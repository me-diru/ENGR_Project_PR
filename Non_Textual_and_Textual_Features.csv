,commit_id,author_date,bugcount,fixcount,la,ld,nf,nd,ns,ent,revd,nrev,rtime,tcmt,hcmt,self,ndev,age,nuc,app,aexp,rexp,oexp,arexp,rrexp,orexp,asexp,rsexp,osexp,asawr,rsawr,osawr,Unnamed: 0.1,Unnamed: 0,BugId,Project,FixHashId,BUG,BIC,Comments,title,description,BFC_message
0,0049967a874a3dca7c6ab8e8e84ca7dea21dc0b2,1395160653,0.0,1.0,29,12,2,2,1,0.999570839,True,4.0,146993.0,41.0,5.0,False,12.0,4829516.0,22.0,3.0,22.0,1561.0,1563.0,22.0,1286.0,1288.0,17.0,826.0,827.0,0.018072289,0.830321285,0.831325301,463,878,1284162,neutron,0049967a874a3dca7c6ab8e8e84ca7dea21dc0b2,1,1,,"Bug #1284162 in neutron: ""Cisco plugin fails test_port_list_filter_by_router_id with ParseError no element found""","When the Cisco nexus plugin is configured on DevStack, the recently added tempest Neutron API test test_port_list_filter_by_router_id fails with the following error:
     ParseError: no element found: line 1, column 0
These failures occur for the following classes:
    NetworksIpV6TestXML
    NetworksTestXML","Cisco plugin fails with ParseError no elem found

When the Cisco nexus plugin is configured on DevStack, the tempest
Neutron API test test_port_list_filter_by_router_id fails with the
following error:
     ParseError: no element found: line 1, column 0

The root cause is that the Cisco Nexus plugin model layer
is encapsulating each dictionary response from the OVS subplugin
into a list, whereas the WSGI layer is expecting a dictionary
response. The fix is not to encapsulate these responses into a list.

Change-Id: I4b589e8e5696c0eae43cafc46912b3d7f29bbdb6
Closes-Bug: #1284162"
1,006a4e5e31a12d049c3d6308c3a2d73f57cd980f,1374767629,0.0,,15,3,2,2,1,0.99107606,True,1.0,344624.0,8.0,4.0,False,31.0,1365650.0,77.0,4.0,157.0,1328.0,1457.0,88.0,1119.0,1191.0,36.0,250.0,283.0,0.041526375,0.281705948,0.318742985,1837,1307878,1307878,Glance,006a4e5e31a12d049c3d6308c3a2d73f57cd980f,0,0,“to avoid problems in the future”,"Bug #1307878 in Glance: ""Fix instances of mutable default arguments to functions/methods""","In a few points throughout the codebase, mutable lists and mutable dicts are being used as default function/method arguments.
In Python, this is an issue since functions are treated as objects that can maintain state between calls. As a result, this only gets set once, and it's possible for it to stack list values over time in cases when you might expect them to be empty. Depending on use, this can cause incredibly complex and yet very subtle bugs in code that reads just fine. In Glance's case, since a few instances of this are in several ACL-related methods in glance.store.*, there is *potential* for security concern (not confirmed).
Here's some additional information illustrating and explaining this behavior in Python:
http://effbot.org/zone/default-values.htm
http://stackoverflow.com/questions/1132941/least-astonishment-in-python-the-mutable-default-argument
There are no comments in the code I've seen that indicate this usage is meant specifically to take advantage of this subtlety in the language. We'd definitely want to document that if it is the case.
Wanted to create this as a discussion point if needed, and as a courtesy to attach it to the patch I'm going to push in a few minutes. The full test suites seem to pass locally, so will be curious what Jenkins has to say.","Allow insecure=True to be set in swiftclient

Currently when we instantiate a swiftclient.Connection we can't pass in
the insecure keyword arg. This means environments which have self-signed
certs might have trouble talking to auth/swift. This fixes that
situation and allows SSL certs to be ignored.

Change-Id: I9663ed333e93529beba4a0736d3b9207bd4f150d"
2,00ac56a100ac7329a01b5f1fa336b99e64778eaf,1401120475,,1.0,16,4,4,3,1,0.960964047,False,,,,,True,,,,,,,,,,,,,,,,,911,1347,1322180,nova,00ac56a100ac7329a01b5f1fa336b99e64778eaf,1,0,"""Legacy bdm in incoming parameters contains a number of devices specified by their names (e.g. ""vdb"" or “/dev/vdc""). Current code considers ""vda"" as root device and doesn't work on “/dev/vda”""","Bug #1322180 in OpenStack Compute (nova): ""Fail to launch an instance from volume by legacy bdm""","Launting an instance from bootable volume passing legacy bdm is available only using vda (no /dev/ prefix) as root device name. This is weird restriction. It prevents to create consistent instance data, because root_device_name instance attribute has /dev/ prefix, but device_name bdm attribute doesn't.
Environment: DevStack
Steps to reproduce:
1 Create bootable volume
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.
2 Boot instance from the volume by legacy bdm.
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=yyy:::1 inst
3 Wait instance status Active, go to instance console and look to 'No bootable device' message.
The reason is in _get_bdm_image_metadata in nova/compute/api.py. There only 'vda' devices are processed as root device for legacy bdm.","Run instance root device determination fix.

Legacy bdm in incoming parameters contains a number of devices
specified by their names (e.g. ""vdb"" or ""/dev/vdc"").
Current code considers ""vda"" as root device and doesn't work on
""/dev/vda"", for example.
This fix extracts the last significant character (digits
are skipped in ""..sda1"")  from the name like ""a"", ""b"", ""c"" and
selects the root device by checking for ""a"".

Change-Id: Iebaa117bb747d30ac9ceabd8805740dec9d1355c
Closes-Bug: #1322180"
3,00f5125745dc72afbc9aeade8b780d7a3be49a30,1394439616,,1.0,5,1,2,2,1,1.0,True,1.0,1043663.0,33.0,15.0,False,145.0,299896.0,602.0,6.0,13.0,3628.0,3632.0,13.0,3038.0,3042.0,8.0,2650.0,2650.0,0.001192053,0.351125828,0.351125828,547,968,1288926,nova,00f5125745dc72afbc9aeade8b780d7a3be49a30,1,1,bug rebooting,"Bug #1288926 in OpenStack Compute (nova): ""incorrect error code when rebooting a rebooting_hard guest""","This is using the latest nova from trunk. In our deployment, we had a hypervisor go down and the tenant issued a hard reboot prior. When attempting a reboot on a guest with the state HARD_REBOOT, nova controller throws this error in it's logs and returns 'ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500)' to the user:
2014-03-06 18:21:00,535 (routes.middleware): DEBUG middleware __call__ Matched POST /tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Route path: '/{project_id}/servers/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>}
2014-03-06 18:21:00,536 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x5242c90>, 'project_id': u'tenant1', 'id': u'778032b2-469d-445e-abde-7b9b0b673324'}
2014-03-06 18:21:00,537 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Action: 'action', body: {""reboot"": {""type"": ""SOFT""}}
2014-03-06 18:21:00,538 (nova.api.openstack.wsgi): DEBUG wsgi _process_stack Calling method <bound method Controller._action_reboot of <nova.api.openstack.compute.contrib.keypairs.Controller object at 0x4c35a50>>
2014-03-06 18:21:00,747 (nova.api.openstack): ERROR __init__ _error Caught error: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
Traceback (most recent call last):
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/__init__.py"", line 125, in __call__
    return req.get_response(self.application)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1320, in send
    application, catch_exc_info=False)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/request.py"", line 1284, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 598, in __call__
    return self.app(env, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 925, in __call__
    content_type, body, accept)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 987, in _process_stack
    action_result = self.dispatch(meth, request, action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/wsgi.py"", line 1074, in dispatch
    return method(req=request, **action_args)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/api/openstack/compute/servers.py"", line 1145, in _action_reboot
    self.compute_api.reboot(context, instance, reboot_type)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/compute/api.py"", line 2073, in reboot
    instance.save(expected_task_state=[None, task_states.REBOOTING])
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/objects/instance.py"", line 472, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/api.py"", line 739, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 128, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2164, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/usr/local/gshare/csi-nova.venv/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py"", line 2215, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard
2014-03-06 18:21:00,750 (nova.api.openstack): INFO __init__ _error http://nova-controller.isg.apple.com:8774/v2/tenant1/servers/778032b2-469d-445e-abde-7b9b0b673324/action returned with HTTP 500
The actual error message back to the user must be something along the lines of 'Unexpected task state: expecting [None, 'rebooting'] but the actual state is rebooting_hard' with a 4xx HTTP code.","Don't allow reboot when instance in rebooting_hard

It shouldn't allow reboot a instance while the instance in
rebooting_hard task state.
If the instance is in rebooting_hard, solf reboot action is
meaningless.

Change-Id: I43147104a7473eaaf1b86f84386bb28b7e3574c3
Closes-bug: #1288926"
4,011d99f300ea5d5f4ce48023bd04a795a4872287,1369792347,3.0,1.0,87,47,2,2,1,0.963535984,True,35.0,24155803.0,286.0,97.0,False,1.0,2387698.5,1.0,13.0,1.0,746.0,746.0,1.0,737.0,737.0,0.0,10.0,10.0,0.083333333,0.916666667,0.916666667,1423,1175695,1175695,neutron,011d99f300ea5d5f4ce48023bd04a795a4872287,1,1, ,"Bug #1175695 in neutron: ""L3 agent restart causes network outage""","When L3 agent is restarted, it destroys all existing namespaces and then recreates them.  This causes a network outage for the affected routers and floating IPs, even if those routers/floating IPs are still valid.  We should be able to preserve existing, valid namespaces across an agent restart and avoid the network outage.","L3 Agent restart causes network outage

When a L3 agent controlling multiple qrouter namespaces
restarts, it destroys all qrouter namespaces even if
some of them are still in use.  As a result, network
traffic could be stopped on the VMs that use the
networks associated with these namespaces.

So what is needed is for the L3 agent to preserve those
qrouter namespaces a L3 agent instance recognizes and to
destroy those it does not know about.

Closes-Bug: #1175695

Change-Id: Idae77886bd195d773878c3d212ccfd56269216fb"
5,0125cf7815acb677b619905e2d9d258a9bae1c48,1407713999,,1.0,3,2,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,1179,1636,1355759,neutron,0125cf7815acb677b619905e2d9d258a9bae1c48,1,1,,"Bug #1355759 in neutron: ""L2populationRpcCallBackTunnelMixin get_agent_ports yields (None, {})""","L2populationRpcCallBackTunnelMixin get_agent_ports yields (None, {}) for unknown networks.
it's useless for consumers.","l2pop: get_agent_ports: Don't yield (None, {})

There's no point to yield None lvm with empty entries.

Closes-Bug: #1355759
Related: blueprint ofagent-l2pop
Change-Id: I42aeb48a062d35038116978d70c8dac4139a5583"
6,015555acb75ee4d9298915951d2bfaf0d19d2b02,1386147080,2.0,1.0,498,219,4,2,1,0.471721032,True,52.0,4688621.0,249.0,50.0,False,8.0,480479.25,18.0,2.0,52.0,441.0,481.0,48.0,440.0,476.0,45.0,414.0,450.0,0.037704918,0.340163934,0.369672131,1659,1237557,1237557,cinder,015555acb75ee4d9298915951d2bfaf0d19d2b02,1,1,Worng logic,"Bug #1237557 in Cinder: ""VMware: size of volume created from image should be based on user input""","With VC VMDK cinder driver, the size of volumes created from image is currently based on the size of the image and the size input by the user is ignored.  So if the image in the following command is 1GB,
    cinder create --image-id <image>  5
The 5GB is effectively ignored and the volume is created as 1GB.
Instead of this behavior, we should create a 5GB volume and then copy in the image.","VMware: Take the volume size from the user input

When we create a volume from an image or a snapshot, we need to
take the size of the volume from the user input and validate whether
the size is appropriate instead of taking the image size as the
volume size directly.

Change-Id: If09933d8ffa989c4dacc0860c19ea332bc21092a
Closes-Bug: #1237557"
7,016e39734e1b70c1b004f1ae366c22b091463932,1380036533,,1.0,26,27,6,2,1,0.863827074,True,2.0,9807.0,12.0,4.0,False,22.0,74390.0,36.0,2.0,136.0,1018.0,1126.0,136.0,865.0,973.0,0.0,8.0,8.0,0.021276596,0.191489362,0.191489362,1593,1229753,1229753,nova,016e39734e1b70c1b004f1ae366c22b091463932,1,1,"“Using blame, here is at least one patch that added code which is causing one of the failures but it wasn't caught in the gate:”","Bug #1229753 in OpenStack Compute (nova): ""Several flake8 errors in new xenserver code""","I don't know why the gate isn't catching these, but I got the latest code this morning and ran a clean tox pep8 which resulted in these failures:
http://paste.openstack.org/show/47427/
Using blame, here is at least one patch that added code which is causing one of the failures but it wasn't caught in the gate:
https://review.openstack.org/#/c/47901/","Fix several flake8 issues in the plugins/xenserver code

Due to some issues in the gate, several flake8 errors got merged
recently. This patch fixes the most recent issues found.

For the H304 relative import errors, we have to skip those because they
are put into /etc/xensource/scripts which is the current working
directory when running the plugin.

For the H231 incompatible python 3 'except x,y' construct, we have to
skip those because this code is written to run on python 2.4 and 'except
x as y' does not work with python 2.4.

Note that in cleaning up some of the H304 failures for relative imports,
I also re-arranged the imports to follow the hacking guide of doing
standard library packages first, then third party packages, and finally
nova-specific packages.

Closes-Bug: #1229753

Change-Id: I1c2211fd6a10d43d7e65cdb4e18530397788cf2c"
8,018ba6302b0f5b909e9f32f44a14c21313a2663e,1385448857,,1.0,40,8,4,3,1,0.898089846,True,2.0,513931.0,17.0,8.0,False,69.0,345523.0,183.0,6.0,34.0,1348.0,1354.0,34.0,1191.0,1197.0,29.0,1175.0,1176.0,0.025125628,0.984924623,0.985762144,81,478,1254978,cinder,018ba6302b0f5b909e9f32f44a14c21313a2663e,1,1,typo in code,"Bug #1254978 in Cinder: ""_update_volume_status() is not called by anyone""","_update_volume_status() method in cinder/volume/driver.py is not called by anyone because misspelling.
_update_volume_status() method in cinder/volume/drivers/eqlx.py is also the same.
I think it should be _update_volume_stats().","Fix _update_volume_stats typos

Fix _update_volume_stats typos in 'cinder/volume/driver.py' and
'cinder/volume/drivers/eqlx.py'.
'_update_volume_status' to '_update_volume_stats'

Change-Id: I0642bdb911ca72517ed655f795e0055f4c4654b8
Closes-Bug: #1254978"
9,0192898e30035028b002f9aed3199be69a18efb3,1393252594,,1.0,25,4,4,3,1,0.784909175,True,3.0,3948616.0,46.0,18.0,False,224.0,28498.0,759.0,4.0,22.0,2657.0,2660.0,22.0,2288.0,2291.0,22.0,1756.0,1759.0,0.003097226,0.236601131,0.237005117,442,857,1282423,nova,0192898e30035028b002f9aed3199be69a18efb3,1,1,,"Bug #1282423 in OpenStack Compute (nova): ""rebuild vm fail when using lvm type image""","I launch a vm with lvm-type image successfully, whose flavor includes ""OS-FLV-EXT-DATA:ephemeral"" flag, the size is 1G.
So, I can see two logical volumes have beed created by ""lvs"" command.
Then I call “nova rebuild $SERVERID $IMAGEID”, try to rebuild it with the same image. Error occur, the vm'state get to ERROR, the two volumes disapear sametime.
nova-compute's log is:
""""""
2014-02-18 19:54:39.290 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvs -o lv_size --noheadings --units b --nosuffix /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:39.626 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): dd bs=1048576 if=/dev/zero of=/dev/cinder-volumes/instance-0000007c_disk.local seek=0 count=1024 oflag=direct execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:47.622 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:48.061 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:48.066 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:49.081 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:49.279 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:49.286 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] ['lvremove', '-f', '/dev/cinder-volumes/instance-0000007c_disk', '/dev/cinder-volumes/instance-0000007c_disk.local'] failed. Retrying. execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:184
2014-02-18 19:54:50.222 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Running cmd (subprocess): lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:147
2014-02-18 19:54:50.359 2368 DEBUG nova.openstack.common.processutils [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
2014-02-18 19:54:50.364 2368 ERROR nova.compute.manager [req-5e803aa8-329b-4536-941b-c6f91c71dcb0 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Setting instance vm_state to ERROR
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Traceback (most recent call last):
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 4967, in _error_out_instance_on_exception
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     yield
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 2037, in rebuild_instance
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     block_device_info=block_device_info)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 835, in destroy
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     destroy_disks, context=context)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 935, in _cleanup
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     self._cleanup_lvm(instance)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 957, in _cleanup_lvm
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     libvirt_utils.remove_logical_volumes(*disks)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 392, in remove_logical_volumes
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     execute(*lvremove, attempts=3, run_as_root=True)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/virt/libvirt/utils.py"", line 50, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     return utils.execute(*args, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     return processutils.execute(*cmd, **kwargs)
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]   File ""/usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146]     cmd=' '.join(cmd))
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] ProcessExecutionError: Unexpected error while running command.
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Command: lvremove -f /dev/cinder-volumes/instance-0000007c_disk /dev/cinder-volumes/instance-0000007c_disk.local
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Exit code: 5
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stdout: ''
2014-02-18 19:54:50.364 2368 TRACE nova.compute.manager [instance: f9dfdb34-4b89-43e2-9b65-1a59d4705146] Stderr: '  One or more specified logical volume(s) not found.\n'
""""""
In nova.virt.libvirt.utils.py:
""""""
def remove_logical_volumes(*paths):
    """"""Remove one or more logical volume.""""""
    for path in paths:
        clear_logical_volume(path)
    if paths:
        lvremove = ('lvremove', '-f') + paths
        execute(*lvremove, attempts=3, run_as_root=True)
""""""
It will try 3 times to remove two volumes (instance-0000007c_disk and instance-0000007c_disk.local).
Then I trace the problem in another example, when it happed again, pdb result show below.
The 1st try:
""""""
2014-02-18 22:33:48.579 24156 DEBUG nova.openstack.common.processutils [req-a97ce6d7-ee76-4349-9d89-7c2e7325f2d5 72d393e2d21d4d71837a4a57e36eb06c 99eced3f5f6d4a8089ec7a8f3e3b5f13] Result was 5 execute /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py:172
> /usr/local/lib/python2.7/site-packages/nova/openstack/common/processutils.py(173)execute()
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('  Logical volume ""instance-0000007e_disk"" successfully removed\n', '  Can\'t remove open logical volume ""instance-0000007e_disk.local""\n')
(Pdb) c
...
""""""
It shows that remove ephemeral disk fail, but image disk success.
2nd try begin:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('  Logical volume ""instance-0000007e_disk.local"" successfully removed\n', '  One or more specified logical volume(s) not found.\n')
(Pdb) c
...
""""""
The ephemeral disk is remove in this time, but image disk have been removed in last step, so the whole command still fail.
3rd try:
""""""
-> if not ignore_exit_code and _returncode not in check_exit_code:
(Pdb) p cmd
['lvremove', '-f', '/dev/cinder-volumes/instance-0000007e_disk', '/dev/cinder-volumes/instance-0000007e_disk.local']
(Pdb) p result
('', '  One or more specified logical volume(s) not found.\n')
(Pdb)
""""""
Openstack think remove_logical_volumes fail, though two volumes have been removed in fact.
It seems that retry to remove open volume is useful, but if the situation that not all volumes were removed once time happen, the retry are useless, and remove_logical_volume must fail.","libvirt: remove_logical_volumes should remove each separately

Currently, remove_logical_volumes() is trying to remove
all of the provided volumes path in a single lvremove command,
while retrying it several times.
This is causing the command to fail, when the deleted volumes
are reappearing in the next iteration of the retry loop.

Handling the removal of each volume separately
and raising an exception for volumes which failed to be removed.
This will increase the chance for individual volume to be removed.

Closes-Bug: #1282423
Change-Id: Ib25f5603929c3038716661a589a41116b4c48909"
10,01a44568cc60bb5a6dd7b55d69b20bba57d1b94b,1378455926,1.0,1.0,28,1,2,2,1,0.216396932,True,6.0,3789279.0,38.0,11.0,False,21.0,345612.0,45.0,5.0,205.0,3458.0,3551.0,205.0,2876.0,2969.0,117.0,2436.0,2515.0,0.019825269,0.409442204,0.422715054,1622,1233561,1233561,nova,01a44568cc60bb5a6dd7b55d69b20bba57d1b94b,1,1, ,"Bug #1233561 in OpenStack Compute (nova): ""server's action confirm_resize return wrong status code v3""","server's action confirm_resize return 204 now, but it should be 202
    @wsgi.response(202)
    @wsgi.serializers(xml=FullServerTemplate)
    @wsgi.deserializers(xml=ActionDeserializer)
    @wsgi.action('confirm_resize')
    def _action_confirm_resize(self, req, id, body):
        context = req.environ['nova.context']
        instance = self._get_server(context, req, id)
        try:
            self.compute_api.confirm_resize(context, instance)
        except exception.MigrationNotFound:
            msg = _(""Instance has not been resized."")
            raise exc.HTTPBadRequest(explanation=msg)
        except exception.InstanceInvalidState as state_error:
            common.raise_http_conflict_for_instance_invalid_state(state_error,
                    'confirm_resize')
        return exc.HTTPNoContent()
The 'return exc.HTTPNoContent()' overwrite the '@wsgi.response(202)'","Fix status code of server's action confirm_resize for v3

The status code of confirm_resize should be 202, but it
returns 204 now. That because it returns HTTPNoContent
that overwrites the status code of decorator 'wsgi.response(202)'

And adds API samples for confirm_resize that used to test
the status code of confirm_resize.

Partially implements blueprint v3-api-unittests
Closes-bug: #1233561

Change-Id: I50a3046866af0a1efdb5c41ded40f973b02843e8"
11,01b2eedd05fb91aa8970c64b753d831257243598,1407285624,,1.0,8,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1154,1611,1353180,neutron,01b2eedd05fb91aa8970c64b753d831257243598,1,1,"“following commit was made https://review.openstack.org/#/c/81334.
That has introduce a problem where neutron-db-manage does not work with some of the options.”","Bug #1353180 in neutron: ""neutron-db-manage current/history/branches/stamp/revision is broken""","In order to fix bug (https://bugs.launchpad.net/neutron/+bug/1288358) following commit was made https://review.openstack.org/#/c/81334.
That has introduce a problem where neutron-db-manage does not work with some of the options.
For example:
root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> current
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 233, in current
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine
root@openstack-ubuntu:/opt/stack/neutron# neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file <X> --config-file <Y> stamp icehouse
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 175, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 91, in do_stamp
    sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 63, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 258, in stamp
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 203, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 212, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 125, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 94, in run_migrations_online
    set_mysql_engine(neutron_config.command.mysql_engine)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/config/cfg.py"", line 2344, in __getattr__
    raise NoSuchOptError(name)
oslo.config.cfg.NoSuchOptError: no such option: mysql_engine","Fixing neutron-db-manage with some options other than upgrade/downgrade

""mysql-engine"" argument was added to upgrade and downgrade option of
neutron-db-manage.
Reference commit: http://tinyurl.com/mzepbmq

migration environment's run_migration_offline/online() gets called
even for other neutron-db-manage options as well such as current,
history, stamp, branches etc. For those options since the argument
can not be set, it throws oslo.config.cfg.NoSuchOptError.
This fix tries to catch it and set the value accordingly.

Closes-bug: #1353180

Change-Id: I044daf04216ec61245ddb51689f8e50be5666e34"
12,01e1c027754b47239b9f9a6c40196512ec262986,1382420555,,1.0,98,35,8,4,1,0.950315721,True,9.0,14142958.0,71.0,35.0,False,32.0,1066596.125,45.0,7.0,11.0,2352.0,2354.0,11.0,1792.0,1794.0,7.0,2103.0,2104.0,0.001250195,0.328801375,0.32895765,5,317,1243037,nova,01e1c027754b47239b9f9a6c40196512ec262986,1,1,Forgot to check float numbers that are not allowed,"Bug #1243037 in OpenStack Compute (nova): ""parameter checking about quota update api is incorrect""","In this API, parameters are checked whether they are integer or not.
But when pass float values to this API, error doesn't occur.
Float values should be forbidden.","Fix parameter checking about quota update api

In quota_classes and quotas update API, parameters are checked
whether they are integer or not.
But when pass float values to these APIs, error doesn't occur.
Float values should be forbidden for the following reasons.
(1)If the value that user submitted and the value registered
   actually are different, user will be confused.
(2)Error messages of these APIs are ""...must be integer..."".
(3)In horizon, if a float is submitted, error message is displayed.

This patch fixes this problem.

Change-Id: I4c7df52ffff2d997da4e6267e25ec2ea4ab6f153
Closes-Bug: #1243037"
13,01e45596fae2818b41c76b41bef46e86c5d33231,1382184363,,1.0,45,1,2,2,1,0.258018669,True,5.0,2961140.0,50.0,21.0,False,7.0,1468347.0,9.0,8.0,148.0,1133.0,1199.0,148.0,1031.0,1097.0,89.0,312.0,346.0,0.191897655,0.667377399,0.739872068,1709,1241602,1241602,neutron,01e45596fae2818b41c76b41bef46e86c5d33231,1,0,“Fix update_device_up method of linuxbridge plugin”,"Bug #1241602 in neutron: ""AttributeError in plugins/linuxbridge/lb_neutron_plugin.py""","I'm running Ubuntu 12.04 LTS x64 + OpenStack Havana with the following neutron package versions:
neutron-common 2013.2~rc3-0ubuntu1~cloud0
neutron-dhcp-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-l3-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-metadata-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge 2013.2~rc3-0ubuntu1~cloud0
neutron-plugin-linuxbridge-agent 2013.2~rc3-0ubuntu1~cloud0
neutron-server 2013.2~rc3-0ubuntu1~cloud0
python-neutron 2013.2~rc3-0ubuntu1~cloud0
python-neutronclient 2.3.0-0ubuntu1~cloud0
When adding a router interface the following error message in /var/log/neutron/server.log:
2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp     port = self.get_port_from_device.get_port(device)
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp AttributeError: 'function' object has no attribute 'get_port'
2013-10-18 15:35:14.862 15675 TRACE neutron.openstack.common.rpc.amqp
2013-10-18 15:35:14.862 15675 ERROR neutron.openstack.common.rpc.common [-] Returning exception 'function' object has no attribute 'get_port' to caller
2013-10-18 15:35:14.863 15675 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", line 44, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 147, in update_device_up\n    port = self.get_port_from_device.get_port(device)\n', ""AttributeError: 'function' object has no attribute 'get_port'\n""]","Fix update_device_up method of linuxbridge plugin

Also add unit tests covering update_device_up and update_device_down
methods

Change-Id: I97f2f9249b684aa5350b3f0621754543e80bec70
Closes-Bug: #1241602"
14,0251b53966eaa9e724377a300ea247367fd778c7,1412513795,,1.0,19,3,2,2,1,0.574635698,False,,,,,True,,,,,,,,,,,,,,,,,1371,1841,1376307,nova,0251b53966eaa9e724377a300ea247367fd778c7,1,1,"“Seems like the commit https://github.com/openstack/nova/commit/6a374f21495c12568e4754800574e6703a0e626f
is the cause.”","Bug #1376307 in OpenStack Compute (nova): ""nova compute is crashing with the error TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'""","nova compute is crashing with the below error when nova compute is started
2014-10-01 14:50:26.854 ^[[00;32mDEBUG nova.virt.libvirt.driver [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mUpdating host stats^[[00m ^[[00;33mfrom (pid=9945) update_status /opt/stack/nova/nova/virt/libvirt/driver.py:6361^[[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 449, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 58, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 167, in _do_send
    waiter.switch(result)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 207, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 490, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 181, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1152, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5946, in update_available_resource
    nodenames = set(self.driver.get_available_nodes())
  File ""/opt/stack/nova/nova/virt/driver.py"", line 1237, in get_available_nodes
    stats = self.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5771, in get_host_stats
    return self.host_state.get_host_stats(refresh=refresh)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 470, in host_state
    self._host_state = HostState(self)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6331, in __init__
    self.update_status()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 6387, in update_status
    numa_topology = self.driver._get_host_numa_topology()
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4828, in _get_host_numa_topology
    for cell in topology.cells])
TypeError: unsupported operand type(s) for /: 'NoneType' and 'int'
2014-10-01 14:50:26.989 ^[[01;31mERROR nova.openstack.common.threadgroup [^[[00;36m-^[[01;31m] ^[[01;35m^[[01;31munsupported operand type(s) for /: 'NoneType' and 'int'^[[00m
Seems like the commit https://github.com/openstack/nova/commit/6a374f21495c12568e4754800574e6703a0e626f
is the cause.","Disable libvirt NUMA topology support if libvirt < 1.0.4

If you're not at a new enough version of libvirt, the compute service
fails on startup because VirtNUMATopologyCellUsage is not fully
populated.

This add a min version check before trying to get host NUMA topology
information.

Closes-Bug: #1376307

Change-Id: I00f6325cb554bc5e34d9f0fe651af39630f35b5d
(cherry picked from commit 8ba0d9188d492028fcf4e65f908aa2d3db571952)"
15,026583e60e9ec2b83049e8ceee16f182560d2562,1393454050,,1.0,6,4,5,4,1,0.967488765,True,6.0,6246829.0,80.0,20.0,False,118.0,73388.4,378.0,5.0,988.0,2768.0,3506.0,763.0,2430.0,2946.0,925.0,1546.0,2225.0,0.124178624,0.207456082,0.298511466,482,899,1285035,nova,026583e60e9ec2b83049e8ceee16f182560d2562,1,1,bug for 1.0,"Bug #1285035 in OpenStack Compute (nova): ""compute FakeDriver : DBerror invalid input syntax for integer: ""1.0""""","by setting nova.conf:
compute_driver = fake.FakeDriver
I got  this error on conductor when nova-compute  update_available_resource :
2014-02-26 08:48:04.631 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
                                                             ^
 'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014, 2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/conductor/manager.py"", line 466, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     result = self.db.compute_node_update(context, node['id'], values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/api.py"", line 228, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return IMPL.compute_node_update(context, compute_id, values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 166, in wrapped
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     compute_ref.update(values)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self.commit()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self._prepare_impl()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     self.session.flush()
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher   File ""/home/croisets/stack/nova/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher     raise exception.DBError(e)
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher DBError: (DataError) invalid input syntax for integer: ""1.0""
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher LINE 1: ...6T07:48:04.627618'::timestamp, hypervisor_version='1.0' WHER...
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher                                                              ^
2014-02-26 08:48:04.631 TRACE oslo.messaging.rpc.dispatcher  'UPDATE compute_nodes SET updated_at=%(updated_at)s, hypervisor_version=%(hypervisor_version)s WHERE compute_nodes.id = %(compute_nodes_id)s' {'hypervisor_version': u'1.0', 'updated_at': datetime.datetime(2014,
2, 26, 7, 48, 4, 627618), 'compute_nodes_id': 1}","Make hypervisor_version an int in fakeVirt driver

This is a follow up to 9e770e62135fe9c2c8ac0121a5a79245b25a7847, which
missed the change in a duplicate copy of host_status.

Add regression test to test_virt_drivers.

This requires changes to the API samples, because they were wrong. virt
drivers use convert_version_to_int which converts a version string to a
4 digit number (""1.0"" becomes to 1000)

Change-Id: I28ce23509e3c9feae183a49a8fc5bf3c7c601295
Closes-Bug: #1285035"
16,026843773b71ab0ac30abb36fe9eed92f5e11f7a,1381148899,,1.0,32,24,3,2,1,0.746232414,True,2.0,2056053.0,14.0,6.0,False,44.0,275619.0,104.0,7.0,1278.0,3528.0,3528.0,1156.0,3191.0,3191.0,463.0,2382.0,2382.0,0.074562108,0.382934276,0.382934276,1632,1235022,1235022,nova,026843773b71ab0ac30abb36fe9eed92f5e11f7a,1,1, ,"Bug #1235022 in OpenStack Compute (nova): ""VMware: errors booting from volume via Horizon""","When using VMwareVC nova driver and VMwareVcVMDK cinder driver, booting from volume via the Horizon UI fails. The instance boots with ERROR status and the log shows ""Image  could not be found"". In addition, the user is unable to access the instances index page in Horizon due to an error 500 (other pages work, however). Steps to reproduce:
(Using horizon)
1. Create a volume from an image
2. Boot an instance from the volume
Expected result:
1. An instance is booted from the volume successfully
2. User is redirected to the instances index page in Horizon
Actual result:
1. Instance fails to boot with status ERROR
2. User is redirected to instances index page but page fails with 500 error. In debug mode, user sees TypeError at /project/instances: string indices must be integers (see link to trace below)
Nova log error:
 Traceback (most recent call last):
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1037, in _build_instance
     set_access_ip=set_access_ip)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
     LOG.exception(_('Instance failed to spawn'), instance=instance)
   File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _spawn
     block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 623, in spawn
     admin_password, network_info, block_device_info)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 208, in spawn
     disk_type, vif_model, image_linked_clone) = _get_image_properties()
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 187, in
     instance)
   File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 184, in
     meta_data = image_service.show(context, image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 290, in show
     _reraise_translated_image_exception(image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 288, in show
     image = self._client.call(context, 1, 'get', image_id)
   File ""/opt/stack/nova/nova/image/glance.py"", line 212, in call
     return getattr(client.images, method)(*args, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 114, in
     % urllib.quote(str(image_id)))
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 272,
     return self._http_request(url, method, **kwargs)
   File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 233,
     raise exc.from_response(resp, body_str)
 ImageNotFound: Image  could not be found.
Horizon error:
    Request Method:	GET
    Request URL:	http://10.20.72.218/project/instances/
    Django Version:	1.5.4
    Exception Type:	TypeError
    Exception Value:
    string indices must be integers
    Exception Location:	/opt/stack/horizon/openstack_dashboard/wsgi/../../openstack_dashboard/    dashboards/project/instances/views.py in get_data, line 92
    Python Executable:	/usr/bin/python
    Python Version:	2.7.3","VMware: fix bug with booting from volumes

When booting from a volume without an image reference would cause
an exception.

Change-Id: Ib0302197b8d4f5e054a8c4426cb0dde7cb82842e
Closes-bug: #1235022"
17,0278a38153f9649aab1cc641bfabd8d5738d2d8c,1378139762,1.0,,43,0,5,5,1,0.921493114,True,31.0,972727.0,112.0,27.0,False,57.0,264376.0,127.0,7.0,0.0,1347.0,1347.0,0.0,1261.0,1261.0,0.0,862.0,862.0,0.001067236,0.921024546,0.921024546,1737,1246228,1246228,Cinder,0278a38153f9649aab1cc641bfabd8d5738d2d8c,1,1,Typo in code,"Bug #1246228 in Cinder: ""Fix typo in types_manager.py""","Found in commit: 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
There is a typo in method name ""_notify_voloume_type_error"". It should be ""_notify_volume_type_error"".","Restrict Volume type deletion with volumes assoc

Updated volume_type_destroy method to throw exception
for volume type delete with associated volumes.

Updated volume_type unit tests

Closes-Bug: #1215329

tag:doc-impact

Change-Id: I7a5d4b473588757d21b461337df493e8046e1d09"
18,027a913a304f8e7000fa3e8ede7776e250a029fb,1411036586,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1332,1800,1370885,nova,027a913a304f8e7000fa3e8ede7776e250a029fb,1,1,,"Bug #1370885 in OpenStack Compute (nova): ""The log info is error in the method '_sync_instance_power_state'""","In the method '_sync_instance_power_state', the log info is wrong.
if self.host != db_instance.host:
            # on the sending end of nova-compute _sync_power_state
            # may have yielded to the greenthread performing a live
            # migration; this in turn has changed the resident-host
            # for the VM; However, the instance is still active, it
            # is just in the process of migrating to another host.
            # This implies that the compute source must relinquish
            # control to the compute destination.
            LOG.info(_(""During the sync_power process the ""
                       ""instance has moved from ""
                       ""host %(src)s to host %(dst)s"") %
                       {'src': self.host,
                        'dst': db_instance.host},
                     instance=db_instance)
            return
The 'src' value should be 'db_instance.host'and the 'dst' value should be the 'self.host'.  The method '_post_live_migration' should be invoked after the live migration completes and it is used to update the database.
In the situation, the instance has been migrated to another host successfully and the database has not been updated. The '_sync_instance_power_state' method is executed. Nova can list it in the dst host with the driver and the data in the database should be the source host.","correct _sync_instance_power_state log message

As reported by a user: the src and dest hosts listed were actually in
reverse order, making this log message quite confusing to
operators. Flipped to the correct requested order.

Change-Id: If9b31bceb56b66af70cddb88dbece7a180e4c2c8
Closes-Bug: #1370885"
19,027b78a214e1f7ab7bbda342a024cb13dae8944d,1355868993,5.0,,902,0,4,2,1,0.751711721,True,13.0,1845570.0,54.0,20.0,False,1.0,341867.5,2.0,3.0,2.0,327.0,327.0,2.0,327.0,327.0,2.0,231.0,231.0,0.01171875,0.90625,0.90625,1817,1269515,1269515,Cinder,027b78a214e1f7ab7bbda342a024cb13dae8944d,1,1, ,"Bug #1269515 in Cinder: ""3PAR iSCSI Driver using 'excessive' iscsi & fc ports""","According to the note at the end of http://docs.openstack.org/trunk/config-reference/content/enable-hp-3par-fibre-channel.html
""You can configure one or more iSCSI addresses by using the hp3par_iscsi_ips option. When you configure multiple addresses, the driver selects the iSCSI port with the fewest active volumes at attach time. The IP address might include an IP port by using a colon (:) to separate the address from port. If you do not define an IP port, the default port 3260 is used. Separate IP addresses with a comma (,). The iscsi_ip_address/iscsi_port options might be used as an alternative to hp3par_iscsi_ips for single port iSCSI configuration.""
The 3PAR Driver should not consume ALL N:S:P iSCSI Paths (and FC!) from the 3PAR to the host, especially when it is configured with a single port iSCSI Configuration when attaching a volume to a host.
Instead, it should simply choose the least used (at the time of connection).
Steps to reproduce:
1. Configure a 3PAR to a blade with more than one path: e.g. 2 iscsi & 2 fc
2. Create an iscsi volume
3. Attach the iscsi volume to an instance
4. Observe that 4 VLUNs are created, one for each path between the 3par & the instance (2 iscsi &  2 fc, yes, fc too even if this is an iscsi volume)","Provide HP 3PAR array iSCSI driver

implements blueprint hp3par-volume-driver

We have the driver broken into 2 files:
hp_3par_common.py and
hp_3par_iscsi.py

The reason we do this is because we have a fibre channel driver
that will be submitted shortly after this is committed.   The
fibre channel driver and the iscsi driver share a lot of the same
code that talks to the 3PAR array for provisioning.  So,
it made sense not to have duplicate code.  The fibre channel driver
will be dependent on the fibre channel support I am actively working
on for nova/cinder grizzly release.

The driver uses a 2 mechanisms to talk to the 3PAR array:
1) a python REST client (hp3parclient) that lives in the pypi
   repository here:
   http://pypi.python.org/pypi/hp3parclient

2) SSH.  We had to pull in some of the ssh code from the base san
   driver to help fix an issue with executing commands on the 3PAR
   array.  The 3PAR has the ability to turn on CSV output for command
   results, which makes this easier to parse.  Unfortunately, there
   is no way to turn CSV mode on permanently for all ssh requests.
   So, we have to turn on the CSV output for every single ssh command
   issued.  Since we use ssh as well, we require the san_* options
   to be set.

We use a dual mechianism because the REST API that ships with the 3.1.2
firmware doesn't support all of the capabilities a cinder driver needs
to export volumes.

When a newer version of the firmware comes out that supports host
management on the 3PAR array, then we will get rid of the SSH code.

Change-Id: I9826ba1a36e27a9be05457ee9236a491dbfd0713"
20,027b78a214e1f7ab7bbda342a024cb13dae8944d,1355868993,5.0,,902,0,4,2,1,0.751711721,True,13.0,1845570.0,54.0,20.0,False,1.0,341867.5,2.0,3.0,2.0,327.0,327.0,2.0,327.0,327.0,2.0,231.0,231.0,0.01171875,0.90625,0.90625,1823,1279478,1279478,Cinder,027b78a214e1f7ab7bbda342a024cb13dae8944d,1,1,,"Bug #1279478 in Cinder: ""3par create volume from snapshot doesn't allow changing size""","In horizon:
1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot using a size greater than the original.
4. Horizon shows the volume with Status=Error
c-vol logs:
2014-02-12 11:11:20.785 ERROR cinder.volume.flows.manager.create_volume [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Volume e07e57c6-7cc1-4b60-8050-8187d99e0006: create failed
2014-02-12 11:11:20.786 DEBUG cinder.openstack.common.lockutils [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Released file lock ""00844d93-9233-4b14-a238-e94916936807-delete_snapshot"" at /opt/stack/data/cinder/cinder-00844d93-9233-4b14-a238-e94916936807-delete_snapshot for method ""_run_flow_locked""... from (pid=28714) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
2014-02-12 11:11:20.786 ERROR cinder.openstack.common.rpc.amqp [req-80c893bb-ba58-4c15-a7a7-ded1ed74247e e1a0f96cf09141ec8e393429e82062df dcac4edbf1aa48afb109bed9529cdd6e] Exception during message handling
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 355, in create_volume
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     _run_flow_locked()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 350, in _run_flow_locked
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     _run_flow()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 346, in _run_flow
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     flow_engine.run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 110, in run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     self._run()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in _run
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     self._revert(misc.Failure())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 75, in _revert
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     misc.Failure.reraise_if_any(failures.values())
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 390, in reraise_if_any
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     failures[0].reraise()
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 397, in reraise
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     six.reraise(*self._exc_info)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/task_action.py"", line 96, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     result = self._task.execute(**kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in execute
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     **volume_spec)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 398, in _create_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     snapshot_ref)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 145, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     snapshot)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 797, in create_volume_from_snapshot
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp     raise exception.InvalidInput(reason=err)
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp InvalidInput: Invalid input received: You cannot change size of the volume.  It must
2014-02-12 11:11:20.786 TRACE cinder.openstack.common.rpc.amqp","Provide HP 3PAR array iSCSI driver

implements blueprint hp3par-volume-driver

We have the driver broken into 2 files:
hp_3par_common.py and
hp_3par_iscsi.py

The reason we do this is because we have a fibre channel driver
that will be submitted shortly after this is committed.   The
fibre channel driver and the iscsi driver share a lot of the same
code that talks to the 3PAR array for provisioning.  So,
it made sense not to have duplicate code.  The fibre channel driver
will be dependent on the fibre channel support I am actively working
on for nova/cinder grizzly release.

The driver uses a 2 mechanisms to talk to the 3PAR array:
1) a python REST client (hp3parclient) that lives in the pypi
   repository here:
   http://pypi.python.org/pypi/hp3parclient

2) SSH.  We had to pull in some of the ssh code from the base san
   driver to help fix an issue with executing commands on the 3PAR
   array.  The 3PAR has the ability to turn on CSV output for command
   results, which makes this easier to parse.  Unfortunately, there
   is no way to turn CSV mode on permanently for all ssh requests.
   So, we have to turn on the CSV output for every single ssh command
   issued.  Since we use ssh as well, we require the san_* options
   to be set.

We use a dual mechianism because the REST API that ships with the 3.1.2
firmware doesn't support all of the capabilities a cinder driver needs
to export volumes.

When a newer version of the firmware comes out that supports host
management on the 3PAR array, then we will get rid of the SSH code.

Change-Id: I9826ba1a36e27a9be05457ee9236a491dbfd0713"
21,028217a0dc6cdc33a37cc303ab9ffb6633036cfe,1392794730,0.0,1.0,70,6,2,2,1,0.524805135,True,5.0,2594518.0,83.0,19.0,False,6.0,1118131.5,7.0,4.0,12.0,2810.0,2812.0,12.0,2378.0,2380.0,10.0,2695.0,2697.0,0.001488901,0.36491608,0.365186789,432,847,1281936,nova,028217a0dc6cdc33a37cc303ab9ffb6633036cfe,1,1,Bug in a special case calling directly,"Bug #1281936 in OpenStack Compute (nova): ""A v3 API  request of ""GET /versions/:(id)"" occurs ""Unexpected API Error.""""","When I request a v3 API request of ""GET /versions/:(id)"" occurs ""Unexpected API Error."".
--------------------
$ curl -i 'http://192.168.1.36:8774/v3/versions/123' -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: <TOKEN>""
HTTP/1.1 500 Internal Server Error
Content-Length: 193
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-baf6f903-b3f1-4c80-9e59-93f1440609a4
Date: Wed, 19 Feb 2014 07:08:13 GMT
{""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<type 'exceptions.TypeError'>"", ""code"": 500}}
--------------------
nova-api logged an error:
--------------------
2014-02-19 16:14:37.075 ERROR nova.api.openstack.extensions [req-282a97a5-8f42-4410-a8dd-ff97457d0241 admin demo] Unexpected exception in API method
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions TypeError: show() got an unexpected keyword argument 'id'
2014-02-19 16:14:37.075 TRACE nova.api.openstack.extensions
--------------------","Fix ""computeFault"" when v3 API ""GET /versions/:(id)"" is called

Usually, the API of ""GET /versions/:(id)"" is not called directly.
However we can call it directly. So this commit fixes it to work
without the ""computeFault ... Unexpected API Error"".

Change-Id: Ic5513e5bce214d6fe90a9ccd7d056ac1dd0a4d19
Closes-Bug: #1281936"
22,02abbef960b12deaa7a911788dbc56f3a0bd555a,1388361320,,1.0,2,2,2,2,1,1.0,True,3.0,166271.0,15.0,7.0,False,9.0,12410954.0,13.0,3.0,1454.0,2451.0,3425.0,1139.0,2003.0,2749.0,1406.0,2297.0,3231.0,0.202650151,0.330980844,0.465504825,190,593,1261475,nova,02abbef960b12deaa7a911788dbc56f3a0bd555a,1,1,Aren’t safe… Bug I think. ,"Bug #1261475 in OpenStack Compute (nova): ""Nova should disable libguestfs' automatic cleanup""","By default libguestfs will register an atexit() handler to cleanup any open libguestfs handles when the process exits. Since libguestfs does not provide any mutex locking in its APIs, the atexit handlers are not safe in multi-threaded processes. If they run they are liable to cause memory corruption as multiple threads access the same libguestfs handle. As such at atexit handlers should be disabled in any multi-threaded program using libguestfs. eg by using
  guestfs.GuestFS (close_on_exit = False)
instead of
  guestfs.GuestFS()","Disable libguestfs' default atexit handlers.

These aren't safe for multi-threaded applications, per the bug
report from Daniel Berrange.

Closes-bug: #1261475

Change-Id: I4cc5eaded2e21f1b2cd55615242a27b9773f0b42"
23,02cfe35694dc69c08d858c756db7221c32bf9f3c,1409140759,,1.0,6,3,2,1,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,998,1439,1333103,neutron,02cfe35694dc69c08d858c756db7221c32bf9f3c,1,1, “Clarify message when no probes are cleared”,"Bug #1333103 in neutron: ""wrong_info_is_displayed_during_no_probe_neutron_debug_probe_clear""","Take a case, when there is no probe present. In this case if we try to clear probes using following CLI -
#neutron-debug probe-clear
it should ideally give a message like - ""Nothing to delete/clear"" or ""No probe is present to clear""
But when I tries above command following is traced -
2014-06-23 11:48:43.924 12056 INFO neutron.common.config [-] Logging enabled!
2014-06-23 11:48:43.925 12056 WARNING neutron.agent.common.config [-] Deprecated: DEFAULT.root_helper is deprecated! Please move root_helper configuration to [AGENT] section.
2014-06-23 11:48:43.925 12056 WARNING neutron.agent.common.config [-] Deprecated: DEFAULT.root_helper is deprecated! Please move root_helper configuration to [AGENT] section.
2014-06-23 11:48:43.927 12056 DEBUG neutron.debug.commands.ClearProbe [-] run(Namespace(request_format='json')) run /opt/stack/neutron/neutron/debug/commands.py:105
2014-06-23 11:48:43.928 12056 DEBUG neutronclient.client [-]
REQ: curl -i http://10.0.9.40:9696/v2.0/ports.json?device_owner=network%3Aprobe&device_owner=compute%3Aprobe&device_id=openstack -X GET -H ""X-Auth-Token: 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-xM-dDkVpSvK15tuQ8nMxnOwa3dwqF5Gmql9A3icH7V1cjnNZfDDwNyvkzvQPoaX5A8gNxKKZ7+4y6LZO0hTc+ib1ikVAjLm--7VG13owGMD2Qd5r5+QJtpPmFujYQaqPIktTJQRFlILvoJlfu+Fo7eidwWDjvIViI0XmcWVBsAJBIzAIH8hmW9w-HYUc0H6E4wK+UXRXLpfCowgNYae30ZBYM4MeWeveXvduVh1iqFFSnWqcqCuKfbiz6RBqWbRWQbcuEw9pXfnQHGTfiQui020qxiuvfFMMiYRVs6-HHwaybUl13RwBEY5MK9FLk8CzqdZB16aHv85djA=="" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""User-Agent: python-neutronclient""
 http_log_req /opt/stack/python-neutronclient/neutronclient/common/utils.py:175
2014-06-23 11:48:44.038 12056 DEBUG neutronclient.client [-] RESP:200 CaseInsensitiveDict({'date': 'Mon, 23 Jun 2014 06:18:44 GMT', 'content-length': '13', 'content-type': 'application/json; charset=UTF-8', 'x-openstack-request-id': 'req-afbd1b2f-ab5e-4441-8477-bede40a652fe'}) {""ports"": []}
 http_log_resp /opt/stack/python-neutronclient/neutronclient/common/utils.py:184
2014-06-23 11:48:44.039 12056 INFO neutron.debug.commands.ClearProbe [-] All Probes deleted
Concentrate in the last of traces --->   ""ClearProbe [-] All Probes deleted""
I dont think it should be like this. In my opinion information ""All probe deleted"" should be modified as per condition if probes are present or not.","Clarify message when no probes are cleared

Log number of probes which were deleted by
using neutron-debug probe-clear
Closes-Bug: #1333103

Change-Id: I5b53f0c4651c6df491d414d7d65f98c4a84a5987"
24,02f4516ad38c5deeefd686b87edf4bf06cffe5af,1388474560,,1.0,0,1,1,1,1,0.0,True,2.0,108068.0,18.0,10.0,False,7.0,5956082.0,11.0,7.0,9.0,2003.0,2005.0,9.0,1779.0,1781.0,4.0,425.0,425.0,0.007587253,0.646433991,0.646433991,235,639,1265132,neutron,02f4516ad38c5deeefd686b87edf4bf06cffe5af,0,0,redundant code,"Bug #1265132 in neutron: ""neutron code redundancy""","I found the code in l3_db.py is redundant,the code is below:
in function _update_router_gw_info:
        if gw_port and gw_port['network_id'] != network_id:
            fip_count = self.get_floatingips_count(context.elevated(),
                                                   {'router_id': [router_id]})
            if fip_count:
                raise l3.RouterExternalGatewayInUseByFloatingIp(
                    router_id=router_id, net_id=gw_port['network_id'])
            if gw_port and gw_port['network_id'] != network_id:
                with context.session.begin(subtransactions=True):
                    router.gw_port = None
                    context.session.add(router)
                self._core_plugin.delete_port(context.elevated(),
                                              gw_port['id'],
                                              l3_port_check=False)
it does not need the third if!","Remove redundant codes

the code is redundant, it should be removed

Change-Id: I050bfc0b4d72037510dc2f3d729a564869a374c2
Closes-Bug: #1265132"
25,02fa15b4caaa414930b6ddf6a3a9fe8019751c26,1410905193,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1183,1640,1355929,nova,02fa15b4caaa414930b6ddf6a3a9fe8019751c26,0,0,Bug in test,"Bug #1355929 in OpenStack Compute (nova): ""test_postgresql_opportunistically fails in stable/havana due to: ""ERROR:  source database ""template1"" is being accessed by other users""""","http://logs.openstack.org/22/112422/1/check/gate-nova-python26/621e0ae/console.html
This is probably a latent bug in the nova unit tests for postgresql in stable/havana, or it's due to slow nodes for the py26 jobs.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRVJST1I6ICBzb3VyY2UgZGF0YWJhc2UgXFxcInRlbXBsYXRlMVxcXCIgaXMgYmVpbmcgYWNjZXNzZWQgYnkgb3RoZXIgdXNlcnNcIiBBTkQgdGFnczpcImNvbnNvbGVcIiBBTkQgYnVpbGRfYnJhbmNoOlwic3RhYmxlL2hhdmFuYVwiIEFORCBidWlsZF9uYW1lOlwiZ2F0ZS1ub3ZhLXB5dGhvbjI2XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDc4NjA5ODg1MjMsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=
3 hits in 7 days, check queue only but multiple changes and all failures.","postgresql: use postgres db instead of template1

In PostgreSQL template1 database is kind of special, as all new databases are
copied from it, when you do CREATE DATABASE. This implies a few restrictions:
e.g. you can't have more than one active session to template1, if one of them
tries to create a database. It will fail with:

    ERROR: source database ""template1"" is being accessed by other users

Using of postgres database instead of template1 resolves this issue.

Closes-Bug: #1355929

Change-Id: I8b1a36163984fbef6be66964456072d32da6a9d0"
26,03277a80d573161abd0d1cb81ec647b53e140063,1401867704,,1.0,15,7,3,3,1,0.74261855,False,,,,,True,,,,,,,,,,,,,,,,,944,1380,1326007,neutron,03277a80d573161abd0d1cb81ec647b53e140063,0,0,Refactoring “Removing check for overlap with fixed ips”,"Bug #1326007 in neutron: ""allowed address pairs - overlap check""","This bug is relate with the following bug:
https://bugs.launchpad.net/neutron/+bug/1321864
The fix patch for the bug delete overlap check for fixed ip and allowed ip address range.
I think that we also need to remove the following code if we do not need to check fixed_ip and allowed address pair overlap:
https://github.com/openstack/neutron/blob/master/neutron/db/allowedaddresspairs_db.py
51 for fixed_ip in port['fixed_ips']:
52 if ((fixed_ip['ip_address'] == address_pair['ip_address'])
53 and (port['mac_address'] ==
54 address_pair['mac_address'])):
55 raise addr_pair.AddressPairMatchesPortFixedIPAndMac()
https://github.com/openstack/neutron/blob/master/neutron/extensions/allowedaddresspairs.py
35class AddressPairMatchesPortFixedIPAndMac(nexception.InvalidInput):
36 message = _(""Port's Fixed IP and Mac Address match an address pair entry."")","Allowed address pair: Removing check for overlap with fixed ips

Some of the overlap check has been removed in the following patch:
https://review.openstack.org/#/c/94508/
But the patch did not remove all the overlap check. I remove the
rest part.

Change-Id: I575ec54c0b3d6dc31ef80819d4258c6d162b4cfd
Closes-Bug: #1326007"
27,03500414cf463905a28ea30d069630dcfda1d90b,1393455616,,1.0,315,181,7,5,1,0.471353241,True,10.0,5525873.0,161.0,26.0,False,32.0,1144333.286,47.0,2.0,10.0,135.0,141.0,10.0,135.0,141.0,7.0,122.0,125.0,0.009184845,0.141216992,0.144661309,487,905,1285335,neutron,03500414cf463905a28ea30d069630dcfda1d90b,0,0,Evolution,"Bug #1285335 in neutron: ""Openvswitch agent should use ovs patch ports instead veths""","https://bugs.launchpad.net/neutron/+bug/1045613 is no more valid because ""patch port support has been added to the upstream Linux kernel OVS implementation"" (comment #4). That's why we can use openvswitch patch ports to interconnect br-int to physnets in order to use the same interconnection technology with br-tun and physnets and increase performance (according to http://www.opencloudblog.com/?p=96).","Use patch ports to interconnect integration/physical bridges

Use by default patch ports to interconnect the integration to
physical bridges for the openvswitch agent in order to increase
performance.

For ovs/kernel without ovs patch port support on kernel side, veths
can be used through the new option OVS.use_veth_interconnection.

Patch port support can be tested using:

    neutron-sanity-check --ovs_patch

DocImpact
Closes-Bug: #1285335
Change-Id: I47106e57bf5b23891d7bf43815959e0d10156fba"
28,035f05eb531cb8dd805685272d46ecd8cb0ad2b3,1378199319,1.0,1.0,13,8,1,1,1,0.0,True,2.0,27735.0,13.0,5.0,False,20.0,1163289.0,42.0,3.0,158.0,2715.0,2807.0,158.0,2294.0,2386.0,156.0,1706.0,1796.0,0.026556157,0.288734777,0.303958051,1503,1220112,1220112,nova,035f05eb531cb8dd805685272d46ecd8cb0ad2b3,1,1,Backwards compatibility,"Bug #1220112 in OpenStack Compute (nova): ""scheduler rpcapi 2.9 is not backwards compatible""","patch 552693e4ad51291c8bfd28cd1939ed3609f6eeac (https://review.openstack.org/#/c/37933/) which added compute scheduler rpcapi version 2.9 does not provide any backwards compatibility, this should be done using 'self.can_send_version'
The result of this is, if compute is running grizzly and trunk is running with
[upgrade_levels]
scheduler=grizzly
in nova.conf
nova-conductor logs:
1e5a084a4838b5d258532645eacc'} from (pid=861) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
2013-09-03 04:32:36.347 DEBUG qpid.messaging.io.ops [-] SENT[4a35cb0]: SessionCompleted(commands=[0-16]) from (pid=861) write_op /usr/lib/python2.7/site-packages/qpid/messaging/driver.py:671
2013-09-03 04:32:36.351 ERROR nova.openstack.common.rpc.amqp [req-b596a82a-0be1-4b90-a93a-d05ae7011e0b demo demo] Exception during message handling
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     **args)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/manager.py"", line 745, in build_instances
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec=legacy_bdm)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/rpcapi.py"", line 112, in run_instance
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec=legacy_bdm_in_spec), version='2.9')
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 169, in cast
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     self._set_version(msg, version)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 72, in _set_version
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp     raise rpc_common.RpcVersionCapError(version_cap=self.version_cap)
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp RpcVersionCapError: Specified RPC version cap, 2.6, is too low
2013-09-03 04:32:36.351 TRACE nova.openstack.common.rpc.amqp","Scheduler rpcapi 2.9 is not backwards compatible

Patch 552693e4ad51291c8bfd28cd1939ed3609f6eeac which added scheduler
rpcapi version 2.9 does not provide any backwards compatibility.

This patch adds the check using 'self.can_send_version', and falls
back to the latest Grizzly compatible version if it the version is
capped.

Also takes the opportunity to make the docstring more consistent in the
scheduler rpcapi SchedulerAPI class.

Closes-bug: #1220112

Change-Id: I5a1e08c1066cae00d47def45f7b87846f679cbbe"
29,038a56c13dafb1993ea2c6cba5a1a06b573cfa03,1393438840,,1.0,4,4,1,1,1,0.0,True,2.0,1099967.0,34.0,2.0,False,13.0,576233.0,23.0,2.0,3.0,1465.0,1466.0,3.0,1266.0,1267.0,3.0,664.0,665.0,0.004618938,0.767898383,0.769053118,486,904,1285289,neutron,038a56c13dafb1993ea2c6cba5a1a06b573cfa03,1,0,Bug because a field has been removed,"Bug #1285289 in neutron: ""get_<resource> functions in N1kv plugin""","The N1kv plugin includes the 'fields' argument in the call to the super class function. Hence, only the specified fields are returned by the get_<..> function in the super class.
The problem is that a field that has been removed may be needed in the subsequent processing in the N1kv's get_<...> function.
The (trivial) solution is to set the fields argument to None in the call to the super class function.
A patch with the solution will be submitted momentarily.","N1kv: Fixes fields argument not None

N1kv plugin includes 'fields' in call to get_<resource> function in
superclass. This can trigger errors if other fields are used in the
subsequent processing in the N1kv plugin's own get_<resource>
function.

Change-Id: I63687a830579a178c6daaa2b8f71bef7eb53d584
Closes-bug: #1285289"
30,03920290d00148437fe14696621bc7446bfc7e46,1407696877,,1.0,3,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1170,1627,1354912,neutron,03920290d00148437fe14696621bc7446bfc7e46,1,1,,"Bug #1354912 in neutron: ""Exception in l3_rbc_base""","The traceback observed in many of tempest jobs:
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 87, in _ensure_host_set_on_ports
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     interface, router['id'])
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 100, in _ensure_host_set_on_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     {'port': {portbindings.HOST_ID: host}})
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 891, in update_port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher     return bound_port._port
2014-08-10 13:01:07.619 27508 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute '_port'
Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkF0dHJpYnV0ZUVycm9yOiAnZGljdCcgb2JqZWN0IGhhcyBubyBhdHRyaWJ1dGUgJ19wb3J0J1wiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3Njk1MjI1MzM1fQ==","Return port context from _bind_port_if_needed

Previously port was returned causing exception being thrown
in rpc handling code.

Change-Id: Iba498d0600d625f0469392b99ac0bc8c1f1ecff7
Closes-Bug: #1354912"
31,03a41f863b160384593ef8df130f369a0c22d393,1376886114,4.0,,475,35,5,5,1,0.599765715,True,17.0,1428600.0,79.0,15.0,False,51.0,126976.0,114.0,1.0,105.0,239.0,321.0,104.0,226.0,308.0,92.0,212.0,282.0,0.107514451,0.246242775,0.32716763,1838,1311182,1311182,Cinder,03a41f863b160384593ef8df130f369a0c22d393,1,1, ,"Bug #1311182 in Cinder: ""GlusterfsException: No base file found""","working with gluster backend for cinder, after failure to delete a snapshot on timeout, I reset the snapshot status to try and delete it again, in which time I immediately fail to delete the snapshot with the following error:
2014-04-22 18:25:48.499 2233 DEBUG cinder.openstack.common.processutils [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Running cmd (subprocess): sudo cinder-rootwrap /etc/cind
er/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa execute /usr/lib/python2.6/site-packages/cinder/open
stack/common/processutils.py:142
2014-04-22 18:25:48.507 2233 DEBUG qpid.messaging.io.raw [-] SENT[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x01\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' writeable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:480
2014-04-22 18:25:48.508 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00:\x00\x00\x00\x00\x00\x00\x00\x00\x02\x02\x01\x00\x00(e20240e0-4cca-44eb-8847-cc486fa240fa:357\x0f\x00\x00\x1c\x00\x00\x00\x00\x00\x00\x00\x00\x02\x
07\x03\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionAttached(name='e20240e0-4cca-44eb-8847-cc486fa240fa:357') write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.509 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: SessionCommandPoint(command_id=serial(0), command_offset=0) write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:25:48.741 2233 ERROR cinder.openstack.common.rpc.amqp [req-ebe13b37-9fb4-496b-8869-cff771b9cd60 97e5450b24624fd78ad6fa6d8a14ef3d c3178ebef2c24d1b9a045bd67483a83c] Exception during message handling
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 435, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     {'status': 'error_deleting'})
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 423, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self.driver.delete_snapshot(snapshot_ref)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 247, in inner
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 557, in delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     self._delete_snapshot(snapshot)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 621, in _delete_snapshot
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp     raise exception.GlusterfsException(msg)
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp GlusterfsException: No base file found for /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa.
2014-04-22 18:25:48.741 2233 TRACE cinder.openstack.common.rpc.amqp
2014-04-22 18:26:03.171 2233 DEBUG qpid.messaging.io.raw [-] READ[3b385a8]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:03.172 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3b385a8]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.raw [-] READ[3e0f710]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2014-04-22 18:26:04.888 2233 DEBUG qpid.messaging.io.ops [-] RCVD[3e0f710]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2014-04-22 18:26:07.577 2233 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.6/site-packages/cinder/openstack/common/periodic_task.py:176
this is the ls output showing that we see the mount:
[root@hostXX ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
-rw-r--r--. 1 qemu qemu 10739843072 Apr 22 18:21 /var/lib/cinder/mnt/59ed8cb64fc8948968a29181234051a2/volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa
here is the snapshot in the storage:
[root@storage Dafna]# ls -l |grep c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d
-rw-r--r-- 2 qemu qemu 10739843072 Apr 22 18:21 volume-c2d7b4c3-d9ef-4676-975b-fb7c83fe3927.bb2fa5f1-4df4-4a62-b077-0a10d862c6fa","QEMU-assisted-snapshots for GlusterFS volumes

Coordinate with Nova to create and delete snaphots for
GlusterFS volumes that are attached to VMs.

Cinder is responsible for creating a QCOW2 file which Nova
will activate in the VM's snapshot chain when a snapshot is
created.

When a snapshot is deleted, Cinder will request for Nova to
perform a block commit/rebase operation to logically delete
the snapshot from the QCOW2 chain.

Implements blueprint qemu-assisted-snapshots

Change-Id: I4a7f0c1bc08d88b0f75d119168dd2077487a62a0"
32,03c4dc3573f640d06b80d686a389f1f3438a6d25,1407341981,,1.0,2,12,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1157,1614,1353414,neutron,03c4dc3573f640d06b80d686a389f1f3438a6d25,0,0,Refactoing “Remove duplicated check for router connect to external”,"Bug #1353414 in neutron: ""duplicated check for router connect to external network""","In the function neutron.db.l3_db.get_assoc_data(),
we get router_id first, then check whether this router is connecting to external network.
But the function we used to get router_id -- neutron.db.l3_db._get_router_for_floatingip(), has already checked this.
As it names says, this function is designed for get router_id, which can connect to floating network.","Remove duplicated check for router connect to external net

In the function get_assoc_data, we check the router which get
from _get_router_for_floatingip whether connect to external net,
but as it has already checked in _get_router_for_floatingip.

Change-Id: I0c00c480273145538d19569dc0a679e9935ad8f9
Closes-bug: #1353414"
33,03d34c975586788dc25249b5e0b962fc0634008c,1410180359,,1.0,23,4,2,2,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,1124,1579,1350355,nova,03d34c975586788dc25249b5e0b962fc0634008c,1,1,,"Bug #1350355 in OpenStack Compute (nova): ""nova-network: inconsistent parameters in deallocate_for_instance()""","Client side of the network RPC API sets 'requested_networks' parameter in deallocate_for_instance() call [1]
while server side expects 'fixed_ips' parameter [2]
[1] https://github.com/openstack/nova/blob/master/nova/network/rpcapi.py#L183
[2] https://github.com/openstack/nova/blob/master/nova/network/manager.py#L555","Nova-net: fix server side deallocate_for_instance()

Client side of the network RPC API sets 'requested_networks'
parameter in deallocate_for_instance() call,
while server side expected 'fixed_ips' parameter.
This patch fixes server side so it handles 'requested_networks'

Closes-Bug: #1350355

Change-Id: If9554a2e98397c5e63e929e1606d99dd2328262c"
34,045e1b11fd058441539e5e6888da7bd873d58c89,1396357171,,1.0,109,104,13,11,2,0.824804791,True,12.0,13303064.0,159.0,56.0,False,191.0,190599.4375,1154.0,5.0,10.0,3557.0,3565.0,8.0,2979.0,2985.0,8.0,3430.0,3436.0,0.001165652,0.444372491,0.445149592,1454,1206032,1206032,nova,045e1b11fd058441539e5e6888da7bd873d58c89,0,0,Refactoring “Rename instance_actions v3”,"Bug #1206032 in OpenStack Compute (nova): ""instance actions should be called server actions in v3 API""","instance is not really a user concept, the user concept, in the api, is servers.
Maybe in the v3 api we should rename instance actions to ""server actions"".","Rename instance_actions v3 to server_actions

* Renames the os-instance_actions v3 plugin to os-server-actions
* Also modifies api specific tests and expected user input JSON
* DocImpact only for v3 Nova api documentation for this plugin.

Closes-Bug: #1206032

DocImpact

Change-Id: Id5049e10f5ef540d0d8e3871d800e9d9cb33da73"
35,048cd541c7066185b3802bded7b2f009859443c3,1395013057,,1.0,1,2,1,1,1,0.0,True,1.0,96309.0,12.0,4.0,False,29.0,3458247.0,40.0,4.0,390.0,2327.0,2686.0,170.0,1925.0,2071.0,108.0,2287.0,2366.0,0.014310096,0.300380727,0.310752265,628,1054,1292997,nova,048cd541c7066185b3802bded7b2f009859443c3,1,1,Correct inheritance of nova.volume.cinder.API,"Bug #1292997 in OpenStack Compute (nova): ""nova.volume.cinder.API incorrectly derives from nova.db.base.Base""","For some reason, nova.volume.cinder.API derives from nova.db.base.Base, which looks like this (in its entirety):
class Base(object):
    """"""DB driver is injected in the init method.""""""
    def __init__(self, db_driver=None):
        super(Base, self).__init__()
        if not db_driver:
            db_driver = CONF.db_driver
        self.db = importutils.import_module(db_driver)  # pylint: disable=C0103
I checked and nova.volume.cinder.API makes no reference at all to self.db, therefore unless I am mistaken, there's no reason for this inheritance.","Correct inheritance of nova.volume.cinder.API

nova.volume.cinder.API was deriving incorrectly from nova.db.base. This
patch simply removes that unnecessary import and changes
nova.volume.cinder.API to derive from object instead.

Change-Id: I5092b7fbb3780317a09a512aed7ecbe1d57a4428
Closes-bug: #1292997"
36,0493c803ae9612f87ed028e1a39e880aead5bdcb,1381873734,3.0,1.0,73,65,3,2,1,0.521325887,True,2.0,2887342.0,21.0,9.0,False,129.0,99395.0,546.0,6.0,1017.0,3946.0,4506.0,783.0,3444.0,3807.0,987.0,2994.0,3531.0,0.155909736,0.472621114,0.557361528,1690,1240247,1240247,nova,0493c803ae9612f87ed028e1a39e880aead5bdcb,1,1, ,"Bug #1240247 in OpenStack Compute (nova): ""API cell always doing local deletes""","It appears a regression was introduced in:
https://review.openstack.org/#/c/36363/
Where the API cell is now always doing a _local_delete()... before telling child cells to delete the instance.  There's at least a couple of bad side effects of this:
1) The instance disappears immediately from API view, even though the instance still exists in the child cell.  The user does not see a 'deleting' task state.  And if the delete fails in the child cell, you have a sync issue until the instance is 'healed'.
2) Double delete.start and delete.end notifications are sent.  1 from API cell, 1 from child cell.
The problem seems to be that _local_delete is being called because the service is determined to be down... because the compute service does not run in the API cell.","Cells: Fix instance deletes

A regression was introduced with object conversion that caused all
instances to be locally deleted in the API cell when cells is enabled.
This would happen before the child cell was told to delete the instance.

Additionally, if an instance record was created in a child cell but
failed to schedule to a host in such a way that the API cell has
no knowledge of the 'host', the instance would be deleted in the API
cell only.

Also, it appears that we would cast deletes to child cells twice due to
logic duplicated in cells_api.py that exists in api.py.

Closes-Bug: #1240247

Change-Id: I94cfddd9cef2d63b2cb31c0f2dae8e211a4418d9"
37,04add2ed0d8b0341ea347ac98834f75cf5b2be1d,1403539186,,1.0,16,1,2,2,1,0.522559375,False,,,,,True,,,,,,,,,,,,,,,,,1000,1442,1333315,nova,04add2ed0d8b0341ea347ac98834f75cf5b2be1d,1,0,“Neutronv2 api does not support neutron without port quota”,"Bug #1333315 in OpenStack Compute (nova): ""Nova boot fails with quota activated in neutron but not on ports""","When nova is deployed with neutron, nova boot fails if quota are activated but not for ports
""Failing"" usecase:
In neutron.conf
[quotas]
quota_items = network,subnet,router,floatingip
neutron quota-show
+---------------------+-------+
| Field               | Value |
+---------------------+-------+
| floatingip          | 2     |
| network             | 5     |
| router              | 1     |
| security_group      | 10    |
| security_group_rule | 100   |
| subnet              | 4     |
+---------------------+-------+
nova boot fails because neutronv2 api expect neutron.show_quota(tenant_id=context.project_id)['quota'].get('port') to return an int but instead None is returned","Neutronv2 api does not support neutron without port quota

If quota on ports are disabled in neutron (see below), neutronv2 api
set port quota to None and quota check raises a TypeError.

  # neutron server /etc/neutron/neutron.conf
  [quotas]
  quota_items=network,subnet,router,floatingip

This change set port quota to -1 (unlimited) if quota on ports are
disabled.

Change-Id: I89c530dd57a7beb35695be124ca8511148378458
Closes-Bug: #1333315"
38,04dd95bef7e88c2602cca74d542654ec7bdfc754,1390545671,,1.0,4,4,1,1,1,0.0,True,2.0,95553.0,11.0,5.0,False,8.0,330748.0,10.0,3.0,54.0,560.0,585.0,54.0,545.0,570.0,42.0,515.0,528.0,0.031617647,0.379411765,0.388970588,319,728,1272182,cinder,04dd95bef7e88c2602cca74d542654ec7bdfc754,0,0,Docstring change and unused variable,"Bug #1272182 in Cinder: ""Fix docstring and remove unused variable in VolumeTransferTestCase""",Fix error docstring and remove unused variable 'volume ' in VolumeTransferTestCase.,"Fix docstring and remove unused variable

Fix error docstring and remove unused variable 'volume ' in
VolumeTransferTestCase.

Change-Id: I6c98af30df88b8f19c23ef5e38e4b5b2eef7436c
Closes-Bug: #1272182"
39,04df85b6e5a098f8f55bb82f04d9769763beb487,1411561412,,1.0,11,11,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1354,1823,1373851,neutron,04df85b6e5a098f8f55bb82f04d9769763beb487,0,0,Optimization “ptimize a query in _get_lla_gateway_ip_for_subnet”,"Bug #1373851 in neutron: ""security groups db queries load excessive data""",The security groups db queries are loading extra data from the ports table that is unnecessarily hindering performance.,"Improve performance of security group DB query

The _select_ips_for_remote_group method was joining the
IP allocation, port, allowed address pair, and security group tables
together in a single query. Additionally, it was loading all of
the port columns and using none of them. This resulted in a
very expensive query with no benefit.

This patch eliminates the unnecessary use of the port table by joining
the IP allocation table directly to the security groups and allowed
address pairs tables. In local testing of the method, this sped it up
by an order of magnitude.

Closes-Bug: #1373851
Change-Id: I12899413004838d2d22b691f1e2f3b18f7ec2c27"
40,04e53669dd3e6acd8086fd06ea073a02207ffb24,1383133104,,1.0,40,17,2,2,1,0.972966127,True,15.0,10859703.0,105.0,41.0,False,65.0,13241.0,261.0,4.0,326.0,884.0,1096.0,315.0,857.0,1058.0,319.0,855.0,1063.0,0.049352252,0.132017273,0.164096237,38,366,1246276,nova,04e53669dd3e6acd8086fd06ea073a02207ffb24,1,1,,"Bug #1246276 in OpenStack Compute (nova): ""xenapi: issues with destroy_vdi masking real errors""","In cleaning up VDIs we call destroy_vdi.
However, should the destroy fail, it masks the real error, for example:
Unable to destroy VDI OpaqueRef:f53f35a9-31e1-af85-87a8-aa0dc7eecd43
  nova/compute/manager.py"", _build_instance
...
  nova/virt/xenapi/vmops.py"", line, in _attach_disks
    DEVICE_SWAP, name_label, swap_mb)
  nova/virt/xenapi/vm_utils.py"", line, in generate_swap
    'swap', swap_mb, fs_type)
 nova/virt/xenapi/vm_utils.py"", line, in _generate_disk
    destroy_vdi(session, vdi_ref)
  nova/virt/xenapi/vm_utils.py"", line, in destroy_vdi
    _('Unable to destroy VDI %s') % vdi_ref)
We should instead use the safe_destroy_vdi call.","xenapi: stop destroy_vdi errors masking real error

We frequently call destroy_vdi in an exception block where we plan to
re-raise the exception.

Should an error occur while destroying the vdi this hides the real error
that caused the problem. To stop this, I have moved those calls to
safe_destroy_vdis and added unit tests to ensure the error from
destroy_vdi does not mask the real issue.

Closes-Bug: #1246276
Change-Id: Ib02f93ab1086f077d3d00e5ca915a26f90552c85"
41,04fa99095cd82f7b6030694996c75d2fd95e7c61,1395930408,,1.0,15,9,2,2,1,0.249882293,True,4.0,81961.0,22.0,9.0,False,47.0,1365343.0,88.0,2.0,8.0,980.0,984.0,8.0,970.0,974.0,5.0,975.0,976.0,0.000779828,0.126852093,0.126982064,690,1116,1298420,nova,04fa99095cd82f7b6030694996c75d2fd95e7c61,1,1,,"Bug #1298420 in OpenStack Compute (nova): ""Libvirt's image caching fetches images multiple times""","When launching several VMs in rapid succession, it is possible that libvirt's image caching will fetch the same image several times.  This can occur when all of the VMs in question are using the same base image and this base image has not been previously fetched. The inline fetch_func_sync method prevents multiple threads from fetching the same image at the same time, but it does not prevent a thread that is waiting to acquire the lock from fetching the image that was being fetched while the lock was still in use. This is because the presence of the image is checked only before the lock has been acquired, not after.","Add exists check to fetch_func_sync in libvirt imagebackend

This change adds a path exists check to fetch_func_sync to deal with
cases where a thread is waiting to acquire the lock while another
thread has the lock and is downloading the same image.  Without this
check the same image template will be downloaded multiple times.

Change-Id: Ib410903c7fd9d6954a8082391f89b42bac6a20d3
Closes-Bug: #1298420"
42,051aa362ff1f52926b1725ab24b3b21bfb5e5e21,1386664987,0.0,,174,220,8,1,1,0.808559436,True,8.0,716860.0,29.0,4.0,False,18.0,1779465.0,40.0,2.0,492.0,608.0,861.0,486.0,557.0,805.0,476.0,568.0,809.0,0.385610348,0.459983832,0.654810024,1828,1285673,1285673,Cinder,051aa362ff1f52926b1725ab24b3b21bfb5e5e21,1,1,"“Updated code where service_get_all_by_topic is used to make use of disabled kwarg instead of filtering afterwards”
“This means it doesn't get all and in fact gets only services that are not disabled.”","Bug #1285673 in Cinder: ""service_get_all_by_topic filters out disabled services""","cinder/db/sqlalchemy/api.py
service_get_all_by_topic includes the filter disabled=False. This means it doesn't get all and in fact gets only services that are not disabled.
This method is used four times in the codebase
cinder/backup/api.py:        API._is_backup_service_enabled
this method filters out disabled services already so is not affected.
cinder/scheduler/host_manager.py:       HostManager.get_all_host_states
this method tries to log messages for disabled services, but can't because they've already been filtered out
cinder/volume/api.py:        API.list_availability_zones
list_availability_zones is not able to show disabled services
cinder/volume/api.py:        API.migrate_volume
not currently filtering out disabled hosts - would require patching","Use mock for scheduler tests

In our effort to move from mox to mock, this patch will convert the
scheduler unit tests to use mock exclusively.  Also fixed some pylint
errors along the way.

Change-Id: If5a94838981a04a38c2797ced07f23abfafbedb7"
43,053b468326e2de6ffecbe77b78cb6a33220f9dec,1388220956,0.0,,15,0,2,2,1,0.836640742,True,3.0,1329196.0,37.0,20.0,False,25.0,4957962.5,64.0,4.0,159.0,745.0,809.0,158.0,554.0,618.0,136.0,583.0,625.0,0.125572869,0.535288726,0.573785518,1831,1290969,1290969,Glance,053b468326e2de6ffecbe77b78cb6a33220f9dec,1,1,“This change should be reverted and depending on the list should be logged as a deprecation warning for 1 cycle.”,"Bug #1290969 in Glance: ""Change to require listing all known stores in glance-api.conf breaks swift-backed glance on upgrade""","This change:
https://review.openstack.org/#q,I82073352641d3eb2ab3d6e9a6b64afc99a30dcc7,n,z
Causes an upgrade failure if users are backed by swift, as it is not in the list of defaults anymore.
This change should be reverted and depending on the list should be logged as a deprecation warning for 1 cycle.","Skip unconfigurable drivers for store initialization

All enabled/known store drivers are currently initialized and configured
by Glance when API daemon startup, and the driver(s) which could not be
configured correctly fail are not skipped. This patch explicitly skips
the stores with bad configuration and logs the same.

Change-Id: Ie14b1fb2f554e12cf82512f1d84390619d35a886
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
44,0540847c89a463f22438c6da6a0d03dc5a86d89f,1405088860,,1.0,22,32,2,2,1,0.764204507,False,,,,,True,,,,,,,,,,,,,,,,,1059,1505,1340778,neutron,0540847c89a463f22438c6da6a0d03dc5a86d89f,1,1,,"Bug #1340778 in neutron: ""common/log.py creates its own logger""",Using log decorator from neutron.common.log causes creating new logger and not respecting logging level of decorated methods.,"Use method's logger in log decorator

Previously decorators shared logger. This patch makes LOG object in
closure for every decorated method in order to use correct logging
level.

Closes-Bug: #1340778
Change-Id: If907acbab34f8bf23e1277e66c3ddd839f668764"
45,056942ad5c0b379ebad06110b45b72f685b6c03d,1391738160,,1.0,40,55,18,5,2,0.77863536,True,3.0,363323.0,45.0,5.0,False,1.0,971087.1515,1.0,3.0,155.0,1265.0,1301.0,154.0,1090.0,1125.0,119.0,614.0,631.0,0.157480315,0.807086614,0.829396325,376,789,1277222,neutron,056942ad5c0b379ebad06110b45b72f685b6c03d,0,0, Reorganize code,"Bug #1277222 in neutron: ""Reorganize code tree for multiple cisco ML2 mech drivers""","Currently there is one ML2 driver for cisco nexus in
    neutron/plugins/ml2/drivers/cisco
It needs to go down a level so other cisco drivers can live alongside it:
    neutron/plugins/ml2/drivers/cisco/apic
    neutron/plugins/ml2/drivers/cisco/nexus
    neutron/plugins/ml2/drivers/cisco/ucs
    neutron/plugins/ml2/drivers/cisco/...","Prepare for multiple cisco ML2 mech drivers

Code tree reorganization in preparation for ML2 mechanism drivers for
other cisco products. The cisco nexus ML2 mechanism driver and its
test cases need to move down into their own subdirectory.

Closes-bug: #1277222

Change-Id: I2ba366332276069545b3deb0bbd39016a893327b"
46,058ea40e7b7fb2181a2058e6118dce3f051e1ff3,1380873280,1.0,1.0,7,44,5,2,1,0.884373851,True,2.0,925606.0,13.0,5.0,False,57.0,206908.4,137.0,3.0,1273.0,1958.0,3012.0,1154.0,1704.0,2640.0,458.0,1864.0,2106.0,0.073913043,0.300322061,0.339291465,1636,1235118,1235118,nova,058ea40e7b7fb2181a2058e6118dce3f051e1ff3,0,0,"“remove deprecated”, refactoring","Bug #1235118 in OpenStack Compute (nova): ""remove deprecated configuration variable for vnc_password""",in H it was decided that the support would be removed in I,"VMware: remove deprecated configuration variable

Removes support for vnc_password configuration variable.

Closes-bug: #1235118

Change-Id: I70fd7d3ee06040d6ce49d93a4becd9cbfdd71f78"
47,05e0ceb242f16a236f3c793305ed24e1f9a43efb,1375912592,,1.0,61,8,2,2,1,0.517582753,True,8.0,1303310.0,50.0,11.0,False,23.0,500663.0,55.0,4.0,97.0,1544.0,1591.0,97.0,1417.0,1464.0,92.0,1473.0,1515.0,0.016714594,0.264917326,0.272465852,1453,1205089,1205089,nova,05e0ceb242f16a236f3c793305ed24e1f9a43efb,0,0,"Feature. “powervm driver needs to support”, “Implements the OS reboot API”","Bug #1205089 in OpenStack Compute (nova): ""powervm driver needs to support reboot operation""","The reboot operation is part of the core nova API but the powervm driver doesn't support it:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/driver.py#L126
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
This is a pretty basic operation that should be supported if the driver is to be considered valid.
Looking here:
http://pic.dhe.ibm.com/infocenter/powersys/v3r1m5/topic/iphcg/chsysstate.htm
It should be a pretty straight-forward operation:
""To perform an immediate restart of a partition (operator panel function 3):
chsysstate -r lpar -o shutdown --immed --restart { -n Name | --id PartitionID } [ -m ManagedSystem ]""","Implement hard reboot for powervm driver

Implements the OS reboot API in the powervm driver. Soft reboot is not
supported as it requires additional configuration on the LPAR when it's
spawned so this patch simply performs an immediate hard reboot.

Also adds tests for start_lpar and stop_lpar which didn't exist before.

Closes-Bug: #1205089

Change-Id: Iad64dda79f46d18c7aa5bee12ed41e7a01853d34"
48,05f688e49fbf543a6c69c0f186279c6732118470,1391696091,,1.0,2,1,1,1,1,0.0,True,1.0,84967.0,12.0,3.0,False,2.0,905803.0,2.0,2.0,67.0,2223.0,2266.0,46.0,1645.0,1670.0,37.0,1998.0,2022.0,0.005215482,0.27436179,0.277655778,359,770,1275771,nova,05f688e49fbf543a6c69c0f186279c6732118470,1,1,Notifications do not work,"Bug #1275771 in OpenStack Compute (nova): ""Notifications do not work: AttributeError: 'RequestContext' object has no attribute 'iteritems'""","When enabling notification with notification_driver = messaging, I get the following:
2014-02-03 14:20:41.152 ERROR oslo.messaging.notify._impl_messaging [-] Could not send notification to notifications. Payload={'priority': 'INFO', '_unique_id': 'da748b32fd144c25adc45ba5b393339d', 'event_type': 'compute.instance.create.end', 'timestamp': '2014-02-03 14:20:41.151419', 'publisher_id': 'compute.devstack', 'payload': {'node': u'devstack', 'state_description': '', 'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', 'created_at': '2014-02-03 14:20:33+00:00', 'access_ip_v6': None, 'disk_gb': 0, 'availability_zone': u'nova', 'terminated_at': '', 'ephemeral_gb': 0, 'instance_type_id': 6, 'instance_flavor_id': '42', 'image_name': u'cirros-0.3.1-x86_64-uec', 'host': u'devstack', 'fixed_ips': [FixedIP({'version': 4, 'floating_ips': [], 'label': u'private', 'meta': {}, 'address': u'10.0.0.2', 'type': u'fixed'})], 'user_id': u'6bcbc8f54d65473c9a0c4a55f64fb580', 'message': u'Success', 'deleted_at': '', 'reservation_id': u'r-jycyyveh', 'image_ref_url': u'http://162.209.87.220:9292/images/7b8d712a-fb31-43b8-8a05-a74d70fd8a11', 'memory_mb': 64, 'root_gb': 0, 'display_name': u'dwq', 'instance_type': 'm1.nano', 'tenant_id': u'cda1741ff4ef47f48fb3d9d76e302add', 'access_ip_v4': None, 'hostname': u'dwq', 'vcpus': 1, 'instance_id': '272c2ec6-bb98-4e84-9377-84c63c7a9ce9', 'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', 'state': u'active', 'image_meta': {u'kernel_id': u'5d1a6130-0e6a-4155-9c05-0174a654da68', u'container_format': u'ami', u'min_ram': u'0', u'ramdisk_id': u'37ad58df-c587-4bed-9062-9428ca14eaf0', u'disk_format': u'ami', u'min_disk': u'0', u'base_image_ref': u'7b8d712a-fb31-43b8-8a05-a74d70fd8a11'}, 'architecture': None, 'os_type': None, 'launched_at': '2014-02-03T14:20:41.070490', 'metadata': {}}, 'message_id': '03b2985a-6bcd-44ff-8303-29618d3c2b01'}
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     version=self.version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 93, in _send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     self._driver.send_notification(target, ctxt, message, version)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send_notification
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     return self._send(target, ctxt, message, envelope=(version == 2.0))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 362, in _send
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     rpc_amqp.pack_context(msg, context)
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py"", line 299, in pack_context
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     context_d = six.iteritems(context.to_dict())
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 484, in iteritems
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging     return iter(getattr(d, _iteritems)(**kw))
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'RequestContext' object has no attribute 'iteritems'
2014-02-03 14:20:41.152 TRACE oslo.messaging.notify._impl_messaging","nova: use RequestContextSerializer for notifications

RequestContext should be serialized when sent via oslo.messaging. The
serializer is correctly used for the general RPC mechanism, but has been
forgotten in the notifier. This patch fixes that.

Change-Id: I56fa8022e34c0e80835e3bde940fda99ed0f9ba8
Closes-Bug: #1275771"
49,061938563097241983b70fc6b85e1c196dfebfeb,1375263368,,1.0,3,80,3,3,1,0.315699722,True,4.0,603051.0,31.0,11.0,False,35.0,3088425.0,57.0,3.0,56.0,423.0,469.0,44.0,410.0,444.0,7.0,389.0,392.0,0.010322581,0.503225806,0.507096774,1461,1208734,1208734,cinder,061938563097241983b70fc6b85e1c196dfebfeb,0,0,Refactoring “deprecated”,"Bug #1208734 in Cinder: ""Drop openstack.common.exception""",The library openstack.common.exceptions is deprecated in Oslo and should be removed.,"Remove usage of obsolete oslo.exception

Change-Id: I6f46f90bd74cc26fc01667e467e3dab38037eec3
Closes-Bug: #1208734"
50,062b1f8c0f6ba09ab6764ea512c3615fc93aaf08,1408239629,,1.0,19,5,3,3,1,0.901227926,False,,,,,True,,,,,,,,,,,,,,,,,1141,1596,1351810,nova,062b1f8c0f6ba09ab6764ea512c3615fc93aaf08,0,0,"Refactoring “Move _is_mapping logic to more central place
“","Bug #1351810 in OpenStack Compute (nova): ""Move _is_mapping logic to more central place""","This bug is a follow-up to Nikola Dipanov's comment in https://review.openstack.org/#/c/109834/2/nova/compute/manager.py.
The logic to identify volumes is currently a nested function in _default_block_device_names, named _is_mapping.  It should be moved to a more general place so others could utilize it.","Move _is_mapping to more central location

The logic to identify volumes is currently a nested function
in _default_block_device_names, named _is_mapping. It should
be moved to a more general location so others could utilize it
and allow it to be properly unit tested. The following patch
moves _is_mapping to nova/virt/block_device.py and renames it
to is_block_device_mapping.

Change-Id: I560abc4b57ca5bd195282af7cd1ab9bbf7600b67
Closes-Bug: #1351810"
51,0663778c531a69bf55ed8f88cd828febd69be44c,1379064959,,1.0,13,2,2,2,1,0.566509507,True,1.0,34248.0,12.0,5.0,False,4.0,354272.0,7.0,3.0,7.0,2307.0,2308.0,7.0,1976.0,1977.0,7.0,2122.0,2123.0,0.00132604,0.351897895,0.35206365,1538,1223559,1223559,nova,0663778c531a69bf55ed8f88cd828febd69be44c,1,1, ,"Bug #1223559 in OpenStack Compute (nova): ""PCI passthrough failing on extra_info""","Havana3 is installed using Packstack on CentOS 6.4.
Nova-compute dies right after start with error ""NameError: global name '_' is not defined"".
Here is the info:
* /etc/nova/nova.conf:
pci_alias={""name"":""test"", ""product_id"":""7190"", ""vendor_id"":""8086"", ""device_type"":""ACCEL""}
pci_passthrough_whitelist=[{""vendor_id"":""8086"",""product_id"":""7190""}]
 With that configuration, nova-compute fails with the following log:
* /var/log/nova/compute.log:
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
    **args)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
    result = getattr(objinst, objmethod)(context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
    self._from_db_object(context, self, db_pci)
NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     x.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 166, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/event.py"", line 116, in wait
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/hubs/hub.py"", line 177, in switch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 192, in main
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 65, in run_service
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     service.start()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 164, in start
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 805, in pre_start_hook
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4773, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 318, in update_available_resource
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._sync_compute_node(context, resources)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 347, in _sync_compute_node
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._update(context, resources, prune_stats=True)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/compute/resource_tracker.py"", line 420, in _update
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self.pci_tracker.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/pci/pci_manager.py"", line 126, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     dev.save(context)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 134, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     ctxt, self, fn.__name__, args, kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/conductor/rpcapi.py"", line 497, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     objmethod=objmethod, args=args, kwargs=kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 85, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/rpcclient.py"", line 63, in _invoke
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return cast_or_call(ctxt, msg, **self.kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = rpc.call(context, real_topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 794, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rpc_amqp.get_connection_pool(conf, Connection))
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 574, in call
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     rv = list(rv)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     raise result
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     **args)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 567, in object_action
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     result = getattr(objinst, objmethod)(context, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/base.py"", line 141, in wrapper
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     return fn(self, ctxt, *args, **kwargs)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.6/site-packages/nova/objects/pci_device.py"", line 242, in save
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup     self._from_db_object(context, self, db_pci)
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup
2013-09-10 12:52:23.774 14749 TRACE nova.openstack.common.threadgroup NameError: global name '_' is not defined","pci passthrough fails while trying to decode extra_info

pci device with empty extra_info should use null dict instead of
None. keep the init of extra_info to make db could access it.

Closes-Bug: #1223559

Change-Id: I2e57eb53dfd4f5168d52dc94c2057fc575458aee
Signed-off-by: Yongli He <yongli.he@intel.com>"
52,06dd7f28d1d3ec8619df0d25ddfe977d538897b3,1404415805,1.0,1.0,5,6,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1040,1484,1337526,cinder,06dd7f28d1d3ec8619df0d25ddfe977d538897b3,1,1,“the field volume_image_metadata is missing from the response.’,"Bug #1337526 in Cinder: ""The volume_image_metadata field is missing from volume list""","When listing all volumes, the field volume_image_metadata is missing from the response.
When enabling debug, this error is logged:
Problem retrieving volume image metadata. It will be skipped. Error: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id' _get_all_images_metadata /usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py:43
If code is updated to log the whole traceback instead, this can be found:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/cinder/api/contrib/volume_image_metadata.py"", line 39, in _get_all_images_metadata
    all_metadata = self.volume_api.get_volumes_image_metadata(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/volume/api.py"", line 686, in get_volumes_image_metadata
    db_data = self.db.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/api.py"", line 552, in volume_glance_metadata_get_all
    return IMPL.volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2447, in volume_glance_metadata_get_all
    return _volume_glance_metadata_get_all(context)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 137, in wrapper
    return f(*args, **kwargs)
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 2436, in _volume_glance_metadata_get_all
    session=session).\
  File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/api.py"", line 195, in model_query
    query = query.filter_by(project_id=context.project_id)
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 1249, in filter_by
    for key, value in kwargs.iteritems()]
  File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/util.py"", line 1218, in _entity_descriptor
    (description, key)
InvalidRequestError: Entity '<class 'cinder.db.sqlalchemy.models.VolumeGlanceMetadata'>' has no property 'project_id'
The problem is caused by a filtering done on project_id.
VolumeGlanceMetadata does not have the project_id field, filtering should be done on the Volume associated to the VolumeGlanceMetadata instead.","volume_image_metadata missing from volume list

The volume_image_metadata field was missing from the volume list
because we tried to filter VolumeGlanceMetadata against a non-existent
project_id field. Filtering should be done on the Volume associated
to the VolumeGlanceMetadata instead.

Move project_id filtering out of model_query as it assumes
the project_id field is in the queried model itself.
Build the filter ourselves against Volume.

Change-Id: I6053708296f2b5e24513dc87ed63da0f67c220ae
Closes-bug: #1337526"
53,06fc675928408e462f01178e0158a72f8518188a,1413325095,,1.0,3,6,2,2,1,0.764204507,False,,,,,True,,,,,,,,,,,,,,,,,1402,1872,1381221,neutron,06fc675928408e462f01178e0158a72f8518188a,0,0,Refactoring “cleanup’,"Bug #1381221 in neutron: ""VPNaaS Cisco cleanup test code""","There is unused arguments in a mock method, and duplication of constants, in the Cisco VPNaaS unit test files.","VPNaaS Cisco unit test clean-up

Removed unused args to mock side-effect function and duplicate
constant.

Change-Id: I5409ce86ccaab86213d65f757f19c1bdf9a66929
Closes-Bug: #1381221"
54,0739b2fe827b3beefb5ad81c063cbc4d00525a9d,1378445239,,1.0,13,7,3,3,1,0.960229718,True,2.0,614282.0,13.0,6.0,False,13.0,32673.0,42.0,4.0,16.0,1017.0,1017.0,16.0,920.0,920.0,16.0,261.0,261.0,0.058823529,0.906574394,0.906574394,1521,1221500,1221500,neutron,0739b2fe827b3beefb5ad81c063cbc4d00525a9d,1,1,,"Bug #1221500 in neutron: ""NotImplementedError is a Python built-in exception""","$ git grep \\.NotImplementedError
neutron/extensions/l3.py:288:        raise qexception.NotImplementedError()
neutron/extensions/l3.py:291:        raise qexception.NotImplementedError()
neutron/plugins/nicira/NeutronPlugin.py:1091:                raise q_exc.NotImplementedError(_(""admin_state_up=False ""","Fix incorrect NotImplementedError

neutron.common.exceptions doesn't define NotImplementedError, but
some codes still use it. We should use builtin
exceptions.NotImplementedError instead.

Note: this patch also fixes a never run code in nvp.

Closes-Bug: #1221500
Change-Id: I5f367ab5edc1e7fbbc2e4eba5fe36d148d4d062d"
55,0756cf8ec3f2c73b4141b4b6dc597e82e47de669,1396621130,1.0,1.0,25,3,2,2,1,0.591672779,True,6.0,2156747.0,36.0,7.0,False,70.0,1292836.0,144.0,1.0,58.0,428.0,469.0,58.0,358.0,399.0,45.0,414.0,442.0,0.029449424,0.265685019,0.283610755,745,1172,1302621,cinder,0756cf8ec3f2c73b4141b4b6dc597e82e47de669,1,1,,"Bug #1302621 in Cinder: ""Only create volume with an acitve image""","When call create volume api with image id """", it will create an empty
volume.
But when create volume with snapshot id """" or srv volume """", it will
return an 404 error.
So it should create volume fail when call create volume api with image id
"""".","Only create volume with an active image

1. Currently cinder doesn't check whether the image is active
when create volume with an image id, we can even create volume
with a deleted/saving/queued image.

2. For deleted image, it only happened when using admin credentials.

When using non-admin credentials, it returned an error that image
is not found by using 'glance image-show' on a deleted image, which
is expected. Also when creating a volume from this deleted image,
it returned  a 404 error upfront. This again is the expected result.

When using admin credentials, we can view the details of deleted image
with 'glance image-show'. If create a volume out of an image ID (which
is now deleted), the operation succeeds, but the volume is left in
'error' state.

3. This accounts for a bug(he volume shouldn't be created) and should be fixed.

Change-Id: If4f59695fbf6636778d0d352aef28d90bb45ee91
Closes-Bug: #1302621"
56,0759ed18a95b438bf775d6905256119f1cee75db,1401719493,,1.0,11,3,3,3,1,0.812040681,False,,,,,True,,,,,,,,,,,,,,,,,941,1377,1325608,neutron,0759ed18a95b438bf775d6905256119f1cee75db,0,0,"“https://github.com/openstack/neutron/commit/634fd1d23fb241bc4990275d5a4da0c3ab66e2de""","Bug #1325608 in neutron: ""base L3's create_router should process extensions""","Change:
https://github.com/openstack/neutron/commit/634fd1d23fb241bc4990275d5a4da0c3ab66e2de
Tweaked base create_router in l3_db.py to process the extensions or not; however it was chosen at the time, to set the process_extensions flag to False. This effectively disable the ability for extensions made to create_router's method to handle extension's attributes correctly.
A more appropriate value should be True, so that extension attributes can be shown during the create process.","Allow L3 base to handle extensions on router creation

By changing the boolean flag, API extensions made to
the router model can be handled correctly: this means
that on router creation, the response body will
contain all the extension attributes being part of
the resource. Prior to this fix, it was only on GETs
or PUTs, leaving the user at loss as to whether
the flag was actually being processed.

Closes-bug: #1325608
Supports-blueprint: neutron-ovs-dvr

Change-Id: I6f913c8417676a789177e00f30eb5875e7aaa3ae"
57,077e3c770ebeebd037ce882863a6b5dcefd644cf,1402408574,1.0,1.0,64,1,2,2,1,0.950079625,False,,,,,True,,,,,,,,,,,,,,,,,965,1403,1328539,nova,077e3c770ebeebd037ce882863a6b5dcefd644cf,1,1,,"Bug #1328539 in OpenStack Compute (nova): ""Fixed IP allocation doesn't clean up properly on failure""","If fixed IP allocation fails, for example because nova's network interfaces got renamed after a reboot, nova will loop continuously trying, and failing, to create a new instance. For every attempted spawn the instance will end up with an additional fixed IP allocated to it. This is because the code is associating the IP, but not disassociating it if the function fails.","Fix resource cleanup in NetworkManager.allocate_fixed_ip

NetworkManager.allocate_fixed_ip isn't cleaning up the resources it
allocates in the event of a failure. This means that if, for example,
_setup_network_on_host fails, the instance will still have a fixed IP
allocated to it. Every attempt to spawn the instance will result in
another fixed ip being allocated.

Closes-Bug: #1328539

Change-Id: Idfa1f4015acb10dc14fcec448257bcfe6971fc99"
58,07841b42828dc133ba258cf7c7f09d6f8906a96f,1397789024,,1.0,5,5,2,2,1,0.721928095,True,1.0,273605.0,19.0,6.0,False,49.0,363945.0,167.0,2.0,0.0,223.0,223.0,0.0,153.0,153.0,0.0,198.0,198.0,0.001231527,0.245073892,0.245073892,781,1208,1306591,swift,07841b42828dc133ba258cf7c7f09d6f8906a96f,1,1,,"Bug #1306591 in OpenStack Object Storage (swift): ""Proxy server generates error log when cache middleware is disabled.""","Proxy server generates error log for every request when cache middleware is disabled.
Here I attach a sample.
Apr 11 20:26:11 swift-proxy proxy-server: STDOUT: ERROR:root:ERROR: swift.cache could not be found in env! (txn: tx61382a02457d47d7b8085-005347d153)","Fix error log of proxy-server when cache middleware is disabled

Change-Id: I0b67721cbb692fb8d2a5dd8b2c4f5383512b7b45
Closes-Bug: #1306591"
59,07a130be1a1f77fc86076f37afe5b6bae2692a2c,1398909485,,1.0,2,2,1,1,1,0.0,True,3.0,660329.0,62.0,17.0,False,17.0,1082120.0,19.0,11.0,74.0,3350.0,3406.0,74.0,2238.0,2294.0,65.0,749.0,798.0,0.055230126,0.627615063,0.668619247,849,1279,1314850,neutron,07a130be1a1f77fc86076f37afe5b6bae2692a2c,1,1,"""This method was removed from that file in commit 53609f29f3c8fcadc545afb891189253c07b33c3”","Bug #1314850 in neutron: ""'module' object has no attribute 'get_engine'""","File ""neutron/lib/python2.7/site-packages/neutron/wsgi.py"", line 98, in start
    session.get_engine(sqlite_fk=True).pool.dispose()
AttributeError: 'module' object has no attribute 'get_engine'
https://review.openstack.org/#/c/77205/
wsgi.py was not updated with these changes and execution results in an exception when get_engine from neutron.openstack.common.db.sqlalchemy.session is invoked.","Reference new get_engine() method from wsgi.py

wsgi.py referenced get_engine in neutron.openstack.common.db.sqlalchemy.session.
This method was removed from that file in commit 53609f29f3c8fcadc545afb891189253c07b33c3
change-id I0e1d86878d3eb924b01e04dced0f90b4e57757d8. This patch references the
replacement method added to neutron.db.api.

Closes-Bug: #1314850
Change-Id: Ifd4db5f49e2aaebc67eccb8bdcef8eea54983112"
60,07ad47ef80dd71e1216331b3b95f5be280455cbd,1390887565,3.0,,73,7,3,2,1,0.941250034,True,11.0,3383849.0,64.0,14.0,False,10.0,1210115.0,23.0,2.0,8.0,1136.0,1138.0,8.0,952.0,954.0,8.0,992.0,994.0,0.00658376,0.726408193,0.727871251,1833,1291181,1291181,Cinder,07ad47ef80dd71e1216331b3b95f5be280455cbd,1,0,"“The blueprint https://blueprints.launchpad.net/cinder/+spec/vmdk-storage-policy-volume-type introduced ""pbm_default_policy""”","Bug #1291181 in Cinder: ""vmware: Use default_volume_type instead of pbm_default_policy""","The blueprint https://blueprints.launchpad.net/cinder/+spec/vmdk-storage-policy-volume-type introduced ""pbm_default_policy"" to allow an admin to specify what storage policy to use when creating a volume with either:
 - no volume_type or
 - with a volume_type with no vmware:storage_policy extra spec.
This could instead use the ""default_volume_type"" config that Cinder already supports. This way the choice of using a volume_type to capture the required policy is not broken.","vmware: default global pbm policy configuration

Adding support for a global pbm policy configuration for the vmdk
driver. Setting the 'pbm_default_policy' in cinder.conf will be
used as the default storage profile name to be used when creating
a volume without associated vmware:storage_profile extra spec.

Also renaming 'vmware-pbm-wsdl' to 'pbm-wsdl-location' to use
nova driver's naming convention.

Implements: blueprint vmdk-storage-policy-volume-type
Change-Id: I7fad167b7be6a479db88fb4d15d07f29afd023b0"
61,07c24d6bbe6b55638f46d9f7dc9123c5d96eb453,1380567393,,1.0,62,8,4,2,1,0.854173578,True,17.0,14399335.0,145.0,59.0,False,22.0,2351488.5,46.0,6.0,18.0,2930.0,2931.0,18.0,2559.0,2560.0,18.0,2030.0,2031.0,0.003084916,0.329761325,0.329923689,1635,1235112,1235112,nova,07c24d6bbe6b55638f46d9f7dc9123c5d96eb453,1,1, ,"Bug #1235112 in OpenStack Compute (nova): ""VMware driver not discovering iscsi targets while attaching cinder volumes""","VMware drivers cannot dynamically add iscsi targets presented to it while attaching a cinder volume. As a result, the instance cannot be attached to a cinder volume and fails with a message 'unable to find iscsi targets'.
This is because the driver fails to scan the Host Bus Adapter with the iscsi target portal (or target host). We need to fix the driver to scan the HBA by specifying the target portal.","VMware: iscsi target discovery fails while attaching volumes

VMware Drivers are unable to discover iscsi targets because
the api calls fail to specify the iscsi server location
(referred in the code as target portal) to retrieve targets.
This is critical for the attach_volume feature to work with
iscsi supported cinder drivers.

Closes-Bug: #1235112

Change-Id: I60c06ae651bb2eb6d466a5ca61a09f289c21d1a3"
62,07d597079781967f5a149f1812ddca3897fa49d9,1384883263,0.0,1.0,22,24,3,3,1,0.810587392,True,3.0,15629276.0,27.0,11.0,False,22.0,598558.0,32.0,6.0,52.0,1045.0,1045.0,52.0,949.0,949.0,47.0,474.0,474.0,0.086330935,0.854316547,0.854316547,1782,1252856,1252856,neutron,07d597079781967f5a149f1812ddca3897fa49d9,1,1,"""Refactor _spawn/destroy_metadata_proxy so”","Bug #1252856 in neutron: ""destroy_metadata_proxy not called from _destroy_router_namespaces""","In preparing the patch for bug 1250596 I found that _destroy_router_namespaces does not properly destroy the metadata proxy.
It may be that it was not called because _destroy_router_namespaces requires a router_info object that is not available in this context.  Maru suggested refactoring _destroy_metadata_proxy and _spawn_metadata_proxy to enable this.","Call _destroy_metadata_proxy from _destroy_router_namespaces

Refactor _spawn/destroy_metadata_proxy so that it can be called
with only the namespace and the router_id.

Change-Id: Id1c33b22c7c3bd35c54a7c9ad419831bfed8746b
Closes-Bug: #1252856"
63,08112f087ffb221513db2872b6db0726d0c30702,1375070930,,1.0,483,206,9,5,1,0.863533715,True,4.0,821400.0,36.0,19.0,False,4.0,1398287.667,6.0,3.0,40.0,576.0,585.0,40.0,566.0,575.0,15.0,97.0,97.0,0.134453782,0.823529412,0.823529412,1448,1202687,1202687,neutron,08112f087ffb221513db2872b6db0726d0c30702,0,0,Clean up,"Bug #1202687 in neutron: ""Cisco plugin db clean up, part II""","The cisco_vlan_ids table is no longer used.
Remove __init__ and __repr__ methods from models since they are provided by model_base.","Cisco plugin db code cleanup, part II

Remove an unused table.
Prefix Cisco tablenames with cisco_.
Make full use of neutron model base class.
DRY out the nexusport binding queries.
Follow coding guidelines for imports.
Improve cisco/db/network_db_v2.py test coverage.
Improve cisco/db/nexus_db_v2.py test coverage.

Change-Id: I8ea110de37552176f2d9bcbff4ca3e0b65dfacdc
Closes-bug: #1202687
Partial-bug: #1190619"
64,0814bff9366974d77d42d03fa11f60fbe469a8c7,1397488343,,1.0,6,4,2,2,1,0.970950594,True,1.0,23551.0,6.0,2.0,False,31.0,1254442.5,43.0,2.0,107.0,625.0,714.0,106.0,597.0,685.0,37.0,585.0,604.0,0.024126984,0.372063492,0.384126984,793,1220,1307560,cinder,0814bff9366974d77d42d03fa11f60fbe469a8c7,1,0,"“LoopingCall has been renamed to FixedIntervalLoopingCall in https://review.openstack.org/#/c/26345/""","Bug #1307560 in Cinder: ""Switch to newer naming in oslo loopingcall""",LoopingCall has been renamed to FixedIntervalLoopingCall in https://review.openstack.org/#/c/26345/,"Switch over to FixedIntervalLoopingCall

Change-Id: Iebe5269ea01acdd4ba011742831ca5e569f66b28
Closes-Bug: #1307560"
65,083324df06828e1b8fb97220b7415c2a28ee16c4,1395323325,,1.0,3,7,1,1,1,0.0,True,2.0,605941.0,47.0,8.0,False,15.0,498807.0,37.0,4.0,55.0,1499.0,1515.0,55.0,1223.0,1239.0,47.0,788.0,801.0,0.047571853,0.781962339,0.794846383,661,1087,1295438,neutron,083324df06828e1b8fb97220b7415c2a28ee16c4,0,0,"Refactoring the code, no bug","Bug #1295438 in neutron: ""BigSwitch plugin: Unnecessarily deletes ports from backend on network delete.""",The Big Switch plugin unnecessarily deletes individual ports from the controller when a network is being deleted. This isn't needed because the controller will automatically clean up the ports when their parent network is deleted.,"Big Switch Plugin: No REST port delete on net del

Moves the REST call for port deletion out of the
_delete_port method into the delete_port method
so it doesn't get called during a delete_network
operation. The backend controller automatically
removes ports that are a member of a network when
it's deleted.

Closes-Bug: #1295438
Change-Id: Ic09eb722f8e9bd3ce298dee90d8415cda1a1aa7a"
66,084b7f62280241a9037b6f9e592c466135e5fec1,1392754962,,1.0,52,19,1,1,1,0.0,True,4.0,3102109.0,93.0,7.0,False,2.0,2359.0,2.0,3.0,662.0,1212.0,1212.0,578.0,998.0,998.0,199.0,439.0,439.0,0.241545894,0.531400966,0.531400966,427,842,1281772,neutron,084b7f62280241a9037b6f9e592c466135e5fec1,1,0,"It worked, but now it is a bug because of evolution","Bug #1281772 in neutron: ""NSX: sync do not pass around model object""","The NSX sync backend previously passed around a sqlalchemy model object
around which was nice because we did not need to query the database an
additional time to update the status of an object. Unfortinately, this
was add done within a db transaction which included a call to NSX which
could cause deadlock this needed to be removed.","NSX: Sync do not pass around model object

The NSX sync backend previously passed around a sqlalchemy model object
around which was nice because we did not need to query the database an
additional time to update the status of an object.

Unfortunately, this was add done within a db transaction which included a
call to NSX which could cause deadlock this needed to be removed. Now a
dict is passed around instead and proper transaction handling
is used when updating objects.

Change-Id: Id50abd8f2143b127d1ca81c360c1c6d400e1d74f
Closes-bug: #1281772"
67,086496bfc45e01cd2905a074d526a7d513bf4ec2,1411235302,,1.0,10,4,2,2,1,0.371232327,False,,,,,True,,,,,,,,,,,,,,,,,1340,1808,1372076,neutron,086496bfc45e01cd2905a074d526a7d513bf4ec2,0,0,Bug in test,"Bug #1372076 in neutron: ""OVS Agent unit tests waste time waiting for timeout""","The OVS Agent unit tests do not prevent it from trying to report its state which eats up >10 seconds per unit test.
Traceback (most recent call last):
  File ""neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 260, in _report_state
    self.use_call)
  File ""neutron/agent/rpc.py"", line 70, in report_state
    return self.call(context, msg)
  File ""neutron/common/log.py"", line 34, in wrapper
    return method(*args, **kwargs)
  File ""neutron/common/rpc.py"", line 161, in call
    context, msg, rpc_method='call', **kwargs)
  File ""neutron/common/rpc.py"", line 187, in __call_rpc_method
    return func(context, msg['method'], **msg['args'])
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 389, in call
    return self.prepare().call(ctxt, method, **kwargs)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 194, in send
    return self._send(target, ctxt, message, wait_for_reply, timeout)
  File ""/home/administrator/code/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/_drivers/impl_fake.py"", line 186, in _send
    'No reply on topic %s' % target.topic)
MessagingTimeout: No reply on topic q-plugin","Mock out all RPC calls with a fixture

Mock out the rpc proxy calls used by various agents to
prevent unit tests from blocking for 10+ seconds while waiting
for a timeout. This happened with the OVS agent unit tests
recently in Change-ID Idd770a85a9eabff112d9613e75d8bb524020234a.

This change results in a reduction from 330.8 seconds to 2.7 seconds
for the neutron.tests.unit.openvswitch.test_ovs_neutron_agent
test module.

Closes-Bug: #1372076
Change-Id: I5e6794dc33c64c8fe309d8e72a8af3385c7d4442"
68,08bfa77aeccb8ca589e3fb5cf9771879818f59de,1409921284,,1.0,36,1,2,2,1,0.800392208,False,,,,,True,,,,,,,,,,,,,,,,,1335,1803,1371022,cinder,08bfa77aeccb8ca589e3fb5cf9771879818f59de,1,1,"(21eb376df58d48ac5b7d57224484f6db1bba6114) There is a bug but I don’t know whether there is a BIC or not “Add a parameter to take advantage of the new(ish) eventlet socket timeout behaviour”. Bug! I would say that there is an omission in the original code, that is solved by adding a parameter","Bug #1371022 in Cinder: ""Idle client connections can persist indefinitely""","Idle client socket connections can persist forever, eg:
$ nc localhost 8776
[never returns]","Add client_socket_timeout option

Add a parameter to take advantage of the new(ish) eventlet socket timeout
behaviour.  Allows closing idle client connections after a period of
time, eg:

$ time nc localhost 8776
real    1m0.063s

Setting 'client_socket_timeout = 0' means do not timeout.

DocImpact

Change-Id: If492810a2f10fa5954f8c8bb708b14be0b77fb90
Closes-bug: #1371022"
69,0985ef78d43ae7d910669c3ca968fd977f2ea2ae,1408748086,,1.0,100,13,4,3,1,0.468183174,False,,,,,True,,,,,,,,,,,,,,,,,1226,1685,1360395,neutron,0985ef78d43ae7d910669c3ca968fd977f2ea2ae,1,1,,"Bug #1360395 in neutron: ""FIP namespace should not track connections for DVR""","With DVR, connections to external network using floating ips flow through the router namespace into the fip namespace and then to the external network. These connections are tracked in router namespace for filter and nating purpose. Currently these connections are also being tracked in the fip namespace because tracking is turned on by default. To avoid unnecessary consumption of resources tracking of such connections should be turned off in fip namespace.","Stop tracking connections in DVR FIP Namespace

For DVR, connections to external network using floating IPs do not
need to be tracked in FIP namespace because they are being
already tracked in router namespace.
This fix adds iptable rules to not track connections.

Change-Id: I378039e311763d114860cb70ded7349d6580855a
Closes-bug: #1360395"
70,09b20b1b8493bef07cfc1caa8c345eba7ae33062,1392289941,,1.0,1,1,1,1,1,0.0,True,3.0,202432.0,24.0,3.0,False,6.0,2229996.0,8.0,2.0,12.0,1313.0,1325.0,12.0,1105.0,1117.0,0.0,1161.0,1161.0,0.000702741,0.81658468,0.81658468,401,815,1279747,cinder,09b20b1b8493bef07cfc1caa8c345eba7ae33062,1,1,typo in code,"Bug #1279747 in Cinder: ""Wrong example of ""nova_endpoint_template"" in cinder.conf""","Wrong example of ""nova_endpoint_template"" in cinder.conf
e.g. http://localhost:8774/v2/%(tenant_id)s
should be
e.g. http://localhost:8774/v2/%(project_id)s","Fix wrong example of ""nova_endpoint_template""

In cinder.conf

e.g. http://localhost:8774/v2/%(tenant_id)s

should be

e.g. http://localhost:8774/v2/%(project_id)s

Change-Id: I7dd3be9bc9d2e8f26619b00b5bdbc66505a984b2
Closes-Bug: #1279747"
71,09c3051fb2819b647f4d8196e6e0fbc66f0ccc04,1385425543,,1.0,1,5,2,2,1,0.918295834,True,4.0,129759.0,35.0,13.0,False,8.0,2544211.0,11.0,4.0,525.0,1122.0,1370.0,427.0,997.0,1182.0,174.0,446.0,492.0,0.307557118,0.785588752,0.866432337,78,475,1254924,neutron,09c3051fb2819b647f4d8196e6e0fbc66f0ccc04,0,0,removes the other redundant notification sent. Just delete ,"Bug #1254924 in neutron: ""security group update notification sent twice""","For several plugins the security group update notification on port-update is sent twice.
1) https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L634
2) https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L101
the notification at #1 is sent but not that at #2 if the security group on the port was not updated.
However if the notication at #2 is sent, then the notification at #1 will always be sent too.
Therefore the notification at #2 is probably redundant
This affects at least the following plugins:
- nec
- ryu
- linuxbridge
- openvswitch
- ryu
This causes a lot more traffic on the agent side thus slowing all the operations on the agent, including wiring VIF ports.","Send only one agent notification on port update

This patch removes the other redundant notification sent in the same
routine to notify whether a security group changed.

Closes-Bug: #1254924
Partial Blueprint neutron-tempest-parallel

Change-Id: I5bad015decdc197c3fa9b58c680564380d513bc4"
72,09cca5fc0c9dd02832421b26acbf691df0c39936,1387342981,,1.0,13,28,4,4,1,0.953336634,True,3.0,3425981.0,32.0,9.0,False,122.0,359626.75,504.0,4.0,24.0,736.0,741.0,22.0,686.0,689.0,24.0,708.0,713.0,0.003630028,0.102947582,0.103673588,176,579,1260178,nova,09cca5fc0c9dd02832421b26acbf691df0c39936,1,0,Error getting version. The code evolved,"Bug #1260178 in OpenStack Compute (nova): ""Error in getting the major and minor version of hypervisor from tuple""","The utils method get_major_minor_version was being used to get the major and minor versions from the hypervisor version. This was used to determine whether the version is compatible with the required version, to then set the device_id on a vm record during vm creation.
This method was required by the feature: https://review.openstack.org/#/c/55117/
The method is not really required, because the hypervisor version in xen is always in the form of a tuple. This can directly be used, instead of using a utils method.
This fix will include:
- Removing the utils get_major_minor_version method
- Correctly handling the hypervisor version as a tuple","Setting the xen vm device id on vm record

Currently, while setting the device_id, the hypervisor
version is being incorrectly handled since the method
get_major_minor_version expects either an int, float,
or string format. However, the xen hypervisor
version is stored in the session product_version as a tuple.
This fix correctly uses the version to set the appropriate
device_id on the vm record. Also, removing the method
get_major_minor_version as it is not needed anymore.

Closes-bug: #1260178

Change-Id: I6b1353cdb7cc856b34f02e3d3679c142ba032533"
73,09dd5eb9c81033fe83b10d4051b2f06125250185,1400744545,,1.0,2,11,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,908,1344,1322076,neutron,09dd5eb9c81033fe83b10d4051b2f06125250185,0,0,"Resfactoring ""fwaas plugin doesn't need to handle delete firewall rule operation
“","Bug #1322076 in neutron: ""fwaas plugin doesn't need to handle delete firewall  rule operation""","If firewall rule is attached to firewall policy, it would raise FirewallRuleInUse excpetion in DB ops, else it is a pure DB delete ops. So it is useless to handle delete_firewall_rule ops in fwaas plugin.","FWaaS plugin doesn't need to handle firewall rule del ops

If firewall rule is attached to firewall policy, it would raise
FirewallRuleInUse excpetion in DB ops, else it is a pure DB delete ops.
So it is useless to handle delete_firewall_rule ops in fwaas plugin.
Closes-Bug: #1322076

Change-Id: I55a000d206f232c79b41230f526007f684db8f4f"
74,09dd7bf800bcfc18a2a57b9e1ce0c3c24653f5ac,1393627755,,1.0,1,4,2,2,1,0.970950594,True,2.0,1408450.0,53.0,14.0,False,12.0,1603300.0,23.0,4.0,203.0,3549.0,3638.0,188.0,2818.0,2897.0,203.0,3410.0,3499.0,0.02728367,0.45619901,0.46810218,598,1022,1291515,nova,09dd7bf800bcfc18a2a57b9e1ce0c3c24653f5ac,1,1,Bug after a change.,"Bug #1291515 in OpenStack Compute (nova): ""Recent Change to default state_path can silently  break existing systems""","the change to the default value of state_path introduced by
I94502bcfac8b372271acd0dbc1710c0e3009b8e1 for the reasons set out
in my -1 review of the same that seems to have been skipped when the
change was accepted.
As implemented the change will break any existing systems that are using
the default value of state_path with no warning period, which goes beyond
the scope of change for UpgradeImpact","Reverts change to default state_path

Reverting the change to the default value of state_path introduced by
I94502bcfac8b372271acd0dbc1710c0e3009b8e1 for the reasons set out
in my -1 review of the same that seems to have been skipped when the
change was accepted.

As implemented the change will break any existing systems that are using
the default value of state_path with no warning period, which goes beyond
the scope of change for UpgradeImpact

A better approach would be to introduce a new config value that is used in
the code, but is over-ridden by ""state_path"" if it is still in the config file,
and log a warning that state_path is deprecated and will be removed in the future.

That way the change to default behavior can be introduced at the start of the
next release cycle by removing the deprecated state_path config value, and
users have until then to prepare.

It does mean that we keep the insane default for a bit longer, but I think that's
much better that breaking a running system.

Closes-bug: #1291515

Change-Id: I7bb70c551ec9616276552d2e32076b66a1d3ce01"
75,09e706b210513968cac2bad78fe81ee9cbb5b5be,1402089346,4.0,1.0,80,27,4,3,1,0.814740342,False,,,,,True,,,,,,,,,,,,,,,,,963,1401,1328331,neutron,09e706b210513968cac2bad78fe81ee9cbb5b5be,1,1,,"Bug #1328331 in neutron: ""Big Switch: servermanager consistency hash doesn't work in HA deployments""","The Big Switch servermanager records the consistency hash to the database every time it gets updated but it does not retrieve the latest value from the database whenever it includes it in an HTTP request. This is fine in single neutron server deployments because the cached version on the object is always the latest, but this isn't always the case in HA deployments where another server updates the consistency DB.","Big Switch: Lock consistency table for REST calls

Adds a new class to hold an SQL table lock throughout
a REST call to prevent servers in an HA deployment from
using a stale consistency hash value.

Also passes the current context down to the server manager
for every request since the consistency hash will need to
be read from the database instead of from a variable on the
server pool object.

Closes-Bug: #1328331
Change-Id: I5f8402c076d7732742c0ae4d9b9d6833d42a0b7b"
76,09f6fe636416ac63e1463d14d469af8f93a6ddf0,1406568954,1.0,1.0,24,1,2,2,1,0.634309555,False,,,,,True,,,,,,,,,,,,,,,,,1110,1564,1349475,cinder,09f6fe636416ac63e1463d14d469af8f93a6ddf0,1,1,,"Bug #1349475 in Cinder: ""Solidfire: wrong AUTH info for transferred volume""","When a volume is transferred from one project to another, this transferred volume cannot be attached to any instance due to wrong AUTH information.  Nova-compute threw exception when trying to attach the volume:
2014-07-25 00:03:57,358.358 42302 ERROR nova.compute.manager [req-04e6cad9-0d3a-4c3c-a9b3-a9d3a0247872 6a08f3e43965436fb028eda1005ec77b b67a57cc0b9d443eac6d2ff8a494b088] [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Failed to attach volume c946565f-d87a-48df-a6f4-baa3bb9d2300 at /dev/vdc
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Traceback (most recent call last):
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/compute/manager.py"", line 3690, in _attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] encryption=encryption)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1083, in attach_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] disk_info)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/driver.py"", line 1042, in volume_driver_method
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return method(connection_info, *args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return f(*args, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 287, in connect_volume
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] self._run_iscsiadm(iscsi_properties, (""--rescan"",))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/virt/libvirt/volume.py"", line 218, in _run_iscsiadm
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] check_exit_code=check_exit_code)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/utils.py"", line 177, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] return processutils.execute(*cmd, **kwargs)
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] File ""/opt/openstack/nova/2013.2.3.r7/lib/python2.7/site-packages/nova/openstack/common/processutils.py"", line 178, in execute
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] cmd=' '.join(cmd))
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] ProcessExecutionError: Unexpected error while running command.
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf iscsiadm -m node -T iqn.2010-01.com.SF:ogav.uuid-c946565f-d87a-48df-a6f4-baa3bb9d2300.200982 -p ISCSITARGET:3260 --rescan
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Exit code: 255
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stdout: ''
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682] Stderr: 'iscsiadm: No portal found.\n'
2014-07-25 00:03:57,358.358 42302 TRACE nova.compute.manager [instance: e02f0e44-bc32-4a5a-ba84-bf338d301682]
How to reproduce:
1) Create a volume X on Solidfire with project A;
2) Transfer volume X to project B;
3) attach volume X to any instance;
In solidfire.py:accept_transfer(), the volume metadata in the backend is changed but the AUTH (CHAP username secret) remains unchanged, which causes the mismatch of the ownership and corresponding AUTH info.  Cinder volume manager will have to be changed as well to adopt cases like this (volume info must be updated to DB once driver completes accept_transfer).","Get updated model info on volume transfer

On some drivers CHAP credentials are tied to accounts
which may be unique for each OS and/or backend Tenant.

For example:
  Volume is created with project_id xyz
  Backend creates an account for xyz with specific CHAP

  Volume is transferred to project_id abc

Currently we don't update the model_info, so even though
on the backend we updated the owner and CHAP settings we
never propogated that back up to the DB object so the volume
can't be used by the new owner.

This patch just adds an option to return model_update
on the accept_transfer call to the driver and updates
the db accordingly.

Also adds a call in the SF driver to actually get the
new model info to be fed back to the manager.

Change-Id: Ie0447cdad69c4fbee99b4b6b1d3cacdfdd14137d
Closes-Bug: #1349475"
77,0a47253307d427c6e668d7cdf3bdf186dfc93858,1382111929,,1.0,2,2,1,1,1,0.0,True,1.0,2056907.0,12.0,8.0,False,18.0,4842215.0,18.0,8.0,11.0,3833.0,3837.0,11.0,3321.0,3325.0,9.0,2886.0,2888.0,0.001567398,0.452507837,0.452821317,1713,1241685,1241685,nova,0a47253307d427c6e668d7cdf3bdf186dfc93858,0,0,Bug in comments,"Bug #1241685 in OpenStack Compute (nova): ""wrong code comments in contrib/consoles.py""","@wsgi.action('os-getVNCConsole')
    def get_vnc_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)
@wsgi.action('os-getSPICEConsole')
    def get_spice_console(self, req, id, body):
        """"""Get text console output.""""""
        context = req.environ['nova.context']
        authorize(context)","Fix the ConsolesController class doc string

The code must have been copied from the console output extension to start
with and the old class doc string was left in place.

Change-Id: I4924e69763027043aace8e34aac55701be2ea823
Closes-bug: #1241685"
78,0a7527c71228c8e776ad40cedd2cf137fd99f43d,1402493046,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,967,1405,1328870,nova,0a7527c71228c8e776ad40cedd2cf137fd99f43d,1,1,,"Bug #1328870 in OpenStack Compute (nova): ""Hyper-V cannot attach volumes when the host is an AD member""",The domain name gets added to the initiator name used by the host when it's an AD member. The method which automatically gets the initiator name when this is not set in the registry does not take this into account. Trying to use a wrong initiator name will lead to an exception when trying to log in to the according iSCSI target.,"Fixes hyper-v volume attach when host is AD member

The domain name gets added to the initiator name used by the host
if it's an Active Directory member.

Currently the method which gets the initiator name does not take
this into account when retrieving the default initiator name.

Trying to use a wrong initiator name will lead to an exception when
trying to log in to the according iSCSI target.

This patch simply appends the domain name (when the host is an AD
member) to the initiator name used to log in to iSCSI targets.

Closes-Bug: #1328870

Change-Id: Ifbe762c685e46081059a01043431b2c4ac5473fc"
79,0a84a7fb24a4605f0da863407512612651890003,1383185875,1.0,1.0,51,2,2,2,1,0.563102824,True,4.0,2199565.0,22.0,6.0,False,43.0,1663669.0,90.0,4.0,328.0,3149.0,3319.0,325.0,2731.0,2899.0,285.0,2225.0,2374.0,0.044061008,0.342936373,0.365891234,17,329,1243488,nova,0a84a7fb24a4605f0da863407512612651890003,1,1,,"Bug #1243488 in OpenStack Compute (nova): ""unplugging vif ERROR in n-cpu log after successful tempest neutron run""","Happens a lot. For example:  http://logs.openstack.org/50/51750/8/check/check-tempest-devstack-vm-neutron/7b73c73/logs/screen-n-cpu.txt.gz
2013-10-22 20:29:59.161 3294 ERROR nova.virt.libvirt.driver [-] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] During wait destroy, instance disappeared.
2013-10-22 20:29:59.163 DEBUG nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] vif_type=ovs instance={'vm_state': u'building', 'availability_zone': None, 'terminated_at': None, 'ephemeral_gb': 0, 'instance_type_id': 6, 'user_data': None, 'cleaned': False, 'vm_mode': None, 'deleted_at': None, 'reservation_id': u'r-5n6tip35', 'id': 18, 'disable_terminate': False, 'display_name': u'Server 88a265da-046a-4fc4-b812-59164e7e35c0', 'uuid': '88a265da-046a-4fc4-b812-59164e7e35c0', 'default_swap_device': None, 'hostname': u'server-88a265da-046a-4fc4-b812-59164e7e35c0', 'launched_on': u'devstack-precise-hpcloud-az3-598823', 'display_description': u'', 'key_data': None, 'kernel_id': u'1b2d8a22-2d48-4240-9987-e27de5999746', 'power_state': 0, 'default_ephemeral_device': None, 'progress': 0, 'project_id': u'd66f874bdc1e49a2adedb607234b2f99', 'launched_at': None, 'config_drive': u'', 'node': u'devstack-precise-hpcloud-az3-598823', 'ramdisk_id': u'8243eabc-098b-44cc-8629-73102741d145', 'access_ip_v6': None, 'access_ip_v4': None, 'deleted': False, 'key_name': None, 'updated_at': datetime.datetime(2013, 10, 22, 20, 29, 57, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'host': u'devstack-precise-hpcloud-az3-598823', 'architecture': u'x86_64', 'user_id': u'ebc6b389f1044bed9ebfdc9ef111d268', 'system_metadata': {u'image_architecture': u'x86_64', u'instance_type_memory_mb': u'64', u'instance_type_swap': u'0', u'instance_type_vcpu_weight': None, u'instance_type_root_gb': u'0', u'instance_type_id': u'6', u'image_image_state': u'available', u'instance_type_name': u'm1.nano', u'image_image_location': u's3bucket--tempest-1023546658/cirros-0.3.1-x86_64-blank.img.manifest.xml', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'image_disk_format': u'ami', u'instance_type_flavorid': u'42', u'image_container_format': u'ami', u'instance_type_vcpus': u'1', u'image_min_ram': u'0', u'image_min_disk': u'0', u'image_base_image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee'}, 'task_state': u'deleting', 'shutdown_terminate': False, 'cell_name': None, 'root_gb': 0, 'locked': False, 'name': 'instance-00000012', 'created_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>), 'locked_by': None, 'launch_index': 0, 'metadata': {}, 'memory_mb': 64, 'vcpus': 1, 'image_ref': u'7979e34a-8f16-4d0b-8d6f-4b521d832aee', 'root_device_name': u'/dev/vda', 'auto_disk_config': False, 'os_type': None, 'scheduled_at': datetime.datetime(2013, 10, 22, 20, 29, 55, tzinfo=<iso8601.iso8601.Utc object at 0x2b0cf50>)} vif=VIF({'ovs_interfaceid': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'network': Network({'bridge': 'br-int', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': 'fixed', 'floating_ips': [], 'address': u'10.1.0.5'})], 'version': 4, 'meta': {'dhcp_server': u'10.1.0.3'}, 'dns': [], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': 'gateway', 'address': u'10.1.0.1'})})], 'meta': {'injected': False, 'tenant_id': u'd66f874bdc1e49a2adedb607234b2f99'}, 'id': u'2b5cb11b-edf9-4e26-a068-bd820c95e106', 'label': u'private'}), 'devname': u'tap725ac9ef-5f', 'qbh_params': None, 'meta': {}, 'address': u'fa:16:3e:31:dd:da', 'type': u'ovs', 'id': u'725ac9ef-5f13-4eb3-b00b-0bbf1aad3809', 'qbg_params': None}) unplug /opt/stack/new/nova/nova/virt/libvirt/vif.py:755
2013-10-22 20:29:59.164 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.178 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.179 DEBUG nova.openstack.common.processutils [req-7e492c69-7a37-438e-9918-f3932e88c617 demo demo] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf ip link show dev qvod0a9091e-91 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-10-22 20:29:59.301 DEBUG nova.openstack.common.processutils [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-10-22 20:29:59.302 ERROR nova.virt.libvirt.vif [req-b59c1299-a2d5-4ad0-a798-e2d6c57734ce demo demo] [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Failed while unplugging vif
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Traceback (most recent call last):
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/virt/libvirt/vif.py"", line 636, in unplug_ovs_hybrid
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     utils.execute('brctl', 'delif', br_name, v1_name, run_as_root=True)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/utils.py"", line 174, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     return processutils.execute(*cmd, **kwargs)
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]   File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]     cmd=' '.join(cmd))
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] ProcessExecutionError: Unexpected error while running command.
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Command: sudo nova-rootwrap /etc/nova/rootwrap.conf brctl delif qbr725ac9ef-5f qvb725ac9ef-5f
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Exit code: 1
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stdout: ''
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0] Stderr: 'interface qvb725ac9ef-5f does not exist!\n'
2013-10-22 20:29:59.302 3294 TRACE nova.virt.libvirt.vif [instance: 88a265da-046a-4fc4-b812-59164e7e35c0]
2","Issue brctl/delif only if the bridge exists

Add a check for existence of the bridge before we try cleaning
up as we see a lot of spurious exceptions in the logs especially
when rebuild_instance is in progress and terminate_instance is
issued

Closes-Bug: #1243488

Change-Id: Ib0e2e4d9506e3a8f63201d28a3fc2c910c77d3bd"
80,0ac15ad5d7902ef30d495acf737c90ba9377566a,1407799782,,1.0,3,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1176,1633,1355502,neutron,0ac15ad5d7902ef30d495acf737c90ba9377566a,0,0,"“This patch clarifies that in order to use NSX distributed
    routers,”","Bug #1355502 in neutron: ""NSX - add note in configuration files regarding distributed routers""","In order to leverage distributed routing with the NSX plugin - the replication_mode parameter should be set to 'service'.
Otherwise the backend wil throw 409 errors resulting in 500 NSX errors.
This should be noted in the configuration files.","NSX: fix wording for configuration option

This patch clarifies that in order to use NSX distributed
routers, the replication_mode parameter should be set to
'service'. This is already its default value, so no further
change is needed beyond clarifying wording.

Change-Id: I446c428ecf8c0bf7f6947e69562d60a7fb4383ed
Closes-Bug: #1355502"
81,0aeffa12a62604ee3238323d969345e41937b642,1412174599,,1.0,40,10,2,2,1,0.881290899,False,,,,,True,,,,,,,,,,,,,,,,,1376,1846,1376945,nova,0aeffa12a62604ee3238323d969345e41937b642,1,1,,"Bug #1376945 in OpenStack Compute (nova): ""os-networks extension displays cidr incorrectly""","The nova-networks extension is improperly converting cidr values to strings:
$ nova network-list
shows a list of ips for cidr:
[u'192.168.50.0', u'192.168.50.1', u'192.168.50.2', u'192.168.50.3',...]
This is possibly due to the extension being updated to use objects, but I don't recall seeing it previously, so it is possible something changed the way an ipnetwork is converted to json so that it now iterates through the object isntead of printing it as a string.","Fix the os_networks display to show cidr properly

Converting network_get and network_get_all to use objects broke
the display of the os_networks extension, because IPAddress
fields in Network objects are dumped as lists by the jsonutils
extension. We therefore must explicitly convert these object
field values to string.

The tests are updated to use objects so that we pick up bugs
like this in the future. Incorrect assertEqual parameter order
is fixed in the tests too since these are comparing dicts and
it's not fun debugging a MismatchError when the reference/actual
values are backwards.

Change-Id: I0f05a9b4d7bbe5fe0a3b110c191455ca7edefcb5
Closes-Bug: #1376945
Co-authored-by: Matt Riedemann <mriedem@us.ibm.com>
(cherry picked from commit da25467aafce9b62dd3fdff9d6cd84121fbee17e)"
82,0b01e846d40f3b343da9ebe1dae89cca8bc2ac66,1409048970,,1.0,98,5,3,2,1,0.462155594,False,,,,,True,,,,,,,,,,,,,,,,,1237,1696,1361840,nova,0b01e846d40f3b343da9ebe1dae89cca8bc2ac66,1,1,“if I revert ecce888c469c62374a3cc43e3cede11d8aa1e799 everything works fine.”,"Bug #1361840 in OpenStack Compute (nova): ""nova boot fails with rbd backend""","Booting a VM in a plain devstack setup with ceph enabled, I get an error like:
libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: -drive file=rbd:vmz/27dcd57f-948f-410c-830f-48d8fda0d968_disk.config:id=cindy:key=AQA00PxTiFa0MBAAQ9Uq9IVtBwl/pD8Fd9MWZw==:auth_supported=cephx\;none:mon_host=192.168.122.76\:6789,if=none,id=drive-ide0-1-1,readonly=on,format=raw,cache=writeback: error reading header from 27dcd57f-948f-410c-830f-48d8fda0d968_disk.config
even though config_drive is set to false.
This seems to be related to https://review.openstack.org/#/c/112014/, if I revert ecce888c469c62374a3cc43e3cede11d8aa1e799 everything works fine.","libvirt: Uses correct imagebackend for configdrive

When the configdrive file is created it must be moved to the
configured imagebackend, not always the raw one.

Otherwise nova can't boot an instance with a configdrive attached
when the rbd backend is configured.

Related to bug #1246201
Closes-bug: #1361840

Change-Id: I59578d9b87a36ff4417a30ff14c5774c54698cf3"
83,0b246bb5f561d694a4fa4f41c90811a64c05b9fb,1410511756,,1.0,42,42,11,7,1,0.865063792,False,,,,,True,,,,,,,,,,,,,,,,,1601,1230127,1230127,glance,0b246bb5f561d694a4fa4f41c90811a64c05b9fb,0,0,Test files,"Bug #1230127 in Glance: ""Use self.assertTrue instead of self.assertEqual(..., True)""","$ find . -name '*.*' | xargs grep self.assertEqual | grep True
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(response['x-image-meta-is_public'], 'True')
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
./tests/integration/legacy_functional/test_v1_api.py:        self.assertEqual(data['image']['is_public'], True)
...
./tests/unit/test_swift_store.py:        self.assertEqual(connection.insecure, True)
./tests/unit/test_swift_store.py:        self.assertEquals(connection.snet, True)
./tests/unit/test_swift_store.py:        self.assertEquals(connection.snet, True)
$ find . -name '*.*' | xargs grep self.assertTrue
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
...
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.read_only)
./tests/unit/test_context_middleware.py:        self.assertTrue(req.context.is_admin)
The tests assertions need to be consistent: use assertTrue/assertFalse instead of assertEqual to test boolean values.","Replaces assertEqual with assertTrue and assertFalse

In order to improve coding practices and be aligned to a standard,
self.assertEqual(...,True) statements where replaced with
self.assertTrue and self.assertFalse.

Change-Id: I4150a343493beae03d6f71c88e84f60d14fb60e1
Closes-Bug: #1230127"
84,0b2cf281be48e76a80e2fc73f00529f22e8d9126,1398656524,,1.0,12,2,2,2,1,0.985228136,True,3.0,719720.0,38.0,8.0,False,214.0,83609.0,1207.0,1.0,133.0,1116.0,1138.0,133.0,1115.0,1137.0,133.0,1110.0,1132.0,0.016992138,0.140882577,0.143672331,730,1156,1301696,nova,0b2cf281be48e76a80e2fc73f00529f22e8d9126,1,1,,"Bug #1301696 in OpenStack Compute (nova): ""automatic confirm resize should not set migration to error when encounter problem""","function in compute/manager.py _poll_unconfirmed_resizes will translate the migrate status from finished to error
whenever it find a problem , consider following case
1) _poll_unconfirmed_resizes running, it found several migrations to be confirmed
2) user want to delete an instance ,its task state will be changed to DELETING by
3) _poll_unconfirmed_resized will found the task state is not None, it will make the migration error status
4) following code in _delete
if instance.vm_state == vm_states.RESIZED:
                self._confirm_resize_on_deleting(context, instance)
will fail because migration status already updated
so we should not set the migration status to 'error' , let it be and report warning message is enough","Keep Migration status in automatic confirm-resize

function in compute/manager.py _poll_unconfirmed_resizes
will translate the migrate status from finished to error
whenever finds a problem ,Consider the following case

1) _poll_unconfirmed_resizes runs and founds several migrations to be confirmed
2) user deletes the instance, so its task state will be changed to DELETING
3) _poll_unconfirmed_resizes will find the task state is not None,
it will make the migration error status
4) following code in _delete
if instance.vm_state == vm_states.RESIZED:
self._confirm_resize_on_deleting(context, instance)
will fail because migration status already updated

User is able to do the migration operations when they found error
so we don't need to handle this kind of error for them

Change-Id: Ia79dd56c9c1fa8add7dc138d838cbfbac4523ac3
Closes-Bug: #1301696"
85,0b311bffc0f22f0b877d6bcad0c4cbcf1701dd54,1390882418,,1.0,25,2,2,2,1,0.605186577,True,2.0,2980393.0,27.0,10.0,False,37.0,1776567.0,76.0,5.0,58.0,4135.0,4140.0,58.0,3432.0,3437.0,58.0,3149.0,3154.0,0.008218415,0.438779774,0.43947625,393,806,1279189,nova,0b311bffc0f22f0b877d6bcad0c4cbcf1701dd54,1,1,Typo? Return instead of raise,"Bug #1279189 in OpenStack Compute (nova): ""delete or query an not exist snapshot raise exception more readable""","In V2 API layer code, it would be better if we raise exception instead of return it
try:
            self.volume_api.delete_snapshot(context, id)
        except exception.NotFound:
            return exc.HTTPNotFound()  ----> use raise instead of return","Raise exception if volume snapshot id not found instead of return

In V2 API, if show or delete a snapshot with an unknown id, exception
will be returned instead of raised. Even though correct error
will be returned to client, it's not aligned to other API layer
code, this patch modify it by raise exception instead of return.

Change-Id: I22d4565ca08e1c0ed5041ce903bf6ba918c14cfd
Closes-Bug: #1279189"
86,0b4e42fe11bf918e18ea8f240d9055b3967b60bb,1406859600,,1.0,60,14,3,3,1,0.787330646,False,,,,,True,,,,,,,,,,,,,,,,,1137,1592,1351123,neutron,0b4e42fe11bf918e18ea8f240d9055b3967b60bb,1,1,“Fix DB Duplicate error”,"Bug #1351123 in neutron: ""DB Duplicate error while scheduling distributed routers""","I observed this error here:
http://logs.openstack.org/77/108177/10/experimental/check-tempest-dsvm-neutron-dvr/9e67d95/logs/screen-q-svc.txt.gz?level=TRACE#_2014-07-31_15_05_48_864
And in few other places. This seems to be triggered (for instance) during the following testcase:
tempest.scenario[...]test_server_connectivity_pause_unpause
It looks like the scheduling process fails because of a duplicate entry to the agent/router binding table. This might be an effect of fix:
https://review.openstack.org/#/c/73234/","Fix DB Duplicate error when scheduling distributed routers

The error was caused by binding the router to an agent
candidate that was already selected during the scheduling
process.

A DB lookup was also saved by passing the router object
around; this led to a minor style cleanup.

Closes-bug: #1351123

Change-Id: Ib71a0140c8a7fbd5b230609d33487f8adba252e7"
87,0b8267aa834e9d86cf2ecd9fd64ccc218df454bd,1391894938,,1.0,6,6,4,4,1,0.959147917,True,1.0,12119.0,15.0,6.0,False,38.0,1104650.0,87.0,2.0,2065.0,1583.0,3215.0,1671.0,1251.0,2536.0,1967.0,1555.0,3093.0,0.269662921,0.213209098,0.423951768,326,735,1272518,nova,0b8267aa834e9d86cf2ecd9fd64ccc218df454bd,0,0,Modify some typos in tests and also deletes 2 returns in code that doesnt make sense. No bug,"Bug #1272518 in OpenStack Compute (nova): ""The _test_{compute,console,network,scheduler}_api methods seem broken.""","The _test_{compute,console,network,scheduler}_api methods, found in nova/tests/{compute,console,network,scheduler}/test_rpcapi.py, all have the following line in the beginning:
    expected_retval = 'foo' if method == 'call' else None
However, the ""method"" parameter is never equal to 'call'. It is probably meant to read:
    expected_retval = 'foo' if rpc_method == 'call' else None
instead.","Fix a couple of unit test typos

There were two cases in rpcapi unit tests where it was checking method instead
of rpc_method.  Fix up the typo and the reason why the typo didn't matter
before.  Also fix two places where rpcapi code had a return for a cast(), which
is now caught by the unit tests.

Closes-bug: #1272518
Change-Id: Ib5e3a002f6b0f7eb8dee8bd84bd53dbf817017f7"
88,0bd4472ef7bdb9d94f988669f34f7eaa53ca0a89,1410793877,,1.0,16,8,3,3,1,0.780107101,False,,,,,True,,,,,,,,,,,,,,,,,1265,1725,1365429,neutron,0bd4472ef7bdb9d94f988669f34f7eaa53ca0a89,1,1,,"Bug #1365429 in neutron: ""All HA routers become active on the same agent""","How to reproduce:
On a setup with two L3 agents, create ten HA routers, the scheduler will place them on both agents, but the same agent will host the active instance of all ten routers. This defeats the idea of load sharing traffic across all L3 agents.
Solutions:
This can be solved in one of two ways:
1) Enable preemptive elections for HA routers. Keepalived enables a configuration value that causes VRRP pre-emptive elections. This way we can set a random VRRP priority for each router instance, and the elections process will guarantee a random distribution of active routers on the available agents. Preemptive elections have a major downside - If an agent that's hosting a master instance drops, the backup router will come in to play, but when the node is fixed the old master will re-assume its role. This second state transition is costly and redundant.
2) With non-preemptive elections the first router instance to come up will become the master. We can exploit this and send the notification from the server to the agents in a random order.","HA routers master state now distributed amongst agents

We're currently running with no pre-emption, meaning that
the first router in a cluster to go up will be the master,
regardless of priority. Since the order in which we sent
notifications was constant, the same agent hosted the
master instances of all HA routers, defeating the idea
of load sharing.

Closes-Bug: #1365429
Change-Id: Ia6fe2bd0317c241bf7eb55915df7650dfdc68210"
89,0bea84ac20fe498bd08f7212a0017196c8cb0812,1402109360,,1.0,95,48,17,12,1,0.751512412,False,,,,,True,,,,,,,,,,,,,,,,,1847,1327473,1327473,Nova,0bea84ac20fe498bd08f7212a0017196c8cb0812,0,0,I don't think this is describing a bug …. It is avoiding a wrong practise in Python,"Bug #1327473 in OpenStack Compute (nova): ""Don't use mutables as default args""","Passing mutable objects as default args is a known Python pitfall.
We'd better avoid this.
This is an  example show the pitfall:
http://docs.python-guide.org/en/latest/writing/gotchas/","Removes the use of mutables as default args

Passing mutable objects as default args is a known Python pitfall.
We'd better avoid this. This commit changes mutable default args with
None, then use 'arg = arg or {}', 'arg = arg or []'. For unit code which
doesn't use the args , just set with None. This commit also adds hacking
check.

Closes-Bug: #1327473
Change-Id: I5a8492bf8ffef8e000b13b6bdfaef1968b96f816"
90,0c202ab3e453e38c09f04978e4fce30d6ee6350c,1393841120,,1.0,36,16,2,2,1,0.961236605,True,6.0,6773889.0,87.0,18.0,False,7.0,3907947.0,11.0,3.0,0.0,178.0,178.0,0.0,178.0,178.0,0.0,172.0,172.0,0.001128668,0.195259594,0.195259594,736,1163,1302272,neutron,0c202ab3e453e38c09f04978e4fce30d6ee6350c,1,1,,"Bug #1302272 in neutron: ""neutron iptables manager is slow modifying a large amount of rules""","Sudhakar Gariganti has noticed that with a very large number of iptables rules that _modify_rules() was taking so long to complete (140 seconds) that VMs couldn't be reliably booted because the rules weren't getting put in place before the initial DHCP requests had timed out.  With a small change the update can be done much quicker, and also allow each node to support a larger set of iptables rules.
I've included a snippet from the related bug for reference, https://bugs.launchpad.net/neutron/+bug/1253993
""We have done significant testing with this patch and want to share few results from our experiments.
We were basically trying to see how many VMs we can scale with the OVS agent in use. With default security groups(which has remote security group), beyond 250-300 VMs, VMs were not able to get DHCP IPs. We were having 16 CNs, with VMs uniformly distributed across them. The VM image had a wait period of 120 secs to receive the DHCP response.
By the time we have around 18-19 VMs on each CN(there were around 6k Iptable rules), each RPC loop was taking close to 140 seconds(if there is any update). And the reason VMs were not getting IPs was that the Iptable rules required for the VM to send out the DHCP request were not in place before the 120 secs wait period. Upon further investigations we discovered that the ""for loop searching iptable rules"" in _modify_rules method of iptables_manger.py is eating a big chunk of the overall time spent.
After this patch, we were able to see close to 680 VMs were able to get IPs. The number of Iptable rules at this point was close to 20K, with around 40 VMs per CN.
To summarize, we were able to increase the processing capability of compute node from 6K Iptable rules to 20K Iptable rules, which helped more VMs get DHCP IP within the 120 sec wait period. You can imagine the situation when the wait time is less than 120 secs.""","Improve iptables_manager _modify_rules() method

As the number of ports per default security group increases, the
number of iptables entries on the Compute Node grows.  Because of
this, there is a gradual increase in the time taken to apply chains
and rules.

Currently we are using list comprehensions to find if a new chain or
rule matches an existing one.  Instead, walk through the list in
reverse to find a matching entry.

Added a new method, _find_last_entry(), to return the entry we are
searching for.

Change-Id: I3585479ffa00be556b8b21dc9dbd6b36ad37f4de
Closes-Bug: #1302272
Related-Bug: #1253993"
91,0c3a47e57317f6efd573388c2c400a6a88745fd8,1409050834,,1.0,104,57,14,13,1,0.900736652,False,,,,,True,,,,,,,,,,,,,,,,,1235,1694,1361611,nova,0c3a47e57317f6efd573388c2c400a6a88745fd8,0,0,"Feature “Adds typed console objects used by virt driver API to
    return information about consoles.”","Bug #1361611 in OpenStack Compute (nova): ""console/virt stop returning arbitrary dicts in driver API""",We have a general desire though to stop returning / passing arbitrary dicts in the virt driver API - On this report we would like to create typed objects for consoles that will be used by drivers to return values on the compute manager.,"console: add typed console objects

Adds typed console objects used by virt driver API to
return information about consoles.

Also updates virt drivers to use them.

Closes-Bug: #1361611
Change-Id: I8f6a857b88659ee30b4aa1a25ac52d7e01156a68"
92,0c409d98bb8f04bc1ba9a47d86c13678ab9221e5,1383059310,,1.0,20,13,5,4,1,0.934832196,True,8.0,1332264.0,51.0,17.0,False,189.0,154548.4,1062.0,2.0,4.0,1254.0,1257.0,3.0,1101.0,1103.0,2.0,1193.0,1194.0,0.000463464,0.18445852,0.184613008,1712,1241681,1241681,nova,0c409d98bb8f04bc1ba9a47d86c13678ab9221e5,1,1, ,"Bug #1241681 in OpenStack Compute (nova): ""compute.instance.delete.end notification emitted without deleted_at""","The compute.instance.delete.end notification used to have a value for deleted_at. It seems that deleted_at is being set in the database, but the final notification for a deleted instance is emitted without it. It is important that the final notification for an instance be sent with up-to-date details as it is potentially the last notification that will be sent.
Sample Notification: http://paste.openstack.org/show/48702/","Send delete.end with latest instance state

Delete.end notifications were getting sent with a blank deleted_at
field. This meant that services listening to notifications for
instance state changes could potentially never get the instances
deleted_at value. The instance usage audit notification would be
the only source for that value.

This change returns the destroyed instance from the conductor API
and sends the delete.end notification with that state, containing
the deleted_at value which was just set.

UpgradeImpact
Closes-Bug: #1241681
Change-Id: I3481e0bada1711c3bff50f249f6a2b40a4ea6855"
93,0c4a94eac94399a524ff758fa7046e98b07951ae,1396923536,,1.0,14,1,2,2,1,0.918295834,True,4.0,574842.0,22.0,9.0,False,24.0,580219.0,59.0,1.0,9.0,469.0,474.0,9.0,467.0,472.0,9.0,449.0,454.0,0.006393862,0.287723785,0.290920716,756,1183,1304115,cinder,0c4a94eac94399a524ff758fa7046e98b07951ae,1,1,,"Bug #1304115 in Cinder: ""Storwize/SVC driver crashes when check volume copy status""","Loopingcall will failed if user delete the volume before it finish copy, .
2014-03-31 16:01:04.086 ERROR cinder.openstack.common.loopingcall [-] in fixed duration looping call
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 76, in
_inner
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall self.f(*self.args, **self.kw)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/_init_.py"",
line 634, in _check_volume_copy_ops
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall volume = self.db.volume_get(ctxt, vol_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/api.py"", line 205, in volume_get
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return IMPL.volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1152, in volume_ge
t
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return _volume_get(context, volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 135, in wrapper
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall return f(*args, **kwargs)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall File ""/opt/stack/cinder/cinder/db/sqlalchemy/api.py"", line 1145, in _volume_g
et
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall raise exception.VolumeNotFound(volume_id=volume_id)
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall VolumeNotFound: Volume 8ff1a61e-21c2-48a7-890e-e7d958172721 could not be found.
2014-03-31 16:01:04.086 TRACE cinder.openstack.common.loopingcall","Storwize/SVC driver crashes when check volume copy status

Storwize/SVC driver does not clear up property of _vdiskcopyops
when deleting a volume. So if a volume which under async copy is
deleted before sync completed, the driver will crash.

When delete a volume, add a check for _vdiskcopyops, if volume
is in the dict, remove it.

Change-Id: I4905404b72a7ac8f90bd92fa4345ba771a78418b
Closes-Bug: #1304115"
94,0c6c57d70826b36db30795fa993cf677a4c2a6e4,1393526494,1.0,1.0,18,2,2,2,1,0.992774454,True,2.0,46086.0,12.0,4.0,False,5.0,15005.0,7.0,2.0,16.0,1182.0,1190.0,16.0,954.0,962.0,16.0,1037.0,1045.0,0.011455526,0.699460916,0.704851752,405,819,1279897,cinder,0c6c57d70826b36db30795fa993cf677a4c2a6e4,0,0,'may return ‘,"Bug #1279897 in Cinder: ""HP LeftHand CLIQ proxy may return incorrect capacity values""","The HP Lefthand CLIQ proxy may return incorrect total_capacity_gb, and free_capacity_gb values if there are more than one cluster configured in the management group. The call to getCluster in _update_backend_status should include the cluster name.","Fix volume stats with multiple LeftHand clusters

When more than one cluster exists in a LeftHand management
group, get_volume_stats may return the stats for the wrong
cluster. This patch fixes the problem by listing the cluster
name in the getClusterInfo query.

Change-Id: I2d5a76869c6bb1b3d514f0546ea00998e5ee32fb
Closes-Bug: #1279897"
95,0c85faa733aef714a8e47f647a931fb27857d4b9,1384200524,,1.0,21,21,4,2,1,0.978033486,True,6.0,4336447.0,41.0,22.0,False,11.0,2794444.0,15.0,9.0,31.0,2038.0,2055.0,26.0,1860.0,1872.0,10.0,463.0,463.0,0.020754717,0.875471698,0.875471698,1751,1250574,1250574,neutron,0c85faa733aef714a8e47f647a931fb27857d4b9,1,1,“Add missing quota flags”,"Bug #1250574 in neutron: ""Quota l3 flags doesn't set in neutron.conf sample file""","The quota flags in l3 extensions [1] are not defined in the configuration file sample [2].
[1] https://github.com/openstack/neutron/blob/master/neutron/extensions/l3.py#L139
[2] https://github.com/openstack/neutron/blob/master/etc/neutron.conf#L272","Add missing quota flags in the config file sample

l3 extensions don't set quota flags in the config sample file
etc/neutron.conf.
Quota is unlimited if its value is minus and not only if it's -1.

Closes-Bug: #1250574
Change-Id: I76c8d1e2676d4e9b0feeed6e504e759fe736a0db"
96,0ca33df5660849ce305f9e9756007d95fcbbfa2b,1403887168,,1.0,0,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1019,1462,1335193,nova,0ca33df5660849ce305f9e9756007d95fcbbfa2b,1,0,"“    In newer kernel versions, the network devices added for lxc containers
    do not get checksums added. “","Bug #1335193 in OpenStack Compute (nova): ""libvirt lxc needs iptables checksum added for dhcp packets""","I tested a devstack today with libvirt-lxc, and was unable to get a dhcp address in cirros 0.3.2.
The reason is that cirros's udhcpc seems to ignore the response if it doesn't have checksums.
the appropriate mangle rule would be written if /dev/vhost-net , but with newer kernels this is also happening on the lxc network devices.
It seems the sane thing to do at this point is just to drop the protection based on '/dev/vhost-net' presence.
--
Related bugs:
 * https://bugs.launchpad.net/ubuntu/+source/isc-dhcp/+bug/930962
 * https://bugzilla.redhat.com/show_bug.cgi?id=910619#c6","add checksums to udp independent of /dev/vhost-net.

In newer kernel versions, the network devices added for lxc containers
do not get checksums added.  The result is the same as previously
occurred when vhost-net became prevalent in kvm guests.

Software that expects the checksums on packets will ignore them.  One example
of such software is udhcpc in cirros.  Without this change, cirros containers
in 3.13 kernels (Ubuntu 14.04) will fail to acquire an address via dhcp.

Closes-Bug: #1335193
Change-Id: Iba305fb7f8236955ca732e467db9e424535be93d"
97,0cbcdcfc5094fcb52679550bc67db8136441adf6,1385557580,,1.0,3,1,1,1,1,0.0,True,1.0,856137.0,26.0,8.0,False,12.0,129833.0,57.0,5.0,531.0,1485.0,1668.0,431.0,1335.0,1460.0,180.0,495.0,521.0,0.312607945,0.856649396,0.901554404,93,491,1255519,neutron,0cbcdcfc5094fcb52679550bc67db8136441adf6,1,1,Bug. Check is an integer to fix it,"Bug #1255519 in neutron: ""NVP connection fails because port is a string""","On a dev machine I've recently create I noticed failures at startup when Neutron is configured with the NVP plugin.
I root caused the failure to port being explicitly passed to HTTPSConnection constructor as a string rather than an integer.
This can be easily fixed ensuring port is always an integer.
I am not sure of the severity of this bug as it might strictly related to this specific dev env, but it might be worth applying and backporting it","Ensure NVP API connection port is always an integer

Convert the 'port' part of a NVP endpoint to int before passing
it to the NVP API client.

Change-Id: I15137d943ab950cb5cc80d7af2971bce3ac1a265
Closes-Bug: #1255519"
98,0cbe012602c7d4240beaa2e4158e885a8ce7de5a,1388993989,1.0,1.0,61,30,9,7,1,0.778511813,True,13.0,10582569.0,79.0,24.0,False,59.0,233301.0,224.0,4.0,13.0,559.0,562.0,13.0,534.0,537.0,13.0,276.0,279.0,0.012750455,0.252276867,0.255009107,1708,1241379,1241379,glance,0cbe012602c7d4240beaa2e4158e885a8ce7de5a,1,1, ,"Bug #1241379 in Glance: ""duplicate upload image""","Image status change ""active"" to ""saving"" when duplicate upload image to glance,details are as follows:
create image:
curl -i -X POST -H ""Content-Type:application/json"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -d '{""name"":""test_img"",""container_format"":""bare"",""disk_format"":""qcow2"",""visibility"":""public""}' http://127.0.0.1:9292/v2/images
upload image:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./test.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file
duplicate upload:
curl -i -H ""Content-Type:application/octet-stream"" -H ""X-Auth-Token:480194c5e9a046b9be8150c80c7f439b"" -X PUT -T ./xx.img http://127.0.0.1:9292/v2/images/04059ffa-56fa-4f67-a294-fdc10372e634/file","Restore image status on duplicate image upload

Glance should not allow already uploaded images to be modified by
another upload. Currently, when configured with the local filesystem
backend, Glance prevents duplicate upload of an already uploaded image
BUT it changes the image status to ""saving"".

This commit adds a status transition state machine to the domain.Image
class and modifies the domain.Image.status setter function to verify
status transitions based on this state machine - only target states that
can be reached from the image's current state are permitted.

Tests have also been agumented to verify that the original image (meta)
data does not change in case of a conflicting upload.

Closes-Bug: #1241379

Change-Id: I62c5acae4c29abf0691d8279b51c59008f9c0047"
99,0cc7444f7577e53f14068a2cb35431713d6d21a0,1409066540,,1.0,30,2,2,2,1,0.997180399,False,,,,,True,,,,,,,,,,,,,,,,,1190,1648,1356679,neutron,0cc7444f7577e53f14068a2cb35431713d6d21a0,1,1,,"Bug #1356679 in neutron: ""Neutron is checking stricter policies than an operator would expect""","I'm trying to set a custom policy.json for Neutron based on new roles I have defined.
In this task, I changed the ""default"" policy from ""rule: admin_or_owner"" to ""rule:admin_only"". After that, a bunch of operations stopped working, including, for instance, a regular user deleting a network or a router of his/her own project. Even with the policy for ""delete_network"" unchanged -- rule:admin_or_owner --, only the admin could delete a network.
I put a print statement in neutron.openstack.common.policy.check method to investigate what was happening. On the following lines you can compare the debug message in the logs with the actual content of the ""rule"" parameter passed to ""check"".
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_network'
(((rule:delete_network and rule:delete_network:provider:physical_network) and rule:delete_network:provider:network_type) and rule:delete_network:provider:segmentation_id)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_port'
(((((((rule:delete_port and rule:delete_port:binding:host_id) and rule:delete_port:allowed_address_pairs) and rule:delete_port:binding:vif_details) and rule:delete_port:binding:vif_ty
pe) and rule:delete_port:mac_address) and rule:delete_port:binding:profile) and rule:delete_port:fixed_ips)
- - -
DEBUG neutron.policy [...] Failed policy check for 'delete_router'
(rule:delete_router and rule:delete_router:distributed)
- - -
DEBUG neutron.policy [...] Failed policy check for 'update_subnet'
(rule:update_subnet and rule:update_subnet:shared)
- in this case, there is no ""update_subnet:shared"" rule, but there is a ""subnets:shared:write"" rule (which doesn't seem to be used).
- - -
These are the tests I've implemented that got broken after changing the default rule. The update tests simply try to rename the resource.
test_delete_network_of_own_project
test_delete_port_own_project
test_add_router_interface_to_router_of_own_project*
test_delete_router_of_own_project
test_remove_router_interface_from_router_of_own_project*
test_update_router_of_own_project
test_update_shared_subnet_of_own_project
* these tests got broken because of this bug: https://bugs.launchpad.net/neutron/+bug/1356678.","Add logging for enforced policy rules

There are a lot of policy rules which should not necessarily
be explicitly specified in policy.json to be checked while enforcement.
There should be a way for an operator to know which policy rules are
actually being enforced for each action.

Added a unit test.

Change-Id: I261d3e230eced9ea514b35cc3f5f8be04f84c751
Closes-Bug: #1356679"
100,0d111c97334c28444708e623a3fde912bc7c25d7,1384368648,,1.0,2,49,5,4,1,0.798169541,True,8.0,7235579.0,77.0,33.0,False,26.0,71881.0,35.0,9.0,42.0,1343.0,1351.0,42.0,1218.0,1226.0,37.0,461.0,468.0,0.07037037,0.855555556,0.868518519,293,700,1269505,neutron,0d111c97334c28444708e623a3fde912bc7c25d7,0,0,delete code,"Bug #1269505 in neutron: ""Remove release_lease from DhcpBase""",https://review.openstack.org/#/c/56263 removes the need to explicitly call release_lease on this class.  It should be removed.,"Remove release_lease from the DHCP driver interface

Neither the midonet nor the linux dhcp driver needs a call to
release_lease.  Removing this from the driver API simplifies more code
in the DHCP agent.

Change-Id: Ib144b2bf3720b1b999205ace1a7d1ffe5ef3b167
Closes-Bug: #1269505"
101,0d131ff0e9964cb6a65f64809270f9d597c2d5d1,1383304827,,1.0,46,4,3,3,1,0.742409527,True,4.0,1845202.0,59.0,19.0,False,27.0,355622.6667,43.0,4.0,145.0,319.0,425.0,145.0,288.0,394.0,104.0,172.0,242.0,0.203094778,0.334622824,0.470019342,42,370,1246737,neutron,0d131ff0e9964cb6a65f64809270f9d597c2d5d1,1,1,,"Bug #1246737 in neutron: ""ML2 plugin deletes port even if associated with multiple subnets on subnet deletion""","On subnet deletion ml2 plugin deletes all the ports associated with this subnet and does not check if a port is associated with other subnets.
Steps to reproduce:
1) create a network with two subnets
2) create dhcp port for the network, port is associated with both subnets
3) delete one of the subnets
4) dhcp port is getting deleted
Though new dhcp port is created shortly I think it's not ok to delete existing dhcp port.","ML2 plugin should not delete ports on subnet deletion

On subnet deletion ports are deleted asynchronously by dhcp agent
so plugin doesn't need to delete them itself.
Instead Ml2Plugin.delete_subnet() method should call update_port()
for each allocation to remove the IP from the port and call the MechanismDrivers.
The patch also adds subnets test suite from test_db_plugin to test_ml2_plugin.

Closes-Bug: #1246737
Change-Id: I7cf0461e9a3cfec4921e2de41fb1ab3fc119fddc"
102,0d447f6edd406547e828c592823ac6624e56e91f,1391099311,,1.0,1,1,1,1,1,0.0,True,2.0,611252.0,12.0,4.0,False,67.0,309023.0,154.0,3.0,53.0,2489.0,2501.0,52.0,2095.0,2107.0,48.0,2386.0,2394.0,0.006789525,0.330746848,0.331855342,345,756,1274611,nova,0d447f6edd406547e828c592823ac6624e56e91f,1,1,Bug with the configuration,"Bug #1274611 in OpenStack Compute (nova): ""nova-network bridge setup fails if the interface address has 'dynamic' flag""","While setting the bridge up, if the network interface has a dynamic address, the 'dynamic' flag will be displayed in the ""ip addr show"" command:
[fedora@dev1 devstack]$ ip addr show dev eth0 scope global
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000
    link/ether 52:54:00:00:00:01 brd ff:ff:ff:ff:ff:ff
    inet 192.168.122.2/24 brd 192.168.122.255 scope global dynamic eth0
       valid_lft 2225sec preferred_lft 2225sec
When latter executing ""ip addr del"" with the IPv4 details, the 'dynamic' flag is not accepted, causes the command to crash and leaves the bridge half configured.","Ignore 'dynamic' addr flag on bridge configuration

When setting the bridge up, if that flag is present but not
ignored, the delete command will fail and may leave the
interface and the bridge in an inconsistent state.

Closes-Bug: #1274611
Related-Bug: #1077066
Change-Id: Iff65d61d724d0de5d8173ac37f7f748aa23e5d8b"
103,0d781d41fef329294891fdda0c1dac4add5ce7c3,1403729793,,1.0,144,3,3,2,1,0.362474341,False,,,,,True,,,,,,,,,,,,,,,,,1004,1446,1334024,nova,0d781d41fef329294891fdda0c1dac4add5ce7c3,1,1,"""The suffix .rescue is missed here and hence, original disk.config is overwritten.”","Bug #1334024 in OpenStack Compute (nova): ""Nova rescue fails for libvirt driver with config drive""","I am using config drive to boot VMs. In icehouse, I observed that nova rescue fails and leaves the VM in SHUTOFF state.
Short error log:
instances/270e299b-90b2-46d5-bf9a-e7f6efe3742e/disk.config.rescue': No such file or directory
Difference in Havana and Icehouse code path:
# Havana
# Config drive
 if configdrive.required_by(instance):
      LOG.info(_('Using config drive'), instance=instance)
      extra_md = {}
      if admin_pass:
          extra_md['admin_pass'] = admin_pass
      for f in ('user_name', 'project_name'):
         if hasattr(context, f):
             extra_md[f] = getattr(context, f, None)
         inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
         with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
             configdrive_path = basepath(fname='disk.config')
             LOG.info(_('Creating config drive at %(path)s'),
                      {'path': configdrive_path}, instance=instance)
def basepath(fname='', suffix=suffix): << Adds suffix .rescue to disk.config.
    return os.path.join(libvirt_utils.get_instance_path(instance),
                                fname + suffix)
# Icehouse:
# Config drive
if configdrive.required_by(instance):
    LOG.info(_('Using config drive'), instance=instance)
    extra_md = {}
    if admin_pass:
        extra_md['admin_pass'] = admin_pass
    for f in ('user_name', 'project_name'):
        if hasattr(context, f):
            extra_md[f] = getattr(context, f, None)
        inst_md = instance_metadata.InstanceMetadata(instance,
                content=files, extra_md=extra_md, network_info=network_info)
        with configdrive.ConfigDriveBuilder(instance_md=inst_md) as cdb:
            configdrive_path = self._get_disk_config_path(instance)
            LOG.info(_('Creating config drive at %(path)s'),
                         {'path': configdrive_path}, instance=instance)
@staticmethod
def _get_disk_config_path(instance):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                        'disk.config')
The suffix .rescue is missed here and hence, original disk.config is overwritten.
Following change fixed the issue for me:
configdrive_path = self._get_disk_config_path(instance, suffix)
@staticmethod
def _get_disk_config_path(instance, suffix=''):
    return os.path.join(libvirt_utils.get_instance_path(instance),
                            'disk.config' + suffix)","Libvirt: Added suffix to configdrive_path required for rescue

It was observed that during nova rescue, nova failed to create
a rescue disk for disk.config. The reason being that .rescue
suffix was missing from the configdrive path. It was working
till Havana but the functionality broke in Icehouse. This commit
fixes the suffix problem.

Unittests have been added to verify following scenarios:
    1. Make sure that .rescue disks are created when not using
        config drive
    2. Make sure that .rescue disks are created when using
        config drive

Closes-Bug: #1334024

Change-Id: I87449ffddd047cb84b7b881757ea4c29927b95da"
104,0d8911115e1b722da2f1e92f444e53b22223ee32,1408913942,1.0,1.0,30,9,2,2,1,0.821280942,False,,,,,True,,,,,,,,,,,,,,,,,1196,1654,1357084,neutron,0d8911115e1b722da2f1e92f444e53b22223ee32,1,1,,"Bug #1357084 in neutron: ""IPv6 slaac is broken when subnet is less than /64""","SLAAC and DHCPv6 stateless work only with subnets with mask /64 and more (/63, /62) because EUI-64 calculated IP takes 8 octets.
If subnet mask is /65, /66, .., /128 SLAAC/DHCP stateless should be disabled.
API call for creating subnet with SLAAC/DHCP stateless and mask more than /64 should fail.
Example:
let's create net and subnet with mask /96:
$ neutron net-create 14
$ neutron subnet-create 14 --ipv6-ra-mode=slaac --ipv6-address-mode=slaac --ip-version=6 2003::/96
Created a new subnet:
...
| allocation_pools  | {""start"": ""2003::2"", ""end"": ""2003::ffff:fffe""} |
| cidr              | 2003::/96                                      |
....                                   |
| gateway_ip        | 2003::1                                        |
...
| ipv6_address_mode | slaac                                          |
| ipv6_ra_mode      | slaac                                          |
...
Let's create port in this network:
$  neutron port-create 14 --mac-address=11:22:33:44:55:66
Created a new port:
...
| fixed_ips             | {""subnet_id"": ""1bfe4522-3b71-4e74-bb80-44c853ff868d"", ""ip_address"": ""2003::1322:33ff:fe44:5566""} |
...
| mac_address           | 11:22:33:44:55:66                                                                                |
...
As you see port gets IP 2003::1322:33ff:fe44:5566 which is not from original network 2003::/96.","Raise exception if ipv6 prefix is inappropriate for address mode

Address prefix to use with slaac and stateless ipv6 address modes
should be equal to 64 in order to work properly.
The patch adds corresponding validation and fixes unit tests
accordingly.

Change-Id: I6c344b21a69f85f2885a72377171f70309b26775
Closes-Bug: #1357084"
105,0d91b1a86a211610f78ac5c0df5a0b02b5a2f3a1,1383736854,,1.0,14,3,2,2,1,0.787126586,True,3.0,15644612.0,20.0,9.0,False,9.0,4351297.0,10.0,5.0,14.0,507.0,511.0,14.0,490.0,494.0,10.0,457.0,457.0,0.009632224,0.401050788,0.401050788,71,399,1248815,cinder,0d91b1a86a211610f78ac5c0df5a0b02b5a2f3a1,1,1,Is using 200 OK instead of 400 Bad Request. Bad code,"Bug #1248815 in Cinder: ""Update quotas shouldn't allow bad keys in the request""","When user passes bad keys to update quota api, it returns 200 OK response. It should return 400 Bad Request error for bad keys.
PUT
​http://127.0.0.1:8776/v2/ad5d29fef329473598031ca4080288a7/os-quota-sets/ad5d29fef329473598031ca4080288a7
Request Body
{
""quota_set"": {
""gigabytes"": 5,
""xyz"": 2
}
}
Actual Response: 200 OK
{""quota_set"": {""gigabytes"": 5, ""snapshots"": 1000, ""volumes"": 10}}
Expected Response: 400
{""badRequest"": {""message"": ""Bad key(s) xyz in quota_set"", ""code"": 400}}
Note: Nova doesn't allow bad keys in the os-quota-sets put method.","Do not allow bad keys while updating quota

Raise 400 (bad request) error instead of 200 (ok) if bad keys
are passed to the update quota request

Closes-Bug: #1248815

Change-Id: Iaefaa4961dd3783dfab15f843cbb2dcb12195a7d"
106,0d95e5316a99b78b4f629a29d862548f87671b43,1387778981,,1.0,161,9,3,3,1,0.899729067,True,14.0,10712532.0,76.0,27.0,False,7.0,55715.33333,10.0,2.0,5.0,220.0,220.0,5.0,220.0,220.0,5.0,187.0,187.0,0.005560704,0.174235403,0.174235403,156,558,1258331,glance,0d95e5316a99b78b4f629a29d862548f87671b43,0,0,New scenario. Evolution,"Bug #1258331 in Glance: ""Glance v2: Image property quotas are unforgiving when quota is exceeded""","Glance v2 image property quota enforcement can be unintuitive.
There are a number of reasons an image may have more properties than the image_propery_quota allows. E.g. if the quota is lowered or when it is first created. If this happens then any request to modify an image must result in the image being under the new quota. This means that even if the user is removing quotas they can still get an 413 overlimit from glance if the result would still be over the limit.
This is not a great user experience and is unintuitive. Ideally a user should be able to remove properties or any other action except for adding a property when they are over their quota for a given image.","Allow some property operations when quota exceeded

Currently in Glance v2 if, for some reason, an image has more
properties than the image_propery_quota allows (e.g. the quota was
lowered after the image was created with properties) then any request
to modify or delete existing properties results in a 413 overlimit
error. Ideally a user should be able to remove properties or any other
action except for adding a property when they are over their quota for
a given image.

This commit does this by adding a new member to the quota.ImageProxy
class to ""remember"" what properties were already present in an image
*before* any new property operations are preformed on the image. After
the new property operations are performed the quotas are checked (before
writing the image info to DB) only if any new properties have been
added.

This commit does not use a subclass of ExtraPropertiesProxy to check
property quotas (in the __setitem__ method) because Glance does not
implement the JSON-patch RFC correctly - in Glance all operations in a
patch are applied and the quota checked only after all operations have
been applied (RFC requires that operations be applied sequentially and
fail on the first failure). Therefore it is possible for the quota to be
temporarily exceeded when a patch is being applied and therefore we
cannot check for quotas as they are being added - we have to wait until
all patch operations have been completed.

Also, as per review discussions in IRC, a new file:
    glance/tests/integration/v2/test_property_quota_violations.py
has been added to perform image property quota related tests (because
the functional test framework is slow).

Change-Id: Icf1b46343463791ed3d2f3ce376f11e409e792ff
Closes-bug: #1258331
Author: David Koo <david.koo@huawei.com>"
107,0da18edec2ca213a9d87778a4679afb891d58a28,1383729883,,1.0,1,1,1,1,1,0.0,True,3.0,1164271.0,19.0,7.0,False,3.0,6168159.0,4.0,3.0,147.0,780.0,880.0,147.0,711.0,811.0,106.0,213.0,284.0,0.20458891,0.40917782,0.544933078,61,389,1248222,neutron,0da18edec2ca213a9d87778a4679afb891d58a28,1,1,,"Bug #1248222 in neutron: ""The Loadbalancer agent's binary name as appears in the agents db table is wrong""","Version
=======
Havana on rhel
openstack-neutron-2013.2-3.el6ost
Description
===========
The Loadbalancer agent's binary in the ""agents"" database table differs from the real binary name and service name.
# mysql -u root ovs_neutron
mysql> select * from agents where agent_type like ""Loadbalancer agent"";
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| id                                   | agent_type         | binary                     | topic                       | host                          | admin_state_up | created_at          | started_at          | heartbeat_timestamp | description | configurations                                                                                                                                                                            |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 7f0b2ac3-5478-4a80-9c09-5856956d0160 | Loadbalancer agent | neutron-loadbalancer-agent | lbaas_process_on_host_agent | shtutgmuralegamrey.redhat.com |              1 | 2013-11-05 13:02:45 | 2013-11-05 13:02:45 | 2013-11-05 14:54:09 | NULL        | {""device_driver"": ""neutron.services.loadbalancer.drivers.haproxy.namespace_driver.HaproxyNSDriver"", ""interface_driver"": ""neutron.agent.linux.interface.OVSInterfaceDriver"", ""devices"": 1} |
+--------------------------------------+--------------------+----------------------------+-----------------------------+-------------------------------+----------------+---------------------+---------------------+---------------------+-------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
1 row in set (0.00 sec)
The binary here is ""neutron-loadbalancer-agent"" while the real binary should probably be ""neutron-lbaas-agent"".
# rpm -ql openstack-neutron  | grep neutron-loadbalancer-agent
<empty output>
# rpm -ql openstack-neutron  | grep neutron-lbaas-agent
/etc/rc.d/init.d/neutron-lbaas-agent
/usr/bin/neutron-lbaas-agent
/usr/share/neutron/neutron-lbaas-agent.upstart
# service neutron-lbaas-agent status
neutron-lbaas-agent (pid  2321) is running...
# service neutron-loadbalancer-agent status
neutron-loadbalancer-agent: unrecognized service
# ps -elf | grep neutron-loadbalancer-agent | grep -v grep
<empty output>
# ps -elf | grep neutron-lbaas-agent | grep -v grep
0 S neutron   2321     1  0  80   0 - 70231 ep_pol 15:02 ?        00:01:10 /usr/bin/python /usr/bin/neutron-lbaas-agent --log-file /var/log/neutron/lbaas-agent.log --config-file /usr/share/neutron/neutron-dist.conf --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/lbaas_agent.ini","LBaaS: fix reported binary name of a loadbalancer agent

Loadbalancer agent's actual binary name differs from what it reports
to Neutron server, so need to fix:
neutron-loadbalancer-agent -> neutron-lbaas-agent

Closes-Bug: #1248222
Change-Id: Ic4e05d0258ba8eb48b96c60d69c1dbf9f27731f6"
108,0da4d72ae6aae012f2c65c928bbab013043d09a1,1410439963,,1.0,34,1,2,2,1,0.591672779,False,,,,,True,,,,,,,,,,,,,,,,,779,1206,1306479,nova,0da4d72ae6aae012f2c65c928bbab013043d09a1,1,1,,"Bug #1306479 in OpenStack Compute (nova): ""ec2 api breaks on non-ascii characters""","When you query the ec2 api, and it returns non-ascii data from the database, It throws something like this
2014-04-11 10:05:19.874 ERROR nova.api.ec2 [req-bc73aa28-0faa-437c-8fb1-e61188658c03 user project] Unexpected error raised: 'ascii' codec can't encode character u'\xe4' in position 1: ordinal not in range(128)
Tha cause seems to be the str() calls when calling xml.createTextNode in the /api/ec2/apirequest.py file.
This should porbably be made safe for non-ascii characters.","ensure that we safely encode ec2 utf8 responses

The ec2 response layer assumed that all strings coming back for
results were always ascii, and exploded horribly if they weren't due
to the use of a str() function late in the game. This updates the code
to use safe_encode correctly when the object gets rendered.

This also adds unit tests for the rendering functionality in
ec2/apirequest so that we can actually feel confident that it does the
right things with unicode.

Change-Id: I348a728e3559fdbf0d1bc66616cc278c3370cd29
Closes-Bug: #1306479"
109,0dd0b6b5ee75bd437b2a82f2e666a6c671b91514,1392837715,0.0,1.0,33,4,2,2,1,0.974024864,True,3.0,478302.0,62.0,12.0,False,7.0,2527315.0,9.0,3.0,474.0,1120.0,1344.0,397.0,980.0,1150.0,217.0,640.0,686.0,0.261704682,0.769507803,0.824729892,439,854,1282394,neutron,0dd0b6b5ee75bd437b2a82f2e666a6c671b91514,0,0,No bug . ‘It is so annoying for debugging or log monitoring.’,"Bug #1282394 in neutron: ""Exceptions due to bad user requests should not be logged as TRACE""","When a user requests non-existing resource or a bad parameter, neutron API module logs it as TRACE level (log.exception). It is so annoying for debugging or log monitoring. These kinds of events are usual behaviors and are not errors of neutron-server.
They should be logged as TRACE level. Errors mapped into 4xx HTTP response are categorized into this category.","Lower log level of errors due to user requests to INFO

Errors due to bad client requests (e.g., NotFound, BadRequest)
are logged as exception/trace level and it is annoying
from the point of operators' view.
This commit changes the log level for errors due to
user requests (HTTP 4xx errors) to INFO.

Closes-Bug: #1282394
Change-Id: Ic5646333db88ce856f9366e914cf961890d30501"
110,0dde14c0cd6ffea8ebff715342852ef17a9c0b70,1399341466,,1.0,2,2,1,1,1,0.0,True,2.0,247237.0,18.0,4.0,False,1.0,2406457.0,15.0,6.0,75.0,1738.0,1750.0,75.0,1454.0,1466.0,66.0,1065.0,1076.0,0.055417701,0.88172043,0.890818859,865,1298,1316382,neutron,0dde14c0cd6ffea8ebff715342852ef17a9c0b70,1,0,"""isn't present in the python 2.6 httplib.”","Bug #1316382 in neutron: ""Big Switch: servermanager HTTPS fails on 2.6""",The HTTPS class with certificate validation in the Big Switch server manager module references a source_address attribute that isn't present in the python 2.6 httplib.,"Big Switch: Check source_address attribute exists

Check that the source_address attribute exists on
HTTPSConnection objects before referencing it since
it's not present on python 2.6 deployments.

Closes-Bug: #1316382
Change-Id: Id82e70f124cba73c33737099027b5c61aea713bb"
111,0e429c71f34c5a7603854171c6843a67c61b9127,1409835619,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1267,1727,1365487,cinder,0e429c71f34c5a7603854171c6843a67c61b9127,1,0,“Qemu-img still uses the legacy name 'vpc' for vhd files which causes this confusion”,"Bug #1365487 in Cinder: ""Cinder fails to upload volume to vhd image""","The method copy_volume_to_image from windows.py specifies the wrong volume format, namely 'vpc'. In this case, the upload_volume method from image_utils will attempt to convert the volume to vhd which will result in an error from qemu as it does not recognize the format. Qemu-img still uses the legacy name 'vpc' for vhd files which causes this confusion.
The solution is to explicitly using vhd as a format when uploading volumes.","Fixes Cinder fails to upload volume to vhd image

The method copy_volume_to_image from windows.py specifies the
wrong volume format, namely 'vpc'. In this case, the upload_volume
method from image_utils will attempt to convert the volume to vhd
which will result in an error from qemu as it does not recognize
the format.

This patch fixes this issue by using 'vhd' as a format when
uploading volumes

Change-Id: Ifa675316e19f9a795e103fd56fcdf4e3ed00aab7
Closes-Bug: #1365487"
112,0e44b7b5418c400af7358a5391f350f2f737929e,1399475142,1.0,1.0,59,33,7,6,1,0.749716953,True,1.0,609394.0,18.0,3.0,False,62.0,384967.1429,127.0,3.0,21.0,288.0,296.0,21.0,285.0,293.0,21.0,283.0,291.0,0.018077239,0.233360723,0.239934265,758,1185,1304181,neutron,0e44b7b5418c400af7358a5391f350f2f737929e,1,1,“Git commit c3706fa2 introduced the force_gateway_on_subnet”,"Bug #1304181 in neutron: ""neutron should validate gateway_ip is in subnet""","I don't believe this is actually a valid network configuration:
arosen@arosen-MacBookPro:~/devstack$ neutron subnet-show  be0a602b-ea52-4b13-8003-207be20187da
+------------------+------------------------------------------------+
| Field            | Value                                          |
+------------------+------------------------------------------------+
| allocation_pools | {""start"": ""10.11.12.1"", ""end"": ""10.11.12.254""} |
| cidr             | 10.11.12.0/24                                  |
| dns_nameservers  |                                                |
| enable_dhcp      | True                                           |
| gateway_ip       | 10.0.0.1                                       |
| host_routes      |                                                |
| id               | be0a602b-ea52-4b13-8003-207be20187da           |
| ip_version       | 4                                              |
| name             | private-subnet                                 |
| network_id       | 53ec3eac-9404-41d4-a899-da4f32045abd           |
| tenant_id        | f2d9c1726aa940d3bd5a8ee529ea2480               |
+------------------+------------------------------------------------+","Make sure that gateway is in CIDR range by default

Git commit c3706fa2 introduced the force_gateway_on_subnet
option that verified that the defined gateway is in the CIDR
range of a newly created or updated subnet. However, the default
value was False for backwards compatability reasons. The default
will change to True and the option will be marked as deprecated.

For IPv6, the gateway must be in the CIDR only if the gateway
is not a link local address.

DocImpact
Change-Id: I04fd1caec6da5dceee3f736b3f91f2468150ba2a
Closes-Bug: #1304181"
113,0e44ba4c273f41195d0361d3c48df76e2e5add76,1393899483,1.0,1.0,51,3,3,2,1,0.860659204,True,2.0,18575.0,14.0,8.0,False,22.0,320246.0,56.0,2.0,2.0,855.0,856.0,2.0,772.0,773.0,2.0,815.0,816.0,0.002008032,0.546184739,0.546854083,516,937,1287476,cinder,0e44ba4c273f41195d0361d3c48df76e2e5add76,0,0,Add initiator_target_map for IBM Storwize/SVC,"Bug #1287476 in Cinder: ""IBM Storwize/SVC driver does not support initiator_target_map""",Fibre Channel zoning code requires that the storage drivers return an initiator_target_map from initialize_connection and terminate_connection.,"Add initiator_target_map for IBM Storwize/SVC

Basic support to return a map of all initiator to all targets.
Updated unit test.

Change-Id: I886dd63021130003b1b2dd391542fb2b41b9d51f
Closes-Bug: #1287476"
114,0ed493b829e8b8844b86a80da3cd70c69b1fe23a,1383495937,,1.0,10,4,2,2,1,0.749595257,True,7.0,2819972.0,34.0,18.0,False,34.0,2332277.0,73.0,8.0,67.0,3910.0,3938.0,67.0,3251.0,3279.0,50.0,2831.0,2842.0,0.007834101,0.435023041,0.43671275,50,378,1247610,nova,0ed493b829e8b8844b86a80da3cd70c69b1fe23a,1,1,Bad status,"Bug #1247610 in OpenStack Compute (nova): ""resource_tracker  didn't report resize statue when resize a shutoff instance""","We can resize a shutoff instance , but resouce_tracker doesn't include this case when judge if an instance is in resize state.
Will show a warning like
""2013-10-31 16:35:18.969 18483 WARNING nova.compute.resource_tracker [-] [instance: 813e5a44-41ba-4ee7-8b7b-442d3fc017a7] Instance not resizing, skipping migration.","Let resource_tracker report right migration status

A shutoff instance can be resized, but resource_tracker only think
active instance is in migration state. This patch let it report
right migration status.

Closes-Bug: #1247610

Change-Id: Ic6ce9c7d8fdecaf812831eb302186133f0ee71b1"
115,0edfa2eaab08a0756bf3ec7818cdc9a8a379ba87,1403258183,,1.0,1,9,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,747,1174,1302670,cinder,0edfa2eaab08a0756bf3ec7818cdc9a8a379ba87,0,0,Bug in test,"Bug #1302670 in Cinder: ""StorwizeSVCDriverTestCase.test_storwize_vdisk_copy_ops fails if there is a context switch to another green thread""","StorwizeSVCDriverTestCase.test_storwize_vdisk_copy_ops test case requires that there will be no context switches between green threads while it's being run.
This is achieved by monkey patching greenthread.sleep() function in setUp():
     self.sleeppatch = mock.patch('eventlet.greenthread.sleep')
     self.sleeppatch.start()
This seems to be a workaround in order to make the test pass, while it's hiding the actual reason of the problem. Any code causing the thread context switch in some other way will break the test (e.g. syncing the latest oslo.db code from oslo.incubator breaks this test, because it uses time.sleep() instead of eventlet.greenthread.sleep(), and the former is not monkey patched in the test).
Monkey patching of greenthread.sleep() in order to prevent thread context switches doesn't seem to be a good solution anyway as  it is an assumption which is not true when Cinder is run in production.","test_storwize_vdisk_copy_ops fails if green thread context switch

There is loopingcall(green thread)will cause _rm_vdisk_copy_op() run
twice.On the second time, the exception KeyError is not caught.

Closes-Bug: #1302670

Change-Id: I788f07c33f76693a13f0d581e687a32c6736a394"
116,0efbcf4e168d030907adbadd6ab7fccf766f4068,1411051066,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1477,1214406,1214406,nova,0efbcf4e168d030907adbadd6ab7fccf766f4068,1,1, ,"Bug #1214406 in OpenStack Compute (nova): ""nova.virt.block_device.get_swap should have better safeguards""","As per markmc's comments on https://review.openstack.org/#/c/39086/24 - the function is used to get the swap out of the list context as the block_device_info data structure (used internally by the virt drivers) needs 'swap' field to be either a single dict or none. However if passed something that is not an obvious list of swap looking things - the function will happily reutrn the passed list.
More safe and correct behaviour would be to return None (or raise).","Return None from get_swap() if input is not swap

If a non-swap parameter is passed to get_swap(), return None,
rather than returning the parameter itself.

Change-Id: Icfb0eb0e4d721522160d0b7a4fd9b98c1871a10a
Closes-bug: #1214406"
117,0f07f8546fda9732a7e3597a2de78156f1fb5a34,1400164668,,1.0,136,107,8,2,1,0.766324294,True,13.0,2523566.0,120.0,36.0,False,16.0,7115875.0,16.0,2.0,4.0,1304.0,1307.0,4.0,1302.0,1305.0,2.0,1262.0,1263.0,0.000375657,0.158151766,0.158276985,94,492,1255532,nova,0f07f8546fda9732a7e3597a2de78156f1fb5a34,0,0,"Is a test bug, but add some commits of other bugs :/","Bug #1255532 in OpenStack Compute (nova): ""Import of unexisting openstack.common.test.py file ""","In keystone/openstack/common/db/sqlalchemy/test_migrations.py, line 30 imports `test` what is not exists. Oslo project contains  appropriate file. Solution ­— synchronize keystone.openstack.common with corresponding file in Oslo.
The bug reproduces under nosetests.","Sync common db and db/sqlalchemy

To use db/sqlalchemy/test_base.py we need the test.py file, which does
not synced to Nova. Latest common db.sqlalchemy uses oslotest. The problem with
unexisting common/test.py is solved implicitly with synchronization of common
code.

Contains oslo-incubator commits:

54f7e7f Prevent races in opportunistic db test cases
8a0f581 Use oslotest instead of common test module
4a591ea Start ping listener also for postgresql
f0e50ed Add a warning to not use get_table for working with ForeignKeys
2fd457b Ignore migrate versioning tables in utf8 sanity check
9fed4ed Fix Keystone doc build errors with SQLAlchemy 0.9
f7705f3 Make table utf-8 charset checking be optional for DB migration
5b7e61c Dispose db connections pool on disconnect
295fcd9 Do not use the 'extend' method on a dict_items object
d1988b9 Set sql_mode callback on connect instead of checkout
a1a8280 Fix excessive logging from db.sqlalchemy.session
dc2d829 Add lockutils fixture to OpportunisticTestCase
d10f871 Adapt DB provisioning code for CI requirements
5920bed Make db utils importable without migrate
9933bdd Get mysql_sql_mode parameter from config
96a2217 Prevent incorrect usage of _wrap_db_error()
6cab37c Python3: define a __next__() method for ModelBase
20a7510 Add from_config() method to EngineFacade
fea119e Drop special case for MySQL traditional mode, update unit tests
a584166 Make TRADITIONAL the default SQL mode
5b9e9f4 Fix doc build errors in db.sqlalchemy

Closes-Bug: #1255532
Change-Id: If26b648dee95abc5aa0bf80d3111265b00141b6d"
118,0f28fbef8bedeafca0bf488b84f783568fefc960,1401277731,,1.0,20,2,2,2,1,0.684038436,False,,,,,True,,,,,,,,,,,,,,,,,915,1351,1322702,nova,0f28fbef8bedeafca0bf488b84f783568fefc960,1,0,"“This is really a bug in libvirt, not openstack”, “This patch fixes a bug in libvirt driver get_host_capabilities where some features can be duplicated”","Bug #1322702 in OpenStack Compute (nova): ""libvirt get_host_capabilities() duplicates features""","get_host_capabilities() in libvirt driver seems to have a bug that will result in duplicated features.
def get_host_capabilities(self):
        """"""Returns an instance of config.LibvirtConfigCaps representing
           the capabilities of the host.
        """"""
        if not self._caps:
            xmlstr = self._conn.getCapabilities()
            self._caps = vconfig.LibvirtConfigCaps()
            self._caps.parse_str(xmlstr)
            if hasattr(libvirt, 'VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES'):
                try:
                    features = self._conn.baselineCPU(
                        [self._caps.host.cpu.to_xml()],
                        libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
                    # FIXME(wangpan): the return value of baselineCPU should be
                    #                 None or xml string, but libvirt has a bug
                    #                 of it from 1.1.2 which is fixed in 1.2.0,
                    #                 this -1 checking should be removed later.
                    if features and features != -1:
                        self._caps.host.cpu.parse_str(features)
                except libvirt.libvirtError as ex:
                    error_code = ex.get_error_code()
                    if error_code == libvirt.VIR_ERR_NO_SUPPORT:
                        LOG.warn(_LW(""URI %(uri)s does not support full set""
                                     "" of host capabilities: "" ""%(error)s""),
                                     {'uri': self.uri(), 'error': ex})
                    else:
                        raise
        return self._caps
The _caps.parse_str() is called in sequence for both capabilites and expand features. Since capabilities will have certain features in a VM, and these will be repeated again in the expand features, the _caps.host.cpu.features will end up with duplicated features. This will cause cpu compare to fail later.
(nova)root@overcloud-novacompute0-un6ckrnp5tzl:~# python
Python 2.7.6 (default, Mar 22 2014, 22:59:38)
[GCC 4.8.2] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> import libvirt
>>> conn = libvirt.open(""qemu:///system"")
>>> from nova.virt.libvirt import config as vconfig
>>> caps = vconfig.LibvirtConfigCaps()
>>> xmlstr = conn.getCapabilities()
>>> caps.parse_str(xmlstr)
>>> features = conn.baselineCPU([caps.host.cpu.to_xml()], libvirt.VIR_CONNECT_BASELINE_CPU_EXPAND_FEATURES)
>>> caps.host.cpu.parse_str(features)
>>> for f in caps.host.cpu.features:
...     print f.name
...
hypervisor
popcnt
hypervisor
popcnt
pni
sse2
sse
fxsr
mmx
pat
cmov
pge
sep
apic
cx8
mce
pae
msr
tsc
pse
de
fpu
>>>","remove cpu feature duplications in libvirt

This patch fixes a bug in libvirt driver get_host_capabilities where
some features can be duplicated. In this patch we use the features returned
from baselineCPU as the only set of features.

Change-Id: Ib708e31e5ae14e93b384d2a8933079ca83ca8aaf
Closes-Bug: #1322702"
119,0f2de5a3353c6951a88542f35be80b9ba9a55c0f,1397174687,,1.0,8,1,2,2,1,0.918295834,True,2.0,310445.0,38.0,3.0,False,5.0,6043593.5,8.0,2.0,23.0,1300.0,1305.0,23.0,1120.0,1125.0,22.0,814.0,819.0,0.020318021,0.719964664,0.724381625,738,1165,1302283,neutron,0f2de5a3353c6951a88542f35be80b9ba9a55c0f,1,1,,"Bug #1302283 in neutron: ""LBaaS Haproxy occurs error if no member is added""","if VIP (only with session_persistence=HTTP_COOKIE)&Pool without one member added, the Haproxy agent would report config file error like the following:
2014-04-02 10:14:10.632 17283 DEBUG neutron.openstack.common.lockutils [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Semaphore / lock released ""deploy_instance"" inner /opt/stack/neutron/neutron/openstack/common/lockutils.py:252
2014-04-02 10:14:10.633 17283 ERROR neutron.services.loadbalancer.agent.agent_manager [req-18b873c4-abd7-4483-824f-f553c29ae549 None] Unable to deploy instance for pool: a61dd6b6-8dbe-4576-b213-f8632892a58c
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Traceback (most recent call last):
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/agent/agent_manager.py"", line 180, in _reload_pool
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self.device_drivers[driver_name].deploy_instance(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     return f(*args, **kwargs)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 280, in deploy_instance
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self.update(logical_config)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 95, in update
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     self._spawn(logical_config, extra_args)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 110, in _spawn
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     ns.netns.execute(cmd)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 466, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     check_exit_code=check_exit_code)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager   File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 76, in execute
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager     raise RuntimeError(m)
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager RuntimeError:
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qlbaas-a61dd6b6-8dbe-4576-b213-f8632892a58c', 'haproxy', '-f', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/conf', '-p', '/opt/stack/data/neutron/lbaas/a61dd6b6-8dbe-4576-b213-f8632892a58c/pid', '-sf', '4524']
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Exit code: 1
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stdout: ''
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager Stderr: '[ALERT] 091/101410 (20752) : config : HTTP proxy a61dd6b6-8dbe-4576-b213-f8632892a58c has a cookie but no server list !\n[ALERT] 091/101410 (20752) : Fatal errors found in configuration.\n'
2014-04-02 10:14:10.633 17283 TRACE neutron.services.loadbalancer.agent.agent_manager
I think one solution: check the member list, if no member exists, Haproxy agent would not deploy the instance or undeploy existed haproxy process.","Fix LBaaS Haproxy occurs error if no member is added

If no member is added and session_persistence.type=HTTP_COOKIE,
haproxy agent would not add cookie persistence option to the backend.
Closes-Bug: #1302283

Change-Id: Ifa2564df924c2555225a749a99c705b3f1caab4a"
120,0f7cfee155c0e3e216b4a1425ec1fbdde6eeb296,1401228497,1.0,1.0,22,4,2,2,1,0.930586129,False,,,,,True,,,,,,,,,,,,,,,,,921,1357,1323322,neutron,0f7cfee155c0e3e216b4a1425ec1fbdde6eeb296,1,1,Feature “Disallow regular user to update firewall's shared attribute”,"Bug #1323322 in neutron: ""fwaas:shrared attribute of tenant's firewall should not have the option to update""","DESCRIPTION:
Shared attribute is not shown when creating firewall.
I understand that, admin can only create shared firewall since it will affect other tenants also
In that case, creating shared firewall is prohibited correctly however I am able to update the firewall from tenant by shared = true
This should not be allowed
Steps to Reproduce:
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared true
Invalid values_specs true
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared false
Invalid values_specs false
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2 --shared
{""NeutronError"": {""message"": ""Policy doesn't allow create_firewall to be performed."", ""type"": ""PolicyNotAuthorized"", ""detail"": """"}}
root@IGA-OSC:~# fwc 7436f673-e1e8-4acf-b8a2-38e70a020105 --name f2
Created a new firewall:
+--------------------+--------------------------------------+
| Field              | Value                                |
+--------------------+--------------------------------------+
| admin_state_up     | True                                 |
| description        |                                      |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id                 | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name               | f2                                   |
| status             | PENDING_CREATE                       |
| tenant_id          | bf4fbb928d574829855ebfd9e5d0e58c     |
+--------------------+--------------------------------------+
root@IGA-OSC:~# fwu f2 --shared true --------------------------------------------------------------------> able to update
Updated firewall: f2
root@IGA-OSC:~# fws f2
+--------------------+--------------------------------------+
| Field              | Value                                |
+--------------------+--------------------------------------+
| admin_state_up     | True                                 |
| description        |                                      |
| firewall_policy_id | 7436f673-e1e8-4acf-b8a2-38e70a020105 |
| id                 | 476dfe06-07f0-404b-8e92-aae953257af9 |
| name               | f2                                   |
| status             | ACTIVE                               |
| tenant_id          | bf4fbb928d574829855ebfd9e5d0e58c     |
+--------------------+--------------------------------------+
Actual Results:
Able to update the shared attribute of tenant's firewall
Expected Results:
tenant's firewall should not be able to update the shared attribute","Disallow regular user to update firewall's shared attribute

Shared firewalls should only be operable by  admins.
Currently only admin can provide shared attribute at firewall creation,
so update_firewall should be consistent with that as well.

Change-Id: I093743514637824207b375d724404d51f778d012
Closes-Bug: #1323322"
121,0f877f2594d415513856af3c528275fce2228ac1,1400601512,,1.0,12,9,2,2,1,0.453716339,False,,,,,True,,,,,,,,,,,,,,,,,901,1337,1321352,neutron,0f877f2594d415513856af3c528275fce2228ac1,1,1,,"Bug #1321352 in neutron: ""Nova notification introduces a hard dependency on novaclient""",The nova notification patch introduces a hard dependency on novaclient when it is a runtime-configurable dependency. The import from novaclient should be conditional on the appropriate nova notification options being enabled in the config.,"Remove hard dependency on novaclient

The nova notification patch introduces a hard dependency on
novaclient when it is a runtime-configurable dependency. The
import from novaclient should be conditional on the
appropriate nova notification options being enabled in the
config.

Change-Id: I2ef4bfa4d53afc7e8c800ad8e2a8737e117af238
Closes-Bug: #1321352"
122,0fcbb7b286888d642d329db1299eb9f3708d1f50,1382430745,2.0,1.0,20,5,2,2,1,0.998845536,True,9.0,7021794.0,57.0,18.0,False,128.0,235223.0,347.0,2.0,7.0,1745.0,1750.0,7.0,1511.0,1516.0,7.0,1649.0,1654.0,0.001249414,0.257691707,0.258472591,8,320,1243083,nova,0fcbb7b286888d642d329db1299eb9f3708d1f50,1,1,,"Bug #1243083 in OpenStack Compute (nova): ""lxc container rootfs is attached to more than one device when rebooted or resized ""","Operation details are as following:
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova reboot lxc1 --hard
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| ID                                   | Name | Status      | Task State     | Power State | Networks         |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | HARD_REBOOT | rebooting_hard | Running     | private=10.0.0.2 |
+--------------------------------------+------+-------------+----------------+-------------+------------------+
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize lxc1 m1.lxc
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------------+------------+-------------+------------------+
| ID                                   | Name | Status        | Task State | Power State | Networks         |
+--------------------------------------+------+---------------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | VERIFY_RESIZE | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk)
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)
ubuntu@lxc-gq:~$ nova resize-confirm lxc1
ubuntu@lxc-gq:~$ nova list
+--------------------------------------+------+---------+------------+-------------+------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks         |
+--------------------------------------+------+---------+------------+-------------+------------------+
| b91adccc-408a-4bac-8835-9843afb904b9 | lxc1 | ACTIVE  | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+---------+------------+-------------+------------------+
ubuntu@lxc-gq:~$ vir list
Id    Name                           State
----------------------------------------------------
9312  instance-00000052              running
ubuntu@lxc-gq:~$ sudo losetup -a
/dev/loop0: [fd01]:136419 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9_resize/disk (deleted))
/dev/loop1: [fd01]:134374 (/opt/stack/data/nova/instances/b91adccc-408a-4bac-8835-9843afb904b9/disk)","Fix lxc rootfs attached two devices in some action

During resizing/rebooting a lxc container, create_domain action
will be called and lxc rootfs will be attached to device again.
So before this action, We need to teardown the container to
make sure lxc_root_device is detached when container is destroyed.

Closes-bug: #1243083
Related-bug: #1208387

Change-Id: Idd1862c7f212dc253b6273b07b020c86a94e658e"
123,0fe3001d639df81ae4e5f77bcfc32bc1bffb014e,1385541936,0.0,1.0,72,1,2,2,1,0.675863572,True,6.0,1438814.0,46.0,10.0,False,139.0,54021.0,407.0,2.0,8.0,1654.0,1656.0,8.0,1359.0,1361.0,8.0,1600.0,1602.0,0.001339884,0.238350454,0.238648206,92,490,1255449,nova,0fe3001d639df81ae4e5f77bcfc32bc1bffb014e,0,0,Add a feature. ‘should use’,"Bug #1255449 in OpenStack Compute (nova): ""Libvirt Driver - Custom disk_bus setting is being lost on instance power on""","Currently, custom disk_bus configuration will be defaulted to virtio when
a user will try to power off + power on or hard reboot his instance.
It is happening since hard_reboot() doesn't consider the image_meta  when constructing
the disk_info","libvirt: Custom disk_bus setting is being lost on hard_reboot

Currently, a non-default disk_bus setting of and image will not be used,
when powering on an instance (or _hard_reboot).

hard_reboot() should use image_meta, if it exist, when constructing
the disk_info

Closes-Bug: #1255449

Change-Id: I477f69c47515ad22165421b4d23ac865c7869873"
124,0ff26160f9a7ae2a4df33c4aaf0dbb59d76ac8d5,1378895430,,1.0,47,3,2,2,1,0.989587521,True,3.0,1145576.0,27.0,9.0,False,4.0,1738110.0,6.0,4.0,360.0,2323.0,2457.0,359.0,2103.0,2237.0,356.0,2103.0,2236.0,0.059490085,0.350608232,0.372771205,1539,1223803,1223803,nova,0ff26160f9a7ae2a4df33c4aaf0dbb59d76ac8d5,1,0,"“V3 API is broken as the JSON format was changed, but the corresponding XML serialisation/deserialisation wasn’t.”, software evolution","Bug #1223803 in OpenStack Compute (nova): ""V3 API server metada xml serialize/deserialize is broken""",XML serialization and deserialization of updating a server metadata through the V3 API is broken. As is deserialization for retrieving a single metadata item,"Fix V3 API server metadata XML serialization

The XML serialisation and deserialization when updating
a single server metadata item or retrieving a single server metadata
item through the V3 API is broken as the JSON format was changed
but the corresponding XML serialisation/deserialisation wasn't.

This changeset fixes this bug and adds testcases for the fixes.

Change-Id: I1bd2a729744387337f86a37dc2ff89606edce424
Closes-Bug: #1223803"
125,0ff8536c0d9b95172c42b1a01d292df58ed2e573,1400482738,,1.0,27,27,9,7,1,0.878192078,False,,,,,True,,,,,,,,,,,,,,,,,894,1330,1320774,neutron,0ff8536c0d9b95172c42b1a01d292df58ed2e573,0,0,Bug in test,"Bug #1320774 in neutron: ""Non-existent assertion methods being called on mocks""","There are various places in the unit tests where non-existent ""assert"" methods are called on mock objects.
e.g. assert_called_once(), assert_called_twice(), assert_called_one_with(), assert_called_twice()
To reproduce, put the mock.py file in the base neutron directory and edit the __getattr__ methods to check if the name begins with 'assert' and raise an exception if true.","Fix non-existent 'assert' calls to mocks

Fixes calls to non-existent 'assert' methods
called on mock objects.

Closes-Bug: #1320774
Change-Id: I7eab28fc9b23cbb39d5507ff3e8004a493c90bed"
126,0ff9373de6e11d7040b6b289cb3239a9ee9a924d,1379154413,,1.0,3,3,1,1,1,0.0,True,1.0,167458.0,7.0,3.0,False,1.0,6259627.0,1.0,3.0,120.0,1595.0,1617.0,120.0,1480.0,1502.0,61.0,280.0,289.0,0.185074627,0.83880597,0.865671642,1527,1221726,1221726,neutron,0ff9373de6e11d7040b6b289cb3239a9ee9a924d,0,0,Test files,"Bug #1221726 in neutron: ""Lbaas tests can't be run alone by tox""","When running 'tox -epy27 services.loadbalancer' the following failures are encountered:
Traceback (most recent call last):
  File ""/home/eugene/quantum/neutron/tests/unit/services/loadbalancer/drivers/haproxy/test_agent.py"", line 32, in setUp
    cfg.CONF.register_opts(agent.OPTS)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1540, in __inner
    result = f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1673, in register_opts
    self.register_opt(opt, group, clear_cache=False)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1544, in __inner
    return f(self, *args, **kwargs)
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 1662, in register_opt
    if _is_opt_registered(self._opts, opt):
  File ""/home/eugene/quantum/.tox/py27/local/lib/python2.7/site-packages/oslo/config/cfg.py"", line 486, in _is_opt_registered
    raise DuplicateOptError(opt.name)
DuplicateOptError: duplicate option: periodic_interval","Fix haproxy agent unit test to be runnable alone by tox

haproxy agent uses periodic_interval option that can interfere
with the same option declared in neutron/service.py when running
'tox services.loadbalancer'

Change-Id: Ibcc91d603f5f31c7a376e0c735e8bf566198231d
Closes-Bug: #1221726"
127,10004672ad1476c55deaad53684a50358da6f656,1386292437,1.0,1.0,21,28,2,2,1,0.88864667,True,8.0,7815284.0,50.0,17.0,False,31.0,814157.5,43.0,5.0,585.0,3936.0,3936.0,545.0,3390.0,3390.0,177.0,2741.0,2741.0,0.026180321,0.403294602,0.403294602,157,559,1258360,nova,10004672ad1476c55deaad53684a50358da6f656,0,0,removes an unneeded call. Refactoring,"Bug #1258360 in OpenStack Compute (nova): ""Remove unneeded call to conductor in network interface""",Remove unneeded call to conductor in network interface,"Remove unneeded call to conductor in network interface

The following patch removes an unneeded call to conductor to retrieve
data already available in instance.

Note: six.text_type() was added and wrapped around info_cache in some of
the tests. This was done because values from the db are in unicode
(text_type). The reason the test changed is because previously we
did not check the type of string from what the conductor was returning.

Closes-bug: #1258360

Change-Id: I45a24af11534911be68fd0964b79fdf726d88de6"
128,100c42ed15be5fc3ca52fb7cba0987e768a18a3f,1394380325,,1.0,5,1,2,2,1,0.918295834,True,1.0,1006213.0,38.0,12.0,False,98.0,650424.5,188.0,8.0,384.0,4997.0,5068.0,383.0,4001.0,4072.0,366.0,3618.0,3678.0,0.048641484,0.479655401,0.487607687,570,991,1290036,nova,100c42ed15be5fc3ca52fb7cba0987e768a18a3f,0,0,typo in commentary and add test,"Bug #1290036 in OpenStack Compute (nova): ""Test refresh_instance_security_rules in nova.tests.virt.test_virt_drivers""","Change https://review.openstack.org/#/c/69600/ adds a new method refresh_instance_security_rules to the nova ComputeDriver interface but it missed adding a test to nova.tests.virt.test_virt_driver to make sure all virt drivers are explicitly handling it.
There is also a typo in the method's docstring.","Fix typo and add test for refresh_instance_security_rules

Change Ia36b0dfb adds virt driver API method
refresh_instance_security_rules but there is a typo in the method's
docstring and it needs to be covered in test_virt_drivers.py.

Closes-Bug: #1290036

Change-Id: Idacffe2087f0bec6290e3422d1e777376b489615"
129,100e962bb989ea3adc8324365dd84f87634691ea,1382038536,,1.0,1,1,1,1,1,0.0,True,2.0,7813.0,7.0,3.0,False,5.0,4531594.0,6.0,2.0,20.0,2074.0,2088.0,20.0,1627.0,1641.0,20.0,1869.0,1883.0,0.003296703,0.293563579,0.295761381,1692,1240292,1240292,nova,100e962bb989ea3adc8324365dd84f87634691ea,1,1, ,"Bug #1240292 in OpenStack Compute (nova): ""vmware: Error in logging causing vcdriver report 0 resources""","I do not have concreate steps to reporduce this but following is the traceback from the logs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 553, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
ValueError: unsupported format character ' ' (0x20) at index 37
Logged from file vim_util.py, line 195
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free ram (MB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:388^[[00m
2013-10-15 16:25:19.677 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: free disk (GB): 0^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: VCPU information unavailable^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:396^[[00m
2013-10-15 16:25:19.678 ^[[00;32mDEBUG nova.compute.resource_tracker [^[[00;36m-^[[00;32m] ^[[01;35m^[[00;32mHypervisor: no assignable PCI devices^[[00m ^[[00;33mfrom (pid=20768) _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:403^[[00m","VMware: Fix ValueError unsupported format character in log message

Fixes a typo with format string in a log message.

Closes-bug: #1240292
Change-Id: Ia86859089351255cb974ed77b0c5f55a5e953b0e"
130,10108f4d2e640a9fc12037ece39a833421d27149,1391109068,,1.0,6,6,2,1,1,1.0,True,7.0,995314.0,54.0,28.0,False,28.0,612831.0,70.0,5.0,0.0,697.0,697.0,0.0,638.0,638.0,0.0,654.0,654.0,0.000728863,0.477405248,0.477405248,51,379,1247743,cinder,10108f4d2e640a9fc12037ece39a833421d27149,1,0,NFS specific,"Bug #1247743 in Cinder: ""cinder failed to create a backup with NFS driver""","Description of problem:
The action cinder backup-create fails when trying to backup a newly created volume.
- The Cinder is using the cinder.volume.drivers.nfs.NfsDriver driver.
- All the other OS components are installed on 1 host and the cinder on a different host.
Version-Release number of selected component (if applicable):
openstack-cinder-2013.2-1.el6ost.noarch
How reproducible:
evevrytime
Steps to Reproduce:
1. create a volume
2. backup the volume
3.
Actual results:
the backup failed
Expected results:
the backup is available
Additional info:
2013-11-04 09:14:04.278 15089 ERROR cinder.openstack.common.rpc.amqp [req-2ae3d45e-40c6-4422-be59-1bba466f086c c7fdf6f628554d56aad363ad501ce412 add3de2deaa445c1a1e71c1721bc8976] Exception during message handling
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 270, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     'fail_reason': unicode(err)})
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/backup/manager.py"", line 263, in create_backup
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     backup_service)
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/nfs.py"", line 352, in backup_volume
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp     raise NotImplementedError()
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp NotImplementedError
2013-11-04 09:14:04.278 15089 TRACE cinder.openstack.common.rpc.amqp","Fixes cinder failed to create/restore a backup with NFS driver

The action cinder backup-create fails when trying to backup a newly
created volume when using cinder.volume.drivers.nfs.NfsDriver. This
patch removes not implemented stubs for backup_volume and
restore_volume. The inherited methods in cinder/volume/driver.py
succeed in creating a backup and in restoring it. After manual
testing of this change backups/restores succeed without the not
implemented stubs. This change also adds the not implemented
stubs to the glusterfs driver until a fix is submitted.

Closes-Bug: #1247743

Change-Id: I5478d00442ce855c91a7f04c5ba2b40733d44902"
131,1027d02977812064b688fe0748c46663c1627cf2,1390592634,,1.0,22,1,4,3,1,0.747270176,True,7.0,1054576.0,101.0,28.0,False,24.0,520626.0,38.0,4.0,25.0,1182.0,1183.0,25.0,1026.0,1027.0,17.0,576.0,577.0,0.024725275,0.792582418,0.793956044,323,732,1272490,neutron,1027d02977812064b688fe0748c46663c1627cf2,1,0, some drivers do not support bulk operations,"Bug #1272490 in neutron: ""ML2 uses bulk support with drivers that don't support it""","ML2 statically sets __native_bulk_support to True. However, some drivers do not support bulk operations so there should be a way to turn off bulk support when using those drivers.","Base ML2 bulk support on the loaded drivers

Changes the ML2 plugin bulk support flag to
be based on the bulk support of the underlying
drivers.

Closes-Bug: #1272490
Change-Id: I28281c9ecc1696b929c7e0125d02a37946948744"
132,103db29c224909cd260f1619a0a4578e172f6649,1381546803,1.0,1.0,10,2,2,2,1,0.918295834,True,8.0,1100000.0,53.0,27.0,False,38.0,3228633.0,64.0,7.0,1.0,3089.0,3090.0,1.0,2766.0,2767.0,1.0,1990.0,1991.0,0.000317864,0.316433566,0.316592498,1679,1239038,1239038,nova,103db29c224909cd260f1619a0a4578e172f6649,1,1, ,"Bug #1239038 in OpenStack Compute (nova): ""migrate server doesn't raise correct exception""","Migrate a non existent server throws HTTPBadRequest exception ,the correct is  InstanceNotFound exception.
live-migrate has the same problem.
The other server actions throw  InstanceNotFound exception.
I think it's a bug.","migrate server doesn't raise correct exception

Currently a BadRequest is raised for the migrate and live-migrate API
operations when the given instance isn't found. This patch changes the
APIs to raise a InstanceNotFound instead to be consistent with the other
admin action APIs which hit the same type of error.

Closes-Bug: #1239038

Change-Id: I28cd367c5885481ea19ace43855b2518a2afd955"
133,105d99d4f52fa849e80829dad3ed12b116820118,1386777986,,1.0,109,69,6,3,1,0.695416652,True,20.0,7120931.0,161.0,69.0,False,133.0,191487.3333,393.0,4.0,27.0,1293.0,1304.0,27.0,1090.0,1101.0,27.0,1224.0,1235.0,0.004099561,0.179355783,0.180966325,1469,1212768,1212768,nova,105d99d4f52fa849e80829dad3ed12b116820118,1,1,“When claims are rejected indicate specific reason” ,"Bug #1212768 in OpenStack Compute (nova): ""When claims are rejected indicate specific reason ""","In resource_tracker.py upon failure, we raise a generic ""ComputeResourcesUnavailable"".  The logging presents good information as to why there was a failure.  We should either provide back data in the ComputeResourcesUnavailble object or raise different exceptions based on what is not available.  This may point to a bigger issue of a broken scheduler.","When a claim is rejected, explain why

Previously, the Claim class offered #test which would return a Boolean
based on availability of memory, disk, CPU and PCI devices.

Now these tests are performed from the constructor and if any test
fails, an exception is thrown. All failure reasons are described in the
exception message. EG:

  ""Insufficient compute resources: Free memory 8192.00 MB <
  requested 16384 MB; Free disk 45.00 GB < requested 50 GB; Free CPUs
  16.00 VCPUs < requested 17 VCPUs.""

Change-Id: I5bf9c81bba747a6c164144e60a92dc6da2f98f4c
Closes-Bug: #1212768"
134,10691597011e8cb462d28c156b39742329bd0339,1401259862,,1.0,3,7,5,3,1,0.967488765,False,,,,,True,,,,,,,,,,,,,,,,,929,1365,1323975,glance,10691597011e8cb462d28c156b39742329bd0339,0,0,Refactoing “remove default=None for config options”,"Bug #1323975 in Glance: ""do not use default=None for config options""",In the cfg module default=None is set as the default value. It's not necessary to set it again when defining config options.,"remove default=None for config options

In the cfg module default=None is set as the default value.

Change-Id: Iad40a7bacef410e54d684cb438d4459f828f70df
Closes-Bug: #1323975"
135,10bf897bf3a10681f5a8769f387599271d57dadb,1401026492,,1.0,8,2,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,918,1354,1323005,neutron,10bf897bf3a10681f5a8769f387599271d57dadb,1,1,,"Bug #1323005 in neutron: ""Metadata workers has a bad default for production""","The Neutron metadata agent supports multiple workers to increase scalability. By default we create a single worker, and the various deployment tools (Should) increase to 4/8/16/x. Any number other than 1 would be a better default.","Increase default metadata_workers, backlog to 4096

While deployment tools might want to change the number of workers
to match the number of cores (Or some fraction of it), any default
other than 1 should be outright better.

Inspired from 19:34:
https://www.youtube.com/watch?v=AF9r_VQrcJ0

DocImpact
Closes-Bug: #1323005
Change-Id: Ie90000183ae67ff391a23ca3213fd23aef5f4dc5"
136,115b6276ac3dd7b3e8c6a42e9d0d80709791b7a2,1393264515,,1.0,1,1,1,1,1,0.0,True,1.0,17605.0,6.0,2.0,False,13.0,299069.0,16.0,2.0,607.0,562.0,913.0,579.0,553.0,879.0,590.0,533.0,870.0,0.404794521,0.365753425,0.596575342,464,879,1284227,cinder,115b6276ac3dd7b3e8c6a42e9d0d80709791b7a2,0,0,tests,"Bug #1284227 in Cinder: ""race condition in test_delete_backup""","The test_delete_backup unit test fails sporadically as such:
2014-02-19 06:55:33.645 | Traceback (most recent call last):
2014-02-19 06:55:33.645 |   File ""cinder/tests/test_backup.py"", line 360, in test_delete_backup
2014-02-19 06:55:33.645 |     self.assertGreater(timeutils.utcnow(), backup.deleted_at)
2014-02-19 06:55:33.645 |   File ""cinder/test.py"", line 280, in assertGreater
2014-02-19 06:55:33.645 |     f(first, second, msg=msg)
2014-02-19 06:55:33.645 |   File ""/usr/lib/python2.7/unittest/case.py"", line 940, in assertGreater
2014-02-19 06:55:33.646 |     self.fail(self._formatMessage(msg, standardMsg))
2014-02-19 06:55:33.646 |   File ""/usr/lib/python2.7/unittest/case.py"", line 408, in fail
2014-02-19 06:55:33.646 |     raise self.failureException(msg)
2014-02-19 06:55:33.646 | AssertionError: datetime.datetime(2014, 2, 19, 6, 53, 14, 473836) not greater than datetime.datetime(2014, 2, 19, 6, 53, 14, 473836)","Fix race in test_delete_backup

Unit test sporadically failed due to timing issue - the deletion
timestamp on the backup was exactly the same as timeutils.utcnow(),
causing the AssertGreater check to fail.  Change to AssertGreaterEqual.

Change-Id: Ia66d0d4bdae3242780a91f0d384cbd17caa8c197
Closes-bug: #1284227"
137,115b8447618073d3383bbd612ebd2fc68c8c5d28,1399411666,,1.0,1,1,1,1,1,0.0,True,2.0,1331805.0,37.0,11.0,False,14.0,21805821.0,28.0,9.0,22.0,1634.0,1634.0,21.0,1307.0,1307.0,4.0,821.0,821.0,0.00310752,0.510876321,0.510876321,706,1132,1300136,cinder,115b8447618073d3383bbd612ebd2fc68c8c5d28,1,1,"""Fix wrong exception reference”","Bug #1300136 in Cinder: ""Failed to delete a never attached volume with lioadm helper""","/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user, I am unable to delete a volume if it was not attached before.
the volume status became: error_deleting
$ cinder create 1
$ cinder delete  daf0a29a-542b-4571-9d4d-5b9b59a7baf3
$ cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |     Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| daf0a29a-542b-4571-9d4d-5b9b59a7baf3 | error_deleting |     None     |  1   |     None    |  false   |             |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
../screen-logs/screen-c-vol.log:
2014-03-31 10:16:43.863 787 ERROR oslo.messaging.rpc.dispatcher [req-8f294457-08a3-4039-9626-ce63a2435bed 46a5bd04d46f49aabcf9edf8214df7a1 d587e4e2b14a41e4b38f62b07a130b3e - - -] Exception during message handling: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 144, in lvo_inner1
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return lvo_inner2(inst, context, volume_id, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     retval = f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 143, in lvo_inner2
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return f(*_args, **_kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 416, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     {'status': 'error_deleting'})
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 400, in delete_volume
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     self.driver.remove_export(context, volume_ref)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     self.target_helper.remove_export(context, volume)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     volume['id'])
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 119, in wrapper
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1344, in volume_get_iscsi_target_num
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher     raise exception.ISCSITargetNotFoundForVolume(volume_id=volume_id)
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher ISCSITargetNotFoundForVolume: No target id found for volume daf0a29a-542b-4571-9d4d-5b9b59a7baf3.
2014-03-31 10:16:43.863 787 TRACE oslo.messaging.rpc.dispatcher
When the lioadm is the iscsi helper the target just exists when the volume attached.
I should be able to delete a volume what was never attached.
The volumes which was attached to a vm before the delete request are deletable.","Fix wrong exception reference

iscsi.py should catch cinder.exception.NotFound instead of
cinder.brick.exception.NotFound.

Change-Id: I77134683712e9706619a7f7fe82caed42ede5887
Closes-Bug: #1300136"
138,115ed3e1cf2cf63bba3a37e61f2a34a186b9f3ad,1395439267,,1.0,38,1,2,2,1,0.477071306,True,13.0,1383614.0,84.0,34.0,False,82.0,279878.0,308.0,6.0,84.0,3830.0,3883.0,45.0,3059.0,3082.0,76.0,2785.0,2832.0,0.010056158,0.363850072,0.369988246,676,1102,1296839,nova,115ed3e1cf2cf63bba3a37e61f2a34a186b9f3ad,1,1,"Attemp to resize, when it doesnt have to","Bug #1296839 in OpenStack Compute (nova): ""xen boot from volume attempts resize""",When attempting a boot-from-volume with a volume size that doesn't match the disk size the compute manager will attempt to resize the volume (which fails). It's fine to press on with the given size.,"Xen: Do not resize root volumes

Boot from volume on Xen wants to resize the volume
when the image size doesn't jive with the volume size.
This patch doesn't attempt the resize.

Closes-Bug: #1296839
Change-Id: Ic03b7997f61f1a22362dafd86664db681e54087e"
139,1167a0f808dd746875be50f31d7ebc56f79d45d6,1385578657,,1.0,38,7,6,6,1,0.902619138,True,5.0,2569115.0,52.0,16.0,False,98.0,165086.0,363.0,5.0,5.0,2637.0,2639.0,4.0,2303.0,2305.0,3.0,2564.0,2564.0,0.000594884,0.381469363,0.381469363,1745,1250206,1250206,nova,1167a0f808dd746875be50f31d7ebc56f79d45d6,1,1,“This seems to be happening because the Instance.destroy() function does not update the instance with the latest state “,"Bug #1250206 in OpenStack Compute (nova): ""Local delete emits delete.end notification without delete_at""","In cases where Nova falls back to doing a local delete at the API (compute host down), it is possible for the delete.end notification to get emitted without deleted_at being set.
Steps to reproduce (with notifications enabled):
1) Create instance and wait for it to go active
2) Take compute host for the instance down
3) Delete instance
4) Observe compute.instance.delete.end notification with a blank deleted_at
This seems to be happening because the Instance.destroy() function does not update the instance with the latest state from the database after calling db.instance_destroy. It is important that the instance is updated with the data from the database as the driver automatically sets the deleted_at value.","Update Instance from database after destroy

In cases where Nova falls back to simply locally deleting an
instance, the Instance object was not updated with the deleted_at
generated by the database implementation. Due to this, the
notification emitted to signify that the delete finished was
getting emitted without a deleted_at value.

This change uses the result from the instance_destroy call to
update the Instance object with the latest state from the database.
Thus, the compute.instance.delete.end notification is emitted
with the proper deleted_at value.

Closes-Bug: #1250206
Change-Id: I55ed37cf89dd6eb8ea901c6f813191a0ce1058e8"
140,119e2a690a64fdfd541010590285f754b3393653,1390244069,0.0,1.0,69,32,3,3,1,0.529867063,True,2.0,10503.0,7.0,4.0,False,154.0,658135.0,654.0,3.0,2002.0,1960.0,3316.0,1631.0,1688.0,2758.0,1904.0,1892.0,3158.0,0.26755618,0.265870787,0.443679775,306,715,1270680,nova,119e2a690a64fdfd541010590285f754b3393653,1,0,Evolution bug: this race may have been introduced by the use of objects,"Bug #1270680 in OpenStack Compute (nova): ""v3 extensions api inherently racey wrt instances""","The pci extension for the v3 API does another instance lookup back to the database for instance objects. The issue being that when you are doing something like a list_* operation on instances, this means that we're making a second trip to the database that's distinct from the first lookup in the request handling. If an instance got deleted between the request and the extension hook running, this will generate a database exception, which turns into an InstanceNot found, and 404s the list operation *if any instance was deleted during the request*
We are managing to hit this quite frequently in tempest with our test_list_servers_by_admin_with_all_tenants (even at only concurency 2)  - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/console.html#_2014-01-20_01_18_11_102
The explosion looks like this - http://logs.openstack.org/80/67480/1/gate/gate-tempest-dsvm-full/24f9aab/logs/screen-n-api.txt.gz?level=INFO#_2014-01-20_00_57_44_352
Logstash picks up these tracebacks really easily. This kind of explosion doesn't always trigger a Tempest failure, because some times this might be in cleanup code, where we protect against 404s (though it probably means we are leaking resources a lot on a normal run).
Logstash query - http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVFJBQ0Ugbm92YS5hcGkub3BlbnN0YWNrXCIgQU5EIG1lc3NhZ2U6XCJJbnN0YW5jZU5vdEZvdW5kOiBJbnN0YW5jZVwiIEFORCBmaWxlbmFtZTpcImxvZ3Mvc2NyZWVuLW4tYXBpLnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzkwMTgzNzk1ODI1fQ==","Join pci_devices when getting all servers in API

The PCI API extension goes back and hits the database for every instance
when getting a list of all instances through the API.  This is causing
problems in testing because it's possible for the instance to be deleted
in between getting the initial list and when the API extension runs.

The API extension really shouldn't be going back to hit the database
anyway, so this patch will resolve that problem.

Change-Id: Id3c8a0b187e399ce2acecd4aaa37ac95e731d46c
Closes-bug: #1270680"
141,11b5487eff312d4d914fbc2f861e18b031421dbe,1386378155,2.0,1.0,239,225,34,18,1,0.837042095,True,7.0,1834340.0,28.0,3.0,False,91.0,283715.0882,478.0,2.0,18.0,192.0,194.0,18.0,192.0,194.0,16.0,160.0,161.0,0.016221374,0.153625954,0.154580153,1789,1253497,1253497,glance,11b5487eff312d4d914fbc2f861e18b031421dbe,0,0,"“should be to remove generate_uuid() from”, “deprecated”","Bug #1253497 in Glance: ""Replace uuidutils.generate_uuid() with str(uuid.uuid4())""","http://lists.openstack.org/pipermail/openstack-dev/2013-November/018980.html
> Hi all,
>
> We had a discussion of the modules that are incubated in Oslo.
>
> https://etherpad.openstack.org/p/icehouse-oslo-status
>
> One of the conclusions we came to was to deprecate/remove uuidutils in
> this cycle.
>
> The first step into this change should be to remove generate_uuid() from
> uuidutils.
>
> The reason is that 1) generating the UUID string seems trivial enough to
> not need a function and 2) string representation of uuid4 is not what we
> want in all projects.
>
> To address this, a patch is now on gerrit.
> https://review.openstack.org/#/c/56152/
>
> Each project should directly use the standard uuid module or implement its
> own helper function to generate uuids if this patch gets in.
>
> Any thoughts on this change? Thanks.
>
Unfortunately it looks like that change went through before I caught up on
email. Shouldn't we have removed its use in the downstream projects (at
least integrated projects) before removing it from Oslo?
Doug","Use uuid instead of uuidutils

Each project should directly use the standard uuid module.
uuidutils will be deprecated/removed in this cycle.

This patch replaces every uuidutils.generate_uuid() with
str(uuid.uuid4()) and uuidutils.is_uuid_like()
with utils.is_uuid_like().

Change-Id: I43642d4f1e137c14134b3d544e367b504b9851ac
Closes-Bug: #1253497"
142,11b9305e3f0dcff7f5f92863cfd3679fd7b6a6a3,1394509094,,1.0,160,2,4,4,1,0.870623345,True,15.0,4385985.0,181.0,53.0,False,35.0,370807.0,46.0,4.0,28.0,920.0,930.0,28.0,918.0,928.0,25.0,904.0,911.0,0.003439608,0.119724831,0.12065088,1501,1219693,1219693,nova,11b9305e3f0dcff7f5f92863cfd3679fd7b6a6a3,1,0,"Database bugs “PostgreSQL gives an error if you try to shove 256+ characters into a 255-character field. MySQL truncates by default, but can be configured to give an error.”","Bug #1219693 in OpenStack Compute (nova): ""create aggregate""","Create aggregate fails when input metadata ""availability_zone"" length >255(models.py value = Column(String(255), nullable=False)),Exception is raised but record in DB is not rolled back.","Check the length of aggregate metadata

PostgreSQL gives an error if you try to shove 256+ characters
into a 255-character field. So we need raise a HTTPBadRequest
explanation if the length of aggregate_metadata.keys or
aggregate_metadata.values is above 255.

Change-Id: I2c778f2237ba5bd2aa8335a0eae80f3aad3e9157
Closes-Bug: #1219693"
143,11c8f8cbca0fc3feed0469346d682225f9301251,1409933134,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1276,1738,1366105,neutron,11c8f8cbca0fc3feed0469346d682225f9301251,1,1,"""misses format for parameter 
“","Bug #1366105 in neutron: ""Debug string in l3 agent misses format for parameter""","neutron/agent/l3_agent.py:
 LOG.debug(""not hosting snat for router: %"", ri.router['id'])","Fixes formatting for debug output in neutron/agent/l3_agent.py

The %s format type was missing.

Change-Id: I4efba513f7b0a6e4a21908bf0b05407e74b524c7
Closes-bug: #1366105"
144,11ca12dd8752a7d8fc13027c21ee572233becb74,1407804138,,1.0,54,24,2,2,1,0.840358672,False,,,,,True,,,,,,,,,,,,,,,,,1171,1628,1355087,neutron,11ca12dd8752a7d8fc13027c21ee572233becb74,1,1,,"Bug #1355087 in neutron: ""when a interface is added after router gateway set,external connectivity using snat fails""","1.create n/w,subnet
2.create a dvr and attach the subnet
3/create external network and attach the router gateway
4.now boot a vm in that subnet
5.ping to external network -successful
6.create a new network,subnet attach it to router created in step 2.
7.boot a vm and ping to external network -fails
8.try to ping to external network using vm created in step 4 -fails
Reason:
=======
when new subnet is added ,all the sg ports inside snat namespace are updated with default gateway of subnet added
say i had subnet 4.4.4.0/24 already attached to router its sg port had ip 4.4.4.2,now when i add new subnet say 5.5.5.0/24 this router
sg port of 4.4.4.0/24 becomes 5.5.5.1 also sg ip of 5.5.5.0/24 also becomes 5.5.5.1 (even though 5.5.5.1 has device owner =network:router_interface_distributed and 5.5.5.2 has device owner as network:router_centralized_snat)","Fix interface add for dvr with gateway

when an interface is added after router gateway set, external
connectivity using snat fails. Instead of just adding the snat port for
the new subnet, method internal_network_added(..) incorrectly re-adds
all the snat ports with wrong cidr.

Change-Id: I7bfe266288670fba0c90990bf350f43ef7829bad
Closes-bug: #1355087"
145,11d8dc7b9eed0e293ac5afab4792d8af7b807f4c,1390841515,,1.0,14,10,1,1,1,0.0,True,3.0,1156145.0,26.0,13.0,False,9.0,51227.0,8.0,7.0,195.0,2997.0,3086.0,184.0,2456.0,2536.0,195.0,2843.0,2932.0,0.027336123,0.39665272,0.409065551,332,742,1273266,nova,11d8dc7b9eed0e293ac5afab4792d8af7b807f4c,1,1,Wrong message log,"Bug #1273266 in OpenStack Compute (nova): ""Error message is malformed when removing a non-existent security group from an instance""","Trying to remove a security group from an instance which is not actually associated with the instance produces the following:
---
$nova remove-secgroup 71069945-5bea-4d53-b6ab-9026bfeebba4 phil
ERROR: [u'Security group %(security_group_name)s not assocaited with the instance %(instance)s', {u'instance': u'71069945-5bea-4d53-b6ab-9026bfeebba4', u'security_group_name': u'phil'}] (HTTP 404) (Request-ID: req-a334b53d-e7cc-482c-9f1f-7bc61b8367e0)
---
The variables are not being populated correctly, and there is a typo:  "" assocaited""","Error message is malformed when removing a sec group from an instance

In the current code the Error message string for a security group which is not
associated with an instance does not get expanded correctly, resulting in users
getting a message:

ERROR: [u'Security group %(security_group_name)s not assocaited with the
instance %(instance)s', {u'instance': u'71069945-5bea-4d53-b6ab-9026bfeebba4',
u'security_group_name': u'phil'}]

In addition fix a minor typo in this message, and clean up a few cases where
error messages are not being passed through _() for i18n.

Change-Id: I85856a135a62bc3415a355e0900ea12ea55f73ae
Closes-Bug: #1273266"
146,121189401a97e5f471a287c453d7f2013c92477a,1385488429,,1.0,7,3,1,1,1,0.0,True,3.0,91231.0,9.0,2.0,False,3.0,62886.0,3.0,2.0,528.0,1095.0,1353.0,429.0,1002.0,1187.0,177.0,461.0,509.0,0.31010453,0.804878049,0.888501742,90,488,1255183,neutron,121189401a97e5f471a287c453d7f2013c92477a,1,1,Sending a lot of notifications,"Bug #1255183 in neutron: ""port update triggered even if sec group did not change""","the code at: https://github.com/openstack/neutron/blob/master/neutron/db/securitygroups_rpc_base.py#L73
will trigger a rebind of the security group of the port, as well as instruct the plugin to notify the agent if the port update request has the 'security_groups' attribute.
Actually these operations are not needed if there's no change from the current value for security groups.
This condition is causing a fair amount of port_update notification to be sent to the agent; the DHCP port is updated quite often, and since the plugin function is called from the RPC dispatcher, the security groups attribute is not missing, but is an empty list.
For this reason the notification is then sent every time a DHCP port is updated.","Rebind security groups only when they're updated

Update the security port bindings for a port only when they actually
differ from the stored value.

This will also avoid sending port_update notifications to the agent
even if nothing actually changes in the port configuration.

Closes-Bug: #1255183
Partial blueprint: neutron-tempest-parallel

Change-Id: I00c29dc97c46478433fdf08069a884bb78e5cd0a"
147,1212f66b188bbe40ac7219908eb47af26ebf2edc,1382351792,,1.0,1,1,1,1,1,0.0,True,1.0,122414.0,12.0,7.0,False,7.0,943156.0,12.0,5.0,54.0,713.0,726.0,54.0,690.0,703.0,38.0,665.0,667.0,0.035135135,0.6,0.601801802,1720,1242549,1242549,cinder,1212f66b188bbe40ac7219908eb47af26ebf2edc,1,1, ,"Bug #1242549 in Cinder: ""Fail to delete volume snapshot created by GPFS driver""","I enable one cinder volume node with GPFS driver. Creating/deleting volume works correctly. However, deleting volume snapshot operation fails.
[root@zhaoqin-RHEL-GPFS1 install]# cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 88a10b05-78d9-495d-beb4-52863c016638 | available | zhaoqin-lvm  |  1   |     lvm     |  false   |             |
| d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | available | zhaoqin-gpfs |  1   |     gpfs    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-create --display_name=gpfs_snapshot d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|      created_at     |      2013-10-21T07:20:36.284189      |
| display_description |                 None                 |
|     display_name    |            gpfs_snapshot             |
|          id         | 6274e3a0-24f0-47dc-914d-235a13047b6a |
|       metadata      |                  {}                  |
|         size        |                  1                   |
|        status       |               creating               |
|      volume_id      | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
|                  ID                  |              Volume ID               |     Status     |    Display Name   | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b |   available    |   gpfs_snapshot   |  1   |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-delete 6274e3a0-24f0-47dc-914d-235a13047b6a
[root@zhaoqin-RHEL-GPFS1 install]# cinder snapshot-list
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
|                  ID                  |              Volume ID               |     Status     |    Display Name   | Size |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+
| 6274e3a0-24f0-47dc-914d-235a13047b6a | d8a9cc4c-b0b8-481e-aa64-1af88bb3cf8b | error_deleting |   gpfs_snapshot   |  1   |
+--------------------------------------+--------------------------------------+----------------+-------------------+------+","Let GPFS driver to rename snapshot with root permission

Deleting GPFS volume snapshot operation fails, because cinder
volume is not running with root permission.

Change-Id: Id00357030da171fddbc6abf82603ceabe4db73ff
Closes-Bug: #1242549"
148,1222366f6dadf7ce2a810c919344054196134db8,1402067738,0.0,1.0,41,10,4,3,1,0.790862379,False,,,,,True,,,,,,,,,,,,,,,,,961,1399,1328288,neutron,1222366f6dadf7ce2a810c919344054196134db8,1,1,,"Bug #1328288 in neutron: ""[mos] openvswitch agent fails with bridges longer than 11 chars""","The openvswitch agent will try to construct veth pairs with names longer than the maximum allowed (15) and fail. VMs will then have no external connectivity.
This happens in cases where the bridge name is very long (e.g. int-br-bonded).","OVS Agent: limit veth names to 15 chars

The maximum length of a veth interface name is 15 characters.
This patch limits the length of the veth names created for
integration bridges by switching to a fixed-length hashing
mechanism if the normal name would exceed the allowed length.

Closes-Bug: #1328288
Change-Id: I432cee62a6dc91f268becbc91f8024c23dd0bfac"
149,1236b09076cca3b4b16538b055e52edde5a4feea,1399683828,,1.0,30,144,2,2,1,0.906370189,True,2.0,67192.0,12.0,3.0,False,73.0,98098.0,178.0,2.0,1862.0,1212.0,3045.0,891.0,1202.0,2069.0,1745.0,1202.0,2918.0,0.219291635,0.15109269,0.366616428,880,1315,1318104,nova,1236b09076cca3b4b16538b055e52edde5a4feea,1,1,,"Bug #1318104 in OpenStack Compute (nova): ""dhcp isolation via iptables does not work""",Attempting to block iptables across the bridge via iptables rules is not working. The iptables rules are never hit. blocking dhcp traffic from exiting the node will need to use ebtables instead.,"Use ebtables to isolate dhcp traffic

Iptables doesn't properly block the broadcast traffic crossing
the bridge, so use ebtables instead. Removes test which is no
longer valid since we are not using iptables anymore.

Change-Id: I43e5f1fe9512dd3ec9595c7203bc46837cef3cad
Closes-Bug: #1318104"
150,1284c4d2e471809635a070289ace0049abf38f18,1394102133,,1.0,1,1,1,1,1,0.0,True,1.0,1007291.0,24.0,6.0,False,178.0,103422.0,910.0,5.0,7.0,3676.0,3681.0,7.0,2911.0,2916.0,4.0,3530.0,3532.0,0.000664628,0.469360627,0.469626479,542,963,1288661,nova,1284c4d2e471809635a070289ace0049abf38f18,1,1,typo in code,"Bug #1288661 in OpenStack Compute (nova): ""log messages typos in rebuild_instance function""","one simple log typos in compute/manager.py rebuild_instance function.
             else:
                    image_ref = orig_image_ref = instance.image_ref
                    LOG.info(_(""disk not on shared storagerebuilding from:""
                               "" '%s'"") % str(image_ref))
should change to
                    LOG.info(_(""disk not on shared storage, rebuilding from:""
                               "" '%s'"") % str(image_ref))","Fix log messages typos in rebuild_instance function

One log message typos in rebuild_instance function.

Change-Id: I0c86a40b6683f3a8a091134b6f37c82a4562370e
Closes-Bug: #1288661"
151,12b8b56807af7ce3bbe330d73864abc87cdadbf1,1410787976,1.0,1.0,25,13,3,3,1,0.922317215,False,,,,,True,,,,,,,,,,,,,,,,,1317,1783,1369508,nova,12b8b56807af7ce3bbe330d73864abc87cdadbf1,1,1,,"Bug #1369508 in OpenStack Compute (nova): ""Instance with NUMA topology causes exception in the scheduler""","This was reported by Michael Turek as he was testing this while the patches were still in flight See: https://review.openstack.org/#/c/114938/26/nova/virt/hardware.py
As described on there - the code there makes a bad assumption about the format in which it will get the data in the scheduler, which results in:
2014-09-15 10:45:44.906 ERROR oslo.messaging.rpc.dispatcher [req-f29a469e-268d-49bf-abfa-0ccb228d768c admin admin] Exception during message handling: An object of type InstanceNUMACell is required here
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/manager.py"", line 175, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     filter_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 300, in _schedule
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     chosen_host.obj.consume_from_instance(context, instance_properties)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 252, in consume_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self, instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/hardware.py"", line 978, in get_host_numa_usage_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     instance_numa_topology = instance_topology_from_instance(instance)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/hardware.py"", line 949, in instance_topology_from_instance
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     cells=cells)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 242, in __init__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self[key] = kwargs[key]
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 474, in __setitem__
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     setattr(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/base.py"", line 75, in setter
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     field_value = field.coerce(self, name, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 388, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     obj, '%s[%i]' % (attr, index), element)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 189, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     return self._type.coerce(obj, attr, value)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/objects/fields.py"", line 474, in coerce
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher     self._obj_name)
2014-09-15 10:45:44.906 TRACE oslo.messaging.rpc.dispatcher ValueError: An object of type InstanceNUMACell is required here","instance_topology_from_instance handles request_spec properly

We special-case how we handle the instance that we extract from the
request_spec data blob in virt.hardware.instance_topology_from_instance,
however, we were making wrong assumptions about the data we get from it
in the scheduler.

This patch fixes how it is handled and also updates the tests and
comments in the code to reflect it.

Change-Id: I38d7078d670bc37ddb40fba82fd0f9cb74045047
Closes-bug: #1369508"
152,130781cb7b6fc1635838a7c3f19d340a0a846637,1386739579,,1.0,0,4,1,1,1,0.0,True,1.0,426750.0,19.0,2.0,False,27.0,1394935.0,49.0,2.0,17.0,348.0,353.0,17.0,305.0,310.0,15.0,310.0,313.0,0.015122873,0.293950851,0.296786389,171,573,1259818,glance,130781cb7b6fc1635838a7c3f19d340a0a846637,0,0,Remove unused exceptions ,"Bug #1259818 in Glance: ""Remove unused exceptions""","Removed the exception classes in the following because not referenced
anywhere.
-MissingArgumentError
-NotAuthorized","Remove unused exceptions

Removed the exception classes in the following because not referenced
anywhere.

 -MissingArgumentError
 -NotAuthorized

Change-Id: I9f8d8e865447d772a9c09141b96075620c6d1b07
Closes-Bug: #1259818"
153,131777688562b1ec020327067d89da1535b99640,1396974910,,1.0,6,1,1,1,1,0.0,True,2.0,3450821.0,20.0,8.0,False,6.0,977877.0,6.0,5.0,360.0,1055.0,1394.0,198.0,929.0,1112.0,34.0,963.0,977.0,0.022364217,0.615974441,0.624920128,761,1188,1304516,cinder,131777688562b1ec020327067d89da1535b99640,1,1,,"Bug #1304516 in Cinder: ""Cinder fails to delete error state volumes on Windows""","If the volume creation fails, we won't be able to delete the volume. It will go from error state to error-deleting. The reason is that the delete_volume method fails when the iSCSI disk does not exist. In fact, it should skip deleting the disk if it does not exist.
Trace: http://paste.openstack.org/show/75340/","Fixes cinder error state volume delete on Windows

If the volume creation fails and the iSCSI disk is not created,
trying to delete the error state volume will fail on Windows.

Deleting the iSCSI disk must be skipped when it does not exist.

Change-Id: I2cd615e71fe457c241bea207c4bd8904c1ffd9cb
Closes-Bug: #1304516"
154,13387c01390e70c9d7811760e603e6306d6d7ea7,1394010058,,1.0,9,26,1,1,1,0.0,True,9.0,6681597.0,34.0,7.0,False,44.0,430821.0,92.0,4.0,8.0,803.0,811.0,8.0,732.0,740.0,0.0,766.0,766.0,0.000667111,0.51167445,0.51167445,1548,1224429,1224429,cinder,13387c01390e70c9d7811760e603e6306d6d7ea7,1,1, ,"Bug #1224429 in Cinder: ""Don't use ModelBase.save() inside of block session.begin()""","Current code use ModelBase.save() always submit a commit even inside a block fo with session.begin().
this is not purpose of using session.begin() to organize some operations  in one transaction
1)session.begin() will return a SessionTransaction instance, then call SessionTransaction.__enter__()
and do something in with block, then call SessionTransaction.__exit__(), in method __exit__() will commit or rollback automatically. See https://github.com/zzzeek/sqlalchemy/blob/master/lib/sqlalchemy/orm/session.py#L454
2) There is also suggestion  metioned in https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/session.py#L71
3) ModelBase.save() begin another transaction see https://github.com/openstack/oslo-incubator/blob/master/openstack/common/db/sqlalchemy/models.py#L47
so we'd better don't use  ModelBase.save() inside of block session.begin()","Don't use ModelBase.save() inside of transaction

'with session.begin()' makes some operations in one transaction.
session.begin() returns a transaction instance, then does some operations,
and will commit or rollback automatically before leaving the block.
ModelBase.save() always submit a commit, and that is not expected.
When we get a persistent object from database, we just modify the
object inside of block 'with session.begin()' and sqlalchemy will
update it, don't need method session.add() or ModelBase.save().

Closes-Bug: #1224429
Change-Id: I4af58e98b2783d3945d92e57680d58e7ae356a67"
155,13c9f3b813f5bb368e311ba0d428fa759d68289a,1394844222,0.0,1.0,81,29,5,5,1,0.785911544,True,6.0,1606765.0,116.0,15.0,False,9.0,583773.2,20.0,3.0,1462.0,1608.0,1867.0,1190.0,1380.0,1548.0,753.0,784.0,845.0,0.776519053,0.808444902,0.871266735,633,1059,1293508,neutron,13c9f3b813f5bb368e311ba0d428fa759d68289a,1,1,Wrong code 500 instead of 400 and add error msg,"Bug #1293508 in neutron: ""NSX plugin: 400 should be returned on invalid certificate""","When a gateway device is created, the client certificate is not stored anywhere on the neutron server, but then passed directly to the backend, which validates the certificate.
Currently the NSX backend raises an exception when the certificate is not valid.
This exception is treated by the NSX plugin as a backend failure and a 500 is then returned.
However, the correct error would a 400 with an appropriate error message.","NSX plugin: return 400 for invalid gw certificate

Gateway certificates are validated by the NSX backend.
The code currently treats a failure in certification
validation as a backend failure and therefore returns
a 500 status code.

This patch changes this behaviour by returning a 400
status code and an appropriate error description.
To this aim a handler for 400 errors has been added to
the NSX API client.

Closes-Bug: #1293508

Change-Id: I196f14337e47cd40710a6d8a30bbe1cac5ffe05b"
156,13e2bd02a5b50973f95eb3d8fc0af4e0702e3381,1394746391,1.0,1.0,67,110,9,2,1,0.927280954,False,,,,,True,,,,,,,,,,,,,,,,,1082,1532,1346092,nova,13e2bd02a5b50973f95eb3d8fc0af4e0702e3381,0,0,"Feature “Move libvirt RBD utilities to a new file
    This will make it easier to share rbd-related code with cinder and glance.”","Bug #1346092 in OpenStack Compute (nova): ""RBD helper utils in libvirt driver code need to be moved to separate module""","The libvirt imagebackend.py file has alot of helper APIs for dealing with the RBD utilities. It is desirable that these all be isolated in a standalone rbd.py file, to be called by the imagebackend.py  This will make the core logic in imagebackend.py easier to follow and make the rbd helpers easier to test.","Use library instead of CLI to cleanup RBD volumes

'rbd list' CLI returns error code when there are no rbd volumes, which
causes problems during live migration of VMs with RBD backed ephemeral
volumes. It's safer to use the library that only raises an exception in
case of a real problem.

The only case where rbd CLI is still justified is import, which is
needed to correctly import sparse image files.

All code related to cleanup of RBD volumes is moved to rbd.py, this
fixes a yo-yo problem with single-use methods scattered across 3
different files, and minimizes impact of this fix on imports in
imagebackend and utils.

Closes-Bug: #1346092
Change-Id: I92cd6b16fbd93b377fe47b15d22efbbf68d02513
Signed-off-by: Dmitry Borodaenko <dborodaenko@mirantis.com>"
157,13fd601415418407bb2d2f6bafffb470e5879df6,1394736538,,1.0,1,1,1,1,1,0.0,True,5.0,2886938.0,78.0,16.0,False,3.0,839198.0,3.0,5.0,0.0,1011.0,1011.0,0.0,893.0,893.0,0.0,492.0,492.0,0.001048218,0.516771488,0.516771488,545,966,1288915,neutron,13fd601415418407bb2d2f6bafffb470e5879df6,0,0,Improve message,"Bug #1288915 in neutron: ""Unclear message when try show quotas of different tenant""","When a quota-show for a different tenant is requested without admin permissions, you get the following message that is confusing and could be improved.
""Non-admin is not authorised to access quotas for another tenant""","Improved quota error message

Removed two negatives and showed a clear message when
tried to see quota of different tenants.
When a user who does not belong to a project tries to access the
quota of that project, the message that gets displayed is not clear.
In this change the user would understand the error more properly.

Change-Id: I2284df07687bb530fe06fbaab38e2971826b7b40
Closes-Bug: #1288915"
158,13fe102fadf8d25b0927ee172b0fb3f681c56bf4,1402639966,,1.0,9,3,2,2,1,0.650022422,False,,,,,True,,,,,,,,,,,,,,,,,971,1410,1329426,neutron,13fe102fadf8d25b0927ee172b0fb3f681c56bf4,1,1,“Add missing keyword raise to get_profile_binding function”,"Bug #1329426 in neutron: ""Cisco n1kv neutron plugin typo in get_profile_binding""",Need to fix typo where raise keyword is missing in get_profile_binding,"Add missing keyword raise to get_profile_binding function

Improve corresponding unit tests.

Change-Id: I8f71ee7aa8fcafbf3ef6e3f9d4f0b89c874af73b
Closes-Bug: #1329426"
159,1408ff71f4d849c319866be92414361d8103acdd,1385714691,,1.0,1,1,1,1,1,0.0,True,1.0,26828.0,5.0,2.0,False,47.0,233382.0,129.0,2.0,19.0,327.0,346.0,19.0,285.0,304.0,0.0,289.0,289.0,0.000968054,0.280735721,0.280735721,105,504,1256217,glance,1408ff71f4d849c319866be92414361d8103acdd,0,0,Typo in comments,"Bug #1256217 in Glance: ""Fix docstring on detail in glance/api/v1/images.py ""","""Returns detailed information for all public, available images"" is inappropriate on detail, because the private image can be returned also.
""Returns detailed information for all  available images"" is better.","Fix docstring on detail in glance/api/v1/images.py

""Returns detailed information for all public, available images"" is
inappropriate on detail, because the private image can be returned also.

Change-Id: Ib173a6813cb70270866476368ffa1ccbb77047aa
Closes-Bug: #1256217"
160,140956515327494a53de6ad09c35690624248f0a,1409222021,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1158,1615,1353506,cinder,140956515327494a53de6ad09c35690624248f0a,0,0,Bug in test,"Bug #1353506 in Cinder: ""NetApp ESeries unit test breaks tests using python-requests""","The unit test for the NetApp E-Series volume driver (cinder/tests/test_netapp_eseries_iscsi.py) is missing cleanup operations to ensure other tests using the python-requests library are not affected by its setup.
By assigning a mock object to requests.Session during setup and not restoring the original during teardown, the test causes any following code trying to invoke requests.Session.send to fail with the message:
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Attached is a test case (test_requests_mock.py) that when run alone will pass but fails when being executed after the NetApp E-Series test in the same run:
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock
[...]
Ran 1 test in 5.007s
OK
cinder$ ./run_tests.sh -V cinder.tests.test_requests_mock cinder.tests.test_netapp_eseries_iscsi
[...]
AttributeError: type object 'FakeEseriesHTTPSession' has no attribute 'send'
Ran 15 tests in 5.030s
FAILED (failures=1)","NetApp fix eseries unit test mock clean

This patch fixes the issue of mock not getting
cleaned for requests in unit tests.

Closes-Bug: #1353506

Change-Id: Iab401021d7f180ff1f2bf3ed79166699112cc367"
161,142c55e82a4d36af887439a94054617a5a0ede9c,1393711654,,1.0,203,566,11,5,1,0.902214638,True,9.0,359646.0,125.0,13.0,False,9.0,6485765.818,14.0,1.0,486.0,1033.0,1303.0,399.0,884.0,1091.0,229.0,577.0,665.0,0.261363636,0.656818182,0.756818182,506,925,1286733,neutron,142c55e82a4d36af887439a94054617a5a0ede9c,0,0,Change in requirements,"Bug #1286733 in neutron: ""nec plugin: delete pre-grizzly ID mapping tables""","Before Grizzly release, data format of OFC ID mapping tables was changed  and there are two types of ID mapping tables for old and new format.  The old mapping tables are only used for resources (networks, ports, tenants, filters) in pre-Grizzly system. pre-Grizzly system is no longer supported and we no longer need to consider old data format. Migrating data from the old mapping tables to the new tables reduces the code complexity.","NEC plugin: delete old OFC ID mapping tables

Before Grizzly release, data format of OFC ID mapping tables was changed
and there are two types of ID mapping tables for old and new format.
This commit migrate data from old mapping tables into new tables,
drop old mapping tables and remove the logic handling the old tables.

In the db migration scripts, built-in compiler of sqlalchemy does not
support ""INSERT INTO table (col1, col2,...) (SELECT ....)"" format,
so a custom sqlalchemy.expression compiling method is defined.

Closes-Bug: #1286733
Change-Id: I7dae6b728ab9e10f1dc5d63418a69ee4c26354ea"
162,1447b8117bc4de27243913208ead210b8f36d635,1385471440,,1.0,20,1,2,2,1,0.453716339,True,2.0,28600.0,8.0,3.0,False,32.0,1367753.0,80.0,2.0,0.0,2250.0,2250.0,0.0,1729.0,1729.0,0.0,2040.0,2040.0,0.000149231,0.304581406,0.304581406,85,482,1255035,nova,1447b8117bc4de27243913208ead210b8f36d635,1,1,Bad argument,"Bug #1255035 in OpenStack Compute (nova): ""Incorrect argument while retrieving quota with project and user in DbQuotaDriver""","The `user_id` passed in db.quota_get() should be a keyword argument.
The prototype of quota_get():
def quota_get(context, project_id, resource, user_id=None)
But In DbQuotaDriver::get_by_project_and_user():
return db.quota_get(context, project_id, user_id, resource)
which should be:
return db.quota_get(context, project_id, resource, user_id=user_id)","Fix incorrect argument position in DbQuotaDriver

The prototype of db.quota_get is:
`quota_get(context, project_id, resource, user_id=None)`
However, in DbQuotaDriver::get_by_project_and_user(), the `user_id`
is incorrectly used as a positional argument.

Closes-Bug: #1255035

Change-Id: I5b6a4c59efe6d9bdc8380e50b7f7c1ff0d2029b1"
163,148fd1bb124fa80f70a8983242dc979a34acceab,1409302478,,1.0,14,1,2,2,1,0.566509507,False,,,,,True,,,,,,,,,,,,,,,,,1248,1707,1363014,nova,148fd1bb124fa80f70a8983242dc979a34acceab,1,1,,"Bug #1363014 in OpenStack Compute (nova): ""NoopQuotasDriver.get_settable_quotas() method always fail with KeyError""","NoopQuotasDriver.get_settable_quotas() tries to call update() on non-existing dictionary entry. While NoopQuotasDriver is not really useful, we still want it to be working.","Fix NoopQuotasDriver.get_settable_quotas()

get_settable_quotas() always failed with KeyError. While this driver
is not really useful, we still want it to be working.

Closes-Bug: #1363014

Change-Id: Ia6986453338cee05f28ff081c6bb1279072c3d80"
164,14903b4ca7d625b8e93fe431f854263bbe26e4bc,1392878591,,1.0,2,2,1,1,1,0.0,True,3.0,702181.0,29.0,9.0,False,1.0,5512128.0,1.0,2.0,47.0,1161.0,1208.0,47.0,944.0,991.0,0.0,1016.0,1016.0,0.000690608,0.702348066,0.702348066,444,859,1282514,cinder,14903b4ca7d625b8e93fe431f854263bbe26e4bc,0,0,"No bug. In case of it, not BIC python3","Bug #1282514 in Cinder: ""python 3 only has  ""__self__"", the ""im_self"" should be replace by ""__self_""""","for code compatible with Python 3, we should use the ""__self__"" instead of ""im_self"".
for example :
cinder/volume/flows/common.py
def make_pretty_name(method):
    """"""Makes a pretty name for a function/method.""""""
    meth_pieces = [method.__name__]
    # If its an instance method attempt to tack on the class name
    if hasattr(method, 'im_self') and method.im_self is not None:
        try:
            meth_pieces.insert(0, method.im_self.__class__.__name__)
        except AttributeError:
            pass
    return ""."".join(meth_pieces)
For reference here(thanks Alex for adding this):
""Changed in version 2.6: For Python 3 forward-compatibility, im_func is also available as __func__, and im_self as __self__.""
http://docs.python.org/2/reference/datamodel.html","Python 3: replace ""im_self"" by ""__self__""

The Python 3 removed the ""im_self"" attribute and only supports the
""__self__"". This patch replaces ""im_self"" by ""__self__"" attribute.

Closes-Bug: #1282514
Change-Id: Ib017fff4b720a5dd7ee27fd01b36a531d9b6ca9c"
165,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1395237635,,2.0,13,7,2,2,1,0.468995594,True,8.0,885009.0,220.0,30.0,False,10.0,142891.0,12.0,5.0,248.0,1281.0,1348.0,243.0,1116.0,1182.0,187.0,794.0,828.0,0.187064677,0.791044776,0.824875622,653,1079,1294603,neutron,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1,1,,"Bug #1294603 in neutron: ""scenario test_load_balancer_basic fails""","http://logs.openstack.org/98/81098/2/check/check-tempest-dsvm-neutron-pg/df24b97/console.html
2014-03-19 09:58:15.379 | Traceback (most recent call last):
2014-03-19 09:58:15.379 |   File ""tempest/test.py"", line 121, in wrapper
2014-03-19 09:58:15.379 |     return f(self, *func_args, **func_kwargs)
2014-03-19 09:58:15.379 |   File ""tempest/scenario/test_load_balancer_basic.py"", line 225, in test_load_balancer_basic
2014-03-19 09:58:15.379 |     self._check_load_balancing()
2014-03-19 09:58:15.379 |   File ""tempest/scenario/test_load_balancer_basic.py"", line 213, in _check_load_balancing
2014-03-19 09:58:15.379 |     ""http://{0}/"".format(self.vip_ip)).read())
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 86, in urlopen
2014-03-19 09:58:15.380 |     return opener.open(url)
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 207, in open
2014-03-19 09:58:15.380 |     return getattr(self, name)(url)
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/urllib.py"", line 345, in open_http
2014-03-19 09:58:15.380 |     errcode, errmsg, headers = h.getreply()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 1102, in getreply
2014-03-19 09:58:15.380 |     response = self._conn.getresponse()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 1030, in getresponse
2014-03-19 09:58:15.380 |     response.begin()
2014-03-19 09:58:15.380 |   File ""/usr/lib/python2.7/httplib.py"", line 407, in begin
2014-03-19 09:58:15.381 |     version, status, reason = self._read_status()
2014-03-19 09:58:15.381 |   File ""/usr/lib/python2.7/httplib.py"", line 365, in _read_status
2014-03-19 09:58:15.381 |     line = self.fp.readline()
2014-03-19 09:58:15.381 |   File ""/usr/lib/python2.7/socket.py"", line 430, in readline
2014-03-19 09:58:15.381 |     data = recv(1)
2014-03-19 09:58:15.381 | IOError: [Errno socket error] [Errno 104] Connection reset by peer","Fix namespace exist() method

Fix namespace exist() method for it shall not be called with a root.
Also, don't run it under the namespace so garbage_collect_namespace
method can run without rootwrap and not withi a ns.

As a result of fixing namespace listing the patch also fixes the
regression introduced (bug/1294603) to loadbalancer agent respawning
haproxy due to inability to list namespaces properly.

Change-Id: I0dc4d01b0c1c04887ec6ad5766ec7c6c96903faa
Closes-Bug: #1297594
Closes-Bug: #1294603"
166,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1395237635,,2.0,13,7,2,2,1,0.468995594,True,8.0,885009.0,220.0,30.0,False,10.0,142891.0,12.0,5.0,248.0,1281.0,1348.0,243.0,1116.0,1182.0,187.0,794.0,828.0,0.187064677,0.791044776,0.824875622,684,1110,1297594,neutron,14b6611efea4c9658c611e8cfcfb42fc7dd82f55,1,1,Fixing commit introduces a bug,"Bug #1297594 in neutron: ""List namespaces properly, fix regression introduced by fix to 1293818""","fix to 1293818 introduced a regression for the code path that is used in loadbalancer namespace driver to list namespaces.
The following tracebacks are seen during tempest runs:
http://logs.openstack.org/14/82514/2/check/check-tempest-dsvm-neutron-pg/73ff42d/logs/screen-q-lbaas.txt.gz?level=TRACE#_2014-03-25_09_49_57_261
As a consequence, this causes agent to resync balancer state and to respawn haproxy process sometimes leading to unavailable service. That in turn makes loadbalancer basic scenario test fail.
Related bugs: 1294603, 1295165","Fix namespace exist() method

Fix namespace exist() method for it shall not be called with a root.
Also, don't run it under the namespace so garbage_collect_namespace
method can run without rootwrap and not withi a ns.

As a result of fixing namespace listing the patch also fixes the
regression introduced (bug/1294603) to loadbalancer agent respawning
haproxy due to inability to list namespaces properly.

Change-Id: I0dc4d01b0c1c04887ec6ad5766ec7c6c96903faa
Closes-Bug: #1297594
Closes-Bug: #1294603"
167,14cb886809e5cccbf799a0dc2e5b99f31b1ab3be,1392984919,1.0,1.0,8,8,5,5,1,0.928383212,True,1.0,156312.0,18.0,3.0,False,30.0,12753.0,86.0,3.0,483.0,2458.0,2567.0,404.0,2131.0,2230.0,226.0,656.0,717.0,0.266745006,0.772032902,0.843713278,452,867,1283019,neutron,14cb886809e5cccbf799a0dc2e5b99f31b1ab3be,0,0,"NO bug. ‘No need to use ""is not"" when comparing values’","Bug #1283019 in neutron: ""nec plugin: No need to use ""is not"" when comparing values""","""is not"" operator compares two objects are identical.
When comparing values, ""!="" should be used.","Use ""!="" instead of ""is not"" when comparing two values

Change-Id: I2cd9575585dde7c44b528077a7bffa16379b1759
Closes-Bug: #1283019"
168,14ebdbe77479a72ad1f324663ec795460e03e99a,1399376104,,1.0,4,2,1,1,1,0.0,True,3.0,217325.0,44.0,7.0,False,7.0,1548189.0,13.0,6.0,286.0,932.0,1040.0,275.0,888.0,987.0,225.0,783.0,846.0,0.186314922,0.64633141,0.698268755,859,1292,1315809,neutron,14ebdbe77479a72ad1f324663ec795460e03e99a,1,1,,"Bug #1315809 in neutron: ""Updation of IKE Policy after the site creation fails ""","Updating the IKE policy after the site creation fails stating ike policy in already in use
Updating IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: url:          http://<controller_ip>:9696/v2.0/vpn/ikepolicies/07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5
_PUT-REST: X-Auth-Token: 86c8be3ce0204aa8bdd1458021eacec4
_PUT-REST: data:         {""ikepolicy"":{""name"": ""IKE1"",""encryption_algorithm"": ""aes-256""}}
{u'NeutronError': {u'message': u'IKEPolicy 07fc4f3b-1b44-4d5c-9949-5f7fa3bc6ae5 is still in use', u'type': u'IKEPolicyInUse', u'detail': u''}}
None","Make VPNaaS 'InUse' exception more clear

In case IpSecPolicy or IKEPolicy is updated while VPN connection that uses it
is already established, IPsecPolicyInUse or IKEPolicuInUse is raised.
Need to clarify their messages to emphasize that policies can't be updated
because they are used by established connection.

Change-Id: I259f9b8bcff7f8ec13ac630285f6e881c6653309
Closes-Bug: #1315809"
169,155cb48bca9b66fa5188e5c1c63adf696cd6d127,1378487457,1.0,1.0,46,1,2,2,1,0.342463772,True,1.0,256778.0,10.0,3.0,False,1.0,1090451.0,2.0,3.0,306.0,1534.0,1585.0,294.0,1437.0,1486.0,54.0,253.0,261.0,0.185185185,0.855218855,0.882154882,1526,1221663,1221663,neutron,155cb48bca9b66fa5188e5c1c63adf696cd6d127,0,0,“Make neutron.common.log.log print module path” feature or bug3,"Bug #1221663 in neutron: ""neutron.common.log.log does not print module path""","neutron.common.log.log is useful for logging arguments of a method.
It outputs class name and method name, but module path is not output.
A module path is useful to search the log message and it is better to contain a module path
[Current]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {}  wrapper /opt/stack/neutron/neutron/common/log.py:33
[Proposed]
2013-09-06 17:00:22.021 25612 DEBUG neutron.common.log [-] neutron.tests.unit.nec.stub_ofc_driver.StubOFCDriver method create_network called with arguments (u'ofc-f1c7f9f5691044f5b5d395d1a7c0', u'ID=8faea3be-66d5-4cbf-8631-810f491c6d0a Name=ext_net at Neutron.', '8faea3be-66d5-4cbf-8631-810f491c6d0a') {}  wrapper /opt/stack/neutron/neutron/common/log.py:33","Make neutron.common.log.log print module path

Closes-Bug: #1221663

neutron.common.log.log is useful for logging arguments of a method.
It outputs class name and method name, but module path is not output.
A module path is useful to search the log message.

Change-Id: Ic2903da750cc13980d5cdee153bb079f7d4ee122"
170,1588de2714a0f28a0da219f319d6a7ac6c1f9bab,1385244048,1.0,1.0,44,1,2,2,1,0.26231122,True,3.0,255008.0,21.0,7.0,False,9.0,1247091.0,12.0,5.0,234.0,3532.0,3591.0,234.0,3063.0,3122.0,219.0,2587.0,2636.0,0.032993401,0.388122376,0.395470906,1796,1253960,1253960,nova,1588de2714a0f28a0da219f319d6a7ac6c1f9bab,1,1,“return the hypervisor_version as an integer rather than a string.”,"Bug #1253960 in OpenStack Compute (nova): ""docker driver reports hypervisor version in wrong format""","devstack, `nova --version` reports 2.15.0.81
VIRT_DRIVER=docker
database: PostgreSQL
when starting nova-compute I'm getting an exception:
2013-11-20 11:41:27.413 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=15320) inner /opt/stack/nova/nova/openstack/common/lockutils.py:251
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/queue.py"", line 107, in switch
    self.greenlet.switch(value)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 164, in start
    self.manager.pre_start_hook()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 788, in pre_start_hook
    self.update_available_resource(nova.context.get_admin_context())
  File ""/opt/stack/nova/nova/compute/manager.py"", line 5065, in update_available_resource
    rt.update_available_resource(context)
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 248, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 326, in update_available_resource
    self._sync_compute_node(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 349, in _sync_compute_node
    self._create(context, resources)
  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 365, in _create
    values)
  File ""/opt/stack/nova/nova/conductor/api.py"", line 236, in compute_node_create
    return self._manager.compute_node_create(context, values)
  File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 356, in compute_node_create
    return cctxt.call(context, 'compute_node_create', values=values)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
    return self._invoke(self.proxy.call, ctxt, method, **kwargs)
  File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
    return cast_or_call(ctxt, msg, **self.kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
    result = rpc.call(context, real_topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
    return _get_impl().call(CONF, context, topic, msg, timeout)
  File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
    rpc_amqp.get_connection_pool(conf, Connection))
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
    rv = list(rv)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
    raise result
RemoteError: Remote error: DBError (DataError) invalid input syntax for integer: ""1.0""
LINE 1: ...L, NULL, 0, 5, 1, 11953, 54, 0, 512, 0, 'docker', '1.0', 'hg...
                                                             ^
(should be in fixed-width font, the circumflex accent points to '1.0')
This appears to be the same issue as #1195139  but with docker driver.  Also related to https://blueprints.launchpad.net/nova/+spec/save-hypervisor-version-as-string
nova-compute starts up OK after applying a similar change as for #1195139 :
--- nova/virt/docker/driver.py.orig
+++ nova/virt/docker/driver.py
@@ -159,7 +159,7 @@
             'local_gb_used': disk['used'] / unit.Gi,
             'disk_available_least': disk['available'] / unit.Gi,
             'hypervisor_type': 'docker',
-            'hypervisor_version': '1.0',
+            'hypervisor_version': utils.convert_version_to_int('1.0'),
             'hypervisor_hostname': self._nodename,
             'cpu_info': '?',
             'supported_instances': jsonutils.dumps([","docker: return hypervisor_version as an int rather than a string

The compute_nodes.hypervisor_version type in the database is an integer
so the docker driver needs to return the hypervisor_version as an
integer rather than a string.

Also adds the missing unit test for get_available_resource.

Closes-Bug: #1253960

Change-Id: I4e2ac13f9203b54e71f1f5cd8e15311921dd85c5"
171,15a93312641bb6121d20c7749481104a1ab34bd0,1404048604,,1.0,176,30,2,2,1,0.996665728,False,,,,,True,,,,,,,,,,,,,,,,,1054,1500,1340315,cinder,15a93312641bb6121d20c7749481104a1ab34bd0,1,1,,"Bug #1340315 in Cinder: ""VMware: Minimum disk size is incorrectly set to 1KB""","The minimum disk size of a backing VM is set to 1KB, but the VIM APIs fail the operation if the disk size is less than 1MB.","VMware:Support for attaching disk to backing

This change adds support for attaching a virtual disk to an existing
backing VM. Currently volume creation from preallocated/sparse image doesn't
honour the disk type in the volume extra spec and adapter type in the image
meta-data. The workflow to address these issues requires the above mentioned
method. This change also sets the disk size of backing to at least 1MB which
is the minimum required by VIM APIs.

Closes-Bug: #1340315
Partial-Bug: #1284284
Partial-Bug: #1287185
Partial-Bug: #1287176

Change-Id: Icdb1f26d5c8eaa42408a14c57c7394d96dda078f"
172,15c04f6869c3ae4bb8906b3783551df0e718d462,1391194732,,1.0,1,2,1,1,1,0.0,True,3.0,211736.0,12.0,6.0,False,13.0,5689162.0,18.0,2.0,83.0,602.0,633.0,69.0,508.0,534.0,54.0,398.0,422.0,0.087025316,0.631329114,0.669303797,325,734,1272503,swift,15c04f6869c3ae4bb8906b3783551df0e718d462,0,0,tests,"Bug #1272503 in OpenStack Object Storage (swift): ""test_connection_pool_timeout in unit.proxy.test_server.TestObjectController failed in a verify job""","Seen here:
http://logs.openstack.org/84/67584/5/gate/gate-swift-python27/cf6c70e/console.html
Just need to evaluate and track.","Attempt to fix periodic memcache timeout test

Not sure this will really fix the issue, but made a small change that
seemed to make more sense for the flow of the test.

Closes-Bug: #1272503
Change-Id: Ifb25ed7c4a42b11f74d865787f63fea4e0beb745"
173,164d1ab46f06141e2c12b944e15ae10f20dace38,1396624592,,1.0,11,4,2,2,1,0.918295834,True,5.0,367349.0,28.0,17.0,False,32.0,810411.5,56.0,5.0,217.0,1134.0,1239.0,181.0,724.0,816.0,160.0,673.0,728.0,0.133499171,0.558872305,0.604477612,746,1173,1302661,glance,164d1ab46f06141e2c12b944e15ae10f20dace38,1,1,“Backwards compatibility? (5fa3d2ef956aa463caf45405a911ef5733085058),"Bug #1302661 in Glance: ""Notifier fails to start when old config params are used and there are missing dependencies""","The current implementation always tries to get a transport from oslo.messaging assuming the transport_url option has been set. This is done to keep backwards compatibility. However, since the default `rpc_backend` is rabbit, it'll always try to load such driver. The problem raises when `kombu` is not installed and the `notifier_strategy` is set to qpid. This will make glance-api fail because it'll try to load the rabbit driver *before* loading the qpid one.","Catch loading failures if transport_url is not set

The current implementation always tries to get a transport from
oslo.messaging assuming the transport_url option has been set. This is
done to keep backwards compatibility. However, since the default
`rpc_backend` is rabbit, it'll always try to load such driver. The
problem raises when `kombu` is not installed and the `notifier_strategy`
is set to qpid. This will make glance-api fail because it'll try to load
the rabbit driver *before* loading the qpid one.

This patch puts the first load attempt in a try / except block. The
error caught (if any) will be re-raised *just* if transport_url has been
configured, otherwise it'll move forward and try to load the notifier
driver using the old configuration options.

The patch also removes an obsolete test.

Change-Id: I00e653704463ea8245e2960050e0d5e6839f278a
Closes-bug: #1302661"
174,1680cf8cef9bd670306a0ef74803409f60f21f28,1396623301,,1.0,4,2,2,2,1,0.918295834,True,2.0,401275.0,26.0,12.0,False,42.0,6892704.5,103.0,9.0,2214.0,5100.0,5102.0,1715.0,3971.0,3973.0,2113.0,3682.0,3682.0,0.273197209,0.475962781,0.475962781,731,1158,1301982,nova,1680cf8cef9bd670306a0ef74803409f60f21f28,1,1,,"Bug #1301982 in OpenStack Compute (nova): ""anti-affinity server boot failure shows stack trace""","If anti-affinity policy is set for two different servers, and only one host is available, a stack trace is shown instead of a error message.  Failure is expected, but a message about why it failed would be helpful.
On a single node devstack setup:
nova server-group-create --policy anti-affinity aagroup
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server1
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=ea784f2b-b262-451a-821a-5ee7f69b3d63 --hint group=0f2b71ea-3dc3-423d-a9f4-41ac5d0fff07 server2
server1 boots fine
server2 has Status Error and Power State No State
Horizon shows:
Fault
Message
unsupported operand type(s) for |: 'list' and 'set'
Code
500
Details
File ""/opt/stack/nova/nova/scheduler/manager.py"", line 140, in run_instance legacy_bdm_in_spec) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 86, in schedule_run_instance filter_properties, instance_uuids) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 289, in _schedule filter_properties) File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 275, in _setup_instance_group filter_properties['group_hosts'] = user_hosts | group_hosts
Created
April 3, 2014, 1:58 p.m.","Fix anti-affinity server-group boot failure

When the second instance is booted to an anti-affinity group
the scheduler fails with an exception due to a missing type conversion.

Closes-Bug: #1301982
Change-Id: I87b2a8d8c7d04b781c789374d6e9c7f4c8567e5d"
175,16c3a33c0b8c37ea76d45a90d6b332cfda416b3d,1386543017,1.0,1.0,818,808,38,17,1,0.707848734,True,5.0,2555294.0,34.0,12.0,False,89.0,506340.3158,527.0,2.0,3.0,647.0,648.0,3.0,491.0,492.0,3.0,493.0,494.0,0.003805899,0.470028544,0.470980019,1811,1257922,1257922,Glance,16c3a33c0b8c37ea76d45a90d6b332cfda416b3d,1,0,"""files still uses json.loads() instead of using the wrapper provided by glance.openstack.common.jsonutils""","Bug #1257922 in Glance: ""Use glance.openstack.common.jsonutils to deal with JSON in test files""","A great number of test files still uses json.loads() instead of using the wrapper provided by glance.openstack.common.jsonutils
All tests that use json directly  should make the switch to the corresponding wrapper in glance.common.openstack.jsonutils","Migrate json to glance.openstack.common.jsonutils

Every call to json.loads/json.dumps have been changed to
jsonutils.loads/jsonutils.dumps respectively. Import json has been removed
also replaced by import glance.openstack.common.jsonutils

654d80b416dc5f413cb791aa838ec8688bf7da44 Create openstack.common.jsonutils

Change-Id: I8ef580e5eb91526dfaef37050ce1f5c6d88d56b5
Closes-bug: #1257922"
176,16d48ca756570ff9b10d5102544d1d5eb0020f29,1387591320,,1.0,0,174,1,1,1,0.0,True,1.0,455515.0,6.0,3.0,False,12.0,4104567.0,16.0,3.0,24.0,663.0,670.0,24.0,502.0,509.0,22.0,521.0,526.0,0.021395349,0.485581395,0.490232558,204,607,1262051,glance,16d48ca756570ff9b10d5102544d1d5eb0020f29,0,0,tests,"Bug #1262051 in Glance: ""Cleanup: tests.functional.store_utils setup_s3 and setup_swift unused""","The following methods contained in glance/tests/functional/store_utils.py  are not used anywhere:
. setup_swift,
. teardown_swift,
. get_swift_uri,
. setup_s3,
. teardown_s3,
. get_s3_uri
Let's get rid of it.","Cleanup: remove unused code from store_utils

store_utils.py contains several methods not used anywhere in the
codebase.
This patch removes the unused code.

Change-Id: I8faa87e1dcce053b19a37c19856eb53529e0ec78
Closes-Bug: #1262051"
177,16dfff5dedffcf4645df3c13b623d1ecd7560d8b,1386778295,,1.0,5,2,3,3,1,0.982141033,True,3.0,6356379.0,32.0,10.0,False,148.0,187286.0,600.0,3.0,4.0,1753.0,1756.0,4.0,1476.0,1479.0,1.0,1689.0,1689.0,0.000292783,0.247401552,0.247401552,155,557,1258275,nova,16dfff5dedffcf4645df3c13b623d1ecd7560d8b,1,1,,"Bug #1258275 in OpenStack Compute (nova): ""Migration record for resize not cleared if exception is thrown during the resize""","Testing on havana.
prep_resize() calls resource tracker's resize_claim() which creates a migration record. This record is cleared during the rt.drop_resize_claim() from confirm_resize() or revert_resize(), however if an exception is thrown before one of these is called or after, but before they clean up the migration record, then the migration record will hang around in the database indefinitely.
This results in an WARNING being logged every 60 seconds for every resize operation that ended with the instance in ERROR state as part of the update_available_resource period task, like the following:
2013-12-04 17:49:15.247 25592 WARNING nova.compute.resource_tracker [req-75e94365-1cca-4bca-92a7-19b2c62b9551 e4857f249aec4160bfa19c12eb805a96 a42cfb9766bf41869efab25703f5ce7b] [instance: 12d2551a-6403-4100-ba57-0995594c9c93] Instance not resizing, skipping migration.
This message is because the resource tracker's _update_usage_from_migrations() logs this warning if a migration record for an instance is found, but the instance's current state is not in a resize state.
These messages will be permanent in the logs even after the instance in question's state is reset, and even after a successful resize has occurred on that instance. There is no way to clean up the old migration record at this point.
It seems like there should be some handling when an exception occurs during resize, finish_resize, confirm_resize, revert_resize, etc. that will drop the resize claim, so the claim and migration record do not persist indefinitely.","Add error as not-in-progress migration status

When a migration status becomes ""error"" the migration is no longer
in progress, i.e. it is in a terminal state.  However, the
migration_get_in_progress_by_host_and_node method returns migrations
that have an ""error"" status, causing, among other things, the
Resource Tracker to continually log messages about how an
Instance is not resizing, creating excessive noise in the logs,
especially since the fact that a migration is entering the ""error""
status is already logged.  This change causes ""error""-status
migrations to not be included when retrieving ""in progress""
migrations.

Closes-Bug: #1258275

Change-Id: I67bfec9f91f89ff38422469a5d86e14c4fffa40b"
178,16e360d3a9183e9fd2b3dc163fe3e1ce4ec18e8b,1393495354,0.0,1.0,58,31,14,2,1,0.852626472,True,1.0,1656665.0,27.0,8.0,False,80.0,244572.7143,149.0,6.0,213.0,4595.0,4634.0,211.0,3612.0,3650.0,204.0,3921.0,3951.0,0.027483577,0.525807749,0.529829736,489,907,1285437,nova,16e360d3a9183e9fd2b3dc163fe3e1ce4ec18e8b,0,0,Improve messages,"Bug #1285437 in OpenStack Compute (nova): ""HTTPNotFound response does not contain enough explanation""","In each RESTful API, there are useful explanation messages for HTTPNotFound response in the implementation.
However, some messages are not output to the response.
For example, the implementation of the pause action API contains ""Server not found"" message for nonexistent server error, but now the message is not output into the response like the following:
$ curl -i 'http://localhost:8774/v2/<project-id>/servers/<nonexistent-server-id>/action' -X POST [..]"" -d '{""pause"": null}'
HTTP/1.1 404 Not Found
Content-Length: 78
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-a5282f5e-7e59-48cf-a86b-fe2e34347a2f
Date: Thu, 27 Feb 2014 10:21:52 GMT
{""itemNotFound"": {""message"": ""The resource could not be found."", ""code"": 404}}
$
When receiving the above message, client may consider ""what is not found? project, server, or action?""
so it is better to output right message into a response.","Fix the explanations of HTTPNotFound response

Nova wsgi framework gets ""explanation"" from HTTPNotFound exception and
outputs it into a response body as ""message"" attribute. Now some error
explanations are passed to HTTPNotFound exception without explanation
argument and they are not output to a response body. As the result,
the default message ""The resource could not be found."" of wsgi library
are output to a response body instead.

This patch fixes them to output right message to a response body.

Change-Id: I80d3a2da6258e94c485ce024eb310a63bcf39773
Closes-Bug: #1285437"
179,1715eb7c8e1f2433df3c081e357f8c40dfe2a28a,1402428049,,1.0,56,4,2,2,1,0.687315093,True,7.0,856694.0,118.0,28.0,False,29.0,3043458.0,43.0,1.0,42.0,43.0,79.0,38.0,42.0,75.0,32.0,40.0,66.0,0.026634383,0.033091203,0.054075868,864,1297,1316190,neutron,1715eb7c8e1f2433df3c081e357f8c40dfe2a28a,1,1,,"Bug #1316190 in neutron: ""When IPv6 addresses are added to the dnsmasq host file, dnsmasq stops issuing leases""","When an IPv6 subnet is created, with ipv6_ra_mode not set, and ipv6_address_mode set to ""slaac"" - ipv6 addresses are added to the host file that dnsmasq uses, and this causes dnsmasq to cease responding to dhcp clients on the v4 side.","Ensure entries in dnsmasq belong to a subnet using DHCP

In certain configurations, Neutron calculates SLAAC addresses for IPv6
subnets and adds them to the fixed_ips field of a port. Since those
subnets are not being managed by DHCP, do not add those fixed_ip entries
to the host file.

Closes-bug: #1316190
Related-bug: #1257446

Change-Id: I77dd55063296990c9df385f331f5de5d42402786"
180,1722378c85dbc9dc2506a2f8a84fef7a27254f57,1397768470,,1.0,8,36,10,8,1,0.794303756,True,6.0,103849.0,80.0,22.0,False,236.0,608091.9,908.0,4.0,15.0,4569.0,4573.0,13.0,3627.0,3629.0,15.0,3586.0,3590.0,0.002044206,0.458285422,0.458796474,810,1238,1309043,nova,1722378c85dbc9dc2506a2f8a84fef7a27254f57,0,0,Bug in test,"Bug #1309043 in OpenStack Compute (nova): ""NetworkCommandsTestCase unit test failing""","Change-Id I663bd06eb50872f16fc9889dde917277739fefce introduced a race condition where if another test doesn't properly reset the _IS_NEUTRON flag, it will fail because it will think that it is using Neutron and error out.","Remove utils.reset_is_neutron() to avoid races

If a unit test does not call utils.reset_is_neutron() when
it is complete, it may introduce a race condition because
the utils.is_neutron() value is cached to the old result.

This patch removes the utils.reset_is_neutron() and instead
makes sure that we never have a cached value for it.  It
also refactors _is_neutron out of vmwareapi.VMOps because
it cache the result unnecessarily.

Change-Id: I2440e308b00979b31c750513d14608808a295c33
Closes-Bug: #1309043"
181,17624e21f4812be80ca535a1f119127c021db54e,1393886822,,1.0,0,32,2,2,1,0.997180399,True,1.0,163409.0,19.0,4.0,False,32.0,724768.5,55.0,4.0,689.0,1354.0,1742.0,591.0,1141.0,1481.0,219.0,713.0,782.0,0.24691358,0.801346801,0.878787879,513,933,1287407,neutron,17624e21f4812be80ca535a1f119127c021db54e,0,0,cleanup,"Bug #1287407 in neutron: ""remove unused method update_fixed_ip_lease_expiration""",should have been removed long ago here: Ifcb4f093c92904ceb896438987d53e692eb7fb26,"Remove unused method update_fixed_ip_lease_expiration

This should have been log removed here:
	Ifcb4f093c92904ceb896438987d53e692eb7fb26

Change-Id: I9395fea531365b82a0cd16f29d6392199e2d9c22
Closes-bug: #1287407"
182,1763c80711993c55f4f13afe56f449b1dd6d3d3a,1382518478,2.0,2.0,72,9,21,19,1,0.839919932,True,22.0,15729244.0,414.0,87.0,False,43.0,278764.3333,99.0,8.0,136.0,1884.0,1900.0,136.0,1744.0,1760.0,95.0,420.0,426.0,0.198347107,0.869834711,0.882231405,163,565,1259144,neutron,1763c80711993c55f4f13afe56f449b1dd6d3d3a,0,0,Delete duplicated code,"Bug #1259144 in neutron: ""bigswitch: no need to duplicate check on network_delete""","Check if network is in use is not needed in bigswitch plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L587-L599
as it is done by db_base_plugin, which is called right after:
https://github.com/openstack/neutron/blob/master/neutron/plugins/bigswitch/plugin.py#L601","Delete disassociated floating ips on external network deletion

Also remove redundant check for network in use in bigswitch plugin

Closes-Bug: #1238439
Closes-Bug: #1259144

Change-Id: I7586f43c2e99be9df491c68bf1e8658994ffd263"
183,1763c80711993c55f4f13afe56f449b1dd6d3d3a,1382518478,2.0,2.0,72,9,21,19,1,0.839919932,True,22.0,15729244.0,414.0,87.0,False,43.0,278764.3333,99.0,8.0,136.0,1884.0,1900.0,136.0,1744.0,1760.0,95.0,420.0,426.0,0.198347107,0.869834711,0.882231405,1675,1238439,1238439,neutron,1763c80711993c55f4f13afe56f449b1dd6d3d3a,1,1, ,"Bug #1238439 in neutron: ""admin can not delete External Network because floatingip ""","HI
in admin role, I create External Network and router.  and create tenant A and userA.
Now the userA login, create network and router, create VM1 and assign Floating IP , access well ,perfectly.
Now I try to in admin roles  delete it.
1:  delete userA ,  no problem
2: delete tenantA ,  no problem
3: delete vm1, no problem
4: delete router, no problem
5: delete External Networknet,  report error,  I try to delete the port in sub panel, also fail.
check the Neutrion server log
TRACE neutron.api.v2.resource L3PortInUse: Port 2e5fa663-22e0-4c9e-87cc-e89c12eff955 has owner network:floatingip and therefore cannot be deleted directly via the port API.","Delete disassociated floating ips on external network deletion

Also remove redundant check for network in use in bigswitch plugin

Closes-Bug: #1238439
Closes-Bug: #1259144

Change-Id: I7586f43c2e99be9df491c68bf1e8658994ffd263"
184,1772a9116d127294354addd830e16877658bbee3,1387525045,,1.0,36,32,7,5,1,0.830256478,True,6.0,1569630.0,25.0,11.0,False,41.0,645727.4286,132.0,2.0,0.0,670.0,670.0,0.0,509.0,509.0,0.0,515.0,515.0,0.000931099,0.480446927,0.480446927,1800,1254210,1254210,glance,1772a9116d127294354addd830e16877658bbee3,1,1, ,"Bug #1254210 in Glance: ""remove, add and save in glance.db.__init__ should not return""",add and save method in ImageMemberRepo are returning a new object. It is expected to modify the values in-place rather than adding them in the new object.,"Remove return stmt of add,save and remove method

Remove the add and save method's return statement in the
ImageMemberRepo class, as the same as ImageRepo class.  Also
modify authorization.py and policy.py and the related unittest.

Closes-Bug: #1254210

Change-Id: I472cd15af8648beea10abc595e905618091f3dab"
185,17912f4c4fba270a15cab47bb68d9e30c34a69b0,1386259533,,1.0,2,3,2,2,1,0.970950594,True,4.0,1304157.0,19.0,9.0,False,66.0,7441.5,165.0,8.0,189.0,1594.0,1594.0,188.0,1350.0,1350.0,25.0,1062.0,1062.0,0.021207178,0.867047308,0.867047308,154,556,1258203,cinder,17912f4c4fba270a15cab47bb68d9e30c34a69b0,1,1,wrong check,"Bug #1258203 in Cinder: ""migrate_volume dest_vg existence is wrong""","if dest_vg != self.vg.vg_name:
        vg_list = volutils.get_all_volume_groups()
            vg_dict = \
                (vg for vg in vg_list if vg['name'] == self.vg.vg_name).next()
`vg['name'] == self.vg.vg_name` should be `vg['name'] == dest_vg`
https://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py?h=stable/havana#n703","LVM migration: Check if name is equal to dest_vg

The existence check of the destination volume group is wrong. It
currently checks if there's a VG with a name equal to the source's vg
name instead of checking if it's equal to the dest_vg.

Closes-bug: #1258203
Change-Id: Ia5d4acb24b94c6aa832107c7eb4b6996985af97f"
186,17b8cedeaa291f973e8311865b6560dc4806888d,1384769394,0.0,1.0,52,1,2,2,1,0.313812964,True,10.0,860399.0,76.0,21.0,False,3.0,2596778.0,5.0,5.0,68.0,727.0,769.0,66.0,658.0,698.0,50.0,237.0,262.0,0.092727273,0.432727273,0.478181818,1773,1252201,1252201,neutron,17b8cedeaa291f973e8311865b6560dc4806888d,1,1, ,"Bug #1252201 in neutron: ""fwaas service can't run in operation system without namespace feature""","Fwaas service can't run in operation system without namespace feature although use_namespaces equals False
$ grep -r ""^use_namespaces ="" /etc/neutron/l3_agent.ini
use_namespaces = False
Bellow is error log:
2013-11-18 14:52:09.782 INFO neutron.agent.l3_agent [-] L3 agent started
2013-11-18 14:52:14.553 ERROR neutron.services.firewall.agents.l3reference.firewall_l3_agent [-] FWaaS RPC info call failed for '770d54af-2bb6-4233-8a39-1d5ad36fea59'.
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 213, in process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     self._process_router_add(ri)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 193, in _process_router_add
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     ri.router['tenant_id'])
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 92, in _get_router_info_list_for_tenant
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     local_ns_list = root_ip.get_namespaces(self.root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 174, in get_namespaces
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     output = cls._execute('', 'netns', ('list',), root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     root_helper=root_helper)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/bak/openstack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent     raise RuntimeError(m)
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent RuntimeError:
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Exit code: 255
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stdout: ''
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent Stderr: 'Object ""netns"" is unknown, try ""ip help"".\n'
2013-11-18 14:52:14.553 TRACE neutron.services.firewall.agents.l3reference.firewall_l3_agent","Fwaas can't run in operating system without namespace feature

Adding the check before generating the local_ns_list
to fix the issue.

Change-Id: If8edb5c0bb0fc0fd9aaf48a3441287f99bcdcf13
Closes-Bug: #1252201"
187,17d131661740e452e9729d9ac8881e6fede23dd7,1383645882,1.0,1.0,12,7,2,2,1,0.629249224,True,8.0,12917730.0,63.0,33.0,False,135.0,23082.0,381.0,7.0,3.0,1014.0,1016.0,3.0,865.0,867.0,1.0,931.0,932.0,0.000306748,0.142944785,0.14309816,57,385,1248019,nova,17d131661740e452e9729d9ac8881e6fede23dd7,1,0,Issue with NFS,"Bug #1248019 in OpenStack Compute (nova): ""OSError occours when try to resize-confirm an instance with status 'VERIFY_RESIZE' using NFS bankend (KVM)""","when using at least two compute nodes using KVM, and use NFS share_storage to test resize an instance.
The configuration of NFS used the introduction about live-migration using NFS in community doc.
when executed command ""nova resize ae6f9472-3080-4e86-8a52-f8e642081d15"", can work well, and the instance's state will change to ""VERIFY_RESIZE',  Then I resize-confirm it, nova met the issue as follow:
{u'message': u""[Errno 39] Directory not empty: '/KVM/stack/data/nova/instances/ae6f9472-3080-4e86-8a52-f8e642081d15_resize'"", u'code': 500, u'details': u'  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 263, in decorated_function |
|                                      |     return function(self, context, *args, **kwargs)                                                                                                                                                                                                               |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2700, in confirm_resize                                                                                                                                                                   |
|                                      |     do_confirm_resize(context, instance, migration_id)                                                                                                                                                                                                            |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner                                                                                                                                                                  |
|                                      |     return f(*args, **kwargs)                                                                                                                                                                                                                                     |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2697, in do_confirm_resize                                                                                                                                                                |
|                                      |     migration=migration)                                                                                                                                                                                                                                          |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 2724, in _confirm_resize                                                                                                                                                                  |
|                                      |     network_info)                                                                                                                                                                                                                                                 |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4623, in confirm_migration                                                                                                                                                            |
|                                      |     self._cleanup_resize(instance, network_info)                                                                                                                                                                                                                  |
|                                      |   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1018, in _cleanup_resize                                                                                                                                                              |
|                                      |     shutil.rmtree(target)                                                                                                                                                                                                                                         |
|                                      |   File ""/usr/lib64/python2.6/shutil.py"", line 221, in rmtree                                                                                                                                                                                                      |
|                                      |     onerror(os.rmdir, path, sys.exc_info())                                                                                                                                                                                                                       |
|                                      |   File ""/usr/lib64/python2.6/shutil.py"", line 219, in rmtree                                                                                                                                                                                                      |
|                                      |     os.rmdir(path)                                                                                                                                                                                                                                                |
|                                      | ', u'created': u'2013-10-22T15:10:50Z'}
cd /KVM/stack/data/nova/instances/be962096-a539-46c7-ae66-9ea383809e9b_resize
[root@cc be962096-a539-46c7-ae66-9ea383809e9b_resize]# ls -al
total 24340
drwxr-xr-x  2 nobody nobody     4096 Oct 18  2013 .
drwxrwxrwx 14 root   root       4096 Oct 18  2013 ..
-rw-r--r--  1 nobody nobody 25034752 Oct 18  2013 .nfs000000000714002e00000001","Libvirt:Instance resize confirm issue against NFS

when I test resize-confirm on NFS (KVM) using at least two
compute nodes, OSError will occur when try to cleanup the
backup instance directory and will make instance status
""Error"".

One way to cleanup resize is using delay_on_retry and
maximum attempts setting. Replace rmtree() because
execute() have more parameters choices.

Closes-Bug: #1248019

Change-Id: Ifb9a500bbba805af0317307c2e7d6903dcd02ad1"
188,17d38f90d1ce28383a5fc061e01f0853cd8debf6,1398369061,,1.0,7,7,2,2,1,0.591672779,True,3.0,466916.0,55.0,16.0,False,16.0,541229.0,24.0,7.0,9.0,861.0,863.0,9.0,771.0,773.0,7.0,620.0,620.0,0.006779661,0.526271186,0.526271186,832,1261,1312392,neutron,17d38f90d1ce28383a5fc061e01f0853cd8debf6,1,1,,"Bug #1312392 in neutron: ""Wrong protocol value for SG IPV6 RA rule""","The ingress SG rule for RA has a wrong value for protocol
field.","Fix protocol value for SG IPV6 RA rule

The ingress SG rule for RA has a wrong value for protocol
field.

Change-Id: I4d3bca4b758540cf857eb13d36ee18f8ebc28272
Closes-Bug: #1312392"
189,180d686d2ff73d24c8259cf9784adef582befaef,1386190903,,1.0,9,9,2,1,1,0.503258335,True,1.0,145822.0,8.0,2.0,False,120.0,282655.5,289.0,2.0,1.0,2379.0,2380.0,1.0,1800.0,1801.0,0.0,2135.0,2135.0,0.000147427,0.314904909,0.314904909,100,499,1255914,nova,180d686d2ff73d24c8259cf9784adef582befaef,0,0,Typo in comments,"Bug #1255914 in OpenStack Compute (nova): ""Misc typos in Openstack Compute (Nova)""","Typos in few files like
1. nova/availability_zones.py +100
Return available and unavailable zones on demands -> Return available and unavailable zones on demand.
2. nova/exception.py +578
Either Network uuid %(network_uuid)s is not present -> Either network uuid %(network_uuid)s is not present
3. nova/exception.py +944
Instance Type %(instance_type_id)s has no extra specs with -> Instance type %(instance_type_id)s has no extra specs with
4. novaclient/base.py +108, 122
typicaly -> typically
And few other","Misc typos in nova

1. on demands -> on demand
2. Either Network -> Either network
3. unexpected task -> Unexpected task
4. PCI Device -> PCI device
5. key manager -> Key manager

Change-Id: I5a3cb64b38e8851902492c15fda7f63c4fa4f7f0
Closes-Bug: #1255914"
190,185f09b7c46edcbb1d39d3b79993fa8c07dafc2c,1406833734,,1.0,2,12,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1200,1658,1357236,neutron,185f09b7c46edcbb1d39d3b79993fa8c07dafc2c,0,0,Refactoring “Also removed custom dispatch”,"Bug #1357236 in neutron: ""Neutron creates oslo.messaging.Server object directly""",oslo.messaging provides a factory method to create a Server object. It should be used to create it. Additionaly Neutron uses custom RPCDispatcher to log incoming messages. This duplicates existing functionality in oslo.messaging.,"Removed direct access to MessagingServer

Factory method provided by the library should be used to create a server
object. This allows to change servers internal implementation without
touching end users.

Also removed custom dispatcher. Its only purpose was to log messages.
If logging is configured accordingly all incoming messages will be
logged from oslo/messaging/_drivers/amqpdriver.py [1]

[1] http://goo.gl/nV9tcu

Closes-Bug: #1357236

Change-Id: Ic208994c5a64fd48528cb41d30a975d68d84af05"
191,186221779a92002ff9fa13c254710c0abb3803be,1387411904,,1.0,29,5,2,2,1,0.977417818,True,2.0,3832639.0,27.0,15.0,False,15.0,326107.0,41.0,5.0,197.0,1300.0,1336.0,192.0,1132.0,1167.0,177.0,1055.0,1082.0,0.140600316,0.834123223,0.855450237,211,614,1262880,cinder,186221779a92002ff9fa13c254710c0abb3803be,1,1,Using wrong value,"Bug #1262880 in Cinder: ""GlusterFS: Use correct base argument when deleting attached snaps""","The base= field is not using the correct value when a libvirt snapshot_delete is issued on the Nova side -- pass correct values for ""file_to_merge"" in the delete_info dict depending on whether or not other snapshots exist.
This fixes an issue where attached snapshot deletion results in an inconsistent qcow2 chain.","GlusterFS: Use correct base argument when deleting attached snaps

When deleting the most recent snapshot, the 'file_to_merge' field
which translates into the base= field for libvirt's blockRebase
call in Nova must be set depending on whether other snapshots exist.

If there are no other snapshots, base = None, which results in
libvirt clearing the qcow2 backing file pointer for the active
disk image.

If there are other snapshots, pass the parent of the file being
deleted as the new base file.  The snapshot info pointer for the
prior base file must also be updated in this case.

Closes-Bug: #1262880
Change-Id: If7bc8259b031d0406346caafb8f688e65a38dba6"
192,1864939720c94b602699f69002c6467fda247deb,1388116285,,1.0,1,1,1,1,1,0.0,True,1.0,87529.0,9.0,2.0,False,34.0,90858.0,97.0,2.0,11.0,230.0,230.0,11.0,229.0,229.0,11.0,197.0,197.0,0.011019284,0.181818182,0.181818182,233,636,1264428,glance,1864939720c94b602699f69002c6467fda247deb,0,0,tests,"Bug #1264428 in Glance: ""Incorrect URL requested in Glance v1 API test_get_images_unauthorized test""","The test code in glance/tests/unit/v1/test_api.py:TestGlanceAPI.test_get_images_unauthorized requests for the wrong URL - instead of requesting for ""/images"" it requests for ""/images/detail"" (making it identical to the test_get_images_unauthorized test).","Correct URL in v1 test_get_images_unauthorized

The test code in
unit/v1/test_api.py:TestGlanceAPI.test_get_images_unauthorized
requests for the wrong URL - instead of requesting for ""/images"" it
requests for ""/images/detail"" (making it identical to the
test_get_images_unauthorized test).

Change-Id: I308e73966bcbb27287012d7b5dcdcc9d2a9f2e18
Closes-Bug: #1264428"
193,186c3e99cf04b22ddbb1046cc9e42d1f7473e8cf,1407453709,,1.0,1,7,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1173,1630,1355297,cinder,186c3e99cf04b22ddbb1046cc9e42d1f7473e8cf,1,1,,"Bug #1355297 in Cinder: ""manage.py is unusable""","cinder/db/sqlalchemy/migrate_repo/manage.py is unusable
It's not possible to use manage.py in its current state due to an exception thrown by oslo.config:
  oslo.config.cfg.ArgsAlreadyParsedError:
    arguments already parsed: cannot register CLI option
This exception is a side-effect of including cinder.openstack.common.logging in most of the migration scripts.","Make manage.py usable

It's not possible to use manage.py in its current state due to
an exception thrown by oslo.config:

  oslo.config.cfg.ArgsAlreadyParsedError:
    arguments already parsed: cannot register CLI option

This exception is a side-effect of including
cinder.openstack.common.logging in most of the migration scripts.

This change therefore suggests removing oslo.config and other imports
from manage.py to make it usable again.

It is now possible to use manage.py:

  tox -evenv -- python cinder/db/sqlalchemy/migrate_repo/manage.py \
    version_control --url=sqlite:///./cinder.db

  tox -evenv -- python cinder/db/sqlalchemy/migrate_repo/manage.py \
    upgrade --url=sqlite:///./cinder.db

Closes-bug: #1355297
Change-Id: I843cfa9a4e4758a6bc8e5c04f064e9d3107b01fa"
194,187140c2fcafb56752f0160832607e374a4a94ff,1384443855,,1.0,1,108,2,2,1,0.66747422,True,11.0,1999897.0,59.0,24.0,False,49.0,1775822.0,89.0,3.0,41.0,1365.0,1389.0,40.0,1201.0,1224.0,32.0,1298.0,1317.0,0.005008347,0.19714676,0.200030354,47,375,1247217,nova,187140c2fcafb56752f0160832607e374a4a94ff,1,1,,"Bug #1247217 in OpenStack Compute (nova): ""Sanitize passwords when logging payload in wsgi for API Extensions""","The fix for bug 1231263 ( https://bugs.launchpad.net/nova/+bug/1231263 ) addressed not logging the clear-text password in the nova wsgi.py module for the adminPass attribute for the Server Change Password REST API, but this only addressed that specific attribute.  Since Nova has support for the ability to add REST API Extensions (in the contrib directory), there could any number of other password-related attributes in the request/response body for those additional extensions.
Although it would not be possible to know all of the various sensitive attributes that these API's would pass in the request/response (the only way to totally eliminate the exposure would be to not log the request/response which is useful for debugging), I would like to propose a change similar to the one that was made in keystone (under https://bugs.launchpad.net/keystone/+bug/1166697) to mask the password in the log statement for any attribute that contains the ""password"" sub-string in it.
The change would in essence be to update the _SANITIZE_KEYS / _SANITIZE_PATTERNS lists in the nova/api/openstack/wsgi.py module to include a pattern for the ""password"" sub-string.
Also, for a slight performance benefit, it may be useful to put a check in to see if debug logging level is enabled around the debug statement that does the sanitize call (since the request/response bodies could be fairly large and wouldn't want to take the hit to do the pattern matches if debug isn't on).","Use password masking utility provided in Oslo

Implement the mask_password function provided in Oslo-incubator's
log.py. Instead of having a specific version in Nova different from
other projects that are essentially doing the same thing.

Change-Id: I7e04b7d31d4d6959b17b1da9654553042eec70f1
Closes-Bug: #1247217"
195,18917d9e0d6fe250cc3b4f7301d37a6c5f5faffb,1396788863,,1.0,66,11,4,2,1,0.65109287,True,11.0,2687107.0,251.0,46.0,False,47.0,2491538.5,118.0,6.0,1892.0,1334.0,2765.0,1568.0,1315.0,2423.0,988.0,1303.0,1831.0,0.127711777,0.16838843,0.236570248,735,1162,1302238,nova,18917d9e0d6fe250cc3b4f7301d37a6c5f5faffb,1,0,"""After https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension,""","Bug #1302238 in OpenStack Compute (nova): ""throw exception if no configured affinity filter""","After https://blueprints.launchpad.net/nova/+spec/instance-group-api-extension, nova has the feature of creating instance groups with affinity or anti-affinity policy and creating vm instance with affinity/anti-affinity  group.
If did not enable ServerGroupAffinityFilter and ServerGroupAntiAffinityFilter, then the instance group will not able to leverage affinity/anti-affinity.
Take the following case:
1) Create a group with affinity
2) Create two vms with this group
3) The result is that those two vms was not created on the same host.
We should  throw exception if using server group with no configured affinity filter","Scheduler: throw exception if no configured affinity filter

If the scheduler hint indicates that the scheduling should perform
either anti-affinity or affinity scheduling and the relevant filter
is not configured then a NoValidHost exception will be thrown.

This is valuable if an existing OpenStack installation is running
and these filters are not defined after an upgrade.

Change-Id: I79bb44ad7481b3ff924687a8d6afdd6c715c0b59
Closes-bug: #1302238"
196,18938d283c5f87ab2e55f9f75626f313e8724bf9,1398775086,,1.0,11,7,5,5,1,0.80810325,True,2.0,570204.0,31.0,7.0,False,20.0,6932.0,37.0,3.0,138.0,3272.0,3281.0,138.0,2601.0,2610.0,135.0,3164.0,3171.0,0.017210833,0.400531511,0.401417363,846,1276,1314176,nova,18938d283c5f87ab2e55f9f75626f313e8724bf9,0,0,"Feature ""We need to improve the hacking rule to avoid markers of author for the tag:”","Bug #1314176 in OpenStack Compute (nova): ""Improve hacking rule for author tag""","We need to improve the hacking rule to avoid markers of author for the tag:
'.. moduleauthor'","Improve hacking rule to avoid author markers

The case with the tag ""moduleauthor"" is not handled when
checking to avoid author makers.
Also fix the position when the tag Author is used.

Change-Id: I17b07b9f5448f9cbb4c5705f655c5392316a6239
Closes-Bug: #1314176"
197,18a1486293b15c5713ac1352b9b9b30fdf2b7413,1385507990,,1.0,33,6,2,2,1,0.73206669,True,9.0,14860132.0,86.0,26.0,False,26.0,4113235.5,38.0,3.0,25.0,2164.0,2180.0,21.0,1809.0,1823.0,19.0,2068.0,2078.0,0.002979738,0.308253874,0.309743743,91,489,1255347,nova,18a1486293b15c5713ac1352b9b9b30fdf2b7413,1,1,,"Bug #1255347 in OpenStack Compute (nova): ""cinder cross_az_attach uses instance AZ value""","When checking if an instance is in the same AZ as a volume nova uses the instances availability_zone attribute. This isn't the correct way to get an instances AZ, it should use the value gotten through querying the aggregate the instance is on","Fix instance cross AZ check when attaching volumes

For CONF.cinder_cross_az_attach=False, we currently only check the desired AZ
during scheduling but don't actually check the final AZ of the node.

When attaching volumes to running instances we need to check the actual
AZ on the instance and not the desired AZ at boot time. The desired AZ
attribute is optional at boot and may not be set or the desired AZ
could be different from the final AZ.
When we are booting with a volume however we don't know the final AZ so we
check on the desired AZ.

Closes-Bug: #1255347
Change-Id: Ied912c171100a450754d315027bf5c407ac067bb"
198,18b4df178b7d708848cad9de7f9073f8fa7f20f9,1393368491,,1.0,10,621,8,4,1,0.625270788,True,2.0,56755.0,12.0,2.0,False,7.0,666220.0,8.0,2.0,55.0,308.0,314.0,55.0,307.0,313.0,49.0,271.0,273.0,0.042698548,0.232280102,0.233988044,446,861,1282715,glance,18b4df178b7d708848cad9de7f9073f8fa7f20f9,0,0,No bug. ‘VMware datastore should use oslo.vmware common code ‘,"Bug #1282715 in Glance: ""VMware datastore should use oslo.vmware common code""",The git repository oslo.vmware is now available: https://github.com/openstack/oslo.vmware/. It is time to consume it and get rid of the vmware folder in the glance folder.,"VMware storage backend should use oslo.vmware

Currently, the VMware store is using its own copy of the
VMwareApiSession to connect to vCenter server and ESX(i).
This patch gets rid of this copy to use the oslo.vmware library.

Closes-Bug: #1282715

Change-Id: I0aa47eada388c09d9835b00fb2c93f50f22675a4"
199,18f417c14a9313d90918285b22263623ba0c7e22,1378796348,,1.0,24,10,5,5,1,0.89658022,True,7.0,1624647.0,37.0,13.0,False,140.0,146856.2,566.0,3.0,214.0,805.0,914.0,214.0,800.0,909.0,126.0,788.0,809.0,0.021233907,0.13191774,0.135428858,1533,1223205,1223205,nova,18f417c14a9313d90918285b22263623ba0c7e22,1,1, ,"Bug #1223205 in OpenStack Compute (nova): ""get console failed because can't load attribute 'pool'""","Try get console as below:
curl -i http://cloudcontroller:8774/v3/servers/570a0058-3094-4201-b313-a337a984f773/consoles -X GET
Then get error in nova-api as below:
2013-09-10 14:59:46.107 ERROR nova.api.openstack.extensions [req-8f981cd0-96ef-4c95-b2b5-2d42a666deed admin admin] Unexpected exception in API method
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 98, in index
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     for console in consoles])
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/consoles.py"", line 30, in _translate_keys
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     pool = cons['pool']
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/openstack/common/db/sqlalchemy/models.py"", line 59, in __getitem__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return getattr(self, key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 168, in __get__
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     return self.impl.get(instance_state(instance),dict_)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/attributes.py"", line 453, in get
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     value = self.callable_(state, passive)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/strategies.py"", line 481, in _load_for_state
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions     (mapperutil.state_str(state), self.key)
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions DetachedInstanceError: Parent instance <Console at 0x635d5d0> is not bound to a Session; lazy load operation of attribute 'pool' cannot proceed
2013-09-10 14:59:46.107 TRACE nova.api.openstack.extensions","Fix console db can't load attribute pool

Lists consoles failed when populate console_type from console
db's pool attribute. The reason was db api console_get_all_by_instance
didn't join with pool. So add param 'columns_to_join' for db api, and
join pool attribute when lists consoles.

Change-Id: I18b44860270a401c55cf02b4334b973fbf357b03
Closes-bug: #1223205"
200,193096a476ca3da2ef7f36f89fdbd897c5bcc320,1389797101,,1.0,8,42,2,2,1,0.881290899,True,5.0,10085377.0,42.0,18.0,False,13.0,1026028.0,32.0,7.0,143.0,875.0,890.0,143.0,827.0,842.0,67.0,753.0,757.0,0.051204819,0.567771084,0.570783133,291,698,1269445,cinder,193096a476ca3da2ef7f36f89fdbd897c5bcc320,1,0,Check newer version,"Bug #1269445 in Cinder: ""Issues in volume creation on unsupported skipactivation""","Cinder volume service traces with the following error when lvchange is not in $PATH. As lvchange is in /sbin on SLE11, and /sbin is not part of the path for non-root, the test aborts with this trace:
2014-01-15 13:55:39.292 9325 ERROR cinder.volume.flows.create_volume [req-04565125-c12a-4081-b7dd-93fe0afc7e17 a0f6c8d7d68241f3bd6d1768775dc239 9d341e8aeb794086894e49e132766cd0] Unexpected build error:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume Traceback (most recent call last):
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     result = runner(context, *args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     self.result = self.task(*args, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1499, in __call__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     **volume_spec)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1300, in _create_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     snapshot_ref)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/volume/drivers/lvm.py"", line 176, in create_volume_from_snapshot
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     self.vg.activate_lv(snapshot['name'], is_snapshot=True)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 487, in activate_lv
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     if self.supports_lvchange_ignoreskipactivation:
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 190, in supports_lvchange_ignoreskipactivation
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     (out, err) = self._execute(*cmd)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     return processutils.execute(*cmd, **kwargs)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 158, in execute
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     shell=shell)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/site-packages/eventlet/green/subprocess.py"", line 44, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     subprocess_orig.Popen.__init__(self, args, 0, *argss, **kwds)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/subprocess.py"", line 623, in __init__
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     errread, errwrite)
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume   File ""/usr/lib64/python2.6/subprocess.py"", line 1141, in _execute_child
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume     raise child_exception
2014-01-15 13:55:39.292 9325 TRACE cinder.volume.flows.create_volume OSError: [Errno 13] Permission denied
Solution/workaround is to run it via the root wrapper, which has /sbin in its trusted paths.","LVM: Robustify skipactivation detection

Running lvchange might fail due to not being in
$PATH for non-root user, and the corresponding OSError
was not caught.

Simply check for lvm2 being 2.02.99 or newer.

Closes-Bug: #1269445
Change-Id: I308bd97cb729e83677f18a693274265a182d794c"
201,193ad7994d536f644d73849512b574d4f5ee7546,1378312608,,1.0,17,17,1,1,1,0.0,True,2.0,223104.0,8.0,3.0,False,41.0,99021.0,63.0,3.0,36.0,570.0,602.0,36.0,553.0,585.0,5.0,529.0,530.0,0.006302521,0.556722689,0.557773109,1510,1220813,1220813,cinder,193ad7994d536f644d73849512b574d4f5ee7546,1,1,"Changed requirements333, the commit before the change was done in may 2012 and the bug report in September 2013","Bug #1220813 in Cinder: ""inconsistent i18n message""","in cinder/exception.py: https://github.com/openstack/cinder/blob/master/cinder/exception.py#L242
message = _(""Invalid metadata"") + "": %(reason)s""
is not consistent i18n usage. In most of cases, we use _() for whole string instead of leaving something out.
More than inconsistent, some text in leaving strings may be not translated, for example, ':' in English is different '：' in Chinese.","fix inconsistent i18n log message

An inconsistent example:
    imessage = _(""Invalid metadata"") + "": %(reason)s""
This is not consistent i18n usage. In most of cases, we use _() for
whole string instead of leaving something out.

More than inconsistent, some text in leaving strings may be not
translated, for example, ':' in English is different '：' in Chinese.

Change-Id: I00e690021c44f1debbe034cc0837a07b292ffd1b
Closes-Bug: #1220813"
202,1957339df302e2da75e0dbe78b5d566194ab2c08,1382740321,2.0,1.0,66,5,3,3,1,0.592076302,True,4.0,12666639.0,32.0,21.0,False,27.0,985902.6667,34.0,6.0,527.0,3497.0,3760.0,496.0,3140.0,3391.0,151.0,2565.0,2605.0,0.02359882,0.398385344,0.40459556,1541,1223859,1223859,nova,1957339df302e2da75e0dbe78b5d566194ab2c08,1,1, ,"Bug #1223859 in OpenStack Compute (nova): ""Network cache not correctly updated during ""interface-attach""""","The network cache is not correctly updated when running ""nova interface-attach"": only the latest allocated IP is used. See this log:
http://paste.openstack.org/show/46643/
Nevermind the error reported when running ""nova interface-attach"": I believe it is an unrelated issue, and I'll write another bug report for it.
I noticed this issue a few months ago, but haven't had time to work on it. I'll try and submit a patch ASAP. See my analysis of the issue here: https://bugs.launchpad.net/nova/+bug/1197192/comments/3","Fix interface-attach removes existing interfaces from db

The following commit 394c693e359ed4f19cc2f7d975b1f9ee5500b7f6 changed
allocate_port_for_instance() to only return the ports that were created
rather than all of the ports on the instance which broke the attach-interface
code.

This patch fixes this issue by remove the sync decorators from:
allocate_for_instance, allocate_port_for_instance, and
deallocate_port_for_instance to just _build_instance_nw_info which
is called in all these cases instead.

Closes-bug: #1223859

Change-Id: I66eb0c0ab926e0a8d1e2c9cfe1f7fd579ea3aa27"
203,1959092bca5646306cfec5303f43627e78f6e2d4,1393932431,1.0,1.0,113,126,2,2,1,0.897519818,True,7.0,1188335.0,125.0,16.0,False,6.0,7751950.5,9.0,3.0,36.0,819.0,822.0,36.0,672.0,675.0,28.0,405.0,408.0,0.032329989,0.452619844,0.455964326,536,957,1288441,neutron,1959092bca5646306cfec5303f43627e78f6e2d4,0,0,tests,"Bug #1288441 in neutron: ""FWaaS tests don't delete firewalls""","When the tests for FWaaS create a firewall and then delete it, the firewall object is still in the database in a PENDING_DELETE state waiting for the agent to remove it. Parent objects then can't be deleted because the firewall object is dependent on them.
Currently, all of the tests just leave the parent objects by using no_delete[1], which leaves things in the database and could lead to potential conflicts later.
1. https://github.com/openstack/neutron/blob/ac8c0c645de001a0d074cdfd9448f9680a5d5e34/neutron/tests/unit/db/firewall/test_db_firewall.py#L732","Mock agent RPC for FWaaS tests to delete DB objs

This changes the firewall service unit tests to
mock the RPC calls from the firewall service to
the agent. This allows the tests to fake the
agent response RPC that removes the firewall
from the DB so the all of the other objects that
the firewall depends on can be deleted.

Closes-Bug: #1288441
Change-Id: I32462ab5557b9c52328bf6a23a12efc2d799644c"
204,1967cee5abb71a23bccd440da9a27309a8d67081,1379018320,,1.0,12,1,2,2,1,0.779349837,True,4.0,2151568.0,18.0,5.0,False,176.0,42057.0,935.0,4.0,47.0,3276.0,3291.0,47.0,2687.0,2702.0,47.0,2983.0,2998.0,0.007964161,0.495105359,0.49759416,1549,1224677,1224677,nova,1967cee5abb71a23bccd440da9a27309a8d67081,1,1, ,"Bug #1224677 in OpenStack Compute (nova): ""_default_block_device_names() throws unhandled exception for instances deleted during build""","During instance build, there is a call to _default_block_device_names() that contains a db call to update the instance. If the instance is deleted before this call it results in a InstanceNotFound exception that goes unhandled and ends up in the compute log. Since this is an expected error, it should be handled correctly.","Move call to _default_block_device_names() inside try block

During instance builds, a call to _default_block_device_names() is
made which contains a db call to update the instance. In case the
instance is gone it will throw an unhandled InstanceNotFound exception.

To properly handle this, move the call to _default_block_device_names()
inside the existing try block next to it that handles this exception.

Change-Id: I3503029f02669e6ae31441653096fa69cba76d45
Closes-Bug: #1224677"
205,197bb467c0fa33700e5397c934fa10d8c16f1fbc,1411247989,,1.0,31,1,3,3,1,0.639824384,False,,,,,True,,,,,,,,,,,,,,,,,1310,1776,1368989,nova,197bb467c0fa33700e5397c934fa10d8c16f1fbc,1,1,Bug It is reporting some bug or a consecutive list of actions that should happen and they can end in a bug (78dbed87b53ad3e60dc00f6c077a23506d228b6c),"Bug #1368989 in OpenStack Compute (nova): ""service_update() should not set an RPC timeout longer than service.report_interval""","nova.servicegroup.drivers.db.DbDriver._report_state() is called every service.report_interval seconds from a timer in order to periodically report the service state.  It calls self.conductor_api.service_update().
If this ends up calling nova.conductor.rpcapi.ConductorAPI.service_update(), it will do an RPC call() to nova-conductor.
If anything happens to the RPC server (failover, switchover, etc.) by default the RPC code will wait 60 seconds for a response (blocking the timer-based calling of _report_state() in the meantime).  This is long enough to cause the status in the database to get old enough that other services consider this service to be ""down"".
Arguably, since we're going to call service_update( ) again in service.report_interval seconds there's no reason to wait the full 60 seconds.  Instead, it would make sense to set the RPC timeout for the service_update() call to to something slightly less than service.report_interval seconds.
I've also submitted a related bug report (https://bugs.launchpad.net/bugs/1368917) to improve RPC loss of connection in general, but I expect that'll take a while to deal with while this particular case can be handled much more easily.","Use reasonable timeout for rpc service_update()

nova.servicegroup.drivers.db.DbDriver._report_state() is called every
service.report_interval seconds from a timer in order to periodically
report the service state. It calls self.conductor_api.service_update().

If this ends up calling nova.conductor.rpcapi.ConductorAPI.service_update(),
it will do an RPC call() to nova-conductor.

If anything happens which causes the RPC reply to be lost or
never sent in the first place, by default the RPC code will
wait 60 seconds for a response (blocking the timer-based
calling of _report_state() in the meantime).
This is long enough to cause the status in the database to get old
enough that other services consider this service to be ""down"".

if rpc_reponse_timeout is smaller than report_interval then we could
use the existing RPC timeout, but wait longer won't hurt. So the patch
didn't check it and only use report_interval.

Change-Id: I88743183bce1a534812cfe6110c3fc2892058c53
Closes-Bug: #1368989"
206,1a0be0e05f8d299820b60cd888d74ed5ed73e265,1406973194,,1.0,25,40,17,15,1,0.884116874,False,,,,,True,,,,,,,,,,,,,,,,,1132,1587,1350937,neutron,1a0be0e05f8d299820b60cd888d74ed5ed73e265,1,1,(1d8afc7593fa04b8f804e13711ee36ea5bda0957)“Move from Python logging to Openstack loggingReplacing usage of python standard logging module with Openstack common logging module.”,"Bug #1350937 in neutron: ""neutron should use openstack logging everywhere""","In several places in the source tree we imports logging
https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L16
but instead it should do:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L52
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L63
arosen@arosen-desktop:/opt/stack/neutron/neutron$ grep -R ""import logging"" *
common/utils.py:import logging as std_logging
db/migration/alembic_migrations/heal_script.py:import logging
plugins/cisco/models/virt_phy_sw_v2.py:import logging
plugins/cisco/common/cisco_credentials_v2.py:import logging as LOG
plugins/cisco/network_plugin.py:import logging
plugins/cisco/nexus/cisco_nexus_snippets.py:import logging
plugins/cisco/nexus/cisco_nexus_plugin_v2.py:import logging
plugins/cisco/nexus/cisco_nexus_network_driver_v2.py:import logging
plugins/ml2/drivers/cisco/nexus/nexus_snippets.py:import logging
plugins/vmware/plugins/base.py:import logging
service.py:import logging as std_logging
tests/base.py:import logging
tests/unit/cisco/test_network_plugin.py:import logging
tests/unit/db/metering/test_db_metering.py:import logging
tests/unit/db/firewall/test_db_firewall.py:import logging
tests/unit/db/loadbalancer/test_db_loadbalancer.py:import logging
tests/unit/ml2/test_helpers.py:import logging
tests/unit/test_servicetype.py:import logging
tests/unit/vmware/apiclient/test_api_eventlet_request.py:import logging","Move from Python logging to Openstack logging

Replacing usage of python standard logging module
with Openstack common logging module. Apart from
the said replacements, this patch also removes
basicConfig() setup from a couple of modules since
its not needed. Also removes unused LOG & imports.

Change-Id: I6a391951e00fb63905b2027270af9f401841d5b9
Closes-Bug: #1350937"
207,1a17d1719d163a360dfd02f9b124f8dc55a5fd0a,1410973945,,1.0,15,1,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1329,1797,1370578,nova,1a17d1719d163a360dfd02f9b124f8dc55a5fd0a,1,1,,"Bug #1370578 in OpenStack Compute (nova): ""Ironic Hostmanager does not pass hypervisor_type to filters""","The Ironic Hostmager does not include the compute node hypervisor values such as type, version,  hostname.
Including these values, which are included by the normal HostManager, is needed to allow the capabilities filter to work in a combined Ironic / virt Nova","Add support for hypervisor type in IronicHostManager

Unlike the default HostManager the IronicHostManager doesn't pass
hypervisor attributes of the compute_node into the HostState.

Adding support for hypervisor_type makes it easy to use the
compute_capabilities filter in a combined Ironic/virt system
with a simple extra_specs value on flavors.

For completeness the following values are added:
hypervisor_type
hypervisor_version
hypervisor_version
cpu_info
supported_instances

Change-Id: I1c81bb9726597609c4e5918eb15b4f2a055e7912
Closes-Bug: #1370578"
208,1a8dd2487d95327b97ebf8e8b4a27420d1432fd4,1394202578,0.0,1.0,82,854,7,3,1,0.676086617,True,5.0,3444808.0,83.0,12.0,False,159.0,1246907.857,658.0,4.0,380.0,3741.0,3888.0,379.0,2973.0,3120.0,362.0,3561.0,3693.0,0.048162399,0.472601831,0.490115431,408,822,1280055,nova,1a8dd2487d95327b97ebf8e8b4a27420d1432fd4,0,0,Remove duplicate code,"Bug #1280055 in OpenStack Compute (nova): ""Replace nova.db.sqlalchemy.utils with openstack.common.db.sqlalchemy.utils""","Most of the code in nova.db.sqlalchemy.utils is also in oslo-incubator.openstack.common.db.sqlalchemy.utils, except for the modify_indexes method which is not actually even used in the nova db migration code anymore now that it's been compacted in icehouse.
Also, the oslo.db code has been getting synced over to nova more frequently lately so rather than keep all of this duplicate code around we should move nova to using the oslo utils code and drop the internal nova one, with maybe moving the modify_indexes method to oslo first, then sync back to nova and then drop nova.db.sqlalchemy.utils from nova.
We will have to make sure that there are no behavior differences in the oslo code such that it would change the nova db schema, but we should be able to use Dan Prince's nova/tools/db/schema_diff.py script to validate that.","Remove duplicate code from nova.db.sqlalchemy.utils

There is a lot of code in nova.db.sqlalchemy.utils which is also in
oslo-incubator's openstack.common.db.sqlalchemy.utils module so this
patch goes through and cleans up Nova.

Notable differences:

1. modify_indexes was nova-only but no longer used after the migration
   compaction work, so it's just removed here. It also didn't really
   work for mysql in cases where the index name was too long so it
   could be table-specific and therefore unusable in some mysql cases.
   See commit 029ebab for history.
2. create_shadow_table was not in oslo-incubator and it's also not
   currently used in nova after the migration compaction, however,
   I leave it here in case future migrations in Nova need to use it.
   This patch does replace the usage of _get_not_supported_column in
   create_shadow_table to use the same method from oslo's DB utils.
3. DeleteFromSelect is still used within nova.db.api per
   commit b36826e so we could move it to oslo-incubator but this
   patch leaves it for now since it's only used in Nova.
4. InsertFromSelect was introduced with commit 2e403b2 but now there
   is a copy in oslo-incubator so we can remove our usage in nova.
   However, oslo doesn't have the unit test that nova does, so the
   unit test has been moved to oslo with change I457acf33.
5. Oslo has is_backend_avail and get_connect_string methods so use
   those in test_migrations.py while doing this update.

Closes-Bug: #1280055

Change-Id: Iefa5b4311f1fe1a5da31cf527521c393f2face7c"
209,1a9f95329f269673efffb35ca403292f7d404333,1389771667,,1.0,6,4,2,2,1,0.721928095,True,1.0,509050.0,7.0,2.0,False,16.0,56674.0,22.0,2.0,66.0,250.0,312.0,66.0,249.0,311.0,4.0,217.0,217.0,0.004484305,0.195515695,0.195515695,227,630,1264193,glance,1a9f95329f269673efffb35ca403292f7d404333,1,1,Fix inconsistent doc string and code,"Bug #1264193 in Glance: ""wrong doc string for glance.db.sqlalchemy.migration.db_sync""","When review patch: https://review.openstack.org/#/c/64076/
I found that:
def db_sync(version=None, current_version=None):
    """"""
Place a database under migration control and perform an upgrade
:retval version number
""""""
    sql_connection = CONF.sql_connection
    try:
        _version_control(current_version or 0)
    except versioning_exceptions.DatabaseAlreadyControlledError as e:
        pass
    if current_version is None:
        current_version = int(db_version())
    if version is not None and int(version) < current_version:
        downgrade(version=version)
    elif version is None or int(version) > current_version:
        upgrade(version=version)
db_sync doesn't return anything, the doc string and code are broken. Since db_sync() accept empty arguments list, so a reasonable version number should be returned instead of remove the incorrect doc string.","Fix inconsistent doc string and code of db_sync

db_sync may downgrade but current doc string only says
upgrade, and it says having return value which seems not
true.

Change-Id: Ib7bbb597f621985aba956302034eb1c5d0b3ff1f
Closes-Bug: #1264193"
210,1aef7b6304af6c19b8ceb81cbd9a3c83bb4880de,1380650714,1.0,1.0,49,1,2,2,1,0.40217919,True,3.0,109662.0,31.0,7.0,False,14.0,180196.0,34.0,4.0,964.0,1165.0,1165.0,851.0,1051.0,1051.0,829.0,1010.0,1010.0,0.783018868,0.953773585,0.953773585,1563,1226337,1226337,cinder,1aef7b6304af6c19b8ceb81cbd9a3c83bb4880de,1,1, ,"Bug #1226337 in Cinder: ""tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern 'qemu-nbd: Failed to bdrv_open'""","When running tempest tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern fails with the server going into an ERROR state.
From the console log:
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 154, in test_volume_boot_pattern
keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 53, in _boot_instance_from_volume
create_kwargs=create_kwargs)
  File ""tempest/scenario/manager.py"", line 390, in create_server
self.status_timeout(client.servers, server.id, 'ACTIVE')
  File ""tempest/scenario/manager.py"", line 290, in status_timeout
self._status_timeout(things, thing_id, expected_status=expected_status)
  File ""tempest/scenario/manager.py"", line 338, in _status_timeout
self.config.compute.build_interval):
  File ""tempest/test.py"", line 237, in call_until_true
if func():
  File ""tempest/scenario/manager.py"", line 329, in check_status
raise exceptions.BuildErrorException(message)
  BuildErrorException: Server %(server_id)s failed to build and is in ERROR status
Details: <Server: scenario-server-89179012> failed to get to expected status. In ERROR state.
The exception:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339/logs/screen-n-cpu.txt.gz#_2013-09-24_04_44_31_806
Logs are located here:
http://logs.openstack.org/64/47264/2/gate/gate-tempest-devstack-vm-full/dced339
-----------------
Originally the failure was (before some changes to timeouts in tempest):
t178.1: tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern.test_volume_boot_pattern[compute,image,volume]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2013-09-16 15:59:44,214 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:44,417 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,348 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:45,495 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:47,644 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:48,762 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:49,879 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 15:59:50,980 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:00:52,581 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:52,897 Authentication (publickey) successful!
2013-09-16 16:00:53,105 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,428 Authentication (publickey) successful!
2013-09-16 16:00:53,431 Secsh channel 1 opened.
2013-09-16 16:00:53,607 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:00:53,875 Authentication (publickey) successful!
2013-09-16 16:00:53,880 Secsh channel 1 opened.
2013-09-16 16:01:58,999 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,288 Authentication (publickey) successful!
2013-09-16 16:01:59,457 Connected (version 2.0, client dropbear_2012.55)
2013-09-16 16:01:59,784 Authentication (publickey) successful!
2013-09-16 16:01:59,801 Secsh channel 1 opened.
2013-09-16 16:02:00,005 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:00,080 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,127 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,192 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:01,414 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:02,494 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:03,615 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:04,724 Starting new HTTP connection (1): 127.0.0.1
2013-09-16 16:02:05,825 Starting new HTTP connection (1): 127.0.0.1
}}}
Traceback (most recent call last):
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 157, in test_volume_boot_pattern
    ssh_client = self._ssh_to_server(instance_from_snapshot, keypair)
  File ""tempest/scenario/test_volume_boot_pattern.py"", line 101, in _ssh_to_server
    private_key=keypair.private_key)
  File ""tempest/scenario/manager.py"", line 453, in get_remote_client
    return RemoteClient(ip, username, pkey=private_key)
  File ""tempest/common/utils/linux/remote_client.py"", line 47, in __init__
    if not self.ssh_client.test_connection_auth():
  File ""tempest/common/ssh.py"", line 148, in test_connection_auth
    connection = self._get_ssh_connection()
  File ""tempest/common/ssh.py"", line 70, in _get_ssh_connection
    time.sleep(bsleep)
  File ""/usr/local/lib/python2.7/dist-packages/fixtures/_fixtures/timeout.py"", line 52, in signal_handler
    raise TimeoutException()
TimeoutException
Full logs here:
http://logs.openstack.org/80/43280/9/gate/gate-tempest-devstack-vm-full/cba22ae/testr_results.html.gz
Looks a bit like the instance failed to boot.","Check for backing lun on iscsi target create

Check to verify the backing lun was actually created and not just
the controller lun.  If it was NOT created, attempt to issue
tgtadm --op new to see if we can recover.

If this fails, then we want to actually fail in Cinder rather than
pretending that everything went well, so we'll log the error and raise.

Change-Id: I3cabab0d214c051267638a627664df2b673236e3
Closes-Bug: #1226337"
211,1af3ca69c250f8cd2f8e60ed596ad2df30353713,1411677886,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1358,1827,1374140,nova,1af3ca69c250f8cd2f8e60ed596ad2df30353713,1,1,,"Bug #1374140 in OpenStack Compute (nova): ""Need to log the orignial libvirtError when InterfaceDetachFailed""","This is not really useful:
http://logs.openstack.org/17/123917/2/check/check-tempest-dsvm-neutron/4bc2052/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-09-25_17_35_11_635
2014-09-25 17:35:11.635 ERROR nova.virt.libvirt.driver [req-50afcbfb-203e-454d-a7eb-1549691caf77 TestNetworkBasicOps-985093118 TestNetworkBasicOps-1055683132] [instance: 960ee0b1-9c96-4d5b-b5f5-be76ae19a536] detaching network adapter failed.
2014-09-25 17:35:11.635 27689 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 393, in decorated_function
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4411, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     self.driver.detach_interface(instance, condemned)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 1448, in detach_interface
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher     raise exception.InterfaceDetachFailed(instance)
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher InterfaceDetachFailed: <nova.objects.instance.Instance object at 0x422fe90>
2014-09-25 17:35:11.635 27689 TRACE oslo.messaging.rpc.dispatcher
The code is logging that there was an error, but not the error itself:
        try:
            self.vif_driver.unplug(instance, vif)
            flags = libvirt.VIR_DOMAIN_AFFECT_CONFIG
            state = LIBVIRT_POWER_STATE[virt_dom.info()[0]]
            if state == power_state.RUNNING or state == power_state.PAUSED:
                flags |= libvirt.VIR_DOMAIN_AFFECT_LIVE
            virt_dom.detachDeviceFlags(cfg.to_xml(), flags)
        except libvirt.libvirtError as ex:
            error_code = ex.get_error_code()
            if error_code == libvirt.VIR_ERR_NO_DOMAIN:
                LOG.warn(_LW(""During detach_interface, ""
                             ""instance disappeared.""),
                         instance=instance)
            else:
                LOG.error(_LE('detaching network adapter failed.'),
                         instance=instance)
                raise exception.InterfaceDetachFailed(
                        instance_uuid=instance['uuid'])
We should log the original libvirt error.","libvirt: log exception info when interface detach failed

The detach_interface method is catching a libvirtError and logging an
error message but it doesn't include the contents of the root
libvirtError, so log that information as well for debugging.

Closes-Bug: #1374140

Change-Id: I042c2956c7393a2ee1ccd026421c8df537aa1a87"
212,1b2570877846598d3545a9c9f31680f2a995491f,1395147192,,1.0,11,2,2,2,1,0.619382195,True,2.0,275212.0,41.0,19.0,False,83.0,438854.0,351.0,6.0,418.0,3517.0,3702.0,397.0,2892.0,3059.0,410.0,2551.0,2733.0,0.053887505,0.33460076,0.358463354,641,1067,1294069,nova,1b2570877846598d3545a9c9f31680f2a995491f,1,1,,"Bug #1294069 in OpenStack Compute (nova): ""XenAPI: Boot from volume without image_ref broken""","https://review.openstack.org/#/c/78194/ changed tempest to clear image_ref for some BFV tests - in particular the test_volume_boot_pattern
This now results in a ""KeyError: 'disk_format'"" exception from Nova when using the XenAPI driver.
http://paste.openstack.org/show/73733/ is a nicer format of the below - but might disappear!
2014-03-18 11:20:07.475 ERROR nova.compute.manager [req-82096fe0-921a-4bc1-9c41-d0aafad4c923 TestVolumeBootPattern-581093620 TestVolumeBootPattern-1800543246] [instance: 2b047f24-675c-4921-8cf3-85584097f106] Error: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] Traceback (most recent call last):
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1306, in _build_instance
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     set_access_ip=set_access_ip)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 394, in decorated_function
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     return function(self, context, *args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1708, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     LOG.exception(_('Instance failed to spawn'), instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1705, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/driver.py"", line 236, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     admin_password, network_info, block_device_info)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 357, in spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     network_info, block_device_info, name_label, rescue)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 526, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     undo_mgr.rollback_and_reraise(msg=msg, instance=instance)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/utils.py"", line 812, in rollback_and_reraise
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     self._rollback()
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     six.reraise(self.type_, self.value, self.tb)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 501, in _spawn
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     disk_image_type = determine_disk_image_type_step(undo_mgr)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 146, in inner
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     rv = f(*args, **kwargs)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vmops.py"", line 414, in determine_disk_image_type_step
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     return vm_utils.determine_disk_image_type(image_meta)
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]   File ""/opt/stack/nova/nova/virt/xenapi/vm_utils.py"", line 1647, in determine_disk_image_type
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]     disk_format = image_meta['disk_format']
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106] KeyError: 'disk_format'
2014-03-18 11:20:07.475 TRACE nova.compute.manager [instance: 2b047f24-675c-4921-8cf3-85584097f106]
I've confirmed that running without https://review.openstack.org/#/c/78194/ passes the tests (although there is a race condition, which is why test_volume_boot_pattern is disabled in the XenServer CI system) but will always fail with https://review.openstack.org/#/c/78194/ applied.","xenapi: boot from volume without image_ref

Currently xenapi does not deal well with image_meta that does not
contain ""disk_format"". This happens when we get image_meta from an
associated volume, rather than from glance.

Change-Id: Id673158442fde27e8d468ca412c9bd557a886e6b
Closes-Bug: #1294069"
213,1b46d36269adc5c64c8cd55a2d9f2c47e42b341f,1403864328,,1.0,12,10,3,2,1,0.906359331,False,,,,,True,,,,,,,,,,,,,,,,,987,1426,1331569,neutron,1b46d36269adc5c64c8cd55a2d9f2c47e42b341f,1,1,"""ip_lib is currently used to list the bridges
in the Open vSwitch neutron agent.

use of ip_lib blocks reuse of the Open vSwitch agent
with userspace only open vSwitchs implementations.” (52e281c736ce0ecac33f5807fa2c5c953925eda9)","Bug #1331569 in neutron: ""Open vSwitch Agent should use ovs_lib to list bridges not ip_lib""","Problem description
===================
In the Open vSwitch  neutron agent, ip_lib is currently used to list the bridges.
as userspace only vswiches do not create bridge local ports in the kernel ip_lib will not correctly detect the bridge configuration.
Proposed Change
==============
use of ip_lib blocks reuse of the Open vSwitch agent with userspace only open vSwitchs implementations.
to enable this use case ovs_lib.get_bridges should be used instead.","changes ovs agent to get bridges via ovs_lib

ip_lib is currently used to list the bridges
in the Open vSwitch neutron agent.

use of ip_lib blocks reuse of the Open vSwitch agent
with userspace only open vSwitchs implementations.

this patch replaces calls to ip_lib with ovs_lib.get_bridges

Closes-Bug: #1331569
Change-Id: I5935d39b314055063a64266bda0cc4c2d1ac05fc"
214,1b46f86148c0366770065fc481984adc6bce5693,1400191185,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,914,1350,1322410,nova,1b46f86148c0366770065fc481984adc6bce5693,0,0,Refactorings “remove uncessary log.exception in api layer”,"Bug #1322410 in OpenStack Compute (nova): ""remove uncessary log.exception in api layer ""","There are some LOG.exception in api layer, which is not necessary , we should downgrade them","Downgrade log when attach interface can't find resources

Downgrade the log when a resource (e.g. instance is deleted concurrently)
can't be found, no need to LOG.exception because an exception
is already raised and no need to log again.

Change-Id: I260e89c8deab11bc9707abbbbc7bb9a4f88640e3
Closes-Bug: #1322410"
215,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1403348202,,4.0,76,2142,37,19,1,0.647407257,False,,,,,True,,,,,,,,,,,,,,,,,343,754,1274523,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,0,0,Add support ,"Bug #1274523 in OpenStack Compute (nova): ""connection_trace does not work with DB2 backend""","When setting connection_trace=True, the stack trace does not get printed for DB2 (ibm_db).
I have a patch that we've been using internally for this fix that I plan to upstream soon, and with that we can get output like this:
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] SELECT services.created_at AS services_created_at, services.updated_at AS services_updated_at, services.deleted_at AS services_deleted_at, services.deleted AS services_deleted, services.id AS services_id, services.host AS services_host, services.""binary"" AS services_binary, services.topic AS services_topic, services.report_count AS services_report_count, services.disabled AS services_disabled, services.disabled_reason AS services_disabled_reason
FROM services WHERE services.deleted = ? AND services.id = ? FETCH FIRST 1 ROWS ONLY
2013-09-11 13:07:51,985 INFO sqlalchemy.engine.base.Engine (0, 3)
2013-09-11 13:07:51.985 28151 INFO sqlalchemy.engine.base.Engine [-] (0, 3)
        File /usr/lib/python2.6/site-packages/nova/servicegroup/drivers/db.py:92 _report_state() service.service_ref, state_catalog)
        File /usr/lib/python2.6/site-packages/nova/conductor/api.py:270 service_update() return self._manager.service_update(context, service, values)
        File /usr/lib/python2.6/site-packages/nova/openstack/common/rpc/common.py:420 catch_client_exception() return func(*args, **kwargs)
        File /usr/lib/python2.6/site-packages/nova/conductor/manager.py:461 service_update() svc = self.db.service_update(context, service['id'], values)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:505 service_update() with_compute_node=False, session=session)
        File /usr/lib/python2.6/site-packages/nova/db/sqlalchemy/api.py:388 _service_get() result = query.first()","Move to oslo.db

Replace common oslo code nova.openstack.common.db by usage
of oslo.db library and remove common code.

Replaced catching of raw sqlalchemy exceptions by catches
of oslo.db exceptions(such as DBError, DBDuplicateEntry, etc).

Co-Authored-By: Eugeniya Kudryashova  <ekudryashova@mirantis.com>

Closes-Bug: #1364986
Closes-Bug: #1353131
Closes-Bug: #1283987
Closes-Bug: #1274523
Change-Id: I0649539e071b2318ec85ed5d70259c949408e64b"
216,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1403348202,,4.0,76,2142,37,19,1,0.647407257,False,,,,,True,,,,,,,,,,,,,,,,,461,876,1283987,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,1,There is a deadlock. But also refactoring code.,"Bug #1283987 in OpenStack Compute (nova): ""Query Deadlock when creating >200 servers at once in sqlalchemy""","Query Deadlock when creating >200 servers at once in sqlalchemy.
--------
This bug occurred when I test this bug:
https://bugs.launchpad.net/nova/+bug/1270725
The original info is logged here:
http://paste.openstack.org/show/61534/
--------------
After checking the error-log, we can notice that the deadlock function is 'all()' in sqlalchemy framework.
Previously, we use '@retry_on_dead_lock' function to retry requests when deadlock occurs.
But it's only available for session deadlock(query/flush/execute). It doesn't cover some 'Query' actions in sqlalchemy.
So, we need to add the same protction for 'all()' in sqlalchemy.","Move to oslo.db

Replace common oslo code nova.openstack.common.db by usage
of oslo.db library and remove common code.

Replaced catching of raw sqlalchemy exceptions by catches
of oslo.db exceptions(such as DBError, DBDuplicateEntry, etc).

Co-Authored-By: Eugeniya Kudryashova  <ekudryashova@mirantis.com>

Closes-Bug: #1364986
Closes-Bug: #1353131
Closes-Bug: #1283987
Closes-Bug: #1274523
Change-Id: I0649539e071b2318ec85ed5d70259c949408e64b"
217,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1403348202,,4.0,76,2142,37,19,1,0.647407257,False,,,,,True,,,,,,,,,,,,,,,,,1153,1610,1353131,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,1,,"Bug #1353131 in OpenStack Compute (nova): ""Failed to commit reservations  in gate""","From: http://logs.openstack.org/31/105031/14/gate/gate-tempest-dsvm-full/c05b927/console.html
2014-08-05 02:54:01.131 | Log File Has Errors: n-cond
2014-08-05 02:54:01.132 | *** Not Whitelisted *** 2014-08-05 02:25:47.799 ERROR nova.quota [req-19feeaa2-e1d4-419b-a7bb-a19bb7000b1d AggregatesAdminTestJSON-2075387658 AggregatesAdminTestJSON-270189725] Failed to commit reservations [u'ceaa6ce7-db8d-4ba6-871a-b29c59f4a338', u'10d7550d-d791-44dd-8396-2fa6eaea7c20', u'e7a322e2-948d-45f7-892f-7ea4d9aa0e7c']
There are a number of errors happening in that file that arent whitelisted.
This one *seems* to be a possible cause of others.as there is then a number of InstanceNotFound errors.","Move to oslo.db

Replace common oslo code nova.openstack.common.db by usage
of oslo.db library and remove common code.

Replaced catching of raw sqlalchemy exceptions by catches
of oslo.db exceptions(such as DBError, DBDuplicateEntry, etc).

Co-Authored-By: Eugeniya Kudryashova  <ekudryashova@mirantis.com>

Closes-Bug: #1364986
Closes-Bug: #1353131
Closes-Bug: #1283987
Closes-Bug: #1274523
Change-Id: I0649539e071b2318ec85ed5d70259c949408e64b"
218,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1403348202,,4.0,76,2142,37,19,1,0.647407257,False,,,,,True,,,,,,,,,,,,,,,,,1261,1721,1364986,nova,1b83b2f11054ddd3f72fe75c5cef496060afcb3a,1,0,“A recent commit to oslo.db changed the way the 'raw' DB exceptions are wrapped”,"Bug #1364986 in OpenStack Compute (nova): ""oslo.db now wraps all DB exceptions""","tl;dr
In a few versions of oslo.db (maybe when we release 1.0.0?), every project using oslo.db should inspect their code and remove usages of 'raw' DB exceptions like IntegrityError/OperationalError/etc from except clauses and replace them with the corresponding custom exceptions from oslo.db (at least a base one - DBError).
Full version
A recent commit to oslo.db changed the way the 'raw' DB exceptions are wrapped (e.g. IntegrityError, OperationalError, etc). Previously, we used decorators on Session methods and wrapped those exceptions with oslo.db custom ones. This is mostly useful for handling them later (e.g. to retry DB API methods on deadlocks).
The problem with Session decorators was that it wasn't possible to catch and wrap all possible exceptions. E.g. SA Core exceptions and exceptions raised in Query.all() calls were ignored. Now we are using a low level SQLAlchemy event to catch all possible DB exceptions. This means that if consuming projects had workarounds for those cases and expected 'raw' exceptions instead of oslo.db ones, they would be broken. That's why we *temporarily* added both 'raw' exceptions and new ones to expect clauses in consuming projects code when they were ported to using of oslo.db to make the transition smooth and allow them to work with different oslo.db versions.
On the positive side, we now have a solution for problems like https://bugs.launchpad.net/nova/+bug/1283987 when exceptions in Query methods calls weren't handled properly.
In a few releases of oslo.db we can safely remove 'raw' DB exceptions like IntegrityError/OperationalError/etc from projects code and except only oslo.db specific ones like DBDuplicateError/DBReferenceError/DBDeadLockError/etc (at least, we wrap all the DB exceptions with our base exception DBError, if we haven't found a better match).
oslo.db exceptions and their description: https://github.com/openstack/oslo.db/blob/master/oslo/db/exception.py","Move to oslo.db

Replace common oslo code nova.openstack.common.db by usage
of oslo.db library and remove common code.

Replaced catching of raw sqlalchemy exceptions by catches
of oslo.db exceptions(such as DBError, DBDuplicateEntry, etc).

Co-Authored-By: Eugeniya Kudryashova  <ekudryashova@mirantis.com>

Closes-Bug: #1364986
Closes-Bug: #1353131
Closes-Bug: #1283987
Closes-Bug: #1274523
Change-Id: I0649539e071b2318ec85ed5d70259c949408e64b"
219,1bc4fe891a359c626ebc8049403d4d72a75b6be7,1407739817,,1.0,25,3,2,2,2,0.966618633,False,,,,,True,,,,,,,,,,,,,,,,,1430,1183152,1183152,swift,1bc4fe891a359c626ebc8049403d4d72a75b6be7,1,1, ,"Bug #1183152 in OpenStack Object Storage (swift): ""statsd timing unhandled exception operation not permitted""","-container-5.2..com: May 22 22:49:25 container-server ERROR __ll__ error with DELETE /d84/58775/AUTH_benchmark/bench_000078/tiny_020610 :
    Traceback (most recent ll last):
      File ""/opt//lib/python2.7/site-packages/swift/container/server.py"", line 474, in __ll__
        res = method(req)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 1458, in wrapped
        return func(*a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 480, in _timing_stats
        ctrl.logger.timing_since(method + '.timing', start_time)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 610, in wrapped
        return func(self.logger.statsd_client, *a, **kw)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 464, in timing_since
        sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 460, in timing
        return self._send(metric, timing_ms, 'ms', sample_rate)
      File ""/opt//lib/python2.7/site-packages/swift/common/utils.py"", line 445, in _send
        return sock.sendto('|'.join(parts), self._target)
      File ""/opt//lib/python2.7/site-packages/eventlet/greenio.py"", line 290, in sendto
        return self.fd.sendto(*args)
    error: [Errno 1] Operation not permitted (txn: tx60ec62fa962d4d96ba5dace5d81eede2)
Just need to audit the error handling, I think this happened under high load and the network was saturated maybe unable to send new udp packets or something...","Catch permissions errors when writing StatsD packets

Closes-Bug: #1183152

Change-Id: I4b2c6e947241c987779a385fdff270d037470a57"
220,1c0235fb592d63e03506670e5690ab6c7ecab0a7,1391974779,1.0,1.0,31,10,2,2,1,0.999570839,True,6.0,1401579.0,81.0,9.0,False,15.0,794831.0,29.0,5.0,28.0,1180.0,1181.0,28.0,1005.0,1006.0,20.0,594.0,595.0,0.027272727,0.772727273,0.774025974,386,799,1278530,neutron,1c0235fb592d63e03506670e5690ab6c7ecab0a7,1,1,sending wrong id,"Bug #1278530 in neutron: ""BigSwitch: Ports on shared network sending wrong netid""","When deleting/updating a port on a shared network, the plugin is currently using the tenant ID of the port owner in the URI rather than the tenant ID of the network owner. This should always be the tenant ID of the network the port is being attached to.","BigSwitch: Fix tenant_id for shared net requests

The URI port requests are sent to on the backend
contains the tenant_id of the network. This corrects
a bug where, on port updates and deletes, the tenant_id
of the port rather than the network was being used,
which was incorrect when attached to a shared network.

Closes-Bug: #1278530
Change-Id: I09ffc767c669416555867e975d0b7057a5cfa0cb"
221,1c0c1eb01d45eb016ba6b2ad63b9e3e652365b34,1394770109,,1.0,19,1,2,2,1,0.609840305,True,3.0,3537633.0,60.0,13.0,False,208.0,166504.5,1156.0,5.0,58.0,2183.0,2221.0,48.0,1879.0,1912.0,58.0,2104.0,2142.0,0.007763158,0.276973684,0.281973684,617,1042,1292339,nova,1c0c1eb01d45eb016ba6b2ad63b9e3e652365b34,0,0,a trace log can be ignored. No bug,"Bug #1292339 in OpenStack Compute (nova): ""instance not found trace log can be ignored while get-console-output""","during we cold-migrating or resizing an instance to another host(or during deleting it), the get_console_output
method may raise an InstanceNotFound excetpion if the instance is not on the hypervisor, this is an expected error,
so we should add the InstanceNotFound excetpion to expected exceptions list in the compute manager.
2014-03-14 11:59:58.884 AUDIT nova.compute.manager [req-6414594a-7fcd-427e-9ed6-1d78f12d40e0 demo demo] [instance: c68b2e95-8299-415a-a837-ff9f7303e6db] Get console output
2014-03-14 11:59:58.901 ERROR oslo.messaging._executors.base [-] Exception during message handling
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base Traceback (most recent call last):
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     incoming.reply(self.callback(incoming.ctxt, incoming.message))
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return self._dispatch(endpoint, method, ctxt, args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/server.py"", line 153, in inner
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return func(*args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     payload)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     six.reraise(self.type_, self.value, self.tb)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return f(self, context, *args, **kw)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/compute/manager.py"", line 293, in decorated_function
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     return function(self, context, *args, **kwargs)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/compute/manager.py"", line 3932, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     output = self.driver.get_console_output(context, instance)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2268, in get_console_output
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     virt_dom = self._lookup_by_name(instance.name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3437, in _lookup_by_name
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base     raise exception.InstanceNotFound(instance_id=instance_name)
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base InstanceNotFound: Instance instance-0000000c could not be found.
2014-03-14 11:59:58.901 TRACE oslo.messaging._executors.base","Ignore InstanceNotFound while getting console output

During we cold-migrating or resizing an instance to another host,
or during deleting it, the get_console_output method may raise an
InstanceNotFound excetpion if the instance is not on the hypervisor,
this is an expected error, so we should add it to expected
exceptions list in compute manager.

Closes-bug: #1292339
Change-Id: I23a99438d73826f01ff5662130dcbfa72dc39a6e"
222,1c1570ff7ac8470d7dba28c893300835b22e0966,1383246474,,1.0,27,1,2,2,1,0.863120569,True,7.0,11511822.0,75.0,29.0,False,38.0,128989.0,78.0,9.0,1404.0,3417.0,3419.0,1263.0,3017.0,3019.0,569.0,2435.0,2435.0,0.087759815,0.375057737,0.375057737,44,372,1246848,nova,1c1570ff7ac8470d7dba28c893300835b22e0966,1,1,,"Bug #1246848 in OpenStack Compute (nova): ""VMWare: AssertionError: Trying to re-send() an already-triggered event.""","When an exceptin occurs in _wait_for_task and a failure occurs, for example a file is requested and it does not exists then another exception is also thrown:
013-10-31 10:49:52.617 WARNING nova.virt.vmwareapi.driver [-] In vmwareapi:_poll_task, Got this error Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 78, in _inner
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     self.f(*self.args, **self.kw)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 941, in _poll_task
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     done.send_exception(excep)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 208, in send_exception
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     return self.send(None, args)
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 150, in send
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall     assert self._result is NOT_USED, 'Trying to re-send() an already-triggered event.'
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall AssertionError: Trying to re-send() an already-triggered event.
2013-10-31 10:49:52.618 TRACE nova.openstack.common.loopingcall","VMware: fix bug for exceptions thrown in _wait_for_task

If the _call_method fails, for example a resource is not found,
then the same call will be invoked again due to the fact that the
polling task was not stopped.

Closes-Bug: #1246848

Change-Id: If4818e41aed5be03395ab22afc0edf9abb3c34e1"
223,1c5797d53a53a990cef3d695f0752742fd213840,1391181962,,1.0,9,3,1,1,1,0.0,True,2.0,6187613.0,29.0,6.0,False,20.0,344705.0,43.0,5.0,374.0,566.0,703.0,360.0,547.0,671.0,178.0,357.0,364.0,0.157293497,0.314586995,0.320738137,440,855,1282400,glance,1c5797d53a53a990cef3d695f0752742fd213840,0,0,tests,"Bug #1282400 in Glance: ""The race condition caused by image created_at break DB driver test_image_paginate case""","FAIL: glance.tests.functional.db.test_registry.TestRegistryDriver.test_image_paginate
tags: worker-0
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""glance/tests/functional/db/base.py"", line 711, in test_image_paginate
    self.assertEqual(extra_uuids, [i['id'] for i in page])
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = ['0be9ce54-f56b-41d5-bef6-9ca75e846b59',
 'a75dc71b-df55-41e3-9218-f465b791b7da']
actual    = [u'a75dc71b-df55-41e3-9218-f465b791b7da',
 u'0be9ce54-f56b-41d5-bef6-9ca75e846b59']
http://logs.openstack.org/58/74858/1/check/gate-glance-python27/732468f/console.html#_2014-02-19_23_03_01_798","Provide explicit image create value for test_image_paginate case

Assign an explicit created_at datetime value to image db fixtures,
it be used to fixes race condition in DB driver test_image_paginate
case.

Closes-Bug: #1282400
Change-Id: I359633af4bb25cf1866b42d6dad3df658995b7bd
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
224,1cd59abef5e9947fa8a98932d71a6b3aeae88ae5,1386063707,,1.0,6,6,2,2,1,1.0,True,2.0,5208627.0,20.0,8.0,False,43.0,658.0,92.0,6.0,25.0,2710.0,2728.0,25.0,2337.0,2355.0,12.0,2551.0,2557.0,0.00192052,0.377012853,0.377899247,128,527,1257222,nova,1cd59abef5e9947fa8a98932d71a6b3aeae88ae5,0,0,test,"Bug #1257222 in OpenStack Compute (nova): ""Fix errors in test_server_start_stop.py""","There are incorrect body dict for stop actions. Such as:
def test_stop_not_ready(self):
        self.stubs.Set(db, 'instance_get_by_uuid', fake_instance_get)
        self.stubs.Set(compute_api.API, 'stop', fake_start_stop_not_ready)
        req = fakes.HTTPRequest.blank('/v2/fake/servers/test_inst/action')
        body = dict(start="""")
        self.assertRaises(webob.exc.HTTPConflict,
            self.controller._stop_server, req, 'test_inst', body)","Fixes errors on start/stop unittest

The action name is incorrect on stop unittest, we should use ""stop""
instead of ""start"".

Change-Id: I973e4f4591945f225e374797aebd1bbe00c3c9bd
Closes-Bug: #1257222"
225,1cdc555d21c532d7e4963ed1784a488769abdd1b,1411049572,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1331,1799,1370769,glance,1cdc555d21c532d7e4963ed1784a488769abdd1b,1,1,,"Bug #1370769 in Glance: ""Ensure all metadata definition code uses six.itertypes""","Similar to https://review.openstack.org/#/c/95467/
According to https://wiki.openstack.org/wiki/Python3 dict.iteritems()
should be replaced with six.iteritems(dict).
All metadata definition code added should ensure that six.iteritems is used.","replace dict.iteritems() with six.iteritems(dict)

All metadata definition code added should ensure that
six.iteritems is used.

Change-Id: Ic93595ffb2a5a8ed7667028d820fac8debd988e6
Closes-Bug: #1370769"
226,1ce493b9e4e0c381146287161ab2a3c75f1fc603,1381405426,1.0,1.0,10,2,2,2,1,0.979868757,True,6.0,33440.0,26.0,16.0,False,10.0,966325.5,16.0,6.0,1428.0,3813.0,4115.0,1375.0,3384.0,3677.0,1367.0,2762.0,3051.0,0.218181818,0.440669856,0.48676236,1668,1237944,1237944,nova,1ce493b9e4e0c381146287161ab2a3c75f1fc603,0,0,Bug in test,"Bug #1237944 in OpenStack Compute (nova): ""boto version breaks gating""","Boto version boto==2.14.0 breaks the gate with
2013-10-10 05:15:05.036 | Traceback (most recent call last):
2013-10-10 05:15:05.036 |   File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 389, in test_group_name_valid_length_security_group
2013-10-10 05:15:05.036 |     self.expect_http()
2013-10-10 05:15:05.036 |   File ""/home/jenkins/workspace/gate-nova-python26/nova/tests/test_api.py"", line 246, in expect_http
2013-10-10 05:15:05.037 |     is_secure).AndReturn(self.http)
2013-10-10 05:15:05.037 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 765, in __call__
2013-10-10 05:15:05.037 |     return mock_method(*params, **named_params)
2013-10-10 05:15:05.038 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 998, in __call__
2013-10-10 05:15:05.038 |     self._checker.Check(params, named_params)
2013-10-10 05:15:05.038 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/mox.py"", line 929, in Check
2013-10-10 05:15:05.039 |     % (' '.join(sorted(still_needed))))
2013-10-10 05:15:05.039 | AttributeError: No values given for arguments: is_secure","Add boto special casing for param changes in 2.14

boto 2.14 has some interface changes with the get_http_connection
call which adds port as a non optional parameter *in the middle of*
existing non optional parameters. It's a special kind of incompatible
change.

We are already special casing around boto in the unit tests, lets
just do more of that if we are running on a newer version.

Co-Authored-By: Dan Smith <dansmith@redhat.com>
Closes-bug: #1237944
Change-Id: If388d872286eca15bf24a582db1bac22224a8b24"
227,1d44bc5d7ac4adb1ad8a3aaabf0c27c593ec07e1,1394748150,,1.0,59,17,2,2,1,0.987475008,True,18.0,4784483.0,194.0,63.0,False,39.0,97035.0,65.0,5.0,728.0,3114.0,3489.0,622.0,2639.0,2952.0,213.0,1964.0,2079.0,0.028180142,0.258756913,0.273900448,614,1039,1292243,nova,1d44bc5d7ac4adb1ad8a3aaabf0c27c593ec07e1,0,0,No bug. should delete all neutron ports on error,"Bug #1292243 in OpenStack Compute (nova): ""deallocate_for_instance should delete all neutron ports on error""","When deleting an instance if an instance has multiple ports and one
of the deletes fail nova should LOG an error and continue trying
to delete the other ports. Previously, nova would stop deleting ports
are the first non 404 error.","neutronv2: attempt to delete all ports

Previously when deleting ports during allocate
or deallocate, the operation would fail on the
first non-404 error encountered. We now loop
through all of the ports and attempt the delete.

In the allocate flow we don't raise any errors
since the instance is set to ERROR anyway and
we're just trying to cleanup.

In the deallocate flow we attempt to delete all
of the ports and if there was a non-404 exception
we raise that at the end so the overall instance
delete operation fails.

Change-Id: Ibf68e0b4770b09ee32f187b85f2ca96984117e0b
Closes-bug: #1292243
Co-Authored-By: Dan Smith <dansmith@redhat.com>
Co-Authored-By: Sean Dague <sean@dague.net>
Co-Authored-By: Matt Riedemann <mriedem@us.ibm.com>"
228,1d4e13574574472e299296892d7c9e8f706aea67,1410389268,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1293,1759,1367864,neutron,1d4e13574574472e299296892d7c9e8f706aea67,1,1,,"Bug #1367864 in neutron: ""User and group association not deleted after router delete in nuage plugin""","In nuage plugin when a router delete operation is performed, the user and group association is not deleted. This is a bug which is caused by a check for a nuage zone attached to the router even after the router is deleted.
This commit associated with this bug will fix the above issue.","Fix to delete user and group association in Nuage Plugin

After a router delete operation, the attached zone to that
router is also deleted. Got rid of code that tried to do
a get operation on the nuage_zone after router delete
operation.
Closes-Bug: #1367864

Change-Id: I01753a472200a961cdcecee703616fd3239abd3c"
229,1d769b0c37f24028bceae568ca66bb0426b28b69,1385093814,1.0,1.0,1,1,1,1,1,0.0,True,1.0,673452.0,14.0,6.0,False,9.0,3956352.0,10.0,6.0,231.0,2848.0,2935.0,231.0,2498.0,2585.0,216.0,1902.0,1980.0,0.032616865,0.286036375,0.297760409,1792,1253875,1253875,nova,1d769b0c37f24028bceae568ca66bb0426b28b69,1,1,"“Commit a52259e introduced some changes to debug log messages in the ImagePropertiesFilter but with that came a bug""","Bug #1253875 in OpenStack Compute (nova): ""Scheduler Filters: ValueError: unsupported format character 'a' (0x61) at index 107""","I've been seeing this in the nova scheduler log a lot this week when looking through gate and check queue failures:
http://paste.openstack.org/show/53791/
Looks like it's probably due to a bad translation.
(9:31:29 PM) clarkb: mriedem: yes that is the sort of traceback that should be fixed. It is possible that that particular error is due to a bad translation
(9:31:58 PM) clarkb: since I think we are using local=en_US on the slaves and en is the source locale
http://logs.openstack.org/42/56642/2/gate/gate-tempest-devstack-vm-neutron/a712b82/logs/screen-n-sch.txt.gz","Fix missing format specifier in ImagePropertiesFilter log message

Commit a52259e introduced some changes to debug log messages in the
ImagePropertiesFilter but with that came a bug due to a missing format
specifier.

Closes-Bug: #1253875

Change-Id: Ic60b7ba179bd43b80c500be084a1809b6b0b32ca"
230,1d91921459e0d67eb668838253dc76371dbdf553,1410776973,,1.0,31,26,2,2,1,0.366578013,False,,,,,True,,,,,,,,,,,,,,,,,1316,1782,1369502,nova,1d91921459e0d67eb668838253dc76371dbdf553,1,1,,"Bug #1369502 in OpenStack Compute (nova): ""NUMA topology _get_constraints_auto assumes flavor object""",Resulting in AttributeError: 'dict' object has no attribute 'vcpus' if we try to start with a flavor that will result in Nova trying to decide on an automatic topology (for example providing only number of nodes with hw:numa_nodes extra_spec),"NUMA _get_constraint auto assumed Flavor object

We call this code in the API that is not converted to using objects at
the moment, so we cannot assume attribute access for dicts.

We also make sure that tests exercising it will fail by using dicts
exclusively until the code is converted to objects.

Change-Id: I6fb7104de5522c51f4706c973d8d8d8697d92b31
Closes-bug: #1369502"
231,1dea1cd710d54d4a2a584590e4ccf59dd3a41faa,1410000111,,1.0,49,1,2,2,1,0.242292189,False,,,,,True,,,,,,,,,,,,,,,,,1327,1795,1370265,nova,1dea1cd710d54d4a2a584590e4ccf59dd3a41faa,1,1,,"Bug #1370265 in OpenStack Compute (nova): ""Crash on describing EC2 volume backed image with multiple devices""","EC2 describe images crashes on volume backed instance snapshot which has several volumes:
$ euca-describe-images
euca-describe-images: error (KeyError): Unknown error occurred.
Steps to reproduce
1 Create bootable volume
$ cinder create --image <image-id> <size>
2 Boot instance from volume
$ nova boot --flavor m1.nano --block-device-mapping /dev/vda=<volume_id>:::1 inst
3 Create empty volume
$ cinder create 1
4 Attach the volume to the instance
$ nova volume-attach inst <empty-volume-id> /dev/vdd
5 Create volume backed snapshot
$ nova image-create inst sn-in
6 Describe EC2 images
$ euca-describe-images","Fix KeyError for euca-describe-images

EC2 describe images crashes on volume backed instance snapshot which has
several volumes.

Change-Id: Ibe278688b118db01c9c3ae1763954adf19c7ee0d
Closes-bug: #1370265"
232,1deb31f85a8f5d1e261b2cf1eddc537a5da7f60b,1406983058,,1.0,71,327,6,2,1,0.419409482,False,,,,,True,,,,,,,,,,,,,,,,,1860,1346637,1346637,Nova,1deb31f85a8f5d1e261b2cf1eddc537a5da7f60b,1,0,"""The ESX driver was deprecated in Icehouse and should be removed in Juno""","Bug #1346637 in OpenStack Compute (nova): ""VMware: remove ESX driver for juno""",The ESX driver was deprecated in Icehouse and should be removed in Juno. This bug is for the removal of the ESX virt driver in nova.,"Remove ESXDriver from Juno.

1. Removes the VMwareESXDriver code in nova/virt/vmwareapi/driver.py
by either deleting the redundant methods or moving them to
VMwareVCDriver.
2. Changes the test cases to use VMwareVCDriver and also removes
duplicate test cases which were previously being testing both
VMwareESXDriver and VMwareVCDriver to just the latter.

DocImpact
Closes-Bug: #1346637
Change-Id: I718fc0ee67dbd625af00c20fa4e34b8a35015437"
233,1deb787c15a0f24c6c079c5e5fe122dc54188cdf,1407998336,,1.0,129,55,13,8,1,0.881914715,False,,,,,True,,,,,,,,,,,,,,,,,1187,1645,1356464,neutron,1deb787c15a0f24c6c079c5e5fe122dc54188cdf,1,1,,"Bug #1356464 in neutron: ""Assigning a FIP to a VIP port does not work, if the VIP resides in a subnet that is associated with a DVR router residing in a service node with no default gateway.""","With DVR routers for FIP to work, there has to be a FIP Namespace that should be created and an IR Namespace.
But when a LBaaS VIP port is created on a Subnet that is part of the DVR router, the IR Namespace is not created for the VIP port since that port is not a Compute Port.
There are some corner cases here, when there are VMs in a node then the IR's for that subnet is already created and so the FIP namespace also will be created.
The issue will only be seen if the compute and the VIP ports are separated apart.","Fix DVR to service LBaaS VIP Ports

Currently, DVR router namespaces are created only
when there is a valid VM port on the compute
node, or for the gateway-port on the service node.
But when an LBaaS VIP port is created the l3 agent
does not create a DVR namespace to service the VIP port.
This fix enables DVR namespaces to be created to
service the LBaaS VIP port.

Also, this fix enables L2 Agent running in DVR
mode, to add-in OVS rules to enable packets to
be routed to such LBaaS VIP Ports which are
resident on DVR routed interfaces.

Therefore, with this fix both East-West and
North-South traffic will be serviced by DVR
for LBaas VIP Ports.

DocImpact

Authored-by: Swaminathan Vasudevan <swaminathan.vasudevan@hp.com>
Co-Authored-By: Vivekanandan Narasimhan <vivekanandan.narasimhan@hp.com>

Change-Id: I698b971d50721fb0512a11569f7d3139d0d456f3
Closes-Bug: #1356464"
234,1df9b26b51ff3a153b93d79e25bef454e08a8e38,1380914920,,1.0,1,1,1,1,1,0.0,True,2.0,15649529.0,12.0,4.0,False,121.0,896.0,335.0,2.0,1674.0,1235.0,2631.0,1443.0,1039.0,2238.0,1581.0,1123.0,2441.0,0.254340836,0.180707395,0.392604502,1416,1106423,1106423,nova,1df9b26b51ff3a153b93d79e25bef454e08a8e38,0,0,"“If https://review.openstack.org/#/c/18042/ is merged, a new configuration option/feature will be added to nova.” Hypothetical bugs, we do not consider them as bugs","Bug #1106423 in OpenStack Compute (nova): ""grizzly: ability to specify the libvirt cache mode for disk devices""","If https://review.openstack.org/#/c/18042/ is merged, a new configuration option/feature will be added to nova. The manuals should be updated to reflect this - configuration references and sample files
In past versions of Nova it was possible to explicitly configure
the cache mode of disks via the libvirt XML template. The curent approach
makes this a derived setting of either “none” or “writethrough” based
on the support of O_DIRECT. Whilst this provides a good set of default
settings it removes the ability of the cloud provider to use other
modes such as “writeback” and “unsafe” which are valuable in certain
configurations.
This change allows the cache mode to be specified on a per-disk type
basis. Leaving the specify_cachemode option set to False retains the
current behaviour.
 189	    cfg.ListOpt('disk_cachemodes',
   190	                 default=[],
   191	                help='Specific cachemodes to use for different disk types '
   192	                     'e.g: [""file=directsync"",""block=none""]'),
 189	    ]	193	    ]
   317	        self.valid_cachemodes = [""default"",
   318	                                 ""none"",
   319	                                 ""writethrough"",
   320	                                 ""writeback"",
   321	                                 ""directsync"",
   322	                                 ""writethrough"",
   323	                                 ""unsafe"",
   324	                                ]","Fix docstring for disk_cachemodes

The docstring for the disk_cachemodes option implied a syntax that is
incorrect for a ListOpt.  Change the example to match exactly what you
would put in the config file for it to work.

Change-Id: Ie999d2799dba6c47facdc771e1cf6081d28d6b85
Closes-Bug: #1106423"
235,1dfd65f4cff1133cff45e103083fc3ae3130877b,1394484591,0.0,1.0,3,9,6,6,1,0.975664138,True,3.0,120607.0,52.0,7.0,False,22.0,315589.6667,49.0,4.0,43.0,1600.0,1611.0,43.0,1381.0,1392.0,35.0,767.0,775.0,0.038338658,0.817891374,0.826411076,583,1004,1290550,neutron,1dfd65f4cff1133cff45e103083fc3ae3130877b,0,0,tests,"Bug #1290550 in neutron: ""Base test case should stop mock patches""","Currently if a unit test creates a patch and does not stop it, the patch will hang around and could potentially affect other tests that rely on the mocked class/method. This can make it difficult for developers creating new tests as unrelated tests could be causing new ones to sporadically fail or vice versa depending on concurrency and test order.","Stop mock patches by default in base test class

Adds a mock.patch.stopall to the base unit test
case cleanup routines to stop patches by default
in unit tests. This behavior can be overwritten
by setting an attribute on the test class if
required.

Also removes the explicit stops in the Cisco n1kv,
VMware, and NEC unit tests to leverage the new
automatic cleanup.

Closes-Bug: #1290550
Change-Id: Ic642430a765ea8deb07ebd88b619da58a58e0edd"
236,1e0ea5217a93fadb915c3dcdf4c260738bed70ac,1397122423,,1.0,0,100,29,11,1,0.788728297,True,3.0,101638.0,53.0,9.0,False,74.0,410028.2759,190.0,5.0,57.0,1647.0,1652.0,57.0,1388.0,1393.0,57.0,952.0,957.0,0.051509769,0.846358792,0.85079929,775,1202,1305656,neutron,1e0ea5217a93fadb915c3dcdf4c260738bed70ac,0,0,Bug in test,"Bug #1305656 in neutron: ""test that inherit from BaseTestCase don't need to clean up the mock.patches""","In BaseTestCase a clean up is added to stop all patches :
self.addCleanup(mock.patch.stopall)
the tests that inherits from BaseTestCase don't need to stop their patches","Remove mock.patch.stop from tests that inherit from BaseTestCase

The tests that inherit from BaseTestCase don't need to stop their
patches, since this is already done in the base class

Change-Id: Ibb1183e521686d6e948046997b32f4044d91d9e7
Closes-bug: #1305656"
237,1e2b7fbf73a9d6b9088396cd41091911d35279ef,1399487534,,1.0,21,7,3,2,1,0.779417821,True,1.0,97912.0,15.0,4.0,False,100.0,520336.6667,378.0,3.0,430.0,2145.0,2397.0,404.0,1813.0,2045.0,422.0,1833.0,2081.0,0.053227633,0.23077891,0.261985655,872,1306,1317208,nova,1e2b7fbf73a9d6b9088396cd41091911d35279ef,1,1,,"Bug #1317208 in OpenStack Compute (nova): ""Xen live-migrate with volume attached ends with FIELD_TYPE_ERROR failure""","The relevant portion of an example stack trace is below:
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packa
ges/nova/virt/xenapi/vmops.py"", line 2316, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     volume_utils.forget_sr(self._session, sr_uuid_map[sr_re
f])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vmops.py"", line 2307, in attach_block_device_volumes
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     hotplug=False)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/volumeops.py"", line 53, in attach_volume
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_ref = vm_utils.vm_ref_or_raise(self._session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 2661, in vm_ref_or_raise
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_ref = lookup(session, instance_name)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/vm_utils.py"", line 1743, in lookup
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     vm_refs = session.call_xenapi(""VM.get_by_name_label"", name_label)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/nova/virt/xenapi/client/session.py"", line 179, in call_xenapi
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     return session.xenapi_request(method, args)
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 133, in xenapi_request
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     result = _parse_result(getattr(self, methodname)(*full_params))
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/rackstack/615.44/nova/lib/python2.6/site-packages/XenAPI.py"", line 203, in _parse_result
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher     raise Failure(result['ErrorDescription'])
2014-05-01 10:25:37.027 13905 TRACE oslo.messaging.rpc.dispatcher Failure: ['FIELD_TYPE_ERROR', 'label']
attach_block_device_volumes() in nova/virt/xenapi/vmops.py calls attach_volume() in nova/virt/xenapi/volumops.py and passes None for instance_name,  and this causes a failure when looking up the vm_ref.","xenapi: fix live-migrate with volume attached

We need to attach the volume SR on the destination host during
pre-live-migrate, so XenAPI spots the volume is present, and does't try
to block migrate the volume to the destination host, instead it just
connects to the remote volume before resuming the VM on the destination
host.

The fix is done by calling connect_volume instead of attach_volume. The
None passed into instance was causing the lookup of the vm_ref to
(correctly) fail, with:
Failure: ['FIELD_TYPE_ERROR', 'label']

Change-Id: I7eaf90582508b42a202da79072c891c216d137d1
Closes-Bug: #1317208"
238,1e3d2fbcb13e01bee0a8f90bd2078b1f5063b4d5,1409744991,,1.0,2,4,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1445,1201873,1201873,nova,1e3d2fbcb13e01bee0a8f90bd2078b1f5063b4d5,1,0, “This bug describes and interaction between Openstack and Ubuntu”,"Bug #1201873 in OpenStack Compute (nova): ""dnsmasq does not use -h, so /etc/hosts sends folks to loopback when they look up the machine it's running on""","from dnsmasq(8):
      -h, --no-hosts
              Don't read the hostnames in /etc/hosts.
I reliably get bit by this during certain kinds of deployments, where my nova-network/dns host has an entry in /etc/hosts such as:
127.0.1.1    hostname.example.com hostname
I keep having to edit /etc/hosts on that machine to use a real IP, because juju gets really confused when it looks up certain openstack hostnames and gets sent to its own instance!","always set --no-hosts for dnsmasq

Many network modes in nova-network disable reading host entries from
/etc/hosts. This includes running vish-ha (multihost), or if you ever
set a dns server. However if you don't do these things we continue to
use /etc/hosts.

It's not clear that this does anything other than create
inconsistency. There is a long standing bug that exists around this
because the impacts include exposing localhost as a dns entry on the
network.

Change-Id: Ibccf0999bfd4b37f2d948af34640260f95c54572
Closes-Bug: #1201873"
239,1e4d070a5b638de58c87363b18a76171dcd0ce38,1395080182,1.0,2.0,31,59,3,2,1,0.738208875,True,1.0,751565.0,34.0,9.0,False,11.0,1786151.667,30.0,3.0,6.0,1400.0,1403.0,6.0,1150.0,1153.0,6.0,1242.0,1245.0,0.004569191,0.811357702,0.813315927,605,1029,1291804,cinder,1e4d070a5b638de58c87363b18a76171dcd0ce38,1,1,Profile attributes not reflecting. Change the way to do it,"Bug #1291804 in Cinder: ""vmware : profile attributes not reflecting in VC UI""","1) create profile say 'default'  having content tagged to nfs data store.
2) Now 'default' profile in /etc/cinder/cinder.conf for ""pbm_default_policy=default""
3) now restart cinder api service.
4) Now create volume with out any volume type ""cinder create --name testvol 1""
5) Now attach to instance.
6) As per default profile define respective volume is present to nfs data store.
how ever default profile  details not appearing in  "" VM storage policies pane"" . See attached snapshot for reference
ssatya@devstack:/etc/cinder$ cat cinder.conf  | grep pbm
pbm_default_policy=default
ssatya@devstack:/etc/cinder$ cinder list | grep in-use
| 4db6af03-7c60-469d-803e-2ba21893fc0d |   in-use  |          testvol2         |  1   |         None         |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| 862219d5-941a-4f96-85a8-9b73a83d4507 |   in-use  |          testvol3         |  1   |         None         |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |
| d33d102f-f863-441d-bf87-29465cef3760 |   in-use  |      ThickSliver_vol1     |  1   |  ThickSliver_volume  |  false   | 26ffc3d5-af49-4c04-926f-9b94563bfd65 |","vmware: Remove pbm_default_policy config option

The pbm_default_policy config option is used to associate a storage
policy with the volume backing if there is no associated volume_type
or the volume_type doesn't have the storage_profile option. A better
approach is to set the storage_profile option in the default_volume_type.
This change removes the pbm_default_policy and queries the policy
using the storage_profile option in the given volume_type.

Closes-Bug: #1291181
Closes-Bug: #1291804
DocImpact

Change-Id: I2519ec1145712ce6927dacde91db468d066af021"
240,1e54cae3d3f562046a50ab20ff60e2879ce80438,1409561054,,1.0,19,14,5,2,1,0.838203272,False,,,,,True,,,,,,,,,,,,,,,,,1869,1363899,1363899,Nova,1e54cae3d3f562046a50ab20ff60e2879ce80438,1,1, ,"Bug #1363899 in OpenStack Compute (nova): ""HyperV Vm Console Log issues""","The size of the console log can get bigger than expected because of a small nit when checking the existing log file size as well
as a wrong size constant.
The method which gets the serial port pipe at the moment returns a list which contains at most one element being the actual pipe path. In order to avoid confusion this should return the pipe path or None instead of a list.","Fixes HyperV VM Console Log

This patch intends to fix some small nits introduced along with
the Nova console log patch.

The size of the console log can get bigger than expected because
of a small nit when checking the existing log file size as well
as a wrong size constant.

The method which gets the serial port pipe at the moment returns
a list which contains at most one element being the actual pipe
path. In order to avoid confusion this should return the pipe path
or None instead of a list.

Change-Id: I5ce9d1bea572f9098971990d011fa97ebf09233e
Closes-Bug: #1363899"
241,1e73e64e7407294dc68af9c09323e2b396335c4a,1387856716,,1.0,31,9,3,3,1,0.756576831,True,6.0,452837.0,22.0,6.0,False,64.0,954038.0,146.0,2.0,40.0,1322.0,1339.0,40.0,1160.0,1177.0,29.0,1169.0,1175.0,0.023529412,0.917647059,0.922352941,222,625,1263820,cinder,1e73e64e7407294dc68af9c09323e2b396335c4a,0,0,Handle a new exception,"Bug #1263820 in Cinder: ""Handle terminate_connection() exception in volume manager""","Due to the fact that we sometimes need to manually terminate a volume's connection through volume api, it's possile that these  backend drivers throw exceptions while doing that.  Currently exceptions are bubbled up to volume API and are not being handled.  This patch logs exception in volume manager and then raises VolumeBackendAPIException to caller.","Handle terminate_connection() exception in volume manager

Due to the fact that we sometimes need to manually terminate a volume's
connection through volume api, it's possile that these backend drivers
throw exceptions while doing that. Currently exceptions are bubbled up to
volume API not being handled. This patch logs exception in volume manager
and then raises VolumeBackendAPIException to caller.

Change-Id: If809f97998f52516af09ec21b3052b67d3a62f36
Closes-bug: #1263820"
242,1eb8fa563d13e46f0a7011143f65e371ae091684,1380251376,2.0,1.0,208,58,4,3,1,0.80434643,True,6.0,7463520.0,69.0,10.0,False,7.0,882422.0,22.0,2.0,4.0,1655.0,1655.0,4.0,1529.0,1529.0,3.0,313.0,313.0,0.010282776,0.807197943,0.807197943,1607,1231770,1231770,neutron,1eb8fa563d13e46f0a7011143f65e371ae091684,0,0," ""In order to support NVP advanced lbaas/firewall UI, it is needed to”","Bug #1231770 in neutron: ""Edge LbaaS/Firewall should return router_id when getting vip/firewall""","In order to support NVP advanced lbaas/firewall UI, it is needed to return the router_id attribute when getting vip/firewall.
Also, it is needed to refactor duplicate code for delete_vip/delete_pool/delete_health_monitor","add router_id to response for CRU on fw/vip objs

Closes-Bug: #1231770
Change-Id: I3763d2af855d2ad28ff89b528264ba981e71c08a"
243,1eda138be81b2405969b80c00f30ba237e250fcd,1400492940,,1.0,25,3,2,2,1,0.985228136,False,,,,,True,,,,,,,,,,,,,,,,,891,1327,1320050,cinder,1eda138be81b2405969b80c00f30ba237e250fcd,0,0,"Feature “Brocade FC SAN lookup service should allow customized hosts key and missing policy""","Bug #1320050 in Cinder: ""Brocade FC SAN lookup service should allow customized hosts key and missing policy""","In the BrcdFCSanLookupService, when initialize. The ssh client should be allowed to load customized host key file instead of only load from the OS host key file. Also for the host key missing policy, the code should also allow to be customized by parsing the kwargs instead of hard-code the ""missing policy"". The ""MIssing Policy"" will not stop the man in the middle attack if the known_hosts is not a match, it should allow customized policy being configured in different scenarios to fit the security need.","BrcdFCSanLookupService should allow customize host key and policy

In BrcdFCSanLookupService, the initialization should allow the
customization of the known_hosts_file and missing_key_policy so that the
hosts key and missing policy can be customized according to the
different scenario and customer aspect. This will not change the default
behavior when no argument is given, but more flexible to allow the
caller to give more options according to different requirements.

Closes-Bug: #1320050
Change-Id: If5767f63ccd2cde5fbea30a6154acf4d28f662b6"
244,1f4595872c2d06359b78795b406ac42fa99cc7a4,1382619586,0.0,1.0,110,2,3,2,1,0.420652198,True,15.0,10293124.0,190.0,45.0,False,5.0,1444401.333,6.0,3.0,3.0,1032.0,1032.0,3.0,970.0,970.0,2.0,397.0,397.0,0.006097561,0.808943089,0.808943089,1719,1242532,1242532,neutron,1f4595872c2d06359b78795b406ac42fa99cc7a4,1,1,“missing”,"Bug #1242532 in neutron: ""mellanox plugin does not update port state up""",There is a missing RPC call from Mellanox neutron agent to neutron server for update device_up once device configuration is applied.,"Add update from agent to plugin on device up

Add RPC message to plugin on device up once configured.
Update device details attribute to use segmentation_id
instead of vlan_id.

Change-Id: I9be8389c01c4c29025bbe868919e39fde3251a58
Closes-Bug: #1242532"
245,1f6381a73f5c99f1f731d6c4f9defb91bd2d042d,1385651833,0.0,1.0,20,12,7,3,1,0.881516664,True,12.0,8838808.0,97.0,26.0,False,47.0,228598.1429,172.0,3.0,186.0,629.0,718.0,185.0,474.0,563.0,140.0,474.0,520.0,0.136893204,0.461165049,0.505825243,1808,1255556,1255556,Glance,1f6381a73f5c99f1f731d6c4f9defb91bd2d042d,0,0,Error in test,"Bug #1255556 in Glance: ""Don't enable all stores by default""","Currently glance ""tries"" to load all stores even though the requirements are not meant. If a store cannot be loaded, it'll be disabled.
Although this is harmless, it is not user-friendly and creates some confusion when reading logs. Instead of trying to load and failing, glance should enable 2 stores by default - filesystem and http - and let the others be enabled manually by the user, which is what GridFS does.
TL;DR: The proposal is to remove all stores but file and http from here[0]
[0] https://git.openstack.org/cgit/openstack/glance/tree/glance/store/__init__.py#n39","Don't enable all stores by default

Glance currently enables all stores by default. This patch changes that
by removing all stores that require manual configuration and leaving
those that work right out of the box.

Current behavior causes a lot of confusion to users since most of those
stores print errors when they're not configured correctly. All extra
stores should be enabled explicitly by users.

This fix makes tests use http locations. All other locations besides the
default ones should be tested in their own test suites.

DocImpact
Closes-bug: #1255556
Change-Id: I82073352641d3eb2ab3d6e9a6b64afc99a30dcc7"
246,1f6972f3fdfb87b09c03f3c2c7ab1870d90e0dc2,1396972661,2.0,1.0,247,54,9,3,2,0.979125539,True,10.0,3631825.0,55.0,3.0,True,,,,,,,,,,,,,,,,,701,1127,1299150,cinder,1f6972f3fdfb87b09c03f3c2c7ab1870d90e0dc2,1,1,Windows dependency?,"Bug #1299150 in Cinder: ""Cinder fails to create volumes on Windows server 2012 R2""","Windows Server 2012 R2 does not support vhd images as iSCSI disks, requiring VHDX images. For this reason, the cinder driver fails to create volumes. For the moment, the default format is vhd. On WSS 2012 R2, we should use vhdx images as default.","Fixes cinder volume create on Windows Server 2012 R2

Windows Server 2012 R2 does not support vhd images as iSCSI disks,
requiring VHDX images. For this reason, the cinder driver fails to
create volumes. For the moment, the default format is vhd.
On WSS 2012 R2, we should use vhdx images as default.

This patch introduces vhd/vhdx related methods for both v1 and v2
wmi virtualization namespaces.

Closes-Bug: #1299150
Change-Id: I272988dc5b0e3b8129c5e4d3c79bea1c292701c8"
247,1f965cac884165ce11c7c81bfdb982e7735e11e9,1371675976,2.0,,15,7,1,1,1,0.0,True,1.0,1824255.0,11.0,6.0,False,21.0,190397.0,53.0,4.0,466.0,1236.0,1350.0,400.0,893.0,971.0,334.0,653.0,688.0,0.402160864,0.785114046,0.827130852,1852,1334711,1334711,Glance,1f965cac884165ce11c7c81bfdb982e7735e11e9,0,0,“This change adds the equivalent documentation for the registry.”,"Bug #1334711 in Glance: ""Document workers parameter for the registry""","The workers parameter applies to the registry as well as the api, ie adding workers=N to glance-registry.conf will increase the number of registry processes.","Make eventlet hub choice configurable.

Some platforms do not support the 'poll' hub.

Change-Id: Ibd6abe499c7f6fdd84f5626b7e8fcb756e45ed57"
248,1fb1b058211f08c0b993372e734ed62cd9267193,1392947575,,1.0,1,1,1,1,1,0.0,True,2.0,471753.0,23.0,7.0,False,2.0,2157287.0,2.0,5.0,28.0,2685.0,2706.0,24.0,2331.0,2349.0,26.0,1765.0,1784.0,0.003646677,0.238519719,0.2410859,385,798,1278291,nova,1fb1b058211f08c0b993372e734ed62cd9267193,1,1,typo getting the msg,"Bug #1278291 in OpenStack Compute (nova): ""log_handler miss  some log information""","log_handler:PublishErrorsHandler just emit the `msg` attribute of record. But many times we log with extra arguments, like ""LOG.debug('start %s', blabla)"", which will result in only show ""start %s"" in notification payload.","Emit message which merged user-supplied argument in log_handler

Sync from Oslo, change-id: I91289cc4a60f5dab89bca852e6f52b4b83831e47

When using PublishErrorsHandler, it will missing user-supplied arguments.
For example, do LOG.info(""blabla %s"", ""foo""), the payload only contains
""blabla %s"", but we expect it like ""blabla foo"". This patch fix it.

Change-Id: I375b0d83783b1866d3bf30ff3ade27e746bb856b
Closes-bug: #1278291"
249,1fbf09814481229e722873474d0be56bb7d53803,1382379360,,1.0,7,34,1,1,1,0.0,True,5.0,873373.0,22.0,6.0,False,8.0,3548781.0,9.0,3.0,8.0,623.0,625.0,8.0,475.0,477.0,6.0,456.0,457.0,0.007049345,0.460221551,0.4612286,1424,1175940,1175940,glance,1fbf09814481229e722873474d0be56bb7d53803,0,0,Test files,"Bug #1175940 in Glance: ""Do not use swift / s3 to test copy file functionality""","Currently, test_copy_to_file test requires swift and / or s3 to test this functionality.
Swift / S3 tests belong to their stores and test_copy_to_file should use a HTTP instance or whatever, to test this functionality.
https://github.com/openstack/glance/blob/master/glance/tests/functional/v1/test_copy_to_file.py","Use HTTP storage to test copy file functionality

The copy file functionality should be tested using Filesystem and HTTP
backend stores instead of S3 and Swift (requiring more configuration).

Closes-Bug: #1175940

Change-Id: I8c45598f2f004006f379861479f6f34150d0f649"
250,1fc9eafab43accc127044351106983e2cb215520,1387238711,,1.0,2,2,1,1,1,0.0,True,2.0,25518.0,6.0,2.0,False,13.0,4743346.0,24.0,2.0,77.0,1370.0,1445.0,68.0,1096.0,1162.0,63.0,1249.0,1310.0,0.009310445,0.181844632,0.19071865,191,594,1261585,nova,1fc9eafab43accc127044351106983e2cb215520,1,0,For windows compatibility,"Bug #1261585 in OpenStack Compute (nova): ""ConfigDrive metadata is incorrectly generated on Windows""","Files must be written with ""wb"" instead of ""w"" in order to support multiple platforms:
https://github.com/openstack/nova/blob/b823db737855149ba847e5b19df9232f109f6001/nova/virt/configdrive.py#L92","Fixes ConfigDrive bug on Windows

Files must be written specifying ""wb"" instead of ""w""
to support binary files on both Windows and Linux.

Change-Id: I069845136df4a11b54c931a71575ca3efb9bc6d0
Closes-Bug: #1261585"
251,1fdb38fd4b43c2685658c1a7aa6a56c92ac31585,1408534839,,1.0,60,21,5,2,1,0.865204655,False,,,,,True,,,,,,,,,,,,,,,,,1836,1301854,1301854,Cinder,1fdb38fd4b43c2685658c1a7aa6a56c92ac31585,1,1,"“Therefore, relying on image size to invoke extend API might
result in VIM API fault, if the virtual disk size is same as the target
size”","Bug #1301854 in Cinder: ""VMware: Create volume from image fails with error "" A specified parameter was not correct""""","Steps to reproduce:
1) Create a cinder volume with 1GB
2) Upload the volume created in step 1 to image
3) Create a cinder volume from the image created in step 2
The volume gets created correctly but the disk extend is throwing an exception with message  "" A specified parameter was not correct"".","VMware: Remove redundant extend disk API call

Extend virtual disk API is called during volume creation from image if
the image size is less than volume size. In the case of streamOptimized
and sparse vmdk images, the size of the virtual disk created from the
image can be greater than image size and equal to the volume size. For
example, streamOptimized image created from a new 1GB volume has size
69120 bytes and the size of the virtual disk created from this image
is 1GB. Therefore, relying on image size to invoke extend API might
result in VIM API fault, if the virtual disk size is same as the target
size (volume size). The fix is to read the current virtual disk size to
decide whether extend disk needs to be invoked or not.

Closes-Bug: #1301854
Change-Id: I990ac0b9aef68b3ef8b6d67188db8d44b5c29599"
252,202fe60b646429aabeb2ef8b1430a1ba9b356061,1381851692,1.0,1.0,9,4,2,2,1,0.619382195,True,2.0,41956.0,16.0,5.0,False,5.0,555179.5,5.0,4.0,24.0,884.0,902.0,21.0,841.0,857.0,4.0,376.0,376.0,0.011061947,0.834070796,0.834070796,1689,1240125,1240125,neutron,202fe60b646429aabeb2ef8b1430a1ba9b356061,1,0,“Handle VLAN interfaces with Linux IP wrapper”,"Bug #1240125 in neutron: ""Linux IP wrapper cannot handle VLAN interfaces""","Interface VLAN name have a '@' character in their names when iproute2 utility list them.
But the usable interface name (for iproute2 commands) is the string before the '@' character, so this interface need a special parse.
$ ip link show
1: wlan0: <NO-CARRIER,BROADCAST,MULTICAST,UP> mtu 1500 qdisc mq state DOWN group default qlen 1000
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
    inet 169.254.10.78/16 brd 169.254.255.255 scope link wlan0:avahi
       valid_lft forever preferred_lft forever
2: wlan0.10@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff
3: vlan100@wlan0: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN group default
    link/ether 6c:88:14:b7:fe:80 brd ff:ff:ff:ff:ff:ff","Handle VLAN interfaces with Linux IP wrapper

Check if the interface name contains '@' character and use the part
before that character as interface name.

Change-Id: I09eb52f4d41054a76fd140d3a3476c6cfe05226b
Closes-Bug: #1240125"
253,203e8bf9daf35319d56612de9c8b75d36fcfafd4,1380567585,,1.0,23,18,1,1,1,0.0,True,4.0,195381.0,35.0,13.0,False,6.0,1027778.0,10.0,2.0,10.0,1026.0,1031.0,10.0,913.0,918.0,9.0,888.0,892.0,0.009460738,0.841059603,0.844843898,1621,1233288,1233288,cinder,203e8bf9daf35319d56612de9c8b75d36fcfafd4,0,0,Bug in comments “should be editted to be more clear.”,"Bug #1233288 in Cinder: ""Edit GPFS config flag help text for clarity""",The help text for gpfs config flags should be editted to be more clear.,"Improve gpfs config flag help text readability

Modify the help text for gpfs configuration flags so that it
reads more clearly.

Closes-Bug: #1233288

Change-Id: Iea41bceb496dfaa1f8625a6e3abb31794838213d"
254,204d3ebec8522cfd01f260720c56e3cd879948cf,1379691208,,1.0,10,9,1,1,1,0.0,True,5.0,2200754.0,40.0,18.0,False,9.0,1806227.0,13.0,6.0,62.0,3051.0,3075.0,62.0,2763.0,2787.0,61.0,2171.0,2194.0,0.010175611,0.356474643,0.360249467,1583,1228449,1228449,nova,204d3ebec8522cfd01f260720c56e3cd879948cf,1,1, ,"Bug #1228449 in OpenStack Compute (nova): ""i18n issues for nova/virt/baremetal/virtual_power_driver.py""","In nova source code, some LOG messages in  nova/virt/baremetal/virtual_power_driver.py are not wrapped with _() which will causei18n problems, we need fix this small but important issue, to make our codes professional.","Fix i18n issue for nova/virt/baremetal/virtual_power_driver.py

There are some log messages in virtual_power_driver.py does not
support i18n, we need to update code to support it.

Closes-bug: #1228449

Change-Id: I6642309bd0e08e356a13d134ac0c743159af2520"
255,20667f7d49eed88ddd528be73293a624b3af1ba0,1384730191,,1.0,195,19,6,2,1,0.821480325,True,3.0,10425850.0,18.0,9.0,False,19.0,1898788.667,45.0,7.0,33.0,1249.0,1253.0,32.0,1087.0,1091.0,33.0,1006.0,1010.0,0.029259897,0.866609294,0.870051635,1790,1253657,1253657,cinder,20667f7d49eed88ddd528be73293a624b3af1ba0,1,1, ,"Bug #1253657 in Cinder: ""NetApp Cinder driver doesn't work with vsadmin role credentials""","If you configure the NetApp cinder driver with clustered mode and use a login/password with only the vsadmin role, many things will not work and APIs will fail with stack traces.","NetApp fix for vsadmin role failure for ssc

The ssc feature does not work with vserver admin
role. If the user is a vsadmin then he does cannot
use the ssc feature fully. This fix checks the access
and issues apis appropriately and also handles error
conditions.

Closes-Bug: #1253657

Change-Id: Ibc4409e5d5e6e36bba41e5f59dd94c05b87b1cde"
256,20b97388efcc3540066ca7743c761c1ec899dd2c,1379588843,,1.0,27,9,4,2,1,0.723308334,True,3.0,692803.0,15.0,5.0,False,15.0,346722.0,22.0,4.0,115.0,1285.0,1312.0,115.0,1237.0,1264.0,74.0,293.0,301.0,0.209497207,0.82122905,0.843575419,1566,1226400,1226400,neutron,20b97388efcc3540066ca7743c761c1ec899dd2c,1,0,“The python neutron client for the V2 API expects the neutron API”,"Bug #1226400 in neutron: ""Not all exception information is propagated through API""","The python neutron client for the V2 API expects the neutron API to send information back such as the type and detail of the exception the body of the message (serialized as a dict). See exception_handler_v20 in client.py. However, the neutron v2 api only returns the exception message. This means that the client can only propagate up a generic NeutronClientException exceptions rather than more specific ones related to the actual problem.
All of the necessary information is present in the v2 api resource class at the point the webob exception is raised to include at least the type of the exception (detail appears not to be used in neutron exceptions) so the format of the message returned should be changed to include a type field and a message field rather than just the message.","Send proper exception info as expected by the neutron client

The python neutron client for the V2 API expects the neutron API
to send information back such as the type and detail of the exception
in the body of the message

Change-Id: I9486d757258c4be72799c41102babe1f7923121c
Closes-bug: #1226400"
257,20dbbab61c1159a3dc6ef838ec65b04f7e9cf560,1412169379,,1.0,15,17,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1368,1838,1376128,neutron,20dbbab61c1159a3dc6ef838ec65b04f7e9cf560,1,1,"""Changes in commit 7f8ae630b87392193974dd9cb198c1165cdec93b moved
    pid files”","Bug #1376128 in neutron: ""Neutron agents won't pick up neutron-ns-metadata-proxy after I->J upgrade""","The pid file path changes from %s.pid to %s/pid during juno, due to this change:
https://github.com/openstack/neutron/commit/7f8ae630b87392193974dd9cb198c1165cdec93b#diff-448060f24c6b560b2cbac833da6a143dL68
That means the l3-agent and dhcp-agent (when isolated metadata is enabled)
will respawn a second neutron-ns-metadata proxy on each namespace/resource
after upgrade (I->J) and agent restart due the inability to find the old PID
file and external process PID.","Fix pid file location to avoid I->J changes that break metadata

Changes in commit 7f8ae630b87392193974dd9cb198c1165cdec93b moved
pid files handled by agent/linux/external_process.py from
$state_path/external/<uuid>.pid  to $state_path/external/<uuid>/pid
that breaks the neutron-ns-metadata-proxy respawn after upgrades
becase the l3 or dhcp agent can't find the old pid file so
they try to start a new neutron-ns-metadata-proxy which won't
succeed, because the old one is holding the port already.

Closes-Bug: #1376128
Change-Id: Id166ec8e508aaab8eea35d89d010a5a0b7fdba1f"
258,20fa2a35c843afe409ad347a40751bb59f9ecf9e,1389060481,,1.0,0,1,1,1,1,0.0,True,1.0,3054233.0,18.0,7.0,False,6.0,4192123.0,6.0,4.0,31.0,3038.0,3059.0,31.0,2362.0,2383.0,15.0,2763.0,2769.0,0.002295223,0.396499785,0.397360493,260,664,1266617,nova,20fa2a35c843afe409ad347a40751bb59f9ecf9e,0,0,unused code,"Bug #1266617 in OpenStack Compute (nova): ""Remove unused code in test_attach_interfaces.py""","Remove unused code in test_attach_interfaces.py:
body = jsonutils.dumps({'port_id': FAKE_PORT_ID1}) is unused in test_attach_interface_without_network_id().
We should remove it.","Remove unused code in test_attach_interfaces.py

""body = jsonutils.dumps({'port_id': FAKE_PORT_ID1})"" is unused in
test_attach_interface_without_network_id(). We should remove it.

Change-Id: Id31b559752d57c9f55fd2bf0de909b8d8f9a0333
Closes-Bug: #1266617"
259,21045a58497818f76f880fe74b3eb5ab09088cf3,1387042286,,1.0,25,5,3,2,1,0.918990046,True,7.0,3013490.0,33.0,14.0,False,10.0,5245550.333,15.0,4.0,3.0,2732.0,2733.0,3.0,1865.0,1866.0,3.0,2568.0,2569.0,0.000583942,0.375036496,0.375182482,9,321,1243101,nova,21045a58497818f76f880fe74b3eb5ab09088cf3,1,0,architecture,"Bug #1243101 in OpenStack Compute (nova): ""nova docker driver cannot find cgroup in /proc/mounts on RHEL""","I'm using the nova docker driver on RHEL 3.10.11-1.el6.x86_64 (rebuilt kernel). Based on the format of /proc/mounts on RHEL, the cgroup devices path cannot be found.
On my box the line in /proc/mounts for cgroups looks like this:
none /sys/fs/cgroup cgroup rw,relatime,perf_event,blkio,net_cls,freezer,devices,memory,cpuacct,cpu,cpuset,clone_children 0 0
In the docker driver the method which searches /proc/mounts for the cgroup path looks like this:
    def _find_cgroup_devices_path(self):
        for ln in open('/proc/mounts'):
            if ln.startswith('cgroup ') and 'devices' in ln:
                return ln.split(' ')[1]
Therefore the method cannot find my cgroup path. I hacked around this with a 1 LOC change to the _find_cgroup_devices_path method which looks for 'cgroup' as the 3rd item in the line split:
if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
The update method in its entirety looks like:
169     def _find_cgroup_devices_path(self):
170         for ln in open('/proc/mounts'):
171             if ln.split(' ')[2] == 'cgroup' and 'devices' in ln:
172                 return ln.split(' ')[1]
Based on the format in /proc/mounts on my ubuntu box, this change *should* work on ubuntu as well as rhel.
I did read that docker is only supported with devstack + unbuntu, so I realize this defect may get deferred or even closed. However I wanted to surface it as I believe future efforts of openstack + docker will need to consider non-ubuntu + devstack envs.","nova docker driver cannot find cgroup in /proc/mounts on RHEL

In order to support the docker hypervisor on rhel/centos or fedora
we had to adapt the function which resolves the cgroup path.

Change-Id: Ie24028f822d91d65d4f90a7941e7b11874a3b425
Closes-Bug: #1243101"
260,212c755c34c195148c668b553ccd9f371df63420,1388696189,1.0,1.0,3,10,2,2,1,0.619382195,True,7.0,8191938.0,63.0,20.0,False,18.0,1037261.0,27.0,5.0,282.0,1577.0,1577.0,266.0,1418.0,1418.0,210.0,594.0,594.0,0.318250377,0.897435897,0.897435897,292,699,1269501,neutron,212c755c34c195148c668b553ccd9f371df63420,0,0,Refactor to remove _recycle_ip ,"Bug #1269501 in neutron: ""With removal of explicit _recycle_ip, the method should be removed from db_base_plugin_v2.py""","https://review.openstack.org/#/c/58017 removes the need to explicitly call _recycle_ip.  More specifically, it makes _recycle_ip a simple pass-through to _delete_ip_allocation.  A follow-on needs to remove _recycle_ip and replace calls with _delete_ip_allocation.","Refactor to remove _recycle_ip

Since _recycle_ip is now just a pass-through to _delete_ip_allocation
it can be removed.

Change-Id: Ifba3da902de599f748038a33ef3bd98ff77c22b9
Closes-Bug: #1269501"
261,2192102d85d6d6742832b4961556ad6a2ac7c089,1410807673,,1.0,41,24,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1318,1786,1369605,nova,2192102d85d6d6742832b4961556ad6a2ac7c089,0,0,Feature,"Bug #1369605 in OpenStack Compute (nova): ""nova.db.sqlalchemy.api.quota_reserve is not very unit test-able""","The method is huge and has lots of conditional blocks.
We should break the big conditional blocks out into private methods so the top-level quota_reserve logic can be unit tested on it's own.
This became an issue in this review for a separate bug fix in the logic:
https://review.openstack.org/#/c/121259/","Break out over-quota processing from quota_reserve()

This moves the over-quota processing out of quota_reserve and into a
private method.

This completes the refactor of the quota_reserve method to isolate the
large chunks of logic into separate private methods.

Closes-Bug: #1369605

Change-Id: Ic910ea042dfe50209f79928e61b3b34b4c734e73"
262,21aeb80a83e157dbc2ba27f16a83c98a37b7067d,1408069348,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1198,1656,1357152,nova,21aeb80a83e157dbc2ba27f16a83c98a37b7067d,1,1,"""commit 374e9766c20c9f83dbd8139aa9d95a66b5da7295”","Bug #1357152 in OpenStack Compute (nova): ""nova.utils.TIME_UNITS['Day'] is only 84400 sec""","Full day is 86400 sec
diff --git a/nova/utils.py b/nova/utils.py
index 65d99aa..0b9afe4 100644
--- a/nova/utils.py
+++ b/nova/utils.py
@@ -90,7 +90,7 @@ TIME_UNITS = {
     'SECOND': 1,
     'MINUTE': 60,
     'HOUR': 3600,
-    'DAY': 84400
+    'DAY': 86400
 }
Based on code from master branch 2014-08-14 HEAD commit 374e9766c20c9f83dbd8139aa9d95a66b5da7295","Correct seconds of a day from 84400 to 86400

Correct a day to 86400 seconds which is used by API rate limit config
Closes-bug: #1357152

Change-Id: Ieceacd00f02b6ed27032cc0ac2f55be8b976a070"
263,21baedfc3185111589535cdc24fff83603a5e3fc,1410508909,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1308,1774,1368597,nova,21baedfc3185111589535cdc24fff83603a5e3fc,1,1,,"Bug #1368597 in OpenStack Compute (nova): ""Wrong status code in @wsgi.response decorator in server's `confirmResize` action""","In server`s action `confirmResize` status code in @wsgi.response decorator is set as 202 but this is overridden/ignored by return statement (return exc.HTTPNoContent()) which return 204 status code  - https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L1080
This is very confusing and we should have expected status code in @wsgi.response decorator as consistence with other APIs.
There is no change required in API return status code but in code it should be fixed to avoid the confusion.","Fix `confirmResize` action status code in V2

In server`s action `confirmResize` status code in @wsgi.response
decorator is set as 202 but this is overridden/ignored by return
statement (return exc.HTTPNoContent()) which return 204 status code.

This is very confusing and we should have expected status code in
@wsgi.response decorator as consistence with other APIs.

NOTE- There is no change in API return status code. API returns the
same 204 return code as previously.

Change-Id: Id8033fb8880306babebc14b319bce5b3b8798774
Closes-Bug: #1368597"
264,222d44532c65ddf3f26532ced217890628352536,1410790814,,1.0,189,55,11,6,1,0.616569875,False,,,,,True,,,,,,,,,,,,,,,,,798,1226,1308342,nova,222d44532c65ddf3f26532ced217890628352536,1,1,,"Bug #1308342 in OpenStack Compute (nova): ""Cannot delete vm instance if send duplicate delete requests ""","I deployed openstack with icehouse rc1 and booted 100 vms on my nodes. After my testing, i  tried to delete my vms at the same time. Then i fount all of my vms` status change to deleting but cannot be deleted. I checked my openstack, the rabbitmq-server crashed . Then i restart rabbitmq-server and my openstack nova services, sended the delete requests again and again, the vms still cannot be deleted. While , in havana, the vms can be deleted if received duplicate delete requests .
I think icehouse should handle duplicate delete requests like havana .","Allow force-delete irrespective of VM task_state

Cannot delete vm instance if duplicate delete requests are sent.

When user sends request to delete an instance, its task_state gets
changed to 'deleting' state. When an instance task_state is already in
'deleting' state and if at that moment the rabbitmq-server get crashed
by some reasons, then the instance task_state remains in 'deleting'
state and user won't be able to delete the instance forever.

At this moment, there is only one way to delete the instances, whose
task_state is in 'deleting' state, by restarting the nova-compute
services where these instances are running.

To avoid restarting the nova-compute service manually, modified the
force-delete api to allow instance deletion irrespective of
instance task_state.

Added new module to use delete_types as constants and replaced all
delete_type string occurances with new constants.

Closes-Bug: #1308342

Change-Id: I4d0e47662a80109ef9622d85455587d487e47c01"
265,22bec67395b52a879ad4eb37b150eead9d4bb444,1398302782,,1.0,106,15,8,4,1,0.813507803,True,7.0,2452984.0,112.0,22.0,False,42.0,1081527.75,61.0,6.0,7.0,448.0,454.0,7.0,440.0,446.0,2.0,395.0,396.0,0.002561913,0.338172502,0.339026473,780,1207,1306488,neutron,22bec67395b52a879ad4eb37b150eead9d4bb444,1,1,,"Bug #1306488 in neutron: ""invalid gre/vxlan id shouldn't be created successfully for ml2""","create vxlan successfully with invalid vxlan id for ml2:
curl -i http://127.0.0.1:9696/v2.0/networks -H ""X-Auth-Token:$token_id"" -H ""Content-Type: application/json"" -X POST -d '{""network"": {""name"": ""vxlan_test"", ""admin_state_up"": true, ""tenant_id"": $tenant_id, ""provider:network_type"": ""vxlan"", ""router:external"": false, ""shared"": false, ""provider:segmentation_id"": -1}}'
response:
HTTP/1.1 201 Created
Content-Type: application/json; charset=UTF-8
Content-Length: 332
X-Openstack-Request-Id: req-1e9af36d-b742-4c76-bfdf-0cea4df6399f
Date: Fri, 11 Apr 2014 09:25:51 GMT
{
    ""network"": {
        ""status"": ""ACTIVE"",
         ""subnets"": [
        ],
         ""name"": ""vxlan_test"",
         ""provider:physical_network"": null,
         ""admin_state_up"": true,
         ""tenant_id"": ""bfed9cd5990c49ad8a42ba36d505c003"",
         ""provider:network_type"": ""vxlan"",
         ""router:external"": false,
         ""shared"": false,
         ""id"": ""5bd9b587-02f8-4432-b0d0-18a126126072"",
         ""provider:segmentation_id"": -1
    }
}","Tunnel ID range validation for VXLAN/GRE networks

Currently, there is no check which validates the values of
tunnel range for VXLAN/GRE networks. The VXLAN VNI is 24 bit
which have range between 1 to 2^24 - 1. Similarly, GRE key field
is 32 bit which have range between 1 to 2^32 - 1.

Closes-Bug: #1306488

Co-Authored-By: romilg romilg@hp.com

Change-Id: Idb3d0f41166df589a1e90394d9319901b5f9b439"
266,22cd7cc27ca8285c1674e370e256ec84d3f2a20a,1385377406,,1.0,2,15,3,2,1,0.926262155,True,3.0,648049.0,17.0,5.0,False,149.0,8199.0,426.0,2.0,35.0,1645.0,1676.0,35.0,1350.0,1381.0,13.0,1591.0,1600.0,0.002096436,0.238394729,0.239742438,75,472,1254811,nova,22cd7cc27ca8285c1674e370e256ec84d3f2a20a,0,0, Deprecate and remove libvirt.api_thread_pool option ,"Bug #1254811 in OpenStack Compute (nova): ""Deprecate and remove libvirt.api_thread_pool option""","We should deprecate the libvirt.api_thread_pool config option in icehouse and remove it in the J release.  Looks like the last time it was touched was in January for some refactoring:
https://github.com/openstack/nova/commit/ce27bca5497600ff9ab7195e27ede94e7cffe5d0
The reasons why this should be removed are detailed here:
https://review.openstack.org/#/c/57000","Remove the api_thread_pool option from libvirt driver

This gave us the option to select if calls to libvirtd are done
in a native thread or directly. This was added because the person
who wrote the thread pool code was afraid of regressions for existing
deployments. But now we have enough features in nova that require
that this is always set to True or else users can have issues with
them. To this point the firewall overrode this option to make
sure that its call was always in a native thread no matter the
value of this option was.

The reasons behind this were discussed during this review:
https://review.openstack.org/#/c/57000/

Closes-Bug: #1254811

DocImpact: removes api_thread_pool flag from the libvirt group

Change-Id: Ifad7f1bdf2af69e4fcd6f9a04bf2f7ff5251afd6"
267,2302ed3f22512590af23a649e09b952189e0fa9d,1390842566,,1.0,6,6,3,1,1,1.0,True,2.0,3032355.0,42.0,9.0,False,6.0,1463541.333,6.0,3.0,11.0,1215.0,1218.0,11.0,1056.0,1059.0,11.0,578.0,581.0,0.016393443,0.790983607,0.795081967,311,720,1271231,neutron,2302ed3f22512590af23a649e09b952189e0fa9d,1,0,Bug by evolution,"Bug #1271231 in neutron: ""securitygroups table is created again while migrating database from havana to icehouse""","There is attempt to create a new table securitygroups by 49f5e553f61f_ml2_security_groups.py while table already exists (created by 3cb5d900c5de_security_groups.py)
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running upgrade havana -> e197124d4b9, add unique constraint to members
INFO  [alembic.migration] Running upgrade e197124d4b9 -> 1fcfc149aca4, Add a unique constraint on (agent_type, host) columns to prevent a race
condition when an agent entry is 'upserted'.
INFO  [alembic.migration] Running upgrade 1fcfc149aca4 -> 50e86cb2637a, nsx_mappings
INFO  [alembic.migration] Running upgrade 50e86cb2637a -> ed93525fd003, bigswitch_quota
INFO  [alembic.migration] Running upgrade ed93525fd003 -> 49f5e553f61f, security_groups
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 143, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/new/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 199, in load_python_file
    module = load_module(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 55, in load_module
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 652, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/new/neutron/neutron/db/migration/alembic_migrations/versions/49f5e553f61f_ml2_security_groups.py"", line 53, in upgrade
    sa.PrimaryKeyConstraint('id')
  File ""<string>"", line 7, in create_table
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 647, in create_table
    self._table(name, *columns, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 149, in create_table
    self._exec(schema.CreateTable(table))
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.OperationalError: (OperationalError) (1050, ""Table 'securitygroups' already exists"") '\nCREATE TABLE securitygroups (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tdescription VARCHAR(255), \n\tPRIMARY KEY (id)\n)\n\n' ()","Move db migration of ml2 security groups to havana

ml2 plugin is a havana feature. Currently securitygroups table are
created in chain of migration after havana release. It causes db
migration failure when migrating from havana to current head because
securitygroups table is attempted to be created although it was already
created by create_all().

Closes-bug: #1271231
Change-Id: If450bd95de7a5fdfdb2219cfbf7eb0c1323044bb"
268,2318f9ba64b4c793eb913330b862d608d86c03b2,1395498609,,1.0,69,8,5,5,1,0.736200821,True,2.0,4746366.0,40.0,12.0,False,45.0,7862791.8,92.0,4.0,108.0,1874.0,1894.0,108.0,1574.0,1594.0,108.0,1568.0,1588.0,0.014227908,0.20480355,0.207414176,664,1090,1295558,nova,2318f9ba64b4c793eb913330b862d608d86c03b2,0,0,Future bug,"Bug #1295558 in OpenStack Compute (nova): ""make unshelving and rescuing as io state which will be counted for ioops filter""","IOopsFilter will take instance in following state
task_states.RESIZE_MIGRATING, task_states.REBUILDING, task_states.RESIZE_PREP, task_states.IMAGE_SNAPSHOT,               task_states.IMAGE_BACKUP
into consideration ,if the instance on this host exceed the maximum number of instance allowed to do I/O
the scheduler will fail
we need to take UNSHELVING and RESCUING into consideration","Add UNSHELVING and RESCUING into IoOPSFilter consideration state

IoOpsFilter will take instance in following state
task_states.RESIZE_MIGRATING, task_states.REBUILDING, task_states.RESIZE_PREP,
task_states.IMAGE_SNAPSHOT, task_states.IMAGE_BACKUP
into consideration ,if the instance on this host exceed the maximum
number of instance allowed to do I/O the scheduler will fail on the host

UNSHELVING and RESCUING are also I/O related so IoOps need to include them

Change-Id: I141470254c824e7e57e6734ae0e016c432b85c00
Closes-Bug: #1295558"
269,232cbfe67ffb7696f115830c711a960af5fa0828,1401842576,,1.0,16,2,3,3,1,0.878347105,False,,,,,True,,,,,,,,,,,,,,,,,946,1382,1326183,nova,232cbfe67ffb7696f115830c711a960af5fa0828,1,1,,"Bug #1326183 in OpenStack Compute (nova): ""detach interface fails as instance info cache is corrupted""","Performing attach/detach interface on a VM sometimes results in an interface that can't be detached from the VM.
I could triage it to the corrupted instance cache info due to non-atomic update of that information.
Details on how to reproduce the bug are as follows. Since this is due to a race condition, the test can take quite a bit of time before it hits the bug.
Steps to reproduce:
1) Devstack with trunk with the following local.conf:
disable_service n-net
enable_service q-svc
enable_service q-agt
enable_service q-dhcp
enable_service q-l3
enable_service q-meta
enable_service q-metering
RECLONE=yes
# and other options as set in the trunk's local
2) Create few networks:
$> neutron net-create testnet1
$> neutron net-create testnet2
$> neutron net-create testnet3
$> neutron subnet-create testnet1 192.168.1.0/24
$> neutron subnet-create testnet2 192.168.2.0/24
$> neutron subnet-create testnet3 192.168.3.0/24
2) Create a testvm in testnet1:
$> nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64-uec --nic net-id=`neutron net-list | grep testnet1 | cut -f 2 -d ' '` testvm
3) Run the following shell script to attach and detach interfaces for this vm in the remaining two networks in a loop until we run into the issue at hand:
--------
#! /bin/bash
c=10000
netid1=`neutron net-list | grep testnet2 | cut -f 2 -d ' '`
netid2=`neutron net-list | grep testnet3 | cut -f 2 -d ' '`
while [ $c -gt 0 ]
do
   echo ""Round: "" $c
   echo -n ""Attaching two interfaces... ""
   nova interface-attach --net-id $netid1 testvm
   nova interface-attach --net-id $netid2 testvm
   echo ""Done""
   echo ""Sleeping until both those show up in interfaces""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       if [ $count -eq 7 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   echo ""Waited for "" $waittime "" seconds""
   echo ""Detaching both... ""
   nova interface-list testvm | grep $netid1 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   nova interface-list testvm | grep $netid2 | awk '{print ""deleting "",$4; system(""nova interface-detach testvm ""$4 "" ; sleep 2"");}'
   echo ""Done; check interfaces are gone in a minute.""
   waittime=0
   while [ $waittime -lt 60 ]
   do
       count=`nova interface-list testvm | wc -l`
       echo ""line count: "" $count
       if [ $count -eq 5 ]
       then
           break
       fi
       sleep 2
       (( waittime+=2 ))
   done
   if [ $waittime -ge 60 ]
   then
      echo ""bad case""
      exit 1
   fi
   echo ""Interfaces are gone""
   ((  c-- ))
done
---------
Eventually the test will stop with a failure (""bad case"") and the interface remaining either from testnet2 or testnet3 can not be detached at all.","Neutron: Atomic update of instance info cache

In the Neutron network API implementation, there is a race condition
between a thread performing periodic task to read and heal instance
network info cache and another thread servicing interface-attach or
interface-detach calls. This patch ensures that instance info cache is
read and then updated in a synchronized block to ensure atomicity.

Please see the bug report for more details.

Change-Id: Ifc76f2b1cce834b3c9927359ac9b957bc9f9c65f
Closes-Bug: #1326183
Co-Authored-By: Dan Smith <dansmith@redhat.com>"
270,2331d3336a6adf4fc13a3b187e91a5d1b1f7c723,1356699526,1.0,,51,4,4,3,1,0.704634348,True,2.0,470512.0,14.0,7.0,False,28.0,1185964.5,74.0,3.0,1.0,330.0,331.0,1.0,330.0,331.0,0.0,239.0,239.0,0.003787879,0.909090909,0.909090909,1818,1270020,1270020,Cinder,2331d3336a6adf4fc13a3b187e91a5d1b1f7c723,1,1,"""but the exception msg only contain info about ""available"". So fix it.”","Bug #1270020 in Cinder: ""Fix exception log msg in attach volume method""","Attach volume happened when volume's status is attaching or available, but the exception msg only contain info about ""available"". So fix it.","Adds synchronization to attach volume.

This patch adds the following changes:
1. Move db update for volume status during attach to volume.manager
2. Add lock synchronization to volume attach operation
3. Related unit tests

Fixes Bug 1087253
Change-Id: I15242766bf4cfa4da67789c485fdf6886983eb45"
271,23340b49b1adee5cb9592b8e6a8471969b9341c7,1407240677,,1.0,51,28,4,4,1,0.866824634,False,,,,,True,,,,,,,,,,,,,,,,,1168,1625,1354664,nova,23340b49b1adee5cb9592b8e6a8471969b9341c7,1,0,"""This is due to the change to objects.”,”Commit 1023e703bd41c2a42b1159af0d9e907e94440b34 added support
    for objects.”","Bug #1354664 in OpenStack Compute (nova): ""Image cache aging: change to objects causes invalid data to be passed""","An extra entry of invalid data is passes to the image cache aging:
USED: {'': (2, 0, ['instance-0000000a', 'instance-0000000a']), '7ee53435-b6d5-4c15-bce4-2f3dfac96ffd': (1, 0, ['instance-0000000a'])}
This is due to the change to objects.","Image caching: update image caching to use objects

Commit 1023e703bd41c2a42b1159af0d9e907e94440b34 added support
for objects. This patch ensures that the parsing of the instances
is correct.

The patch does the following:
1. In the method _list_running_instances it treats all instances
   as an object
2. Fixes the bug mentioned below. The code was making a string from
   None. This would lead to invalid image details.
3. Ensures that the unit tests actually pass instance objects instead
   of an instance dict
4. In relevant tests updates assertEquals to have the expected parameter
   first

Closes-bug: #1354664

Change-Id: Ib028baab4d4c823f22c371a83f7a813c24d77570"
272,23790183d20f59ddcb6025e8497aa16e443bb929,1400082933,,1.0,1,1,1,1,1,0.0,True,2.0,573862.0,25.0,9.0,False,70.0,399105.0,164.0,2.0,20.0,1225.0,1243.0,10.0,1224.0,1233.0,18.0,1202.0,1218.0,0.002381549,0.150789672,0.152795187,876,1310,1317654,nova,23790183d20f59ddcb6025e8497aa16e443bb929,1,1,,"Bug #1317654 in OpenStack Compute (nova): ""default dhcp lease time of 120s is too short""",The default dhcp lease time is fairly short (120s). Is this simply a hold-over from the time before force_dhcp_release was the default and dhcp_release was expected throughout or is there another reason for the default to be so brief?,"Default dhcp lease time of 120s is too short

As part of reasonable-defaults, increase the default DHCP
lease time to a more sane value of 24 hours.  This change
also aligns Nova with the current default DHCP lease time
in Neutron.

DocImpact: This changes the default dhcp_lease_time from
120 seconds to 86400 seconds (24 hours).  If the Icehouse
or previous release behavior is desired, set the config
dhcp_lease_time to 120.

Change-Id: Ia666ff31f299c57d46605705e9ebea21630e0722
Closes-Bug: #1317654"
273,238689570145fdafe8e61967bb56133f167d39ab,1395912825,,1.0,1,1,1,1,1,0.0,True,4.0,729707.0,53.0,10.0,False,8.0,1953998.0,8.0,3.0,1135.0,1292.0,1292.0,965.0,1120.0,1120.0,676.0,807.0,807.0,0.650961538,0.776923077,0.776923077,1642,1236372,1236372,neutron,238689570145fdafe8e61967bb56133f167d39ab,1,1, ,"Bug #1236372 in neutron: ""Router without active ports fails to be deleted, are reports wrong error message""","Version
=======
Havana, RHEL, neutron+ovs, python-neutron-2013.2-0.3.3.b3.el6ost
Description
===========
It's impossible to delete a router while it still has inactive ports.
The error message states that the router still has active ports.
# neutron router-port-list router1
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                            |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
| 052a48e4-0868-4675-bef7-8f763dd697b4 |      | fa:16:3e:60:73:8d | {""subnet_id"": ""c5d63940-71e8-4338-865e-f5364fbe4e78"", ""ip_address"": ""10.35.214.1""}   |
| 065cef02-a949-45c9-b3df-e005dbf96c9a |      | fa:16:3e:a6:6d:d8 | {""subnet_id"": ""044bcc05-f37b-4d1d-a700-c91c4381fbc8"", ""ip_address"": ""10.35.211.1""}   |
| 7a020243-90e5-439d-90fb-ec96b07843e7 |      | fa:16:3e:04:0c:1f | {""subnet_id"": ""4081fbca-3e59-4be5-a98e-3c9e0d13d3a6"", ""ip_address"": ""10.35.212.1""}   |
| 7af56958-674e-472b-8dbe-09b60501a6e6 |      | fa:16:3e:1a:07:a4 | {""subnet_id"": ""ef8e7c03-f17f-4c3c-9afe-252aca1283fd"", ""ip_address"": ""10.35.170.102""} |
| f034cb8a-2a09-4d41-b46c-a08fe208461e |      | fa:16:3e:de:9d:32 | {""subnet_id"": ""cca4edc7-2872-4c1e-a270-3b0beb60f421"", ""ip_address"": ""10.35.213.1""}   |
+--------------------------------------+------+-------------------+--------------------------------------------------------------------------------------+
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                             |
| status                | ACTIVE                                                                               |
| status                | ACTIVE                                                                             |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-update $i --admin_state_up False ; done | grep status
# for i in `neutron router-port-list router1 | grep subnet_id | cut -d"" "" -f 2` ; do neutron port-show $i ; done | grep status
| status                | DOWN                                                                               |
| status                | DOWN                                                                               |
| status                | DOWN                                                                               |
| status                | DOWN                                                                                 |
| status                | DOWN                                                                               |
# neutron router-delete router1
Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
From /var/log/neutron/server.log
================================
2013-10-07 16:39:40.429 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource     raise l3.RouterInUse(router_id=id)
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:39:40.429 2341 TRACE neutron.api.v2.resource
2013-10-07 16:40:34.870 2341 ERROR neutron.api.v2.resource [-] delete failed
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_db.py"", line 266, in delete_router
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource     raise l3.RouterInUse(router_id=id)
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource RouterInUse: Router 727edf71-f637-402e-9fd9-767c372922ee still has active ports
2013-10-07 16:40:34.870 2341 TRACE neutron.api.v2.resource","Changed the message line of RouterInUse class

Implements: Changed the message line of RouterInUse class,
so user will not get confused with active or inactive ports
while deleting the router.

Closes-Bug: #1236372
Change-Id: I7a82550e2c6051f9e4940a8f1dd4401059e0a8d8"
274,2392313f562ba6a90ed1ec3fbc507862043fa44f,1381557158,,1.0,17,1,2,2,1,0.99107606,True,4.0,262894.0,21.0,8.0,False,127.0,178896.0,532.0,4.0,77.0,2024.0,2073.0,77.0,1692.0,1741.0,76.0,1798.0,1846.0,0.012229987,0.285736976,0.293360864,1651,1236930,1236930,nova,2392313f562ba6a90ed1ec3fbc507862043fa44f,1,1,“nova compute api should throw exception for such state.” Feature,"Bug #1236930 in OpenStack Compute (nova): ""attempting to reboot a shutdown/suspened/crashed/paused instance appears to have failed, but then surprisingly succeeds two minutes later""","I am running Havana from precise-proposed in the UCA (nova 1:2013.2~b3-0ubuntu1~cloud0).
To reproduce:
- start an instance
- reboot (sudo reboot) the compute node on which it is running
-  after the compute node is done booting, the instance will be off:
root@xen10:~# nova list
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| ID                                   | Name | Status  | Task State | Power State | Networks                |
+--------------------------------------+------+---------+------------+-------------+-------------------------+
| 4824dce8-d876-4022-a446-3fc8d708ac62 | test | SHUTOFF | None       | Shutdown    | novanetwork=172.20.46.3 |
+--------------------------------------+------+---------+------------+-------------+-------------------------+
(note that although my hostname has ""xen"" in it, I'm using KVM. Haven't updated DNS yet...)
- attempt to reboot the instance (nova reboot 4824dce8-d876-4022-a446-3fc8d708ac62)
# nova show 4824dce8-d876-4022-a446-3fc8d708ac62
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| status                               | SHUTOFF                                                  |
| updated                              | 2013-10-08T15:28:47Z                                     |
| OS-EXT-STS:task_state                | rebooting                                                |
The reboot fails. The compute node will log:
2013-10-08 11:28:55.579 1400 WARNING nova.compute.manager [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] trying to reboot a non-running instance: (state: 4 expected: 1)
- attempt to start the instance (nova start 4824dce8-d876-4022-a446-3fc8d708ac62):
produces console output:
ERROR: Instance 4824dce8-d876-4022-a446-3fc8d708ac62 in task_state rebooting. Cannot start while the instance is in this state. (HTTP 400) (Request-ID: req-732224e1-8c34-4754-84f7-7a8476673185)
- wait about 120 seconds, and the compute node will log:
2013-10-08 11:30:56.082 1400 WARNING nova.virt.libvirt.driver [req-11fe1624-22f6-4348-81c5-185d0ce0d3a0 a70453729dd84bfd8f31019b1bb91e40 46ab32189ab64a4c92f8f64e6c9ed028] [instance: 4824dce8-d876-4022-a446-3fc8d708ac62] Failed to soft reboot instance. Trying hard reboot.
Afterwards, the instance will be running.
It's confusing that the reboot logs a failure for a very obvious reason (an instance that is not running can't be *re*booted), yet the instance's state remains as ""rebooting"". I had expected that the reboot had failed, and openstack was in some consistant state. I was then again suprised when in fact it *was* still rebooting -- it just took two minutes to do so. Less confusing would be to catch the original error, and report the reboot as failed. The log messages are confusing, because the first sets the expectation that a non-running instance can't be rebooted, but it can (two minutes later).","compute api should throw exception if soft reboot invalid state VM

When user perform soft reboot to a VM which in suspended/paused/
stopped/error state, nova compute api should throw exception for
such state.

Change-Id: Ic365c6360f6b7407d9de0dac6ff1093484692cf4
Closes-Bug: #1236930"
275,23e820abb8685573f97f5e3764225a97dd695127,1395997033,,1.0,3,2,2,1,1,0.721928095,True,1.0,505957.0,8.0,4.0,False,8.0,4025707.5,8.0,3.0,1222.0,1652.0,1659.0,958.0,1304.0,1311.0,1069.0,1261.0,1261.0,0.688545689,0.812097812,0.812097812,699,1125,1299131,cinder,23e820abb8685573f97f5e3764225a97dd695127,1,1,Windows dependency?,"Bug #1299131 in Cinder: ""Cinder fails to associate targets to initiators on Windows""","Due to a small nit, the initiator name and the target name are inverted when passing the arguments to the method which associates the iSCSI target to an initiator. For this reason, this operation will fail.
The connection to the iSCSI target cannot be initialized properly as the method which gets portal information is missing the return value.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/windows/windows.py#L71-72
Trace: http://paste.openstack.org/show/74581/","Fixes cinder volume attach on Windows

The initiator name and the target name are inverted when passing
the arguments to the method which associates the iSCSI target to
an initiator. For this reason, this operation will fail.

Also, the connection to the iSCSI target cannot be initialized
properly as the method which gets portal information is missing
the return value.

Change-Id: I31034cd407d12d69a410f8c9e86a2e6821349dad
Closes-Bug: #1299131"
276,23fa48fa7b86f0767ff4bde9095393a8dfa2e1f7,1365035943,0.0,,248,84,16,9,1,0.889634655,True,6.0,4790006.0,28.0,17.0,False,52.0,723044.0,254.0,4.0,81.0,608.0,618.0,81.0,567.0,577.0,63.0,131.0,139.0,0.083660131,0.17254902,0.183006536,1814,1267103,1267103,Glance,23fa48fa7b86f0767ff4bde9095393a8dfa2e1f7,0,0,“Here the list of misspelled word found in glance source” las misspellings no son un bag verdad3,"Bug #1267103 in Glance: ""missellings on sources ""","Here the list of misspelled word found in glance source
(using misspellings)
http://paste.openstack.org/show/60786/","Adding help text to the options that did not have it.

Many of the config options did not have help text set.  This patch
adds text for those.

patchset2 adds 5 skipped options

patchset3 adds a missing underscore before the helptext bracket

patchset4 fixes pep8 errors

patchset5 notes the units (seconds) in sql_retry_interval, as noted
by Flavio Percoco Premoli

patchset6 fixes comments from Alex Meade

Fixes bug: 1162449
Change-Id: Ic75e499456e389df2ec527594bbcd54c0b464ee3"
277,2418a9dce86280fbf269f68f7d7c9b739f662ef1,1411130514,,1.0,60,14,2,2,1,0.99155285,False,,,,,True,,,,,,,,,,,,,,,,,1225,1684,1360119,nova,2418a9dce86280fbf269f68f7d7c9b739f662ef1,1,0,"“FYI, Nova has always been broken in this regard, but libvirt was buggy and did not enforce uuid,name stability for nwfilters until release 1.2.7”","Bug #1360119 in OpenStack Compute (nova): ""Nova tries to re-define an existing nwfilter with the same name but different uuid""","Hello,
    I have successfully compiled libvirt 1.2.7 and qemu 2.1.0 but had some troubles with nova-compute. It appears like libvirt is throwing back an error if a nwfilter is already present.
Here is my debug log:
2014-08-22 08:22:25.032 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-nodhcp' already exists with uuid 59970732-ca52-4521-ba0c-d001049d8460)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.033 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-base' already exists with uuid b5aa80ad-ea4a-4633-84ac-442c9270a143)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
2014-08-22 08:22:25.034 15354 DEBUG nova.virt.libvirt.firewall [req-0959ec86-3939-4e38-9505-48494b44a9fa f1d21892f9a0413c9437b6771e4290ce 9cad53a0432d4164837b8c0b35d91307] nwfilterDefineXML may have failed with (operation failed: filter 'nova-vpn' already exists with uuid b61eb708-a9a5-4a16-8787-cdc58310babc)! _define_filter /usr/lib/python2.6/site-packages/nova/virt/libvirt/firewall.py:239
Here is the original function:
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        self._conn.nwfilterDefineXML(xml)
And here is the ""patched"" function"":
    def _define_filter(self, xml):
        if callable(xml):
            xml = xml()
        try:
            self._conn.nwfilterDefineXML(xml)
        except Exception, e:
            LOG.debug(_('nwfilterDefineXML may have failed with (%s)!'), e)
I'm not a python expert but I think that patch could be adapted to raise an error ONLY if the nwfilter rule doesn't already exist.
Dave","libvirt: avoid changing UUID when redefining nwfilters

libvirt >= 1.2.7 enforces that when you re-define a network
filter you can't change the UUID. ie name + uuid must match.
Since Nova was not including any UUID in the XML it sent, it
would always get a random UUID generated, which would cause
failures when re-defining an existing filter. The result
was that Nova would fail to start up and fail to migrate
if there was an existing guest running. The fix is to query
libvirt to see if the nwfilter already exists, and extract
the UUID from its XML and use that when re-defining it.

Closes-bug: #1360119
Change-Id: I9d4b2c6c8f0c9a23ed79ed8e0b5ac0d4418851a4"
278,24755ff21f0ee7643ba1cdeb339b260cde5683e9,1392366841,,1.0,3,44,3,2,1,0.347632479,True,1.0,123030.0,8.0,2.0,False,5.0,2468374.667,5.0,2.0,7.0,807.0,813.0,7.0,781.0,787.0,1.0,774.0,774.0,0.00139958,0.542337299,0.542337299,406,820,1280033,cinder,24755ff21f0ee7643ba1cdeb339b260cde5683e9,0,0,‘we don't need this module now ‘,"Bug #1280033 in oslo.messaging: ""Remove dependent module py3kcompat""","Everything in module py3kcompat is ready in six > 1.4.0, we don't need this module now . It was removed from oslo-incubator recently, see  https://review.openstack.org/#/c/71591/.  This make us don't need maintain this module any more, use six directly.","Remove dependent module py3kcompat

Module py3kcompat was removed from oslo-incubator, we can use
six directly.

Change-Id: I6c4266b71312ae53ecaedbd2ce1a865e60eb8d50
Closes-Bug: #1280033"
279,24a19bfd362996cd9833a167d6eb86fdcf743b75,1402564527,,1.0,4,8,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,983,1422,1331249,neutron,24a19bfd362996cd9833a167d6eb86fdcf743b75,0,0,REFACTORING “Big Switch: Remove unnecessary initialization code”,"Bug #1331249 in neutron: ""Big Switch: Uneccesary init method and server_timeout param""","Both the Big Switch Plugin and ML2 driver allow a server_timeout param in the initialization methods that can never be set by a user so it doesn't serve a purpose [1][2]. With those removed, the entire __init__ method of the base class can be removed as well.
1. https://github.com/openstack/neutron/blob/d379170109982a53544d01566ba9231d66b24ed4/neutron/plugins/bigswitch/plugin.py#L171
2. https://github.com/openstack/neutron/blob/1a116d24a955c9e45fa8a29998d09da0350be4ab/neutron/plugins/ml2/drivers/mech_bigswitch/driver.py#L46","Big Switch: Remove unnecessary initialization code

Removes a server_timeout parameter that was never set by anything
in the Big Switch plugin and the ML2 driver.

Also eliminates an __init__ method that didn't do anything except
incorrectly log an unset server pool warning before the inheriting
class setup the server pool.

Closes-Bug: #1331249
Change-Id: I77dd22295305b96d4702f1ea0e319dc5f82c71a8"
280,24d6b57d1cb0c4f91545f85ea9c766d1e0ec50a0,1389167574,,1.0,64,52,5,2,1,0.818055207,True,20.0,2730450.0,65.0,19.0,False,93.0,257253.2,263.0,1.0,54.0,363.0,407.0,49.0,362.0,401.0,47.0,344.0,384.0,0.036585366,0.262957317,0.293445122,114,513,1256763,cinder,24d6b57d1cb0c4f91545f85ea9c766d1e0ec50a0,1,1,confusing message,"Bug #1256763 in Cinder: ""Extending a volume without enough quota isn't caught at API level""","I think this should be caught at API level, it means instant feedback for the user and also the volume goes to ""error_extending"" which can confuse a user thinking the volume is no longer usable.","Validate the quota in the API layer for volume extend

The user needs a friendly message feedback if the quota exceeds when
the volume is about to extend.

Change-Id: Ifd523ac5e9039861cf87711dc5c4842b5cb524c2
Closes-Bug: #1256763"
281,24e4110eb284078775496501ff81630eb1619c11,1412681681,,1.0,0,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1392,1862,1379510,neutron,24e4110eb284078775496501ff81630eb1619c11,0,0,“If the topology sync fails”,"Bug #1379510 in neutron: ""Big Switch: sync is not retried after failure""","If the topology sync fails, no other sync attempts will be made because the server manager clears the hash from the DB before the sync operation. It shouldn't do this because the backend ignores the hash on a sync anyway.","Big Switch: Don't clear hash before sync

This patch removes the step of clearing the consistency
hash from the DB before a topology sync. This will ensure
that inconsistency will be detected if the topology sync
fails.

This logic was originally there to make sure the hash header
was not present on the topology sync call to the backend.
However, the hash header is ignored by the backend in a sync
call so it wasn't necessary.

Closes-Bug: #1379510
Change-Id: I2d58fa2aea3b692834d64192d06ace727c7df8a0"
282,24eb8234ff4b907b8c88f26fb0c427cb7a452140,1397184624,,1.0,32,15,2,1,1,0.784992089,True,6.0,2066728.0,32.0,15.0,False,8.0,551197.5,12.0,5.0,361.0,1687.0,1993.0,196.0,1402.0,1561.0,43.0,1519.0,1519.0,0.027972028,0.966306421,0.966306421,689,1115,1298201,cinder,24eb8234ff4b907b8c88f26fb0c427cb7a452140,1,1,Bug. Add a exception,"Bug #1298201 in Cinder: ""If volume copy not finished, user add a copy of the volume, storwize/svc driver will failed""","2014-03-27 14:22:13.716 ERROR cinder.volume.drivers.san.san [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c4178
0fac445b3b6e9a1ff2c805d8d] Error running SSH command: svctask addvdiskcopy -rsize 2% -autoexpand -warning 20% -grainsize 256 -easytier on -mdi
skgrp Anna_Test volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef
2014-03-27 14:22:13.717 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b8
79ba6 c9c41780fac445b3b6e9a1ff2c805d8d] CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u
'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.718 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Volume a9b15c37-f70f-4a47-8f29-101d7e3970ef: driver error when trying to retype, falling back to generic mechanism.
2014-03-27 14:22:13.719 ERROR cinder.volume.manager [req-ef2dc69d-74f5-4551-b4bd-74ec725f0390 08328d4607504beda2c6bb189b879ba6 c9c41780fac445b3b6e9a1ff2c805d8d] Bad or unexpected response from the storage volume backend API: CLI Exception output:
 command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']
 stdout:
 stderr: CMMVC6352E The command failed because the number of copies of this virtual disk (VDisk) would exceed the limit.
2014-03-27 14:22:13.719 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1232, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     diff, host)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 734, in retype
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     self.configuration)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 703, in add_vdisk_copy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     new_copy_id = self.ssh.addvdiskcopy(vdisk, dest_pool, params)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 304, in addvdiskcopy
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     return self.run_ssh_check_created(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 60, in run_ssh_check_created
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     out, err = self._run_ssh(ssh_cmd)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/ssh.py"", line 41, in _run_ssh
2014-03-27 14:22:13.719 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=msg)
2014-03-27 14:22:13.719 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: CLI Exception output:
2014-03-27 14:22:13.719 TRACE cinder.volume.manager  command: ['svctask', 'addvdiskcopy', '-rsize', '2%', '-autoexpand', '-warning', '20%', '-grainsize', '256', '-easytier', 'on', '-mdiskgrp', u'Anna_Test', u'volume-a9b15c37-f70f-4a47-8f29-101d7e3970ef']","Add exception catch if Storwize/SVC driver failed when
retyping

If adding a copy for a volume of Storwize/SVC which is under
migration, Storwize/SVC command will fail because the
number of the copies of the volume exceed the limit which
is 2.

Modifying the volume type when adding a copy for the volume,
Storwize/SVC driver will also fail, should add exception catch.

Also add exception catch when getting the volume from the database.
This change is necessary to enable resolving bug 1305550
(https://bugs.launchpad.net/cinder/+bug/1305550 - Failed retype with
driver raised exception should set volume status to ""error"" ).

Change-Id: I1cf4b6606b5228c8adcb2bde21f345fa979bc13e
Closes-Bug: #1298201"
283,25103df197c1f366eac8dd3069fabc01d3bd18e9,1396248416,,1.0,114,45,6,3,1,0.913527483,True,6.0,4521215.0,112.0,11.0,False,7.0,825008.3333,17.0,1.0,22.0,223.0,239.0,22.0,219.0,235.0,21.0,198.0,213.0,0.020932445,0.189343482,0.203615604,695,1121,1298865,neutron,25103df197c1f366eac8dd3069fabc01d3bd18e9,1,1,Wrong order of actions,"Bug #1298865 in neutron: ""NVP advanced service plugin should check router status before deploying a service""","With NVP advanced service plugin, router creation is asynchronous while all service call is synchronous, so it is possible that advanced service request is called before edge deployment completed.
One solution is to check the router status before deploying an advanced service. If the router is not in ACTIVE status, the service deployment request would return ""Router not ready"" error.","Check NVP router's status before deploying a service

With NVP advanced service plugin, router creation is asynchronous while
all service call is synchronous, so it is possible that advanced service
request is called before edge deployment completed.
The solution is to check the router status before deploying an advanced service.
If the router is not in ACTIVE status, the service deployment request would return
""Router not in 'ACTIVE' status"" error.

Change-Id: Idde71c37f5d5c113ac04376ed607e0c156b07477
Closes-Bug: #1298865"
284,251159c90a69dfb61d9927093d5eae41cd99e2a7,1389304150,,1.0,31,21,2,1,1,0.706274089,True,3.0,70097.0,20.0,9.0,False,3.0,6483335.0,3.0,4.0,614.0,783.0,1222.0,563.0,635.0,1039.0,159.0,309.0,389.0,0.236686391,0.458579882,0.576923077,277,682,1267619,neutron,251159c90a69dfb61d9927093d5eae41cd99e2a7,1,0,Issues by evolution,"Bug #1267619 in neutron: ""Fix Migration 50e86cb2637a""","The following migration 50e86cb2637a called
op.rename_table('neutron_nvp_port_mapping', 'neutron_nsx_port_mappings')
though the table name was actually quantum_nvp_port_mapping. Because of this
the quantum_id->nvp_id mapping was never migrated over to the new table and
you would be left with a quantum_nvp_port_mapping table hanging around.
In addition, the downgrade would rename the table to neutron_nvp_port_mapping
instead of quantum_nvp_port_mapping. This patch addresses this issues.","Fix Migration 50e86cb2637a and 38335592a0dc

When the rename of quantum->neutron occurred here ee3fe4e8 it also renamed
the the table creation from quantum_nvp_port_mapping to
neutron_nvp_port_mapping. This went undetected for a long time because
when neutron-server starts up it pushes down the scheme for tables that
are not there so the table would be created.

Because of this the following migration 50e86cb2637a called
op.rename_table('neutron_nvp_port_mapping', 'neutron_nsx_port_mappings')
though the table name being used was quantum_nvp_port_mapping. Because of this
the quantum_id->nvp_id mapping was never migrated over to the new table and
you would be left with a quantum_nvp_port_mapping table hanging around.

In addition, the downgrade would rename the table to neutron_nvp_port_mapping
instead of quantum_nvp_port_mapping. This patch addresses this issues.

Change-Id: I4f80b7b9dc56996ecd83826ee65918f5311c7c4f
Closes-bug: #1267619"
285,252a83890abfb825497f308147aeafc6ee1e0731,1395394891,,1.0,6,7,4,2,1,0.869074667,True,1.0,9465.0,20.0,4.0,False,40.0,792318.0,60.0,4.0,196.0,609.0,742.0,181.0,491.0,610.0,154.0,329.0,426.0,0.152409046,0.324483776,0.41986234,665,1091,1295703,neutron,252a83890abfb825497f308147aeafc6ee1e0731,1,1,There is a bug. They decided to delete the priority,"Bug #1295703 in neutron: ""ci-overcloud job failing ""Error while processing VIF ports""""","ci overcloud jobs started failing between 5 and 8 AM GMT
Error from http://logs.openstack.org/73/79873/5/check-tripleo/check-tripleo-overcloud-precise/859d4d4/
var/log/upstart/neutron-openvswitch-agent.log ( on contoller and 1 compute)
[-] Error while processing VIF ports
Traceback (most recent call last):
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1230, in rpc_loop
    sync = self.process_network_ports(port_info)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1084, in process_network_ports
    devices_added_updated)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 984, in treat_devices_added_or_updated
    details['admin_state_up'])
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 893, in treat_vif_port
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 593, in port_bound
    physical_network, segmentation_id)
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 459, in provision_local_vlan
    (segmentation_id, ofports))
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 190, in mod_flow
    flow_str = _build_flow_expr_str(kwargs, 'mod')
  File ""/opt/stack/venvs/neutron/local/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 546, in _build_flow_expr_str
    raise exceptions.InvalidInput(error_message=msg)
InvalidInput: Invalid input for operation: Cannot match priority on flow deletion or modification.
2014-03-21 05:20:56.329 7601 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
merge times and traceback details seem to match up with
https://review.openstack.org/#/c/58533/19
currently I'm testing a revert to see if it fixes things","Don't set priority when calling mod_flow

This commit [1] changed the behavior of modifying flows to not allow priority
to be set on flow modification. The agent code which was calling mod_flow was
not updated to respect this behavior. The unit tests were also not updated in
this regard. I've corrected this behavior now, and added a negative UT case to
catch this in the future.

[1] https://review.openstack.org/#/c/58533/

Closes-Bug: #1295703

Change-Id: I20737637cc84567a5ec19efaad18e7f334007f18"
286,257183a8a9130f2b444f7f96ec8582da89684528,1407945829,,1.0,56,8,2,2,1,0.965201699,False,,,,,True,,,,,,,,,,,,,,,,,1146,1601,1352659,nova,257183a8a9130f2b444f7f96ec8582da89684528,1,1,,"Bug #1352659 in OpenStack Compute (nova): ""race in server show api""","Because of the instance object lazy loading its possible to get into situations where the API code is half way through assembling data to return to the client when the instance disappears underneath it. We really need to ensure everything we will need is retreived up front so we have a consistent snapshot view of the instance
[req-5ca39eb3-c1d2-433b-8dac-1bf5f338ce1f ServersAdminNegativeV3Test-1453501114 ServersAdminNegativeV3Test-364813115] Unexpected exception in API method
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 410, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     return self._view_builder.show(req, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 268, in show
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     _inst_fault = self._get_fault(request, instance)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/api/openstack/compute/views/servers.py"", line 214, in _get_fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     fault = instance.fault
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/base.py"", line 67, in getter
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     self.obj_load_attr(name)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/instance.py"", line 520, in obj_load_attr
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     expected_attrs=[attrname])
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/base.py"", line 153, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     result = fn(cls, context, *args, **kwargs)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/objects/instance.py"", line 310, in get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/api.py"", line 676, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1715, in instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     columns_to_join=columns_to_join, use_slave=use_slave)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions   File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 1727, in _instance_get_by_uuid
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions     raise exception.InstanceNotFound(instance_id=uuid)
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions InstanceNotFound: Instance fcff276a-d410-4760-9b98-4014024b1353 could not be found.
2014-08-04 06:37:25.738 21228 TRACE nova.api.openstack.extensions
  http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-nova-v3-full-master/a278802/logs/screen-n-api.txt","Direct-load Instance.fault when lazy-loading

This breaks out instance.fault lazy-loading from the other attributes,
since we have a direct and more efficient way of fetching the fault than
re-querying the entire instance with the fault attached.

This also should help address fault-related races in the API where a
list of instances is queried, one of those is deleted, and the fault
attribute later triggers an InstanceNotFound whilst trying to do the
lazy-load.

Change-Id: Iceb552663db93fa2a01fb90ece0c1eebecdb783f
Closes-bug: #1352659"
287,258aad077279b2020b47f4136747c1e5aa4bf5a8,1382437766,,1.0,1,1,1,1,1,0.0,True,3.0,3850015.0,32.0,20.0,False,8.0,903693.0,9.0,10.0,23.0,823.0,842.0,23.0,751.0,770.0,5.0,287.0,288.0,0.012578616,0.603773585,0.605870021,12,324,1243148,neutron,258aad077279b2020b47f4136747c1e5aa4bf5a8,1,1,,"Bug #1243148 in neutron: ""Mistake in usage drop_table in downgrade in migration""","In method downgrade_cisco in migration folsom_initial instead of local method drop_tables op.drop_tables is used
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 468, in downgrade
    downgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 541, in downgrade_cisco
    op.drop_tables(
AttributeError: 'module' object has no attribute 'drop_tables'","Fix downgrade in migration

In method downgrade_cisco in migration folsom_initial instead of
local method drop_tables op.drop_tables is used

Closes-Bug: #1243148

Change-Id: Ida6c52653c065e49c85141a7d1e61d394c1d709e"
288,2593469103aa7d9d2bcb759b78d5f8637911a1e0,1386062846,,1.0,50,4,4,2,1,0.714485499,True,11.0,4200239.0,60.0,18.0,False,45.0,277931.0,121.0,4.0,64.0,3059.0,3092.0,55.0,2307.0,2336.0,60.0,2817.0,2847.0,0.009014334,0.416432688,0.420865967,116,515,1256838,nova,2593469103aa7d9d2bcb759b78d5f8637911a1e0,1,1,Race condition,"Bug #1256838 in OpenStack Compute (nova): ""Race between imagebackend and imagecache""","After ImageCacheManager judges a base image is not used recently and marks it as to be removed, there is some time before the image is actually removed. So if an instance using the image is launched during the time, the image will be removed unfortunately.","Fix race conditions between imagebackend and imagecache

The race may occur in the situation:
* There is a base file that is not used for a long time enough
  to be removed by imagecache.
* imagebackend is provisioning a virtual disk from the base file.
* imagecache is removing the base file.

Then, the base file is removed even though it is about to be used.

To fix this, these changes are in this patch:

* A new function imagecache.refresh_timestamp(base_file) updates
  the owner and mtime of the base file with the lock dedicated
  to the base file.
* imagebacked calls refresh_timestamp(base_file) before provision
  a disk from the base file.
* imagecache.ImageCacheManager._remove_base_file(base_file) uses
  the same lock as used by refresh_timestamp()

Closes-Bug: #1256838
Change-Id: I7c897cf6071d87a2c4532fb3a73863d649d02782"
289,25a5a8526cf8182fbb6d7f8acf224ee1666da5c2,1402299990,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,957,1395,1327974,neutron,25a5a8526cf8182fbb6d7f8acf224ee1666da5c2,0,0,Bug in test,"Bug #1327974 in neutron: ""hyperv unit test agent failure""","The hyperv unit appear to not properly mock all cases of report_state calls so an occasional exception will be thrown on an unrelated patch.[1]
1. http://logs.openstack.org/01/96201/6/gate/gate-neutron-python27/2b0de5e/console.html","Start an unstarted patch in the hyperv unit tests

This starts a patch that was setup but never started
for a loopingcall that was allowing an occasional
exception to be thrown from the agent on unrelated patches.

Closes-Bug: #1327974
Change-Id: I6cfdb128c2a78e9c087d79a6db850603f2d03fa8"
290,25c5291addcd3ed0ef9df567a828866f1681e014,1406709734,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1123,1578,1350320,neutron,25c5291addcd3ed0ef9df567a828866f1681e014,1,1,,"Bug #1350320 in neutron: ""Use correct section for log message if interface_driver import fails""",If import of interface_driver in lbaas namespace_driver fails then message for logger raises exception because of taking interface_driver from haproxy section while actual interface_driver is in default section.,"Use correct section for log message if interface_driver import fails

If import of interface_driver in namespace_driver fails then message for
logger raises exception because of taking interface_driver from haproxy
section while actual interface_driver is in default section.

Closes-Bug: #1350320
Change-Id: Ic1e73c0cdccb2fff02f2adfbad677503c29ba88a"
291,25f117bacf1cb110260d96e764ae8fac9440c14e,1408967569,,1.0,5,12,3,1,1,0.926262155,False,,,,,True,,,,,,,,,,,,,,,,,1230,1689,1361176,neutron,25f117bacf1cb110260d96e764ae8fac9440c14e,1,0,“After commit commit 466e89970f11918a809aafe8a048d138d4664299 migrations should not anymore explicitly specify the engine used for mysql.”,"Bug #1361176 in neutron: ""DB: Some tables still explicitly set mysql_engine""","After commit commit 466e89970f11918a809aafe8a048d138d4664299 migrations should not anymore explicitly specify the engine used for mysql.
There are still some migrations which do that, and they should be amended.","Do not explicitly set mysql_engine

Setting the mysql_engine when creating a table is not needed
anymore as this is automatically done by an event listener
added with commit 466e89970f11918a809aafe8a048d138d4664299

Change-Id: I0df978dbf2bb44f17102da3c17fd4e93c9e4df8d
Closes-Bug: #1361176"
292,2634616e3d94b57600090f3262c96e2233a521f8,1396562043,,1.0,0,11,1,1,1,0.0,True,1.0,323588.0,28.0,12.0,False,24.0,631635.0,30.0,5.0,3.0,3143.0,3145.0,3.0,2411.0,2413.0,2.0,2641.0,2642.0,0.000387797,0.341520165,0.341649431,734,1161,1302211,nova,2634616e3d94b57600090f3262c96e2233a521f8,0,0,Bug in test,"Bug #1302211 in OpenStack Compute (nova): ""RbdTestCase.test_cache_base_dir_exists is duplicated in test_imagebackend""","RbdTestCase in nova/tests/virt/libvirt/test_imagebackend.py now has two slightly different versions of the same test case:
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L759
https://github.com/openstack/nova/blob/dc8de426066969a3f0624fdc2a7b29371a2d55bf/nova/tests/virt/libvirt/test_imagebackend.py#L806
The redundant version was added in:
https://review.openstack.org/82840
I think it should be removed, it doesn't do anything the original test case doesn't already have.","remove redundant copy of test_cache_base_dir_exists

Second copy of RbdTestCase.test_cache_base_dir_exists was accidentally
introduced in https://review.openstack.org/82840.

Change-Id: I16b2e472b2499f01379fdca9c860da0b95e291d8
Closes-bug: #1302211
Signed-off-by: Dmitry Borodaenko <dborodaenko@mirantis.com>"
293,2648aa3561d23e1215e0cc6f446253e5df56c8f6,1394835249,,1.0,2,24,2,1,1,0.391243564,True,2.0,1759106.0,33.0,12.0,False,23.0,505397.0,48.0,9.0,428.0,2099.0,2154.0,367.0,1713.0,1740.0,277.0,919.0,923.0,0.287487073,0.95139607,0.955532575,532,953,1288379,neutron,2648aa3561d23e1215e0cc6f446253e5df56c8f6,1,1,deadlock,"Bug #1288379 in neutron: ""db Deadlock detected when running 'delete_port""","I'm seeing Deadlocks when deleting large numbers of VMs in a multinode system.
The port delete fails, and then ports are left behind after the VMs are deleted.
VMs cannot be created as the IP are not released.
The ports have to be manually deleted.
2014-02-24 15:18:12.606 1819 ERROR neutron.api.v2.resource [-] delete failed
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/api/v2/base.py"", line 432, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 634, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     super(OVSNeutronPluginV2, self).delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1403, in delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     self._delete_port(context, id)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 1425, in _delete_port
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     a['ip_address'])
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 415, in _recycle_ip
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     ip_address)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/db/db_base_plugin_v2.py"", line 449, in _delete_ip_allocation
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     subnet_id=subnet_id).delete()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2581, in delete
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     delete_op.exec_()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 816, in exec_
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     self._do_exec()
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/persistence.py"", line 942, in _do_exec
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     params=self.query._params)
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 531, in _wrap
2014-02-24 15:18:12.606 1819 TRACE neutron.api.v2.resource     _raise_if_deadlock_error(e, get_engine().name)","Stop removing ip allocations on port delete

The _delete_port() method was manually removing related
IPAllocation instances despite the existence of a perfectly
good cascade deletion relationship in the model.  This patch
puts an end to that nonsense and the potential for deadlock that
it represented.

Closes-bug: #1288379
Related-Bug: #1283522

Change-Id: Ib31550fa9000fc75768a327cb6cc1c419e06568f"
294,26871aa60ba663d56951fcd449167bfadbe01522,1408588994,,1.0,3,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1109,1562,1349268,nova,26871aa60ba663d56951fcd449167bfadbe01522,1,1,,"Bug #1349268 in OpenStack Compute (nova): ""OverLimit: VolumeLimitExceeded: Maximum number of volumes allowed (10) exceeded""","The instance will be ERROR when booting instance from volume, if the volume quota is not enough. And there is even no useful error message to show to the user. Following is the related nova-compute.log:
2014-07-27 17:56:19.372 17060 ERROR nova.compute.manager [req-4e876b97-be8a-486b-98e2-7d707266755d 98fa3fd418914a9288b5560e1bb6944e 5254621adfd949a9a3b975f68119e269] [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Instance failed block device setup
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] Traceback (most recent call last):
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1690, in _prep_block_device
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     self.driver, self._await_block_device_map_created))
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 363, in attach_block_devices
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     map(_log_and_attach, block_device_mapping)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 361, in _log_and_attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     bdm.attach(*attach_args, **attach_kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 311, in attach
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     '', '', image_id=self.image_id)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/nova/volume/cinder.py"", line 303, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     item = cinderclient(context).volumes.create(size, **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/v1/volumes.py"", line 187, in create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     return self._create('/volumes', body, 'volume')
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/base.py"", line 153, in _create
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     resp, body = self.api.client.post(url, body=body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 209, in post
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     return self._cs_request(url, 'POST', **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 173, in _cs_request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     **kwargs)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]   File ""/usr/lib/python2.7/dist-packages/cinderclient/client.py"", line 156, in request
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]     raise exceptions.from_response(resp, body)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc] OverLimit: VolumeLimitExceeded: Maximum number of volumes allowed (10) exceeded (HTTP 413) (Request-ID: req-07dcc4c4-182f-4d73-b054-806f31cb7e71)
2014-07-27 17:56:19.372 17060 TRACE nova.compute.manager [instance: 2a124872-3332-4f54-bb28-f0a96a7ed7bc]
2014-07-27 17:56:19.693 17060 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 282, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 268, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 335, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 311, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     e, sys.exc_info())
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 298, in decorated_function
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2077, in run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     do_run_instance()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     return f(*args, **kwargs)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2076, in do_run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1209, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     notify(""error"", fault=e)  # notify that build failed
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1193, in _run_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     instance, image_meta, legacy_bdm_in_spec)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1347, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     LOG.exception(msg, instance=instance)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1304, in _build_instance
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     context, instance, bdms)
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1707, in _prep_block_device
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher     raise exception.InvalidBDM()
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher InvalidBDM: Block Device Mapping is Invalid.
2014-07-27 17:56:19.693 17060 TRACE oslo.messaging.rpc.dispatcher","Log warning message if volume quota is exceeded

It would be useful to the user if a warning message is logged
when the volume quota is exceeded instead of a debug message.
This would allow the root cause to be more easily determined
instead of turning on the debug option.

Change-Id: I19dc078cbacf8f39480e8a7f4c4edb764e4696eb
Closes-Bug: #1349268"
295,26d8231b0bf4050453bcb6122484c801eff852ca,1391962831,,1.0,2,1,1,1,1,0.0,True,1.0,67753.0,24.0,9.0,False,6.0,2064365.0,7.0,9.0,3.0,972.0,973.0,3.0,854.0,855.0,3.0,536.0,537.0,0.00520156,0.698309493,0.699609883,383,796,1278271,neutron,26d8231b0bf4050453bcb6122484c801eff852ca,0,0,I think is not a bug. ,"Bug #1278271 in neutron: ""SG rpc: skip logging when devices are added to the set""",In review https://review.openstack.org/#/c/63100 extra logging is done when no devices match the updated security group.,"Skip extra logging when devices is empty

Change-Id: Id9ae8aef0555109bd88600965adb2744ab924430
Closes-Bug: #1278271"
296,26de1b1d829849665dae921b8be739194b84515d,1410509402,,1.0,112,29,3,2,1,0.707074385,False,,,,,True,,,,,,,,,,,,,,,,,1307,1773,1368595,cinder,26de1b1d829849665dae921b8be739194b84515d,1,1,,"Bug #1368595 in Cinder: ""IBM Storwize: unable to retype to the new qos, if QoS is empty""","QoS should be changed after retyping the volume.
So far for storwzie driver, if the new type does not have QoS configurations, the old QoS configurations remain in the volume.","IBM Storwize driver: Retype the volume with correct empty QoS

* Currently for Storwzie driver, if the new type does not have QoS
configurations, the old QoS configurations remain in the volume after
retyping it. It should be retyped into a volume with empty QoS for the
Storwize driver.
* Refactor three dicts into one for better maintainance of the QoS keys
for Storwize driver.

DocImpact

Change-Id: I2b2801a4ef72ef02c11392ed00b56f5263a8a7e4
Closes-Bug: #1368595"
297,26e9fad61f86bedd16179ede9736b56c4a9a5752,1385057582,0.0,1.0,133,202,7,4,1,0.828702469,True,4.0,1412080.0,28.0,9.0,False,11.0,1739500.857,17.0,4.0,16.0,1145.0,1146.0,16.0,1055.0,1056.0,11.0,480.0,480.0,0.021276596,0.852836879,0.852836879,1436,1190622,1190622,neutron,26e9fad61f86bedd16179ede9736b56c4a9a5752,0,0,Test files,"Bug #1190622 in neutron: ""Improve unit test coverage for Cisco plugin nexus code""","Improve unit test coverage for ...
quantum/plugins/cisco/nexus/cisco_nexus_network_driver_v2	100	24	0	26	1	72%
quantum/plugins/cisco/nexus/cisco_nexus_plugin_v2	117	23	0	12	2	79%","Improve unit test coverage for Cisco plugin nexus code

Closes-Bug: #1190622

This fix improves the unit test coverage for the Cisco Nexus plugin's
cisco_nexus_network_driver_v2.py and cisco_nexus_plugin_v2.py
source files from:
cisco_nexus_network_driver_v2: 72%
cisco_nexus_plugin_v2:         79%
To:
cisco_nexus_network_driver_v2: 99%
cisco_nexus_plugin_v2:         94%

Much of what the remaining ""uncovered"" code (coverage tool
reports as partially covered) can be attributed to the
coverage tool not being aware that execution does not
continue at the end of a save_and_reraise_exception()
context block (i.e. the exception will be reraised, but
the coverage tool isn't aware of this).

This fix and coverage results are dependent on the fix for #1246080.

Change-Id: Ie0e11fc0a12502739fb39bb4b30deb04dd31b7b0"
298,271a280d0e8f86b96d24970f588184b1cfc34ef8,1409781574,,1.0,59,23,4,2,1,0.647602921,False,,,,,True,,,,,,,,,,,,,,,,,1242,1701,1362308,neutron,271a280d0e8f86b96d24970f588184b1cfc34ef8,1,0,"This is strange! It looks like there is no BIC, but a change in the requirements. One of the fixing commits is huge and changes many, many files","Bug #1362308 in neutron: ""Use dict_extend_functions mechanism to populate provider network attributes in Nuage Plugin""",Use dict_extend_functions mechanism to handle populating additional provider network attributes into Network model.,"Use dict_extend_functions to populate provider network attributes

Use dict_extend_functions mechanism to handle populating additional
provider network attributes into Network model in Nuage plugin.

Change-Id: I72e5afe3d03ae223fcd8e75f1f4f07624c3a7daf
Closes-Bug: #1362308"
299,272912b6d82af00a6f7bafc26d8ce7a79b09ef26,1385656247,0.0,1.0,33,2,2,2,1,0.775512658,True,3.0,8432296.0,57.0,23.0,False,3.0,7333704.0,3.0,9.0,45.0,1553.0,1558.0,45.0,1414.0,1419.0,38.0,545.0,548.0,0.066213922,0.926994907,0.932088285,104,503,1256041,neutron,272912b6d82af00a6f7bafc26d8ce7a79b09ef26,1,1, Fix a typo in log exception in the metering agent ,"Bug #1256041 in neutron: ""Typo in the metering agent when there is an exception when it invokes a driver method""",Typo in the metering agent when there is an exception when it invokes a driver method.,"Fix a typo in log exception in the metering agent

This patch fixes a typo when there is a exception
when the agent try to invoke a method of the driver.

Change-Id: I4fcf19fa84f178348abfb3563bbfd52f0dfcc095
Closes-bug: #1256041"
300,2737c76cb2fb436f117a4f635aebca7a01691d88,1383072252,,1.0,44,5,2,2,1,0.768281409,True,3.0,1291433.0,24.0,17.0,False,34.0,798216.0,71.0,9.0,284.0,962.0,962.0,284.0,916.0,916.0,258.0,711.0,711.0,0.229609929,0.631205674,0.631205674,30,342,1244415,cinder,2737c76cb2fb436f117a4f635aebca7a01691d88,1,1,,"Bug #1244415 in Cinder: ""check_ssh_injection not handling quoted args correctly""","check_ssh_injection in cinder/utils.py is disallowing args with spaces even when the arg is quoted. This leads to an SSHInjectionThreat being raised when a volume driver needs to send a quoted arg containing spaces, e.g. when a storage pool name contains a space.","Allow spaces in quoted SSH command arguments

The check_ssh_injection() method was rejecting arguments with spaces
even when they were quoted, this was causing problems with some volume
driver commands such as commands for a storage pool with spaces in the
name.

Closes-Bug: #1244415
Change-Id: Ie4b809e1b39fdb752cf634e6d3c0a3924d8ac52b"
301,273b169029e6693cb862be38df1c70e540808645,1393504060,,1.0,1,1,1,1,1,0.0,True,1.0,50009.0,16.0,4.0,False,3.0,12025301.0,2.0,4.0,31.0,1334.0,1354.0,31.0,1136.0,1156.0,13.0,712.0,714.0,0.016,0.814857143,0.817142857,492,910,1285641,neutron,273b169029e6693cb862be38df1c70e540808645,1,0,Evolution bug,"Bug #1285641 in neutron: ""different fully qualified class name for VPNaaS migrations""","In migrations 52ff27f7567a_support_for_vpnaas.py and  338d7508968c_vpnaas_peer_address_.py different class names are set:  neutron.services.vpn.plugin.VPNDriverPlugin and neutron.services.vpn.plugin.VPNPlugin.
This cause the following exception:
neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini upgrade head
No handlers could be found for logger ""neutron.common.legacy""
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running upgrade None -> folsom, folsom initial database
INFO  [alembic.migration] Running upgrade folsom -> 2c4af419145b, l3_support
INFO  [alembic.migration] Running upgrade 2c4af419145b -> 5a875d0e5c, ryu
INFO  [alembic.migration] Running upgrade 5a875d0e5c -> 48b6f43f7471, DB support for service types
INFO  [alembic.migration] Running upgrade 48b6f43f7471 -> 3cb5d900c5de, security_groups
INFO  [alembic.migration] Running upgrade 3cb5d900c5de -> 1d76643bcec4, nvp_netbinding
INFO  [alembic.migration] Running upgrade 1d76643bcec4 -> 2a6d0b51f4bb, cisco plugin cleanup
INFO  [alembic.migration] Running upgrade 2a6d0b51f4bb -> 1b693c095aa3, Quota ext support added in Grizzly
INFO  [alembic.migration] Running upgrade 1b693c095aa3 -> 1149d7de0cfa, initial port security
INFO  [alembic.migration] Running upgrade 1149d7de0cfa -> 49332180ca96, ryu plugin update
INFO  [alembic.migration] Running upgrade 49332180ca96 -> 38335592a0dc, nvp_portmap
INFO  [alembic.migration] Running upgrade 38335592a0dc -> 54c2c487e913, 'DB support for load balancing service
INFO  [alembic.migration] Running upgrade 54c2c487e913 -> 45680af419f9, nvp_qos
INFO  [alembic.migration] Running upgrade 45680af419f9 -> 1c33fa3cd1a1, Support routing table configuration on Router
INFO  [alembic.migration] Running upgrade 1c33fa3cd1a1 -> 363468ac592c, nvp_network_gw
INFO  [alembic.migration] Running upgrade 363468ac592c -> 511471cc46b, Add agent management extension model support
INFO  [alembic.migration] Running upgrade 511471cc46b -> 3b54bf9e29f7, NEC plugin sharednet
INFO  [alembic.migration] Running upgrade 3b54bf9e29f7 -> 4692d074d587, agent scheduler
INFO  [alembic.migration] Running upgrade 4692d074d587 -> 1341ed32cc1e, nvp_net_binding
INFO  [alembic.migration] Running upgrade 1341ed32cc1e -> grizzly, grizzly
INFO  [alembic.migration] Running upgrade grizzly -> f489cf14a79c, DB support for load balancing service (havana)
INFO  [alembic.migration] Running upgrade f489cf14a79c -> 176a85fc7d79, Add portbindings db
INFO  [alembic.migration] Running upgrade 176a85fc7d79 -> 32b517556ec9, remove TunnelIP model
INFO  [alembic.migration] Running upgrade 32b517556ec9 -> 128e042a2b68, ext_gw_mode
INFO  [alembic.migration] Running upgrade 128e042a2b68 -> 5ac71e65402c, ml2_initial
INFO  [alembic.migration] Running upgrade 5ac71e65402c -> 3cbf70257c28, nvp_mac_learning
INFO  [alembic.migration] Running upgrade 3cbf70257c28 -> 5918cbddab04, add tables for router rules support
INFO  [alembic.migration] Running upgrade 5918cbddab04 -> 3cabb850f4a5, Table to track port to host associations
INFO  [alembic.migration] Running upgrade 3cabb850f4a5 -> b7a8863760e, Remove cisco_vlan_bindings table
INFO  [alembic.migration] Running upgrade b7a8863760e -> 13de305df56e, nec_add_pf_name
INFO  [alembic.migration] Running upgrade 13de305df56e -> 20ae61555e95, DB Migration for ML2 GRE Type Driver
INFO  [alembic.migration] Running upgrade 20ae61555e95 -> 477a4488d3f4, DB Migration for ML2 VXLAN Type Driver
INFO  [alembic.migration] Running upgrade 477a4488d3f4 -> 2032abe8edac, LBaaS add status description
INFO  [alembic.migration] Running upgrade 2032abe8edac -> 52c5e4a18807, LBaaS Pool scheduler
INFO  [alembic.migration] Running upgrade 52c5e4a18807 -> 557edfc53098, New service types framework (service providers)
INFO  [alembic.migration] Running upgrade 557edfc53098 -> e6b16a30d97, Add cisco_provider_networks table
INFO  [alembic.migration] Running upgrade e6b16a30d97 -> 39cf3f799352, FWaaS Havana-2 model
INFO  [alembic.migration] Running upgrade 39cf3f799352 -> 52ff27f7567a, Support for VPNaaS
INFO  [alembic.migration] Running upgrade 52ff27f7567a -> 11c6e18605c8, Pool Monitor status field
INFO  [alembic.migration] Running upgrade 11c6e18605c8 -> 35c7c198ddea, remove status from HealthMonitor
INFO  [alembic.migration] Running upgrade 35c7c198ddea -> 263772d65691, Cisco plugin db cleanup part II
INFO  [alembic.migration] Running upgrade 263772d65691 -> c88b6b5fea3, Cisco N1KV tables
INFO  [alembic.migration] Running upgrade c88b6b5fea3 -> f9263d6df56, remove_dhcp_lease
INFO  [alembic.migration] Running upgrade f9263d6df56 -> 569e98a8132b, metering
INFO  [alembic.migration] Running upgrade 569e98a8132b -> 86cf4d88bd3, remove bigswitch port tracking table
INFO  [alembic.migration] Running upgrade 86cf4d88bd3 -> 3c6e57a23db4, add multiprovider
INFO  [alembic.migration] Running upgrade 3c6e57a23db4 -> 63afba73813, Add unique constraint for id column of TunnelEndpoint
INFO  [alembic.migration] Running upgrade 63afba73813 -> 40dffbf4b549, nvp_dist_router
INFO  [alembic.migration] Running upgrade 40dffbf4b549 -> 53bbd27ec841, Extra dhcp opts support
INFO  [alembic.migration] Running upgrade 53bbd27ec841 -> 46a0efbd8f0, cisco_n1kv_multisegment_trunk
INFO  [alembic.migration] Running upgrade 46a0efbd8f0 -> 2a3bae1ceb8, NEC Port Binding
INFO  [alembic.migration] Running upgrade 2a3bae1ceb8 -> 14f24494ca31, DB Migration for Arista ml2 mechanism driver
INFO  [alembic.migration] Running upgrade 14f24494ca31 -> 32a65f71af51, ml2 portbinding
INFO  [alembic.migration] Running upgrade 32a65f71af51 -> 66a59a7f516, NEC OpenFlow Router
INFO  [alembic.migration] Running upgrade 66a59a7f516 -> 51b4de912379, Cisco Nexus ML2 mechanism driver
INFO  [alembic.migration] Running upgrade 51b4de912379 -> 1efb85914233, allowedaddresspairs
INFO  [alembic.migration] Running upgrade 1efb85914233 -> 38fc1f6789f8, Cisco N1KV overlay support
INFO  [alembic.migration] Running upgrade 38fc1f6789f8 -> 4a666eb208c2, service router
INFO  [alembic.migration] Running upgrade 4a666eb208c2 -> 338d7508968c, vpnaas peer_address size increase
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 145, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 80, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 124, in upgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 205, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 105, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 89, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 681, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 48, in upgrade
    type_=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 322, in go
    return fn(*arg, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/operations.py"", line 300, in alter_column
    existing_autoincrement=existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/mysql.py"", line 42, in alter_column
    else existing_autoincrement
  File ""/usr/local/lib/python2.7/dist-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
    self.errorhandler(self, exc, value)
  File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
    raise errorclass, errorvalue
sqlalchemy.exc.ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ipsec_site_connections' doesn't exist"") 'ALTER TABLE ipsec_site_connections CHANGE peer_address peer_address VARCHAR(255) NULL' ()","Different class names for VPNaaS migrations

In migrations 52ff27f7567a_support_for_vpnaas.py and
338d7508968c_vpnaas_peer_address_.py different class names are set:
neutron.services.vpn.plugin.VPNDriverPlugin and ne
utron.services.vpn.plugin.VPNPlugin.

Closes-bug: #1285641

Change-Id: Ieda8d3a8a022179867010b6e47d09cf7f43f5da9"
302,273d8765115f0936d24549db8698dbe7e8d2ec1a,1385137687,,1.0,4,1,1,1,1,0.0,True,1.0,231887.0,7.0,4.0,False,14.0,13498792.0,19.0,3.0,7.0,685.0,688.0,7.0,639.0,642.0,7.0,632.0,635.0,0.006762468,0.535080304,0.53761623,1798,1254089,1254089,cinder,273d8765115f0936d24549db8698dbe7e8d2ec1a,1,1, ,"Bug #1254089 in Cinder: ""wait_child() without greenthread.sleep() uses excessive CPU""","https://bugs.launchpad.net/oslo/+bug/1095346 tracked a fix in service.py: wait() to prevent the cinder-volume process from hogging the CPU while looping in _wait_child()
However, the wait() function will also loop in _wait_child() when the parent catched SIGTERM and the parent is waiting to reap the children. This also causes excessive CPU usage if the child does not die quickly for some reason.
I've an instance of cinder-volume  that has been running for several days in this wait state. An strace of the parent shows:
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
wait4(0, 0x7fff081b7780, WNOHANG, NULL) = 0
......forever....
this is because the parent has caught SIGTERM is and is looping on wait_child without the eventlet.greenthread.sleep(.01)
during the normal running state of cinder-volume the parent does an epoll for the eventlet.greenthread.sleep(.01):
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984})  = 0 (Timeout)
wait4(0, 0x7fff25185430, WNOHANG, NULL) = 0
select(0, NULL, NULL, NULL, {0, 9984})  = 0 (Timeout)
My CPU is pegged at 100% with this process (shown in top):
  PID USER      PR  NI  VIRT  RES  SHR S  %CPU %MEM    TIME+  COMMAND
27313 cinder    20   0  103m  25m 4080 R   100  0.0  56343:11 cinder-volume
To fix this, the second call to _wait_child() needs the same sleep as the first:
    def wait(self):
        """"""Loop waiting on children to die and respawning as necessary.""""""
        while self.running:
            wrap = self._wait_child()
            if not wrap:
                # Yield to other threads if no children have exited
                # Sleep for a short time to avoid excessive CPU usage
                # (see bug #1095346)
                eventlet.greenthread.sleep(.01)
                continue
            LOG.info(_('wait wrap.failed %s'), wrap.failed)
            while (self.running and len(wrap.children) < wrap.workers
                   and not wrap.failed):
                self._start_child(wrap)
        if self.sigcaught:
            signame = {signal.SIGTERM: 'SIGTERM',
                       signal.SIGINT: 'SIGINT'}[self.sigcaught]
            LOG.info(_('Caught %s, stopping children'), signame)
        for pid in self.children:
            try:
                os.kill(pid, signal.SIGTERM)
            except OSError as exc:
                if exc.errno != errno.ESRCH:
                    raise
        # Wait for children to die
        if self.children:
            LOG.info(_('Waiting on %d children to exit'), len(self.children))
            while self.children:
                self._wait_child()
Patch is :
--- a/cinder/service.py
+++ b/cinder/service.py
@@ -323,7 +323,10 @@ class ProcessLauncher(object):
         if self.children:
             LOG.info(_('Waiting on %d children to exit'), len(self.children))
             while self.children:
-                self._wait_child()
+               wrap = self._wait_child()
+               if not wrap:
+                   eventlet.greenthread.sleep(.01)
+                   continue","Add greenthread.sleep() to parent wait()

Add an eventlet.greenthread.sleep() to wait() to keep
cinder-volume parent from looping after catching SIGTERM
and hogging the CPU. This was already added to the running
state but is missing from the SIGTERM caught state when
the parent waits to reap the child.

Change-Id: Ia871e31db5bf9ec4e86f926f8f6c4f0f4ecb7925
Closes-Bug: #1254089"
303,27466f2a8f518cfda2935ddd882a99f6736296af,1381394128,,1.0,4,2,2,2,1,1.0,True,1.0,1181494.0,13.0,7.0,False,11.0,2402300.0,15.0,7.0,1.0,2887.0,2888.0,1.0,2288.0,2289.0,0.0,2628.0,2628.0,0.000159566,0.419498963,0.419498963,1665,1237836,1237836,nova,27466f2a8f518cfda2935ddd882a99f6736296af,1,1,Bug in a msg333 “Fix error message of os-cells sync_instances api”,"Bug #1237836 in OpenStack Compute (nova): ""error message of os-cells sync_instances api is incorrect""","Though parameters of this api are 'updated_since' , 'project_id' and 'deleted',
the error message is ""Only 'updated_since' and 'project_id' are understood.""
It shoud be ""Only 'updated_since', 'project_id' and 'deleted' are understood.""","Fix error message of os-cells sync_instances api

Because parameters of this api are 'updated_since' , 'project_id'
and 'deleted',add 'deleted' to the message ""Only 'updated_since'
and 'project_id' are understood.""

Change-Id: Ic2b2314861030b029be5df490e123d8976711193
Closes-Bug: #1237836"
304,2749bb9138f5946d1ef3030c7fe349fbd5920139,1391895728,1.0,1.0,3,8,1,1,1,0.0,True,4.0,776342.0,56.0,27.0,False,101.0,950597.0,245.0,5.0,2066.0,3351.0,4661.0,1672.0,2643.0,3651.0,1968.0,2272.0,3526.0,0.269762981,0.311412522,0.483216879,350,761,1274992,nova,2749bb9138f5946d1ef3030c7fe349fbd5920139,1,1,Race condition,"Bug #1274992 in OpenStack Compute (nova): ""Utils.is_neutron can create race conditions""","It appears that using the utils.is_neutron() function can create race conditions, if not used carefully. Review #56381 was attempting to check the presence of Neutron, in order to determine if hairpinning should be enabled on the bridge that Nova creates. Only after creating a new configuration flag in Nova.conf and checking the setting, were the unit tests passing and the race condition resolved.
My hunch is that the global variable that utils.is_neutron( ) creates is the root cause of the issue.
https://review.openstack.org/#/c/56381/","Make is_neutron() thread-safe

The utils method is_neutron() was not thread safe and could have returned the
wrong value if two greenthreads called it at the same time.  Specifically, a
greenthread switch could occur when the import is being done.  At that point, it
looks like the check has already been completed, but the result has not yet been
recorded.

Closes-bug: #1274992
Change-Id: I5a6eedaeb4c0deee4b31509946ae5daedf98389d"
305,275a165cf0e74112d7ec9addacb1f84d703977c8,1397001343,,1.0,35,13,3,3,1,0.969519149,True,1.0,41506.0,10.0,5.0,False,94.0,718035.0,271.0,5.0,1186.0,3801.0,4321.0,986.0,2916.0,3303.0,1167.0,3544.0,4049.0,0.150379812,0.456418179,0.521436848,765,1192,1304724,nova,275a165cf0e74112d7ec9addacb1f84d703977c8,1,1,,"Bug #1304724 in OpenStack Compute (nova): ""DBNotAllowed raised if trying to create network with VlanManager from nova-manage network create""","Steps to reproduce:
- Setup a devstack from scratch using nova-network
- delete the default network
  # nova-manage network delete 10.0.0.0/24
- change nova.conf to use VlanManager:
network_manager = nova.network.manager.VlanManager
- restart nova-network
- create a new network with a vlan id:
nova-manage network create --label=network --fixed_range_v4 10.0.1.0/24 --vlan 42
- boot a vm on the cirros image:
nova --debug boot --flavor 1 --image 0b969819-2d85-4f7f-af76-125c5bb5789f test
Expected behavior: The new VM goes to Active state
Actual behavior: The new VM goes to Error state, also nova-network log has this exception:
a7-abaf-78db50a4b62c] network allocations from (pid=13676) allocate_for_instance /opt/stack/nova/nova/network/manager.py:494
2014-04-07 15:32:02.137 ERROR nova.network [req-87a65a9e-9196-4203-9de2-f6911d2aef4b admin demo] No db access allowed in nova-network: File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 128, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/network/floating_ips.py"", line 119, in allocate_for_instance
    **kwargs)
  File ""/opt/stack/nova/nova/network/manager.py"", line 497, in allocate_for_instance
    requested_networks=requested_networks)
  File ""/opt/stack/nova/nova/network/manager.py"", line 1837, in _get_networks_for_instance
    networks = self.db.project_get_networks(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1370, in project_get_networks
    return IMPL.project_get_networks(context, project_id, associate)
  File ""/opt/stack/nova/nova/cmd/network.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())
I think the exception was introduced by this patch that disables direct database access from nova-network: https://review.openstack.org/#/c/79716/
However, VlanManager still relies on database access for the given scenario, and there are 3 other places in manager.py that rely on direct db access:
devuser@ubuntu:/opt/stack/nova$ grep self.db nova/network/manager.py -n
1389: vifs = self.db.virtual_interface_get_by_instance(context,
1446: vif = self.db.virtual_interface_get_by_address(context,
1837: networks = self.db.project_get_networks(context, project_id)
1914: not self.db.network_in_use_on_host(context, network['id'],
Therefore, I cannot currently use conductor with nova-network VlanManager, which is a regression from Havana.
===
devstack defaults the network_manager to FlatDHCPManager so we don't test VlanManager in the gate.","Fix straggling uses of direct-to-database queries in nova-network

There were a few remaining calls directly to the database module
left in nova-network. These would fail because of the wedge we
put in place specifically to catch this case. However, we don't
test VlanManager in the gate, which means we didn't catch them
until now.

This required adding two more methods, one each to Network and
NetworkList, but they are both extremely simple.

Change-Id: Iecc382074f060da1bd6f740d7bde0b20a0be2fcf
Closes-bug: #1304724"
306,276a3b422d54b6976fed8c23ed54bd9bb707e4ff,1379450582,,1.0,5,4,2,2,1,0.764204507,True,1.0,61960.0,6.0,2.0,False,4.0,4372032.0,6.0,2.0,17.0,839.0,839.0,17.0,777.0,777.0,9.0,272.0,272.0,0.028571429,0.78,0.78,1569,1226814,1226814,neutron,276a3b422d54b6976fed8c23ed54bd9bb707e4ff,0,0,Feature or bug3 “Allows 'external' keyword in addition to 'any' keyword”,"Bug #1226814 in neutron: ""BigSwitch plugin: missing keyword in router rules""","In this BigSwitch plugin router rules support, the plugin only accepts the 'any' keyword. However, it should accept both the 'external' keyword as well, which looks the same to neutron (0.0.0.0/0) but has a different meaning on the backend controller.","BigSwitch plugin: allow 'external' keyword in router rules

Allows 'external' keyword in addition to 'any' keyword
in BigSwitch router rules support.

Closes-Bug: #1226814
Change-Id: Ia92513e89b0c95c5b3b98921e126fdd21cc4f405"
307,2779fdd14279e017f35780fe343b1c7243898397,1389359063,,1.0,1,5,1,1,1,0.0,True,1.0,127963.0,15.0,7.0,False,2.0,13889834.0,2.0,7.0,566.0,1135.0,1448.0,452.0,1058.0,1278.0,215.0,489.0,561.0,0.317647059,0.720588235,0.826470588,282,687,1267841,neutron,2779fdd14279e017f35780fe343b1c7243898397,1,1,Fix description,"Bug #1267841 in neutron: ""ext_gw_mode refers to DNAT""","The extension module for the L3_ext_gw_mode extension has:
- a RouterDNATdisabled exception which is not used anywhere
- a description which refers to disabling DNAT, which is not allowed by the extension
The above are 'vestigial' items from a previous release of the extension which allowed to disable DNAT (floating IPs)","Fix extension description and remove unused exception

Change-Id: Id0a40a8decb46e306256196c7fc79d687d4f8075
Closes-Bug: #1267841"
308,279b6e98bc72c906ac2c3f8665a1acdc99b30833,1410634617,,1.0,21,14,6,2,1,0.952573494,False,,,,,True,,,,,,,,,,,,,,,,,1353,1822,1373535,nova,279b6e98bc72c906ac2c3f8665a1acdc99b30833,1,1,"“Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make the obj_make_compatible calls consistent,”","Bug #1373535 in OpenStack Compute (nova): ""obj_make_compatible is wrong""","Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make the obj_make_compatible calls consistent, but it actually changed them the wrong way.
Change https://review.openstack.org/#/c/121663/ addresses the bug but is sitting on top of a change that might be too risky at this point for juno-rc1, so this should be a separate fix.","Undo changes to obj_make_compatible

Commit 7cdfdccf1bb936d559bd3e247094a817bb3c03f4 attempted to make
the obj_make_compatible calls consistent, but it actually changed
them the wrong way. This fixes them to unwrap nova_object.data
and updates the test that was testing the reverse.

Change-Id: I1a5db4e9817390ba3c2423d33387f780c890de64
Closes-Bug: #1373535"
309,27a5296f4d7f045abf9534155081025b214ce088,1410440879,,1.0,22,2,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1323,1791,1370019,nova,27a5296f4d7f045abf9534155081025b214ce088,1,1,,"Bug #1370019 in OpenStack Compute (nova): ""unshelve and resize instance unnecessarily logs ‘image not found’ error/warning messages""","unshelve and resize instance (created by bootable volume) unnecessarily logs ‘image not found’ error/warning messages
In both the cases, it logs following misleading error/warning messages in the compute.log when image_id_or_uri is passed as None to nova/compute/utils->get_image_metadata method.
14-09-05 03:41:54.834 ERROR glanceclient.common.http [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin] Request returned failure status 404.
14-09-05 03:41:54.834 WARNING nova.compute.utils [req-80c9db2e-cc3d-481c-a5a3-babd917a3698 admin admin]  [instance: d5b137ab-19a1-484a-a828-6a229ec66950] Can't access image : Image  could not be found.","Suppressed misleading log in unshelve, resize api

If instance is booted from volume, unshelve and
resize instance unnecessarily logs 'image not found'
error and warning messages in the compute.log when
image_id_or_uri is passed as None to
nova/compute/utils->get_image_metadata method.

To avoid this error and warning message, calling
image_api.get() method only if image_id_or_uri is
not 'None'.

Closes-Bug: #1370019
Change-Id: Ibf8c02fa153fc096e4e8a1c054ef62c935006091"
310,27b405e46c62e7084ac8db038f4b213a30dc3d23,1394208810,,1.0,13,3,2,2,1,0.988699408,True,4.0,1181163.0,91.0,18.0,False,14.0,713456.0,14.0,5.0,50.0,2879.0,2890.0,49.0,2432.0,2442.0,42.0,2752.0,2755.0,0.005703674,0.365167794,0.365565725,559,980,1289164,nova,27b405e46c62e7084ac8db038f4b213a30dc3d23,1,1,No exception handled.. Error raised,"Bug #1289164 in OpenStack Compute (nova): ""FloatingIpNotFoundForHost isn't handled when showing floating ips by host""","When showing the floating ips by host, I got this error.
$ nova floating-ip-bulk-list --host xxx
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-9a17c8cf-b1cd-4092-b853-7e24126db7e8)
and in the nova-api log I got this message:
014-03-05 04:01:40.270 ERROR nova.api.openstack [req-9a17c8cf-b1cd-4092-b853-7e24126db7e8 admin demo] Caught error: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack Traceback (most recent call last):
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 125, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return req.get_response(self.application)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2014-03-05 04:01:40.270 TRACE nova.api.openstack     application, catch_exc_info=False)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2014-03-05 04:01:40.270 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 598, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self.app(env, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     response = self.app(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return resp(environ, start_response)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 929, in __call__
2014-03-05 04:01:40.270 TRACE nova.api.openstack     content_type, body, accept)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 991, in _process_stack
2014-03-05 04:01:40.270 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1078, in dispatch
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return method(req=request, **action_args)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 48, in show
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return self._get_floating_ip_info(context, id)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips_bulk.py"", line 57, in _get_floating_ip_info
2014-03-05 04:01:40.270 TRACE nova.api.openstack     floating_ips = db.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/api.py"", line 356, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return IMPL.floating_ip_get_all_by_host(context, host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 110, in wrapper
2014-03-05 04:01:40.270 TRACE nova.api.openstack     return f(*args, **kwargs)
2014-03-05 04:01:40.270 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 918, in floating_ip_get_all_by_host
2014-03-05 04:01:40.270 TRACE nova.api.openstack     raise exception.FloatingIpNotFoundForHost(host=host)
2014-03-05 04:01:40.270 TRACE nova.api.openstack FloatingIpNotFoundForHost: Floating ip not found for host xxx.
2014-03-05 04:01:40.270 TRACE nova.api.openstack
The FloatingIpNotFoundForHost exception should be handled.","Catch FloatingIpNotFoundForHost exception

When showing floating ips for a given host, the FloatingIpNotFoundForHost
exception isn't handled when no floating ips can be found for the host.
This patch fixes this bug.

Closes-Bug: #1289164
Change-Id: Ic25ffd66945fd40ba10d69515e0cbfeae840313c"
311,280af85945d7ce8be60135fd364f22e1e8782b87,1393738452,,1.0,69,26,2,2,1,0.986449742,True,6.0,5752096.0,74.0,18.0,False,39.0,703389.0,79.0,6.0,87.0,2711.0,2716.0,87.0,2398.0,2403.0,87.0,1801.0,1806.0,0.011758418,0.240780331,0.241448423,358,769,1275755,nova,280af85945d7ce8be60135fd364f22e1e8782b87,0,0,Exception with more information,"Bug #1275755 in OpenStack Compute (nova): ""v2 volume api should return volumenotfound""","currently, NotFound exception was raised when no volume found in v2 api
this will not be helpful to user especially there is no 'ec2 error code' returned","Raise more information on V2 API volumes when resource not found

NotFound exception is caught in volumes.py on v2 api
and raise HTTPNotFound exception but exception information are
not well specified. This patch adds more information
to the raised exception.

Change-Id: I2bd48503460662fcc648b31ade7156059b370f2b
Closes-Bug: #1275755"
312,281e2a10628eb394b6b905286070c4f25ce9f134,1377852350,1.0,1.0,16,1,2,2,1,0.672294817,True,2.0,172725.0,11.0,4.0,False,28.0,233370.0,63.0,3.0,3.0,2601.0,2603.0,3.0,2312.0,2314.0,2.0,1731.0,1732.0,0.000509857,0.29435758,0.294527532,1498,1218779,1218779,nova,281e2a10628eb394b6b905286070c4f25ce9f134,1,1,“Fix wrong method call in baremetal”,"Bug #1218779 in OpenStack Compute (nova): ""'PXE' object has no attribute 'get_pxe_config_file_path'""","The baremetal driver's throwing the following error:
ERROR nova.virt.baremetal.driver [req-aa418c3f-a45b-40a2-b0b3-dff16886a8ab 94242353fea44662973dbe7ce3dc980c 93e2f6898a394052a6c1b7b22aedcf1f] Exception no pxe bootfile-name path: 'PXE' object has no attribute 'get_pxe_config_file_path'
The error happens because of this line in the baremetal/driver.py file.
nova/virt/baremetal/driver.py:511:            bootfile_path =            self.driver.get_pxe_config_file_path(instance)
Looking at the baremetal/pxe.py you'll see that the `get_pxe_config_file` method is not part of the PXE class, but that method exist in the file.","Fix wrong method call in baremetal

The `get_pxe_config_file` method is not part of the PXE class.

Change-Id: Ife4f5332af05fd82a865ecdbddf7c5431eb29c1d
Closes-Bug: #1218779"
313,288e3127440158f177beaae1972236def4916251,1394219386,,1.0,27,2,2,2,1,0.578794625,True,3.0,361990.0,53.0,11.0,False,1.0,1170.0,9.0,3.0,41.0,1339.0,1342.0,41.0,1131.0,1134.0,33.0,727.0,730.0,0.036598493,0.783638321,0.7868676,560,981,1289192,neutron,288e3127440158f177beaae1972236def4916251,1,1,Fix certificate file helper functions ,"Bug #1289192 in neutron: ""BigSwitch: certificate file helper functions incorrect""","The BigSwitch plugin has helper methods for writing certificates to the file system that are incorrectly defined.
They are missing the self argument that will be passed in.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L368
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L319
The unit tests were also incorrect in this case since they were refactored at right before the merge to avoid any file-system writes during unit tests.","BigSwitch: Fix certificate file helper functions

Fixes function definitions for file-system calls
in certificate functions for BigSwitch plugin.

Closes-Bug: #1289192
Change-Id: Ifea8506ea0d751e0d5b08511eafd04d2fa26be23"
314,28b37c1a707f5e958221b4ee28c4832d081eb706,1404899955,,1.0,31,13,4,4,1,0.794255639,False,,,,,True,,,,,,,,,,,,,,,,,1042,1486,1337821,nova,28b37c1a707f5e958221b4ee28c4832d081eb706,1,1,,"Bug #1337821 in OpenStack Compute (nova): ""VMDK Volume attach fails while attaching to an instance that is booted from VMDK volume""","I have booted an instance from a volume, successfully booted,
now another volume, i try to attach to same instance, it is failing.
see the stack trace..
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher     raise exception.InvalidDevicePath(path=root_device_name)
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher InvalidDevicePath: The supplied device path (vda) is invalid.
2014-07-04 08:56:11.391 TRACE oslo.messaging.rpc.dispatcher
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] Returning exception The supplied device path (vda) is invalid. to caller
2014-07-04 08:56:11.396 ERROR oslo.messaging._drivers.common [req-648122d5-fd39-495b-a3a7-a96bd32091d6 admin admin] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 401, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped\n    payload)\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped\n    return f(self, context, *args, **kw)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 286, in decorated_function\n    pass\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 314, in decorated_function\n    kwargs[\'instance\'], e, sys.exc_info())\n', '  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 302, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 4201, in reserve_block_device_name\n    return do_reserve()\n', '  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner\n    return f(*args, **kwargs)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 4188, in do_reserve\n    context, instance, bdms, device)\n', '  File ""/opt/stack/nova/nova/compute/utils.py"", line 106, in get_device_name_for_instance\n    mappings[\'root\'], device)\n', '  File ""/opt/stack/nova/nova/compute/utils.py"", line 155, in get_next_device_name\n    raise exception.InvalidDevicePath(path=root_device_name)\n', 'InvalidDevicePath: The supplied device path (vda) is invalid.\n']
The reason behind this issue is: because of the root device_name being set 'vda' in the case of boot from volume, The future volume attaches to the VM fail saying ""The supplied device path (vda) is invalid""","Prepend '/dev/' to supplied dev names in the API

Make sure that all user input device names end up in the database with
the '/dev/' prefix. This will make things more consistent and avoid
issues when attempting to parse device names.

This includes two main sources - block device mapping that can be passed
as part of a request or as image metadata, and root device name, passed
as image metadata.

Closes-bug: #1337821
Change-Id: Ibda82f511be99f1a68f2f77c72601a1b006be7a0"
315,28b8ecc15d874644952bb74539c61becaf5f7d20,1390334055,,1.0,3,2,1,1,1,0.0,True,2.0,321245.0,13.0,7.0,False,12.0,2387831.0,26.0,4.0,183.0,812.0,894.0,177.0,758.0,837.0,173.0,771.0,844.0,0.128698225,0.571005917,0.625,312,721,1271249,cinder,28b8ecc15d874644952bb74539c61becaf5f7d20,0,0,tests,"Bug #1271249 in Cinder: ""Possibly broken unit tests for TGT and LIO iSCSI backends""","I'm guessing (though I'm not really sure) that these two items in cinder/tests/test_iscsi.py are inaccurate:
from TgtAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'tgt-admin --update iqn.2011-09.org.foo.bar:blaa',
            'tgt-admin --force '
            '--delete iqn.2010-10.org.openstack:volume-blaa',
            'tgtadm --lld iscsi --op show --mode target'])
from LioAdmTestCase.setUp():
        self.script_template = ""\n"".join([
            'cinder-rtstool create '
            '/foo iqn.2011-09.org.foo.bar:blaa test_id test_pass',
            'cinder-rtstool delete iqn.2010-10.org.openstack:volume-blaa'])
Note how the IQNs for creation and deletion don't match.
The trouble is that if you replace those hardcoded IQNs with %(target)s, the unit tests break, possibly indicating an issue with the underlying brick implementations themselves.","Fixed inconsistency in iqn

Brick's TargetAdmin classes have a
default value set in __init__ for
iscsi_target_prefix.   The unit test was changing
the value returned for the internal _get_target,
but wasn't changing the CONF.iscsi_target_prefix to
match.  So the default value in the __init__ was
being used and hence the differences.

This patch sets the configuration's iscsi_target_prefix
which alters the values passed into the driver.  This
also makes the tests consistent between the iqn of the volume
at create time and delete time.

Closes-Bug: #1271249
Change-Id: Ie3308cf68adcbdff6057ea795af1299dded82e14"
316,29250949012e9c0a60b0ddb56ddbf18d7b68106b,1412371921,,1.0,66,7,3,2,1,0.717180057,False,,,,,True,,,,,,,,,,,,,,,,,1378,1848,1377307,neutron,29250949012e9c0a60b0ddb56ddbf18d7b68106b,1,0,"“When DVR is enabled and enable_isolated_metadata=True,
the DHCP agent should only inject a metadata host route” change in the requirements","Bug #1377307 in neutron: ""Metadata host route added when DVR and isolated metadata enabled""","When DVR is enabled and enable_isolated_metadata=True in dhcp_agent.ini, the agent should only inject a metadata host route when there is no gateway on the subnet.  But it does it all the time:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0  src 10.0.0.5
169.254.169.254 via 10.0.0.4 dev eth0
The ""opts"" file for dnsmasq confirms it was the Neutron code that configured this.
The code in neutron/agent/linux/dhcp.py:get_isolated_subnets() is only looking at ports where the device_owner field is DEVICE_OWNER_ROUTER_INTF, it also needs to look for DEVICE_OWNER_DVR_INTERFACE.  Simlar changes have been made in other code.
Making that simple change fixes the problem:
$ ip r
default via 10.0.0.1 dev eth0
10.0.0.0/24 dev eth0  src 10.0.0.5
I have a patch I'll get out for this.","Teach DHCP Agent about DVR router interfaces

When DVR is enabled and enable_isolated_metadata=True,
the DHCP agent should only inject a metadata host route
when there is no port with the gateway IP address configured
on the subnet.  Add a check for DEVICE_OWNER_DVR_INTERFACE
when we look at each port's device_owner field, otherwise
it will always add this route to the opts file when DVR
is enabled.

Change-Id: I3ff3bb85105b8215b36535983016d8c0ff3d8cb7
Closes-bug: #1377307"
317,295efa00081c770a57a5541652d9d3ffd5c1074d,1408423771,,1.0,8,1,2,2,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,1214,1673,1358668,neutron,295efa00081c770a57a5541652d9d3ffd5c1074d,1,1,,"Bug #1358668 in neutron: ""Big Switch: keyerror on filtered get_ports call""","If get_ports is called in the Big Switch plugin without 'id' being one of the included fields, _extend_port_dict_binding will fail with the following error.
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_restproxy_plugin.py"", line 87, in test_get_ports_no_id
    context.get_admin_context(), fields=['name'])
  File ""neutron/plugins/bigswitch/plugin.py"", line 715, in get_ports
    self._extend_port_dict_binding(context, port)
  File ""neutron/plugins/bigswitch/plugin.py"", line 361, in _extend_port_dict_binding
    hostid = porttracker_db.get_port_hostid(context, port['id'])
KeyError: 'id'","Big Switch: Check for 'id' in port before lookup

Check for the presence of the 'id' key before trying
to do a lookup based on the value. This is necessary
because a 'fields' param to get_port(s) may exclude
the 'id' field before the host_id lookup.

Closes-Bug: #1358668
Change-Id: If30567d5deaeabd2de6c6287fac7a81695a41cc5"
318,2981cdbb03c6f1239a58fedb260796667b8154ab,1396297959,0.0,,69,29,3,3,1,0.828210102,True,1.0,2397598.0,15.0,8.0,False,34.0,1880971.0,53.0,6.0,430.0,916.0,1114.0,362.0,821.0,980.0,415.0,829.0,1014.0,0.267180475,0.533076429,0.651894669,1867,1356368,1356368,Cinder,2981cdbb03c6f1239a58fedb260796667b8154ab,1,1,“https://github.com/openstack/cinder/commit/0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 appears to have introduced a bug”,"Bug #1356368 in Cinder: ""Able to show volumes associated with different project""","https://github.com/openstack/cinder/commit/0505bb268942534ad5d6ecd5e34a4d9b0e7f5c04 appears to have introduced a bug making it possible to show the details of a volume under another project_id.
$ . ~/devstack/openrc demo demo
$ cinder list
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
|                  ID                  |   Status  |  Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | available | demo1 |  1   | lvmdriver-1 |  false   |             |
+--------------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder list --all-tenants 1
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
|                  ID                  |            Tenant ID             |   Status  |  Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
| c99fe35a-47ef-448a-af31-edf7d04cd44a | 807f2a0ef357420e9a70ac1a5fef7a4c | available | demo1 |  1   | lvmdriver-1 |  false   |             |
+--------------------------------------+----------------------------------+-----------+-------+------+-------------+----------+-------------+
$ cinder show ad3194a3-8304-4d17-8f66-f1a1a261d339
+------------------------------+--------------------------------------+
|           Property           |                Value                 |
+------------------------------+--------------------------------------+
|         attachments          |                  []                  |
|      availability_zone       |                 nova                 |
|           bootable           |                false                 |
|          created_at          |      2014-08-12T11:06:33.000000      |
|         description          |                 None                 |
|          encrypted           |                False                 |
|              id              | ad3194a3-8304-4d17-8f66-f1a1a261d339 |
|           metadata           |                  {}                  |
|             name             |                admin1                |
| os-vol-tenant-attr:tenant_id |   e66effcd5db642dfabceabc76ea78196   |
|             size             |                  1                   |
|         snapshot_id          |                 None                 |
|         source_volid         |                 None                 |
|            status            |              available               |
|           user_id            |   7e540d1944a74bd4a90aa9a8b15d09b0   |
|         volume_type          |             lvmdriver-1              |
+------------------------------+--------------------------------------+","GET details REST API next link missing 'details'

When executing a pagination query a ""next"" link is included in the
API reply when there are more items then the specified limit.

See pagination documentation for more information:
http://docs.openstack.org/api/openstack-compute/2/content/
Paginated_Collections-d1e664.html

The caller should be able to invoke the ""next"" link (without
having to re-format it) in order to get the next page of data.
The documentation states ""Subsequent links will honor the
initial page size. Thus, a client may follow links to traverse
a paginated collection without having to input the marker parameter.""

The problem is that the ""next"" link is always scoped to the non-
detailed query.

For example, if you execute ""/v2/<tenant>/volumes/detail?limit=1"",
the ""next"" link does not have the URL for a detailed query and is
formatted as ""/v2/<tenant>/volumes?limit=1&marker=<marker>"". In this
case the ""next"" link needs to be scoped to ""/v2/<tenant>/volumes/detail"".

The user could work around this issue my manually inserting '/details'
into the ""next"" link URL.

Test code is included to verify that the '/details' URL is correctly added
when the ""next"" link is included in a detailed pagination query. Also,
existing tests were changed to ensure that the correct controller function
(ie, 'index' vs. 'detail) are invoked for the appropriate query
(ie, non-detailed or detailed) -- 'index' was previously alwayed invoked
for detailed URL requests.

Change-Id: Ib00d6deb25255fac1db0f7bf4ecd3c8d30e1c39d
Closes-bug: 1299247"
319,29e889bde9364f30c3f1cbeb7ee835b036451cba,1378225602,,1.0,14,1,2,2,1,0.566509507,True,1.0,32985.0,6.0,2.0,False,5.0,1711244.5,12.0,2.0,169.0,896.0,1039.0,169.0,821.0,964.0,16.0,764.0,764.0,0.017951426,0.80781415,0.80781415,1504,1220286,1220286,cinder,29e889bde9364f30c3f1cbeb7ee835b036451cba,1,1, ,"Bug #1220286 in Cinder: ""Volume operations fail for thin provisioning the first time it's executed""","The create_thin_pool method is called when thin provisioning is enabled and the pool doesn't exist. At the end of this method, self.vg_thin_pool is set[0] with the full pool_path which makes volume's creation fail since it tries to build the pool_path again.[1]
This is the command it generates:
sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvcreate -T -V 1g -n volume-b0c7232b-4214-4d13-ac20-4894333b627d stack-volumes/stack-volumes/stack-volumes-pool
This stack-volumes/stack-volumes/stack-volumes-pool should be stack-volumes/stack-volumes-pool
[0] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L340
[1] https://github.com/openstack/cinder/blob/master/cinder/brick/local_dev/lvm.py#L353","Set vg_thin_pool to pool name instead of pool_path

create_thin_pool is setting vg_thin_pool to the pool path instead of the
pool_name. This makes volumes creation fail when the create_thin_pool
method is called. This happens because create_volume builds the pool
path itself as create_thin_pool does.

Keeping the pool name in vg_thin_pool instead of the path makes more
sense and allows it to be used in other places in the brick. Also, most
commands return both vg_name and pool_name separated.

Change-Id: Ibf5cd746fc050eab5ce6aff13dd70c1e8066b228
Closes-Bug: #1220286"
320,29e93f2248df880f411bbf83316fd67d93a33b7d,1383616844,1.0,1.0,1,1,1,1,1,0.0,True,1.0,11021.0,11.0,4.0,False,4.0,824843.0,5.0,3.0,21.0,391.0,391.0,21.0,329.0,329.0,13.0,107.0,107.0,0.026923077,0.207692308,0.207692308,62,390,1248377,neutron,29e93f2248df880f411bbf83316fd67d93a33b7d,0,0,Tests,"Bug #1248377 in neutron: ""cisco n1kv unit test failing depending on threading""","The test_n1kv_plugin unit tests have segment IDs that overlap with segment IDs in test_n1kv_db.
https://github.com/openstack/neutron/blob/ba242d91c54b36628e03332a91adfef40e32ac32/neutron/tests/unit/cisco/n1kv/test_n1kv_plugin.py#L259
https://github.com/openstack/neutron/blob/64c0d5d272fd48c84b044a771ca6122700b69a29/neutron/tests/unit/cisco/n1kv/test_n1kv_db.py#L47
This causes a unit test for segment uniqueness to fail when they end up executing simultaneously. (e.g. http://logs.openstack.org/85/54485/10/check/gate-neutron-python27/e18c12f/console.html)","Fix segment range in N1KV test to remove overlap

Adjusts a segment range in an N1KV unit test so
it doesn't overlap with a segment range in another
test that may be executed simultaneously.

Closes-Bug: #1248377
Change-Id: I84a15a2c772f01d7be7ba91aa86de09b934ef5e9"
321,29f2d666e9ad64fd646d4711fd32cf325e039f5b,1385647371,0.0,1.0,19,3,2,2,1,0.266764988,True,18.0,1496110.0,112.0,35.0,False,139.0,2101.0,411.0,4.0,145.0,631.0,734.0,145.0,583.0,686.0,142.0,584.0,685.0,0.021229216,0.086846793,0.101840855,1765,1251589,1251589,nova,29f2d666e9ad64fd646d4711fd32cf325e039f5b,1,1, ,"Bug #1251589 in OpenStack Compute (nova): ""host update disable/enable report HTTP 400""","liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
ERROR: Bad request (HTTP 400) (Request-ID: req-f8c083dc-a327-48dc-b705-293f03e834dd)
liugya@liugya-ubuntu:~$ nova service-list
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary           | Host          | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor   | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:46.000000 | None            |
| nova-compute     | liugya-ubuntu | nova     | disabled | up    | 2013-11-15T11:06:42.000000 |                 | << disabled already
| nova-cert        | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:38.000000 | None            |
| nova-network     | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:39.000000 | None            |
| nova-scheduler   | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:40.000000 | None            |
| nova-consoleauth | liugya-ubuntu | internal | enabled  | up    | 2013-11-15T11:06:45.000000 | None            |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+","libvirt: host update disable/enable report HTTP 400

When disable/enable host with host-update command, end user will
get HTTP 400 error even if the host was updated(disabled/enabled)
successfully.

The root cause is that after the host was disabled/enabled, nova
api need depend on host status result to check if the host was
disabled or enabled, but libvirt driver did not return host status
to nova api.

Change-Id: I0688134eecb0628b680c8ed01741f00f80eec53f
Closes-Bug: #1251589"
322,2a0c679b00ecf8d047a4372bc38f1b2662348f48,1392862607,,1.0,56,55,4,3,2,0.723770719,True,2.0,517943.0,37.0,11.0,False,14.0,9018.4,26.0,4.0,0.0,1235.0,1235.0,0.0,1048.0,1048.0,0.0,636.0,636.0,0.001191895,0.759237187,0.759237187,436,851,1282352,neutron,2a0c679b00ecf8d047a4372bc38f1b2662348f48,1,1,"Bug, I think is not specific. It is a bug in the code","Bug #1282352 in neutron: ""Neutron Tempest XML tests fail for Cisco N1KV plugin""","While running Neutron Tempest tests for Cisco N1KV plugin, XML tests fail with the following trace:
FAIL: tempest.api.network.test_networks.BulkNetworkOpsTestXML.test_bulk_create_delete_network[gate,smoke]
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-02-13 11:08:50,454 Request: POST http://192.168.255.184:9696/v2.0/networks
2014-02-13 11:08:50,454 Request Headers: {'Content-Type': 'application/xml', 'Accept': 'application/xml', 'X-Auth-Token': '<Token omitted>'}
2014-02-13 11:08:50,454 Request Body: <?xml version=""1.0"" encoding=""UTF-8""?>
<networks xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network ><name >network--331846832</name></network><network ><name >network--859697792</name></network></networks>
2014-02-13 11:08:51,051 Response Status: 201
2014-02-13 11:08:51,051 Glance request id req-b2eb0771-614b-4edd-a4f8-df8c9f60e0dd
2014-02-13 11:08:51,051 Response Headers: {'content-length': '962', 'date': 'Thu, 13 Feb 2014 19:08:51 GMT', 'content-type': 'application/xml; charset=UTF-8', 'connection': 'close'}
2014-02-13 11:08:51,051 Response Body: <?xml version='1.0' encoding='UTF-8'?>
<networks xmlns=""http://openstack.org/quantum/api/v2.0"" xmlns:quantum=""http://openstack.org/quantum/api/v2.0"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--331846832</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>908124c9-f76c-4861-a875-21f9793d09f7</id></network><network><status>ACTIVE</status><subnets quantum:type=""list"" /><name>network--859697792</name><admin_state_up quantum:type=""bool"">True</admin_state_up><tenant_id>770dbef985aa4f3583880717ea3ac943</tenant_id><n1kv:profile_id>1e82eb72-5514-4972-a76a-b1202e46c057</n1kv:profile_id><shared quantum:type=""bool"">False</shared><id>50fff0cb-e592-4916-b7f2-1047c9174705</id></network></networks>
}}}
Traceback (most recent call last):
  File ""tempest/api/network/test_networks.py"", line 311, in test_bulk_create_delete_network
    resp, body = self.client.create_bulk_network(2, network_names)
  File ""tempest/services/network/network_client_base.py"", line 186, in create_bulk_network
    body = {'networks': self.deserialize_list(body)}
  File ""tempest/services/network/xml/network_client.py"", line 43, in deserialize_list
    return parse_array(etree.fromstring(body), self.PLURALS)
  File ""lxml.etree.pyx"", line 2754, in lxml.etree.fromstring (src/lxml/lxml.etree.c:54631)
  File ""parser.pxi"", line 1578, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:82748)
  File ""parser.pxi"", line 1457, in lxml.etree._parseDoc (src/lxml/lxml.etree.c:81546)
  File ""parser.pxi"", line 965, in lxml.etree._BaseParser._parseDoc (src/lxml/lxml.etree.c:78216)
  File ""parser.pxi"", line 569, in lxml.etree._ParserContext._handleParseResultDoc (src/lxml/lxml.etree.c:74472)
  File ""parser.pxi"", line 650, in lxml.etree._handleParseResult (src/lxml/lxml.etree.c:75363)
  File ""parser.pxi"", line 590, in lxml.etree._raiseParseError (src/lxml/lxml.etree.c:74696)
XMLSyntaxError: Namespace prefix n1kv on profile_id is not defined, line 2, column 211","Fixes Tempest XML test failures for Cisco N1kv plugin

Renamed N1kv_profile class to N1kv.
Modified N1kv attribute extension class to fix XML namespace issues.

Change-Id: I29b541730fc95c5e16667b64efd180be31e25b17
Closes-Bug: #1282352"
323,2a2d566be3d69295f56d2db5b6004f12d0ec5b9f,1393897340,3.0,1.0,349,349,46,30,1,0.909540086,True,1.0,51679.0,15.0,2.0,False,71.0,511667.913,210.0,2.0,691.0,1133.0,1572.0,593.0,983.0,1364.0,221.0,653.0,743.0,0.248322148,0.731543624,0.832214765,1830,1287462,1287462,Neutron,2a2d566be3d69295f56d2db5b6004f12d0ec5b9f,1,0,"""The following patch renames the rest of q_exc to n_exc which were left when quantum was renamed to neutron"" ------ Aqui me vuelve la duda, habria algun commit que olvido cambiar esto33 Quien seria el responsible33","Bug #1287462 in neutron: ""replace rest of q_exc to n_exc in code base""",The following patch renames the rest of q_exc to n_exc which were left when quantum was renamed to neutron.,"replace rest of q_exc to n_exc in code base

The following patch renames the rest of q_exc to n_exc which were
left when quantum was renamed to neutron.

used: find . -name ""*.py""  -print | xargs sed -i 's/q_exc/n_exc/g'

Change-Id: I04041030694b28c400b6c9c552433f2eaa4a37ba
Closes-bug: #1287462"
324,2a630f71af21334a1f3dd213205efcba1ef38ac4,1380701953,,1.0,1,2,1,1,1,0.0,True,1.0,72016.0,7.0,1.0,False,4.0,9063058.0,5.0,1.0,12.0,968.0,973.0,12.0,854.0,859.0,9.0,832.0,834.0,0.009407338,0.783631232,0.7855127,1627,1234002,1234002,cinder,2a630f71af21334a1f3dd213205efcba1ef38ac4,1,1,,"Bug #1234002 in Cinder: ""allowed_rpc_exception_modules in cinder.conf.sample is incorrect""","cinder.openstack.common.exception has already been deleted.
It seems to be a leak in 'Change-Id:
I6f46f90bd74cc26fc01667e467e3dab38037eec3'.","Remove incorrect class in cinder.conf.sample

cinder.openstack.common.exception has already been deleted.
It seems to be a leak in 'Change-Id:
I6f46f90bd74cc26fc01667e467e3dab38037eec3'.

Change-Id: Ic2000280f31e354da39a9bbb0d60142b431f58a6
Closes-Bug: #1234002"
325,2aaec3a81148e6c0cefd8f5b989dbcecf872a680,1386803411,0.0,1.0,21,1,1,1,1,0.0,True,3.0,300386.0,21.0,11.0,False,13.0,199007.0,60.0,3.0,546.0,831.0,1183.0,443.0,776.0,1037.0,195.0,319.0,409.0,0.314606742,0.51364366,0.658105939,151,553,1258150,neutron,2aaec3a81148e6c0cefd8f5b989dbcecf872a680,1,1,deadlock,"Bug #1258150 in neutron: ""nicira: tempest fails to delete routers in parallel""","when parallel tempest tests are enabled, the nicira plugin shows erros when deleting routers.
Parallel operations indeed cause the usual eventlet/mysql deadlock in delete_router as the nvp operation is nested within the db transaction.
The root cause for the deadlock is that the nvp api client uses eventlet to dispatch requests.
while a solution might be to rework the API client, an easier, backportable solution would be to move the NVP operation out of the transaction and ensuring consistency in case of failure.
note: in the same delete_router routine also the metada access network handling should be moved out of the transaction.","NVP plugin: Do backend router delete out from db transaction

Performing the NVP API operation from within a DB transaction
increases the risk of a deadlock between sqlalchemy and eventlet.

With this patch, the operation is moved outside of the db transaction
and appropriate mechanism are put in place for:
i) ensuring neutron db consistency in case of NVP failures
ii) avoiding deleting from backend if neutron logic does not allow it

This patch also synchronizes the routine for removing a router
gateway port from NVP.

Change-Id: I58d156e303e7a56ceb8c62766c192e154b0a3bb4
Closes-Bug: #1258150"
326,2b16e91b3f0fe06153a3a890c159c52974a57b91,1392865623,1.0,1.0,17,4,3,3,1,0.79987395,True,4.0,1018407.0,73.0,6.0,False,3.0,116456.3333,7.0,2.0,17.0,538.0,552.0,17.0,458.0,472.0,16.0,303.0,316.0,0.020238095,0.361904762,0.377380952,438,853,1282366,neutron,2b16e91b3f0fe06153a3a890c159c52974a57b91,1,1,,"Bug #1282366 in neutron: ""NVP FWaaS occurs error when creating firewall without policy""","VCNS related error should VcnsBadRequest other than BadRequest.
When creating a firewall without policy, it would report the following error:
Traceback (most recent call last):
  File ""neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""neutron/api/v2/base.py"", line 411, in create
    obj = obj_creator(request.context, **kwargs)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 893, in create_firewall
    self._vcns_update_firewall(context, fw, router_id)
  File ""neutron/plugins/nicira/NeutronServicePlugin.py"", line 842, in _vcns_update_firewall
    self.vcns_driver.update_firewall(context, edge_id, fw_with_rules)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 231, in update_firewall
    fw_req = self._convert_firewall(context, firewall)
  File ""neutron/plugins/nicira/vshield/edge_firewall_driver.py"", line 131, in _convert_firewall
    for rule in firewall['firewall_rule_list']:
TypeError: 'NoneType' object is not iterable
}}}","Fix NVP FWaaS errors when creating firewall without policy

Change-Id: I7ced6fe91a2d27c3739c54aa90489976532b3ecc
Closes-Bug: #1282366"
327,2b2decfd1b5b0d68813bc7e5b5e4349a1ab592aa,1394138287,,1.0,40,1,6,6,1,0.942703926,True,5.0,2425422.0,69.0,27.0,False,156.0,406021.0,618.0,12.0,31.0,2900.0,2900.0,31.0,2454.0,2454.0,24.0,2750.0,2750.0,0.003321818,0.36553282,0.36553282,529,950,1288296,nova,2b2decfd1b5b0d68813bc7e5b5e4349a1ab592aa,1,1,Conflict. Not allow duplicates names.,"Bug #1288296 in OpenStack Compute (nova): ""Update aggregate allows duplicate names""","The behaviour to manage naming conflicts is different between aggregate creation and aggregate update.
Aggregate create doesn't let you create 2 aggregates with the same name.
Aggregate update lets you update an aggregate to a name that already exists.
It seems to me it should be consistent, and probably both check for conflict.
Here's an example, using a recent devstack:
$ nova aggregate-create test
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 14 | test | -                 |       |          |
+----+------+-------------------+-------+----------+
$ nova aggregate-create test
ERROR: There was a conflict when trying to complete your request. (HTTP 409) (Request-ID: req-6711e05e-4efc-4a2d-9117-52d034c74a4f)
$ nova aggregate-create test2
+----+-------+-------------------+-------+----------+
| Id | Name  | Availability Zone | Hosts | Metadata |
+----+-------+-------------------+-------+----------+
| 15 | test2 | -                 |       |          |
+----+-------+-------------------+-------+----------+
$ nova aggregate-update 15 test
Aggregate 15 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 15 | test | -                 |       |          |
+----+------+-------------------+-------+----------+
$ nova aggregate-list
+----+--------------------+-------------------+
| Id | Name               | Availability Zone |
+----+--------------------+-------------------+
| 14 | test               | -                 |
| 15 | test               | -                 |
+----+--------------------+-------------------+
Nova api logs from when the aggregate creation fails as expected:
2014-04-05 14:45:34.865 INFO nova.api.openstack.compute.contrib.aggregates [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Aggregate test already exists.
2014-04-05 14:45:34.865 INFO nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] HTTP exception thrown: There was a conflict when trying to complete your request.
2014-04-05 14:45:34.865 DEBUG nova.api.openstack.wsgi [req-aeb307d4-c4c9-4684-8b13-1d81f0b8c692 admin demo] Returning 409 to user: There was a conflict when trying to complete your request. from (pid=1517) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1223","Update aggregate should not allow duplicated names

It is not allow to create 2 aggregates with the same name
but if update an existing one with a name already used
by other aggregate that check should be made.
This patch doesn't allow to change the name to an existing
one.

Change-Id: Icfe315594beaec94229e5a4a4c1bb80fc366b66e
Closes-Bug: #1288296"
328,2b375c0f15fd43af23fbd28b85929d63a753548b,1385587567,,1.0,38,1,2,2,1,0.391243564,True,2.0,1907363.0,53.0,10.0,False,16.0,4627326.5,26.0,4.0,534.0,2042.0,2207.0,434.0,1821.0,1957.0,183.0,309.0,392.0,0.315068493,0.530821918,0.672945205,96,495,1255680,neutron,2b375c0f15fd43af23fbd28b85929d63a753548b,1,1,Sending a lot of notifications,"Bug #1255680 in neutron: ""ml2: too many port_update notifications  sent because of port binding""","process_port_binding is always called when a port is updated: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L617
However, its current implementation: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L202
Return True, hence triggering a notification in many cases.
The code has a check for returning false when the host is not set and the vif is already bound: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L213
However, this is perhaps not triggered in many cases.
As an example, in this job: http://logs.openstack.org/20/57420/9/experimental/check-tempest-devstack-vm-neutron-isolated-parallel/269a314
Of 538 update_port calls to the plugin, process_port_binding turns need_port_update_notify from False to Truue in
It is therefore worth ensuring no notification is sent if the host doesn't change (the other binding parameters should not change on update and even if they can they should not trigger a notification) in 72 cases out of 82 cases where the flag was false before invoking process_port_binding
Looking at the logs - the port should not have been notified because of bindings in any case.","Do not trigger agent notification if bindings do not change

The method _process_port_bindings for the ml2 plugin should not
return True when the host binding does not change, otherwise an
unnecessary notification will be sent to the agent

Closes-Bug: #1255680
Related-Bug: #1253896
Partially Implements: neutron-tempest-parallel

Change-Id: I8a40090af347ca430ff6c8e2211fa34bb2bd0f8c"
329,2b9805105eac9594a038c8e09f4b6cfc6255c677,1389879456,1.0,1.0,60,3,2,2,1,0.631263018,True,6.0,3393143.0,83.0,26.0,False,7.0,5390688.5,9.0,7.0,58.0,1235.0,1242.0,58.0,1084.0,1091.0,51.0,570.0,575.0,0.073758865,0.809929078,0.817021277,1816,1269189,1269189,Neutron,2b9805105eac9594a038c8e09f4b6cfc6255c677,1,1, ,"Bug #1269189 in neutron: ""iptables_manager doesn't honor wrap target when removing a rule""","iptables_manager permits to use a variable begining by a $ in order to reference a wrapped chain, the issue is that the remove rule method doesn't expand this variable, thus if a rule has been added with a variable, it is impossible to remove it since the iptables manager compares the expanded rule and the not expanded one, of course not equal.","Fix wrap target in iptables_manager

This patch fixes issues when using wrap target
for add_rule and remove_rule methods.

Change-Id: I01926719ef5ccf229748b9ceb1553e5314ab623e
Closes-bug: #1269189"
330,2bc0877d3cbe5e6ed9bad0907f61011014b75a2d,1395951746,,1.0,25,3,2,2,1,0.940285959,True,5.0,2952297.0,63.0,16.0,False,184.0,302494.0,940.0,1.0,122.0,932.0,954.0,122.0,931.0,953.0,122.0,927.0,949.0,0.015976101,0.120535134,0.123392648,759,1186,1304184,nova,2bc0877d3cbe5e6ed9bad0907f61011014b75a2d,1,1,,"Bug #1304184 in OpenStack Compute (nova): ""instance stuck into rebuild state when nova-compute unexpected restart""","when rebuild an instance, and nova-compute died unexpectedly, then the instance will be in REBUILD state forever unless admin take actions
[root@controller ~]# nova list
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| ID                                   | Name   | Status  | Task State | Power State | Networks                                   |
+--------------------------------------+--------+---------+------------+-------------+--------------------------------------------+
| a9dd1fd6-27fb-4128-92e6-93bcab085a98 | test11 | REBUILD | rebuilding | Running","Update instance state after compute service died for rebuilded instance

When nova-compute died unexpectedly or rpc message lost unexpectly
and an rebuild operation is ongoing, the instance will be in REBUILD
state forever after the nova-compute service restarts.
Only if admin take actions such as reset its state.
Like what we did for build instance (general spawn)
operation, the instance might not be used so it need to be set to ERROR
state after compute service restarts.

Change-Id: I37748edb3ca0501b6f2fb41fbd75b7b8aa00d59c
Closes-Bug: #1304184"
331,2bca0c9011d9868aff3e80f4f6f432d29f07adf6,1395952823,,1.0,59,15,2,2,1,0.956888666,True,3.0,58411.0,15.0,4.0,False,46.0,3673218.0,99.0,3.0,2198.0,2193.0,3640.0,1709.0,1980.0,3015.0,2099.0,2162.0,3513.0,0.272727273,0.280909091,0.456363636,677,1103,1296913,nova,2bca0c9011d9868aff3e80f4f6f432d29f07adf6,0,0,Evolution. Add implementation,"Bug #1296913 in OpenStack Compute (nova): ""GroupAntiAffinityFilter scheduler hint no longer works""","Passing a scheduler hint for the GroupAntiAffinityFilter no longer works:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
ERROR (NotFound): The resource could not be found. (HTTP 404) (Request-ID: req-21430f41-e6ca-46db-ab5c-890a1d1dbd01)
screen-n-api.log contains message:
Caught error: Instance group foo could not be found.","Add new style instance group scheduler filters

Prior to Icehouse, there was a different type of handling of the
'group' scheduler hint that got lost in the completion of the server
groups API.  This patch completes the code necessary to provide
backwards compatibility with the old behavior.

Previously, the policy for groups was simply based on what scheduler
filters you had enabled.  You could have either the affinity or
anti-affinity filter enabled and that would be applied to all groups.
These filters now act on groups with a policy type of 'legacy'.

New filters have been added that can be enabled simultaneously and act
based on the policy set on the group via the server group API.

DocImpact

Change-Id: Ia65c151396415ca48725cb3c756f33efa01d2fe5
Closes-bug: #1296913"
332,2bd97d205b98292be709b0257bda5fc489eb643a,1401704042,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,923,1359,1323715,neutron,2bd97d205b98292be709b0257bda5fc489eb643a,0,0,Bug in test,"Bug #1323715 in neutron: ""network tests fail on policy check after upgrade from icehouse to master (juno)""","Lots of tempest tests fail after upgrade
http://logs.openstack.org/51/94351/3/check/check-grenade-dsvm-neutron/ac837a8/logs/testr_results.html.gz
2014-05-26 21:47:20.109 364 INFO neutron.wsgi [req-7c96bf86-6845-4143-92d0-2bb32f5767d7 None] (364) accepted ('127.0.0.1', 60250)
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:619
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:678
2014-05-26 21:47:20.110 364 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1041
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Storing token in cache _cache_put /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1151
2014-05-26 21:47:20.111 364 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 47d465f7c2e44c048f63066dff93093c with project_id : d3e7af8cf42d4613beb315dc19444d40 and roles: _member_  _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:940
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] No route matched for GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:97
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Matched GET /ports.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2014-05-26 21:47:20.112 364 DEBUG routes.middleware [-] Match dict: {'action': u'index', 'controller': <wsgify at 87437776 wrapping <function resource at 0x5358500>>, 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2014-05-26 21:47:20.122 364 DEBUG neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to find ':' as separator in tenant_id. __call__ /opt/stack/new/neutron/neutron/policy.py:243
2014-05-26 21:47:20.123 364 ERROR neutron.policy [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.policy Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.policy   File ""/opt/stack/new/neutron/neutron/policy.py"", line 239, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.policy     parent_res, parent_field = do_split(separator)
2014-05-26 21:47:20.123 364 TRACE neutron.policy   File ""/opt/stack/new/neutron/neutron/policy.py"", line 234, in do_split
2014-05-26 21:47:20.123 364 TRACE neutron.policy     separator, 1)
2014-05-26 21:47:20.123 364 TRACE neutron.policy ValueError: need more than 1 value to unpack
2014-05-26 21:47:20.123 364 TRACE neutron.policy
2014-05-26 21:47:20.123 364 ERROR neutron.api.v2.resource [req-ee5c1651-9d0c-43c9-974d-a0c888c08468 None] index failed
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 309, in index
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return self._items(request, True, parent_id)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 264, in _items
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     request.context, obj_list[0])
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/api/v2/base.py"", line 145, in _exclude_attributes_by_policy
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     might_not_exist=True):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/policy.py"", line 346, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return policy.check(*(_prepare_check(context, action, target)))
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 169, in check
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     result = rule(target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 732, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     return _rules[self.match](target, creds)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/openstack/common/policy.py"", line 366, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     if rule(target, cred):
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource   File ""/opt/stack/new/neutron/neutron/policy.py"", line 261, in __call__
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource     reason=err_reason)
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource PolicyCheckError: Failed to check policy tenant_id:%(tenant_id)s because Unable to verify match:%(tenant_id)s as the parent resource: tenant was not found
2014-05-26 21:47:20.123 364 TRACE neutron.api.v2.resource","Pass object to policy when finding fields to strip

During the evaluation of fields to strip in responses to list
operations, pass also the first object in the list to the
policy engine.
This will avoid errors in policy evaluation if during
an upgrade from icehouse policy.json was not updated to remove
attribute-level policies dependent on resource values.

Closes-Bug: #1323715

Change-Id: Iaa6ed3bbf4a07ce0b614a4197cabdfa1cb36d427"
333,2bebf1090156da5a773df723026c16bc40d8a138,1388031005,,1.0,1,8,1,1,1,0.0,True,1.0,611030.0,16.0,3.0,False,16.0,2379172.0,46.0,2.0,59.0,228.0,286.0,59.0,227.0,285.0,1.0,195.0,195.0,0.001839926,0.180312787,0.180312787,228,631,1264204,glance,2bebf1090156da5a773df723026c16bc40d8a138,0,0,Tests,"Bug #1264204 in Glance: ""redundant code in glance.tests.unit.utils:FakeDB.reset""","glance.tests.unit.utils:FakeDB uses glance.db.simple.api
FakeDB  implements reset with:
    @staticmethod
    def reset():
        simple_db.DATA = {
            'images': {},
            'members': [],
            'tags': {},
            'locations': [],
            'tasks': {},
            'task_info': {}
        }
while api already has:
def reset():
    global DATA
    DATA = {
        'images': {},
        'members': [],
        'tags': {},
        'locations': [],
        'tasks': {},
        'task_info': {}
    }
the redundant code in glance.tests.unit.utils:FakeDB.reset should be replaced with simple_db.reset()","Refactor tests.unit.utils:FakeDB.reset

glance.tests.unit.utils:FakeDB uses glance.db.simple.api which has
already implemented reset() functionality, the redundant code in
FakeDB should be removed.

Change-Id: I140e932e9cc531900cee503e20d3549549b13d27
Closes-Bug: #1264204"
334,2bfc7171c23d0595aa7f8680271778bc58cb28ba,1392132520,1.0,1.0,80,5,4,3,1,0.846896775,True,16.0,6849565.0,182.0,42.0,False,179.0,581757.0,925.0,5.0,23.0,3221.0,3221.0,23.0,2722.0,2722.0,23.0,2278.0,2278.0,0.003270646,0.310575089,0.310575089,329,738,1272830,nova,2bfc7171c23d0595aa7f8680271778bc58cb28ba,0,0,Add more documentation,"Bug #1272830 in OpenStack Compute (nova): ""document expected behavior when setting poll config option to 0""","We have a whole bunch of config options that control periodic tasks that can be turned off when setting the option to 0 or -1. We should document in  the config option help messages what the expected behavior when setting these options to < 1.
Related patch: https://review.openstack.org/#/c/60641","Add warning to periodic_task with interval 0

The behaviour of Oslo's @periodic_task decorator is that
a repeat interval of 0 means that the method will be
called regularly.  Several Nova methods which use that
decorator also have code for returning early (ie no-op)
if the interval is 0.  This is inconsistent for users, and
means there is timing-related code where it doesn't belong.

This patch adds a warning when a user takes a value of 0
from config and uses it as an interval in a @periodic_task.

Ideally all code will use Oslo's convention that 0 means
run at the default rate and a negative interval will mean
""don't run"".  After this warning has been in place for a full
release cycle that can be the case.

Change-Id: Ia227f4c4e69ecf361ab02d1d17a3010303650104
DocImpact: Warns of upcoming change to behaviour when
           *_interval is set to 0
Closes-Bug: #1272830
Partial-Bug: #1276203"
335,2c5f99a4b1376a46b8c1e1f4e82a015c2d29aa1f,1412961225,,1.0,74,67,9,4,1,0.665315789,False,,,,,True,,,,,,,,,,,,,,,,,1395,1865,1379811,neutron,2c5f99a4b1376a46b8c1e1f4e82a015c2d29aa1f,0,0,"“he VPN logging code should use the new marker functions for info, warning, error”","Bug #1379811 in neutron: ""VPN: Update logging to use i18n hints""","The VPN logging code should use the new marker functions for info, warning, error and critical log levels to separate log messages into different catalogs for translation priority. We also need to ensure that the new i18n guidelines are followed.","Update VPN logging to use new i18n functions

For log messages in neutron/services/vpn and neutron/db/vpn, replace
_() marker functions with log-level-specific marker functions: _LI(),
_LW(), _LE() from oslo.i18n.

Also, remove _() functions for debug log messages as debug level log
messages should not be translated.

Change-Id: I07fcf25bb6344c47e74d6ee23f9bc08e4b560679
Closes-Bug: #1379811"
336,2c94b11c40433e44c481aed6b590a563471dc358,1391213160,,1.0,2,1,1,1,1,0.0,True,1.0,251261.0,8.0,2.0,False,1.0,8384731.0,1.0,2.0,41.0,277.0,281.0,41.0,276.0,280.0,38.0,241.0,243.0,0.034240562,0.212467076,0.214223003,351,762,1275126,glance,2c94b11c40433e44c481aed6b590a563471dc358,0,0, Add VMware store the strategy module ,"Bug #1275126 in Glance: ""Add VMware store the strategy module""",The VMware store needs to be added to strategy module: https://github.com/openstack/glance/blob/9567c2b6a06aa1e8205f9f30beca63d77500dd1d/glance/common/location_strategy/store_type.py#L55,"Add VMware storage backend to location strategy

The store to scheme map used in the location strategy module
doesn't contain the VMware datastore backend.

Change-Id: I5d1faeb464dab79c300a525bab6e7e0ecb4ea489
Closes-Bug: #1275126"
337,2cae463f18a8a0a579e6dd22ec918af79934ac00,1379091074,,1.0,3,3,2,2,1,0.918295834,True,3.0,23960.0,12.0,6.0,False,8.0,550434.0,17.0,3.0,27.0,976.0,993.0,24.0,878.0,892.0,16.0,843.0,849.0,0.017102616,0.849094567,0.855130785,1556,1225094,1225094,cinder,2cae463f18a8a0a579e6dd22ec918af79934ac00,1,0,“nms.folder.create_with_opts not supported on NexentaSto”,"Bug #1225094 in Cinder: ""Nexenta NFS volume driver error in _do_create_volume method""","NMS method nms.folder.create_with_opts not supported on NexentaStor Appliance version 3.1.4.2, instead of this method must be used nms.folder.create_with_props","nms.folder.create_with_opts not supported on Nexenta 3.1.4.2

nms.folder.create_with_opts method not supported on early NexentaStor
version (3.1.4.2). replace if with create_with_props.

Closes-Bug: #1225094
Change-Id: I0c861ae8d966d53c6484e7ded8cab71f395fa059"
338,2cf64655da38d228b8892a86f005353c84ab212b,1298840069,0.0,,408,312,8,4,2,0.782866047,False,,,,,True,,,,,,,,,,,,,,,,,1824,1280100,1280100,Glance,2cf64655da38d228b8892a86f005353c84ab212b,1,0,“StringIO.StringIO is incompatible for python 3”,"Bug #1280100 in Glance: ""StringIO.StringIO is incompatible for python 3""","Import StringIO
StringIO.StringIO()
should be :
Import six
six.StringIO() or six.BytesIO()
StringIO works for unicode
BytesIO works for bytes
For Python3 compatible.","Adds ability for Swift to be used as a full-fledged backend.
Adds POST/PUT capabilities to the SwiftBackend
Adds lots of unit tests for both FilesystemBackend and SwiftBackend
Removes now-unused tests.unit.fakeswifthttp module"
339,2d13161643faf2b663b50d2e191232fcdeefd815,1379340897,,1.0,10,5,2,2,1,0.721928095,True,2.0,797594.0,17.0,9.0,False,22.0,611888.0,23.0,8.0,6.0,3408.0,3410.0,5.0,2794.0,2796.0,0.0,2317.0,2317.0,0.000165289,0.383140496,0.383140496,1552,1224960,1224960,nova,2d13161643faf2b663b50d2e191232fcdeefd815,1,0,BIC or evolution333 “Nova was sending the host that previously hosted the vm”,"Bug #1224960 in OpenStack Compute (nova): ""wrong port-binding info after live-migration ""","When live migrating a VM, nova-compute send the new port-binding info to neutron. But the host sent in the port-binding info is the previous host, not the host on which the VM is migrated.
I've attached a patch to resolv this, but I will ask for review as soon as I will hae time to investigate deeper.","send the good binding to neutron after live-migration

Nova was sending the host that previously hosted the vm

Closes-Bug: #1224960

Change-Id: If4677c0d1e11a8cf16f9d49e9ca854ca2fcfb698"
340,2d20b87aef5a7d00cb36826b8907032912fb17fc,1376072126,0.0,1.0,56,23,3,3,1,0.427449964,True,5.0,2321114.0,35.0,12.0,False,105.0,24914.0,250.0,4.0,102.0,3809.0,3833.0,102.0,3242.0,3266.0,95.0,2774.0,2793.0,0.017155111,0.495889921,0.499285204,1458,1207914,1207914,nova,2d20b87aef5a7d00cb36826b8907032912fb17fc,1,1, ,"Bug #1207914 in OpenStack Compute (nova): ""nova stacktraces when neutron throws a quota error""","From nova-compute log when exceeding a quota.
/json; charset=UTF-8'} {""NeutronError"": ""Quota exceeded for resources: ['port']""}
 http_log_resp /opt/stack/python-neutronclient/neutronclient/common/utils.py:179
2013-08-02 21:30:38.115 30306 DEBUG neutronclient.v2_0.client [-] Error message: {""NeutronError"": ""Quota exceeded for resources: ['port']""} _handle_fault_response /opt/stack/python-neutronclient/neutronclient/v2_0/client.py:756
2013-08-02 21:30:38.115 30306 ERROR nova.compute.manager [-] Instance failed network setup after 1 attempt(s)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager Traceback (most recent call last):
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/compute/manager.py"", line 1280, in _allocate_network_async
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     security_groups=security_groups)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/api.py"", line 49, in wrapper
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     res = f(self, context, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 310, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     LOG.exception(msg, port_id)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 287, in allocate_for_instance
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     port_client.create_port(port_req_body)['port']['id'])
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     ret = self.function(instance, *args, **kwargs)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 276, in create_port
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     return self.post(self.ports_path, body=body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 872, in post
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     headers=headers, params=params)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 795, in do_request
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     self._handle_fault_response(status_code, replybody)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 765, in _handle_fault_response
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     exception_handler_v20(status_code, des_error_body)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 81, in exception_handler_v20
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager     message=error_dict)
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager NeutronClientException: Quota exceeded for resources: ['port']
2013-08-02 21:30:38.115 30306 TRACE nova.compute.manager
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
Furthermore nova doesn't show its a quota issue but instead says:
| fault                                | {u'message': u'NoValidHost', u'code': 500, u'created': u'2013-08-02T21:31:04Z'} |","Handle port over-quota when allocating network for instance

This patch adds a check in the neutron API for a port create failure due
to over-quota in neutron and raises the new exception PortLimitExceeded.

Also moves the port-create block of code into it's own method to
try and clean up some of the large allocate_for_instance method.

Closes-Bug: #1207914
Related-Bug: #1209446

Change-Id: I4000c8ab550e032363f138a86e1b87f6ab2f5ff2"
341,2d2e30dba5c571c05dd4191c1b023fe4014c1751,1385571906,,1.0,0,2,2,2,1,1.0,True,1.0,309277.0,10.0,7.0,False,87.0,26985.0,224.0,7.0,243.0,2729.0,2827.0,243.0,2520.0,2618.0,228.0,1597.0,1685.0,0.034072311,0.237762238,0.250855527,108,507,1256427,nova,2d2e30dba5c571c05dd4191c1b023fe4014c1751,0,0,Duplicate exception,"Bug #1256427 in OpenStack Compute (nova): ""Duplicate FlavorNotFound handled in server create API""","The FlavorNotFound exception is handled in multiple places in the server create API, the first being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L951
and the latter being:
https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/servers.py#L970
They both return a 400 response and only the former is used, so remove the latter.
This is both in the v2 and v3 API.","Remove duplicate FlavorNotFound exception handling in server create API

The v2 and v3 openstack server create APIs have duplicate handling for
the FlavorNotFound exception. This patch removes the duplicate handling
which is found in a generic catch-all block for each API.

Closes-Bug: #1256427

Change-Id: I78344cd588178c501398147f3afcd418e0744b0d"
342,2d72790bf83473e0c9b73cc290f711fcda7a5803,1398187669,,1.0,5,3,1,1,1,0.0,True,2.0,1635457.0,26.0,3.0,False,2.0,1252660.0,10.0,1.0,73.0,153.0,226.0,73.0,106.0,179.0,64.0,67.0,131.0,0.055698372,0.058269066,0.11311054,857,1290,1315574,neutron,2d72790bf83473e0c9b73cc290f711fcda7a5803,0,0,Bug in test,"Bug #1315574 in neutron: ""Big Switch unit test unnecessary magic mock""",The Big Switch unit tests use Magic Mocks to stop external calls (e.g. HTTP and RPC). This is a waste of memory most of the time since no assertions are made on the Mocks.,"Big Switch: Don't use MagicMocks unnecessarily

Replace the mocks purely being used to block external
calls with functions that just return and don't record
calls.

Closes-Bug: #1315574
Change-Id: I79533135b5cb456d941ddffa62f5365970146a7e"
343,2d8b905f474a879ad6a9a441e79bf7b05d2d39a5,1380898586,,1.0,25,9,6,5,1,0.992530109,True,2.0,404682.0,12.0,6.0,False,66.0,639646.5,167.0,3.0,57.0,674.0,698.0,55.0,653.0,676.0,50.0,628.0,646.0,0.047663551,0.587850467,0.604672897,1637,1235148,1235148,cinder,2d8b905f474a879ad6a9a441e79bf7b05d2d39a5,1,1,Feature or bug3 “Provide the user with useful information”,"Bug #1235148 in Cinder: ""Over quota message does not give user enough information""","The message cinder returns to the user when a create volume/snapshot request would exceed quota does not give the user the information they need to correct the problem. The error message returned is:
Requested volume or snapshot exceeds allowed Gigabytes quota.
The following goes to the api log:
Quota exceeded for 12345678, tried to create 2048G volume (2048G of 4048G already consumed)
However, in general, the user won't have access to the api.log, so those numbers should be passed to the user.","Provide user with more information on quota fail

Provide the user with useful information when a snapshot or volume
create fails because it would cause the user to exceed available quota.
Specifically report the user's current gigabyte usage and quota
allocations.

Closes-Bug: #1235148

Change-Id: Ib4c5dbcbd172c69834c2824791af755380f38e01"
344,2d9488be71244cd7fcee624764b2f6c0cc5fe3a1,1402339830,,1.0,4,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,960,1398,1328222,neutron,2d9488be71244cd7fcee624764b2f6c0cc5fe3a1,1,1,"""Sync function is missing network information”","Bug #1328222 in neutron: ""BigSwitch: Sync function is missing network information""",The Big Switch full topology synchronization function isn't including all of the information about a network. It's only including the ports and floating IP addresses. The subnets are missing as well as the tenant of the network.,"Big Switch: Add missing data to topology sync

Adds data to the topology sync function that was
missing in the case where it is called from the
ML2 driver.

Closes-Bug: #1328222
Change-Id: I1cc3b0430f63f62c881f28139d1e5ebf22766357"
345,2da351af7d2a10a3055020d572d357c56ba2689b,1396783747,1.0,1.0,27,1,2,2,1,0.67694187,True,5.0,169259.0,100.0,19.0,False,4.0,3033320.0,4.0,4.0,66.0,843.0,888.0,66.0,727.0,772.0,57.0,554.0,590.0,0.052727273,0.504545455,0.537272727,727,1153,1301432,neutron,2da351af7d2a10a3055020d572d357c56ba2689b,1,1,,"Bug #1301432 in neutron: ""ODL ML2 driver doesn't warn if the url parameter is not set""","If you don't define the ""url"" parameter in the [ml2_odl] section of the ML2 configuration file, the ODL driver will run happily but no request will be made to the ODL service. See http://git.openstack.org/cgit/openstack/neutron/tree/neutron/plugins/ml2/drivers/mechanism_odl.py?id=a57dc2c30ab78ba74cfc51b8fdb457d3374cc87d#n313
The parameter should have a sane default value (eg http://127.0.0.1:8080/controller/nb/v2/neutron) and/or a message should be logged to warn the deployer.","Enforce required config params for ODL driver

Raise a config error during initialization if
there is no URL, username, or password specified
in the config for the OpenDayLight ML2 driver.

Closes-Bug: #1301432
Change-Id: I65fc94d3eaaade3d1402d1c82d2c1edfa7133d5a"
346,2ddc905ebba6e6b2bd5be2ca45a14115ef5a6e8f,1399385862,,1.0,2,2,1,1,1,0.0,True,1.0,693710.0,18.0,6.0,False,14.0,614961.0,17.0,6.0,4.0,2668.0,2668.0,4.0,2147.0,2147.0,4.0,2310.0,2310.0,0.000630199,0.291278044,0.291278044,862,1295,1316152,nova,2ddc905ebba6e6b2bd5be2ca45a14115ef5a6e8f,1,1,,"Bug #1316152 in OpenStack Compute (nova): ""nova cell showing should raise more information on v2 API""","nova cell showing should raise more information on v2 API, instead of ""The resource could not be found. ""","Fix the explanation of HTTPNotFound for cell showing v2 API

This patch adds the explanation to cell showing in v2 API, when
the cell is not found, the exception of HTTPNotFound will return
more infomation.

Change-Id: I25dee08a04ad974af5309999527e0f173d2c2cab
Closes-Bug: #1316152"
347,2df48336d0e0381522e9ec100cb56579367cfa1f,1410339963,,1.0,3,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1282,1745,1367172,glance,2df48336d0e0381522e9ec100cb56579367cfa1f,1,1,,"Bug #1367172 in Glance: ""glance-manage db_load_metadefs should use 'with open as...'""",Currently script which loads metadeta definitions to database uses try except block with open(file) inside. There is better solution in Python for file streams - use 'with open(file) as...'. With this approach we will be sure that every resource is cleaned up when the code finishes running.,"Change open(file) to with block

Currently script which loads metadata definitions to database
uses open(file) to stream files. This isn't the best practice
because in case of any errors this code may not be safe.
To solve this problem this patch changes open(file) to
'with open(file) as...:' block. It allows us to ensure that every
resource is cleaned up when the code finishes running.

Change-Id: I4a85700061ed5050f4356f1b39ea128931626c83
Closes-Bug: #1367172"
348,2e4837971ff512342fb47d3b43a57b67d0ab36d3,1405805717,,1.0,275,266,10,2,1,0.792057173,False,,,,,True,,,,,,,,,,,,,,,,,1088,1538,1346496,cinder,2e4837971ff512342fb47d3b43a57b67d0ab36d3,1,1,Wrong logic?,"Bug #1346496 in Cinder: ""The Cinder Windows storage driver must not require Hyper-V""","The Cinder storage driver is currently depending on Hyper-V WMI features for VHD conversions.
This must be urgently removed and replaced with Win32 calls available on any supported Windows SKU as otherwise CI tests cannot be performed and the Windows Storage Server edition cannot be supported.","Remove Hyper-V dependency in the Windows Cinder Volume Driver

Currently, the Windows Cinder Volume driver depends on the Hyper-V
feature in order to be able to access the WMI namespaces that
are used for some of the volume related operations.

Those WMI calls must be urgently replaced with Win32 calls in order
to provide CI tests performed on Windows Storage Server edition.

Change-Id: Ib4b8bccee3c7ee86e6cc44e1b16d20d455e61612
Closes-Bug: #1346496"
349,2e6b2404155156ca336dadeacc8874645ca07bfc,1404184655,,1.0,24,4,2,2,1,0.491237342,False,,,,,True,,,,,,,,,,,,,,,,,1009,1451,1334278,nova,2e6b2404155156ca336dadeacc8874645ca07bfc,1,1,,"Bug #1334278 in OpenStack Compute (nova): ""limits with tenant parameter returns wrong maxTotal* values""","When querying for the ""absolute limits"" of a specific tenant
the maxTotal* values reported aren't correct.
How to reproduce:
for example using devstack...
OS_TENANT_NAME=demo (11b2b129994844798c98f437d9809a9c)
OS_USERNAME=demo
$nova absolute-limits
+-------------------------+-------+
| Name                    | Value |
+-------------------------+-------+
| maxServerMeta           | 128   |
| maxPersonality          | 5     |
| maxImageMeta            | 128   |
| maxPersonalitySize      | 10240 |
| maxTotalRAMSize         | 1000  |
| maxSecurityGroupRules   | 20    |
| maxTotalKeypairs        | 100   |
| totalRAMUsed            | 128   |
| maxSecurityGroups       | 10    |
| totalFloatingIpsUsed    | 0     |
| totalInstancesUsed      | 2     |
| totalSecurityGroupsUsed | 1     |
| maxTotalFloatingIps     | 10    |
| maxTotalInstances       | 10    |    <-----------------------
| totalCoresUsed          | 2     |
| maxTotalCores           | 10    |   <-----------------------
+-------------------------+-------+
OS_TENANT_NAME=admin (b0f08277004b43aab516ae7dbf36ff51)
OS_USERNAME=admin
$nova absolute-limits
+-------------------------+--------+
| Name                    | Value  |
+-------------------------+--------+
| maxServerMeta           | 128    |
| maxPersonality          | 5      |
| maxImageMeta            | 128    |
| maxPersonalitySize      | 10240  |
| maxTotalRAMSize         | 151200 |
| maxSecurityGroupRules   | 20     |
| maxTotalKeypairs        | 100    |
| totalRAMUsed            | 1152   |
| maxSecurityGroups       | 10     |
| totalFloatingIpsUsed    | 0      |
| totalInstancesUsed      | 18     |
| totalSecurityGroupsUsed | 1      |
| maxTotalFloatingIps     | 10     |
| maxTotalInstances       | 30     |
| totalCoresUsed          | 18     |
| maxTotalCores           | 30     |
+-------------------------+--------+
$nova absolute-limits --tenant 11b2b129994844798c98f437d9809a9c
+-------------------------+--------+
| Name                    | Value  |
+-------------------------+--------+
| maxServerMeta           | 128    |
| maxPersonality          | 5      |
| maxImageMeta            | 128    |
| maxPersonalitySize      | 10240  |
| maxTotalRAMSize         | 151200 |
| maxSecurityGroupRules   | 20     |
| maxTotalKeypairs        | 100    |
| totalRAMUsed            | 128    |
| maxSecurityGroups       | 10     |
| totalFloatingIpsUsed    | 0      |
| totalInstancesUsed      | 2      |
| totalSecurityGroupsUsed | 1      |
| maxTotalFloatingIps     | 10     |
| maxTotalInstances       | 30     |     <-------------------
| totalCoresUsed          | 2      |
| maxTotalCores           | 30     |    <-------------------
+-------------------------+--------+
note: arrows show the wrong values.
Seems that maxTotal* shows the values for the current tenant and not what is specified by ""--tenant""
as expected.
tested in havana and icehouse-1","API: Enable support for tenant option in nova absolute-limits

When querying for the absolute limits of a specific tenant,
the tenant option is ignored. There are no attempts to extract
the tenant from the request. Instead, nova uses context.project_id
as the project_id in QUOTAS.get_project_quotas. The following
patch extracts the tenant_id from the request (if any) and passes
that to QUOTAS.get_project_quotas to obtain the proper quota.

Change-Id: If5f91de020ed8a40fa04fc001c7c4c92681f4ad1
Closes-Bug: #1334278"
350,2e758452ddcb8871f7dbc6c859e3414923cd18bc,1386134494,,1.0,64,10,4,4,1,0.876598996,True,7.0,2551782.0,36.0,10.0,False,44.0,124709.5,81.0,4.0,116.0,2274.0,2303.0,115.0,1959.0,1987.0,108.0,2168.0,2189.0,0.016086187,0.320100354,0.323199528,139,540,1257661,nova,2e758452ddcb8871f7dbc6c859e3414923cd18bc,0,0, should handle boolean string parameters,"Bug #1257661 in OpenStack Compute (nova): ""should handle boolean string parameters through migrate_live API""","If specifying false string (""False"") as ""block_migration"" parameter of migrate_live API like the following, nova considers it as True.
$ curl -i 'http://10.21.42.81:8774/v2/[..]/servers/[..]/action' -X POST [..] -d '{""os-migrateLive"": {""disk_over_commit"": ""False"", ""block_migration"": ""False"", ""host"": ""localhost""}}'
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency.","Add boolean convertors for migrate_live API

migrate_live API contains ""block_migration"" and ""disk_over_commit"" as
the parameters. They should be boolean.
There are the other boolean parameters such as ""on_shared_storage"" of
evacuate API, they can be passed with either boolean or string(e.g. ""True"").
For API consistency, this patch adds boolean convertors to migrate_live
API.

Change-Id: Ic3040811977c834bfd4b0b4527795382e11227ed
Closes-Bug: #1257661"
351,2e7de07c5a7c8f9d11c00499f7e85ac30f71d025,1409037778,1.0,,4845,3,26,10,1,0.859226874,False,,,,,True,,,,,,,,,,,,,,,,,1874,1367564,1367564,Glance,2e7de07c5a7c8f9d11c00499f7e85ac30f71d025,1,1,“should handle type specific prefix”. Is it a bug3,"Bug #1367564 in Glance: ""metadata definition property show should handle type specific prefix""","metadata definition property show should handle type specific prefix
The metadata definitions API supports listing namespaces by resource type.  For example, you can list only namespaces applicable to images by specifying OS::Glance::Image
The API also support showing namespace properties for a specific resource type.  The API will automatically prepend any prefix specific to that resource type.  For example, in the OS::Compute::VirtCPUTopology namespace, the properties will come back with ""hw_"" prepended.
However, if you then ask the API to show the property with the prefix, it will return a ""not found"" error.   To actually see the details of the property, you have to know the base property (without the prefix).  It would be nice if the API would attempt to auto-resolve any automatically prefixed properties when showing a property.
This is evident from the command line.  If you look at the below interactions, you will see the namespaces listed, then limited to a particular resource type, then the properties shown for the namespace, and then a failure to show the property using the automatically prepended prefix.
* Apologize for formatting.
$ glance --os-image-api-version 2 md-namespace-list
+------------------------------------+
| namespace                          |
+------------------------------------+
| OS::Compute::VMware                |
| OS::Compute::XenAPI                |
| OS::Compute::Quota                 |
| OS::Compute::Libvirt               |
| OS::Compute::Hypervisor            |
| OS::Compute::Watchdog              |
| OS::Compute::HostCapabilities      |
| OS::Compute::Trust                 |
| OS::Compute::VirtCPUTopology       |
| OS::Glance:CommonImageProperties   |
| OS::Compute::RandomNumberGenerator |
+------------------------------------+
$ glance --os-image-api-version 2 md-namespace-list --resource-type OS::Glance::Image
+------------------------------+
| namespace                    |
+------------------------------+
| OS::Compute::VMware          |
| OS::Compute::XenAPI          |
| OS::Compute::Libvirt         |
| OS::Compute::Hypervisor      |
| OS::Compute::Watchdog        |
| OS::Compute::VirtCPUTopology |
+------------------------------+
$ glance --os-image-api-version 2 md-namespace-show OS::Compute::VirtCPUTopology --resource-type OS::Glance::Image
+----------------------------+----------------------------------------------------------------------------------+
| Property                   | Value                                                                            |
+----------------------------+----------------------------------------------------------------------------------+
| created_at                 | 2014-09-10T02:55:40Z                                                             |
| description                | This provides the preferred socket/core/thread counts for the virtual CPU        |
|                            | instance exposed to guests. This enables the ability to avoid hitting            |
|                            | limitations on vCPU topologies that OS vendors place on their products. See      |
|                            | also: http://git.openstack.org/cgit/openstack/nova-specs/tree/specs/juno/virt-   |
|                            | driver-vcpu-topology.rst                                                         |
| display_name               | Virtual CPU Topology                                                             |
| namespace                  | OS::Compute::VirtCPUTopology                                                     |
| owner                      | admin                                                                            |
| properties                 | [""hw_cpu_cores"", ""hw_cpu_sockets"", ""hw_cpu_maxsockets"", ""hw_cpu_threads"",        |
|                            | ""hw_cpu_maxcores"", ""hw_cpu_maxthreads""]                                          |
| protected                  | True                                                                             |
| resource_type_associations | [""OS::Glance::Image"", ""OS::Cinder::Volume"", ""OS::Nova::Flavor""]                  |
| schema                     | /v2/schemas/metadefs/namespace                                                   |
| visibility                 | public                                                                           |
+----------------------------+----------------------------------------------------------------------------------+
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology hw_cpu_cores
Request returned failure status 404.
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  Could not find property hw_cpu_cores<br /><br />
 </body>
</html> (HTTP 404)
ttripp@ubuntu:/opt/stack/glance$ glance --os-image-api-version 2 md-property-show OS::Compute::VirtCPUTopology cpu_cores
+-------------+---------------------------------------------------+
| Property    | Value                                             |
+-------------+---------------------------------------------------+
| description | Preferred number of cores to expose to the guest. |
| name        | cpu_cores                                         |
| title       | vCPU Cores                                        |
| type        | integer                                           |
+-------------+---------------------------------------------------+","Glance Metadata Definitions Catalog - API

Implements: blueprint metadata-schema-catalog

A common API hosted by the Glance service for vendors, admins,
services, and users to meaningfully define available key / value
pair and tag metadata. The intent is to enable better metadata
collaboration across artifacts, services, and projects for
OpenStack users.

This is about the definition of the available metadata that can
be used on different types of resources (images, artifacts,
volumes, flavors, aggregates, etc). A definition includes the
properties type, its key, it's description, and it's constraints.
This catalogue will not store the values for specific instance
properties.

 - REST API for CRUD on metadef namespace
 - REST API for CRUD on metadef objects
 - REST API for CRUD on metadef properites
 - REST API for CRUD on metadef resource types
 - REST API for JSON schemas on metadef API's

Change-Id: I8e6d88ffee9a9337bf82b1da85648ba638a154ab
DocImpact
Co-Authored-By: Lakshmi N Sampath <lakshmi.sampath@hp.com>
Co-Authored-By: Wayne Okuma <wayne.okuma@hp.com>
Co-Authored-By: Travis Tripp <travis.tripp@hp.com>
Co-Authored-By: Pawel Koniszewski <pawel.koniszewski@intel.com>
Co-Authored-By: Michal Jastrzebski <michal.jastrzebski@intel.com>
Co-Authored-By: Michal Dulko <michal.dulko@intel.com>"
352,2e8bc44c9d0586813587a5711f8e857a31393326,1395097246,,1.0,87,12,2,2,2,0.815256077,True,2.0,245555.0,16.0,4.0,False,29.0,2464166.0,66.0,3.0,208.0,584.0,682.0,170.0,504.0,567.0,150.0,578.0,618.0,0.161497326,0.619251337,0.662032086,1835,1292784,1292784,Swift,2e8bc44c9d0586813587a5711f8e857a31393326,1,1,"""There was a path on container recreate that would sometimes allow db to get reinitialized without updating put_timestamp""","Bug #1292784 in OpenStack Object Storage (swift): ""race with container recreate""","So I always hated the way that the status field worked on our db's - but when looking at it I thought I spotted a race, and now I'm pretty sure it's there.  Can someone check out this test case (redbo, put on your concurrency hat).
Basically if you PUT on a container if the file on disk already exists it will update the put_timestamp, if not it goes into initialize which *may* raise AlreadyExists - but in the second case we don't update put_timestamp.
Replication was fixing it eventually...","Fix race on container recreate

There was a path on container recreate that would sometimes allow db to get
reinitialized without updating put_timestamp.  Replication would of course fix
it up, but that node would think it's database was deleted till then desipite
just ok'ing a request with a newer X-Timestamp than the deleted_timestamp on
disk.

Change-Id: I8b98afb2aac2e433b6ecb5c421ba0d778cef42fa
Closes-Bug: #1292784"
353,2e90d62727c093efbf60a7d86b1ee5262b041bbb,1409216282,,1.0,0,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1239,1698,1362129,nova,2e90d62727c093efbf60a7d86b1ee5262b041bbb,1,1,,"Bug #1362129 in OpenStack Compute (nova): ""For rbd image backend, disk IO rate limiting isn't supported""","when using rbd as disk backend.   images_type=rbd in nova.conf
disk IO tunning doesn't work as described https://wiki.openstack.org/wiki/InstanceResourceQuota","Fix wrong disk type limitation for disk IO throttling

The disk I/O limits feature in QEMU block layer allows I/O throttling
to be used across all underlying storage types.

Change-Id: If99107955fc976b00ee101d4c6a685c29e655f82
Closes-Bug: #1362129"
354,2e94076ca43ee3f31b1fc7f46b4c137d36bcd7db,1331852948,0.0,,50,17,6,3,1,0.801300549,True,2.0,318474.0,14.0,8.0,False,24.0,9530.5,84.0,2.0,0.0,569.0,569.0,0.0,569.0,569.0,0.0,189.0,189.0,0.004854369,0.922330097,0.922330097,1844,1317847,1317847,Glance,2e94076ca43ee3f31b1fc7f46b4c137d36bcd7db,1,1,"It depends when the translation policy enters, after the code or before3 Since I do not have the info I would say it is a BIC","Bug #1317847 in Glance: ""debug level logs should not be translated""","According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation.","Disambiguates HTTP 401 and HTTP 403 in Glance. Fixes bug 956513.

Change-Id: I82865293f60eabaf3bc40b40dc9c8612b12a6d1b"
355,2ea6eadfd574770f19c817f9451ab4e7ad6d77b9,1392838659,,1.0,3,3,1,1,1,0.0,True,1.0,352037.0,22.0,5.0,False,6.0,944.0,8.0,4.0,475.0,1811.0,2027.0,398.0,1408.0,1579.0,218.0,641.0,706.0,0.262589928,0.769784173,0.847721823,441,856,1282401,neutron,2ea6eadfd574770f19c817f9451ab4e7ad6d77b9,0,0,tests,"Bug #1282401 in neutron: ""API UT: should not use expected_errors=False when expecting success HTTP response""","In neutron/tests/unit/test_api_v2_resource.py,
there are tests which expects HTTP success response but expect_errors=True is specified.
There are no direct negative impact now, but it is better to specify expect_errors=False (default value) to avoid a confusion for test authors.","Should specify expect_errors=False for success response

In API unit tests, expect_errors=False should be specified
when expecting 2xx API response.

Closes-Bug: #1282401
Change-Id: Ib2d57aab16f73f234b3cb6ecfeecf2779bf17223"
356,2eca65eb5febae59eaca9e949ecd6a6f9492350a,1385763827,,1.0,55,36,10,8,1,0.831625209,True,10.0,3757079.0,53.0,21.0,False,52.0,419183.0,274.0,4.0,3.0,630.0,633.0,3.0,475.0,478.0,1.0,487.0,488.0,0.001932367,0.471497585,0.472463768,132,532,1257282,glance,2eca65eb5febae59eaca9e949ecd6a6f9492350a,1,0,Issue of version,"Bug #1257282 in Glance: ""Bump hacking to 0.8""","Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues.","Bump hacking to 0.8 and get python 3.x compatibility

Bump hacking dependency to 0.8 to get python 3.x compatibility
Fixes done in order to avoid errors after enabling hacking 0.8

Change-Id: Ic878fe2e1bd3f65f7f95a9b5c7a192dac81b749d
Closes-Bug: #1257282"
357,2ed2f3aff8eee70e02eb995780af292057cfcad4,1384178403,,1.0,17,2,3,2,1,0.982630164,True,10.0,11807686.0,51.0,24.0,False,11.0,441549.0,12.0,6.0,16.0,1359.0,1364.0,16.0,1204.0,1209.0,11.0,1115.0,1115.0,0.010452962,0.972125436,0.972125436,1744,1249971,1249971,cinder,2ed2f3aff8eee70e02eb995780af292057cfcad4,1,1, ,"Bug #1249971 in Cinder: ""update quota sets API does not handle all fail cases""","The update quota sets API should check whether 'quota_set' is in body.
The code to  update quota sets does the following:
for key in body['quota_set'].keys():","Update quota-set throw 500 error

The server doesn't check whether the parameter ""quota_set"" or
""quota_class_set"" is in request
body.So the 500 error has been thrown.

We should catch the KeyError and transfer the KeyError to
400(HTTPBadRequest) instead of 500.

Change-Id: I01260c77efa50324f3d203888689cdb1e94d2c21
Closes-Bug: #1249971"
358,2ed3e28442e7dd8ded485cdf96a2053386441051,1409946713,,1.0,44,7,5,4,1,0.887498987,False,,,,,True,,,,,,,,,,,,,,,,,1422,1168318,1168318,nova,2ed3e28442e7dd8ded485cdf96a2053386441051,1,1,"Wrong logic, “should not return empty QemuImgInfo objects”","Bug #1168318 in OpenStack Compute (nova): ""qemu_image_info should not return empty QemuImgInfo objects""","In nova/virt/images.py, if qemu-img command fails or the image is missing, instead of returning en empty QemuImgInfo it should probably throw some exception to inform the caller of the situation instead of hiding the problem like it does today.","Raise an exception if qemu-img fails

Raise an exception if the qemu image doesn't
exist or qemu-img fails.

Co-Authored-By: Chuck Short <chuck.short@canonical.com>
Closes-Bug: #1168318
Change-Id: I6b4123590e7d2934de0bc6add900d708d5986039"
359,2ee0d651b18a5840033f9338f3b605b5a64769f1,1412026867,,1.0,35,2,2,2,1,0.753197991,False,,,,,True,,,,,,,,,,,,,,,,,1366,1836,1375478,nova,2ee0d651b18a5840033f9338f3b605b5a64769f1,1,1,,"Bug #1375478 in OpenStack Compute (nova): ""image metadata not copied when bdm v2 source=snapshot used""","If an instance is booted using the block device mapping v2 API and source=snapshot is used, no image metadata will be copied into the instance system_metadata which can cause issues further in the boot process.  Since properties like os_type are missed which may be used by a virt driver.","Use image metadata from source volume of a snapshot

When booting an instance with the block device mapping V2 API and
source=snapshot, the image metadata is not retrieved and stored in
instance system_metadata.  Metadata is not exposed on a volume snapshot
so it is grabbed from the volume that the snapshot is from.

Change-Id: I96d6c2198b2f94470ab9dfee5e4bddb13992d8d5
Closes-bug: #1375478"
360,2f121680eba9404dc723872cde4d16dea90a0c3f,1405005967,,1.0,29,1,2,2,1,0.566509507,False,,,,,True,,,,,,,,,,,,,,,,,1067,1516,1342055,nova,2f121680eba9404dc723872cde4d16dea90a0c3f,1,1,,"Bug #1342055 in OpenStack Compute (nova): ""Suspending and restoring a rescued instance restores it to ACTIVE rather than RESCUED""","If you suspend a rescued instance, resume returns it to the ACTIVE state rather than the RESCUED state.","Store original state when suspending

We were losing the original state when suspending a rescued instance. This
patch ensures that a suspended rescued instance is restored to the rescued
state on resume.

Closes-Bug: #1342055
Change-Id: I89be60d65eddcfe561794c210b1057fdb157cf41"
361,2f3d774eb51eac9a370813362047a2cb3d124d86,1404398660,,1.0,44,6,5,3,1,0.937271886,False,,,,,True,,,,,,,,,,,,,,,,,1012,1454,1334651,nova,2f3d774eb51eac9a370813362047a2cb3d124d86,1,1,,"Bug #1334651 in OpenStack Compute (nova): ""Nova api service outputs error messages when SIGHUP signal is sent""","When SIGHUP signal is send to nova-api service, it stops all the nova-api processes and while restarting the nova-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
2014-06-24 15:52:55.185 CRITICAL nova [-] AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova Traceback (most recent call last):
2014-06-24 15:52:55.185 TRACE nova   File ""/usr/local/bin/nova-api"", line 10, in <module>
2014-06-24 15:52:55.185 TRACE nova     sys.exit(main())
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/cmd/api.py"", line 56, in main
2014-06-24 15:52:55.185 TRACE nova     launcher.launch_service(server, workers=server.workers or 1)
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 340, in launch_service
2014-06-24 15:52:55.185 TRACE nova     self._start_child(wrap)
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 324, in _start_child
2014-06-24 15:52:55.185 TRACE nova     launcher.restart()
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 145, in restart
2014-06-24 15:52:55.185 TRACE nova     self.services.restart()
2014-06-24 15:52:55.185 TRACE nova   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 478, in restart
2014-06-24 15:52:55.185 TRACE nova     restart_service.reset()
2014-06-24 15:52:55.185 TRACE nova AttributeError: 'WSGIService' object has no attribute 'reset'
2014-06-24 15:52:55.185 TRACE nova
Steps to reproduce:
1. Run nova-api service as daemon.
2. Send SIGHUP signal to nova-api service
   kill -1 <parent_process_id_of_nova_api>","Nova-api service throws error when SIGHUP is sent

Added reset method in WSGIService class.

After adding reset method when SIGHUP signal is sent to
wsgi service parent process,then it sends SIGHUP signal
to all of its child processes. Each child process handles
SIGHUP signal by first stopping the service and then calls
service start method again. When it stops the service, it
kills the eventlet thread, which internally closes the wsgi
server socket object. This server socket object is now not
usable again and it throws following error, while restarting
the service:

error: [Errno 9] Bad file descriptor

To resolve 'Bad file descriptor' error, creating duplicate
socket object, every time service starts.

When the wsgi service is stopped, it sets the green pool
size to 0, so resizing the green pool to default value,
on service restart.

Closes-Bug: #1334651

Change-Id: If1035deaf8b31f2712d88f0112fdd2e9e9dc7cb0"
362,2f49ed4b5dbb5c954fc7a9b42ee7b170c38c775c,1385638673,2.0,1.0,84,11,5,3,1,0.883395216,True,12.0,9546055.0,81.0,38.0,False,138.0,120051.8,381.0,6.0,49.0,3656.0,3659.0,49.0,3174.0,3177.0,47.0,2710.0,2711.0,0.007129066,0.402643695,0.402792217,95,494,1255609,nova,2f49ed4b5dbb5c954fc7a9b42ee7b170c38c775c,1,1,Race condition,"Bug #1255609 in OpenStack Compute (nova): ""VMware: possible collision of VNC ports""","We assign VNC ports to VM instances with the following method:
def _get_vnc_port(vm_ref):
    """"""Return VNC port for an VM.""""""
    vm_id = int(vm_ref.value.replace('vm-', ''))
    port = CONF.vmware.vnc_port + vm_id % CONF.vmware.vnc_port_total
    return port
the vm_id is a simple counter in vSphere which increments fast and there is a chance to get the same port number if the vm_ids are equal modulo vnc_port_total (10000 by default).
A report was received that if the port number is reused you may get access to the VNC console of another tenant. We need to fix the implementation to always choose a port number which is not taken or report an error if there are no free ports available.","VMware: fix the VNC port allocation

There is small chance for VNC port collisions with the current
implementation which choose the port number based on the MoRef id
of the VM.
This patch fixes this by running a query for all allocated ports
and then selects one which is not taken.

Change-Id: If7c3b14dd49ed05c5fde819c5a36d5608650cbbc
Closes-Bug: #1255609"
363,2f5360753308eb8b10581fc3c026c1b66f42ebdc,1344706311,9.0,,1176,37,20,11,1,0.732956679,True,7.0,3562905.0,40.0,21.0,False,16.0,253021.25,36.0,6.0,14.0,985.0,988.0,14.0,985.0,988.0,4.0,67.0,67.0,0.069444444,0.944444444,0.944444444,1829,1286809,1286809,Cinder ,2f5360753308eb8b10581fc3c026c1b66f42ebdc,0,0,Removing code is not a bug,,Error retrieving bug report 1286809 for project Cinder : 404 Not Found,Adds new volume API extensions\n\nAdds following extensions:\n1. Create volume from image\n2. Copy volume to image\n\nAdded unit tests.\n\nImplements: blueprint create-volume-from-image\nChange-Id: I9c73bd3fa2fa2e0648c01ff3f4fc66f757d7bc3f
364,2f62736e47367404ac56525689412d123d69c283,1383740377,,1.0,5,3,1,1,1,0.0,True,1.0,645032.0,10.0,6.0,False,20.0,770893.0,41.0,6.0,71.0,578.0,637.0,71.0,554.0,613.0,5.0,503.0,505.0,0.005249344,0.440944882,0.442694663,59,387,1248216,cinder,2f62736e47367404ac56525689412d123d69c283,0,0,Remove deprecated command in tests,"Bug #1248216 in Cinder: ""Need remove deprecated module commands""","The commands module was deprecated since version 2.6 and it has been
removed in Python 3. Use the subprocess module instead.
See http://docs.python.org/2/library/commands#module-commands","Don't use deprecated module commands

The commands module was deprecated since version 2.6 and it has been
removed in Python 3. Use the subprocess module instead.
See http://docs.python.org/2/library/commands#module-commands

Closes-Bug: #1248216
Change-Id: I4091179c6b312ae8534b30adc518eff67d68320a"
365,2fb996642a597e7050ecc13ef6f3ad4aaae34997,1383816927,0.0,1.0,25,1,2,2,1,0.391243564,True,2.0,321170.0,29.0,9.0,False,9.0,1024926.0,10.0,5.0,148.0,683.0,754.0,148.0,621.0,692.0,107.0,256.0,301.0,0.205714286,0.48952381,0.575238095,60,388,1248219,neutron,2fb996642a597e7050ecc13ef6f3ad4aaae34997,1,1,,"Bug #1248219 in neutron: ""ExtraRoute_db_mixin._get_extra_routes_by_router_id always returns all routes""","Tonight I created a new router and attached an new subnet to it, then I tried to detach this subnet but failed. I debug the code and found ExtraRoute_db_mixin._get_extra_routes_by_router_id always returns all routes, because of the third line:
1.    def _get_extra_routes_by_router_id(self, context, id):
2.        query = context.session.query(RouterRoute)
3.        query.filter(RouterRoute.router_id == id)
4.        return self._make_extra_route_list(query)
   It should be:
          query = query.filter(RouterRoute.router_id == id)","ExtraRoute: fix _get_extra_routes_by_router_id()

router_id filter wasn't properly applied to the query
so the func returned all extra routes of all routers.

Closes-Bug: #1248219
Change-Id: I77733ee7994c8fccbbc966ccabd83b8fe4def8d5"
366,2fc92d9bd7b4e212a3f28e57565a4cf260982f30,1410861405,,1.0,738,433,2,2,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1876,1369516,1369516,Nova,2fc92d9bd7b4e212a3f28e57565a4cf260982f30,0,0,"Bug in test files, but due to evolution.","Bug #1369516 in OpenStack Compute (nova): ""Convert libvirt driver test suites to use NoDBTestCase ""","A large number of libvirt test classes inherit from the TestCase class, which means they incur the overhead of database setup
nova/tests/virt/libvirt/test_blockinfo.py:class LibvirtBlockInfoTest(test.TestCase):
nova/tests/virt/libvirt/test_blockinfo.py:class DefaultDeviceNamesTestCase(test.TestCase):
nova/tests/virt/libvirt/test_dmcrypt.py:class LibvirtDmcryptTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class CacheConcurrencyTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtConnTestCase(test.TestCase,
nova/tests/virt/libvirt/test_driver.py:class HostStateTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class IptablesFirewallTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class NWFilterTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtUtilsTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtDriverTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeUsageTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtNonblockingTestCase(test.TestCase):
nova/tests/virt/libvirt/test_driver.py:class LibvirtVolumeSnapshotTestCase(test.TestCase):
nova/tests/virt/libvirt/test_imagebackend.py:class EncryptedLvmTestCase(_ImageTestCase, test.TestCase):
nova/tests/virt/libvirt/test_vif.py:class LibvirtVifTestCase(test.TestCase):
Some of these do not even use the database so can be trivially changed. Others will need significant refactoring work to remove database access before they can be changed to NoDBTestCase","libvirt: convert conn test case to avoid DB usage

The LibvirtConnTestCase uses the database for alot of random
stuff. With a bunch more mocking, it can be converted to use
NoDBTestCase which improves perf from 50s to 9s.

Updates the comment in _get_cpu_numa_config_from_instance to
be explicit about which test cases remain to be converted
after this change.

Closes-bug: #1369516
Change-Id: I1b585cba39913ff4609c6d86dcafc07e6c04e2b8"
367,2fee082e9267c75a1e676361d975e52ac9ad86f8,1404307333,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1035,1479,1336767,nova,2fee082e9267c75a1e676361d975e52ac9ad86f8,1,1,,"Bug #1336767 in OpenStack Compute (nova): ""Instance disappeared during wait for destroy should be a warning not an error""","Currently Nova logs an error if a libvirt domain disappears during while waiting for it to be destroyed, but the code actually treats this (correctly) as a recoverable situation since the end result is the required one.  Hence this should be logged as a warning not  an error.
This may help wit some of the gate failures:  https://bugs.launchpad.net/nova/+bug/1300279","Change instance disappeared during destroy from Error to Warning

Currently Nova logs an error if a libvirt domain disappears during
while waiting for it to be destroyed, but the code actually treats
this (correctly) as a recoverable situation since the end result
is the required one. Hence this should be logged as a warning not
an error.

Change-Id: Ie72cd11e22c5f049d3263e58bec286914798147a
Closes-bug: #1336767"
368,3001bc67edacb89a852b06a7d840da41f42e0e5b,1385451629,,1.0,20,6,4,4,1,0.889674919,True,4.0,3544770.0,34.0,17.0,False,43.0,13731.0,74.0,9.0,10.0,3448.0,3456.0,10.0,2990.0,2998.0,2.0,2302.0,2302.0,0.000447962,0.343885322,0.343885322,82,479,1254988,nova,3001bc67edacb89a852b06a7d840da41f42e0e5b,0,0,removes redundant conde,"Bug #1254988 in OpenStack Compute (nova): ""The body validation is redundant for createBackup""","The body validation is redundant for createBackup, should remove the redundant KeyError for 'createBackup'.
 In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action.","Remove redundant body validation for createBackup

The body validation is redundant for createBackup, should remove it. In
fact that this extension is registered as WSGI action implies that the
body must contain a key with the same name as action.

Change-Id: I327688b2df50782bc38b97a4750f83a607623ce4
Closes-Bug: #1254988"
369,3012eab2f7e70796ddc06d211dbc76bcf62c3736,1397000962,,1.0,40,4,2,2,1,0.998509099,True,13.0,3011135.0,115.0,30.0,False,165.0,79215.5,513.0,4.0,520.0,1702.0,2136.0,297.0,1626.0,1849.0,463.0,1655.0,2037.0,0.059747618,0.213237188,0.262425959,764,1191,1304721,nova,3012eab2f7e70796ddc06d211dbc76bcf62c3736,1,1,,"Bug #1304721 in OpenStack Compute (nova): ""virNodeListDevices may not be supported""","If libvirt wasn't compiled with the appropriate flags, listDevices may return NOT_SUPPORTED.
This is currently not gracefully handled, which prevents the compute node from starting up.
http://paste.openstack.org/show/75375/
A better fix is to handle the error, and issue a warning when we detect this case.","libvirt: Handle `listDevices` unsupported exception

It's possible that `listDevices` will return an exception if it's not
currently supported.

If this happens, the compute service will fail to start up.

The proposed solution is to catch this error, log a warning, and allow
the compute-service to continue initialization.

Change-Id: If152abd1080e9ba395de5d8ea10f5e0c476d4e2b
Closes-Bug: #1304721"
370,3082a074f819f0479fc941b30dc7cfdbb8aab394,1407842004,,1.0,7,10,2,2,1,0.522559375,False,,,,,True,,,,,,,,,,,,,,,,,1172,1629,1355251,neutron,3082a074f819f0479fc941b30dc7cfdbb8aab394,0,0,Bug in test,"Bug #1355251 in neutron: ""db plugin sort testing function should not use sorted() on assertion""","tests/unit/test_db_plugin.py line 585
(https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_db_plugin.py#L585)
sorted() function is used within assertEqual() function.
This breaks tests objective which is to test sorting
Every unit test using this function (_test_list_with_sort) for sorting tests will always succeed.
sorted() should not be used for proper test results","Removing sorted() function from assertEqual()

Removing unnecessary sorted() function from assertEqual() function
calls in functions used for sorting and pagination testing

Using sorted() breaks sorting tests objective which is to test sorting.
This is causing every unit test using this functions
(_test_list_with_sort, _test_list_with_pagination, _test_list_with_pagination_reverse)
for sorting tests to always succeed

Switched the parameters for assertEqual() function within the fixed code to meet
its signature - assertEqual(self, expected, observed, message='')

Test neutron.tests.unit.vmware.vshield.test_lbaas_plugin.TestLoadbalancerPlugin.test_list_vips
was succeeding because of this bug, although it should fail. It failed after the bug was fix.
Test was fixed.

Change-Id: Ia36b81d9a7fd32cae3b567ac1899b56481eb62ce
Closes-Bug: #1355251"
371,30871e8702737edbbfbcbbb5f21858873b37685c,1411990430,,1.0,25,50,2,2,1,0.300391417,False,,,,,True,,,,,,,,,,,,,,,,,1356,1825,1373993,nova,30871e8702737edbbfbcbbb5f21858873b37685c,1,0,Library “This should be changed to use the requests lib.”,"Bug #1373993 in OpenStack Compute (nova): ""Trusted Filter uses unsafe SSL connection""","HTTPSClientAuthConnection uses httplib.HTTPSConnection objects. In Python 2.x those do not perform CA checks so client connections are vulnerable to MiM attacks.
This should be changed to use the requests lib.","Fix unsafe SSL connection on TrustedFilter

TrustedFilter was using httplib which doesn't check for CAs.
Here the change is using Requests and verifies local CAs by default (or another
one if provided)
This effort is related to CVE 2013-2255.

SecurityImpact

Closes-Bug: #1373993

Change-Id: I0b8e6319a4cc39876b1e396ef705f0fc5def1e44"
372,30e5dae520535fbd759f4c12e184970afef854de,1397638636,,1.0,1,1,1,1,1,0.0,True,1.0,1131011.0,48.0,11.0,False,3.0,3741296.0,4.0,7.0,0.0,2960.0,2960.0,0.0,2555.0,2555.0,0.0,972.0,972.0,0.000874891,0.851268591,0.851268591,800,1228,1308421,neutron,30e5dae520535fbd759f4c12e184970afef854de,1,1,copy & paste error,"Bug #1308421 in neutron: ""local variable 'physical_network' referenced before assignment""","In my case, triggered by running neutron-usage-audit. Fix underway.
Traceback (most recent call last):
  File ""~/bin/neutron-usage-audit"", line 10, in <module>
    sys.exit(main())
  File ""~/neutron/cmd/usage_audit.py"", line 37, in main
    plugin = manager.NeutronManager.get_plugin()
  File ""~/neutron/manager.py"", line 211, in get_plugin
    return cls.get_instance().plugin
  File ""~/neutron/manager.py"", line 206, in get_instance
    cls._create_instance()
  File ""~/neutron/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""~/neutron/manager.py"", line 200, in _create_instance
    cls._instance = cls()
  File ""~/neutron/manager.py"", line 112, in __init__
    plugin_provider)
  File ""~/neutron/manager.py"", line 140, in _get_plugin_instance
    return plugin_class()
  File ""~/neutron/plugins/linuxbridge/lb_neutron_plugin.py"", line 263, in __init__
    db.sync_network_states(self.network_vlan_ranges)
  File ""~/neutron/plugins/linuxbridge/db/l2network_db_v2.py"", line 87, in sync_network_states
    'physical_network': physical_network})
UnboundLocalError: local variable 'physical_network' referenced before assignment","Fix uninitialized variable reference

This was probably a copy&paste error. While physical_network is used in
earlier loops, it isn't defined here. The identically named member variable was
probably intended.

Change-Id: I09e38321cf8670c62f4fdf9d99da1df737b2fc68
Closes-Bug: #1308421"
373,30f73de399d9490f8f23d493c3cc20813ef71481,1391550138,0.0,1.0,8,2,2,2,1,0.721928095,True,2.0,22600.0,14.0,6.0,False,37.0,759850.0,90.0,4.0,51.0,1168.0,1201.0,51.0,1124.0,1157.0,38.0,1126.0,1147.0,0.005374122,0.155298333,0.15819209,398,812,1279572,nova,30f73de399d9490f8f23d493c3cc20813ef71481,1,1,,"Bug #1279572 in OpenStack Compute (nova): ""DBError in nova-compute with baremetal driver after stats table change""","As of the following commit: https://github.com/openstack/nova/commit/8a7b95dccdbe449d5235868781b30edebd34bacd our nova-compute service on the seed node is throwing DBErrors.  If I reset my Nova tree to the previous commit and downgrade the database to 232 then I am able to use nova successfully again.  With that commit all boots fail with a No hosts found message, presumably related to the following messages in the nova-compute log:
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/hubs/hub.py"", line 187, in switch
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/eventlet/greenthread.py"", line 194, in main
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     service.start()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/service.py"", line 193, in start
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 835, in pre_start_hook
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/manager.py"", line 5121, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 353, in update_available_resource
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self._sync_compute_node(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 384, in _sync_compute_node
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     self._update(context, resources)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/compute/resource_tracker.py"", line 456, in _update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     context, self.compute_node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/api.py"", line 241, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self._manager.compute_node_update(context, node, values)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/rpcapi.py"", line 364, in compute_node_update
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     prune_stats=prune_stats)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     wait_for_reply=True, timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/transport.py"", line 87, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     timeout=timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 390, in send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     return self._send(target, ctxt, message, wait_for_reply, timeout)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 383, in _send
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup     raise result
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup RemoteError: Remote error: DBError (ProgrammingError) (1064, 'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \': ""\'amd64\'"", u\'baremetal_driver\': ""\'nova.virt.baremetal.pxe.PXE\'""} WHERE compute\' at line 1') 'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u'cpu_arch': u'amd64', u'baremetal_driver': u'nova.virt.baremetal.pxe.PXE'}, 1L)
Feb 12 23:08:24 localhost nova-compute: 2014-02-12 23:08:23.980 4572 TRACE nova.openstack.common.threadgroup [u'Traceback (most recent call last):\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/_executors/base.py"", line 36, in _dispatch\n    incoming.reply(self.callback(incoming.ctxt, incoming.message))\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__\n    return self._dispatch(endpoint, method, ctxt, args)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/conductor/manager.py"", line 458, in compute_node_update\n    result = self.db.compute_node_update(context, node[\'id\'], values)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/api.py"", line 228, in compute_node_update\n    return IMPL.compute_node_update(context, compute_id, values)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 110, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 166, in wrapped\n    return f(*args, **kwargs)\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 614, in compute_node_update\n    compute_ref.update(values)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 447, in __exit__\n    self.rollback()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 58, in __exit__\n    compat.reraise(exc_type, exc_value, exc_tb)\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 444, in __exit__\n    self.commit()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalchemy/orm/session.py"", line 354, in commit\n    self._prepare_impl()\n', u'  File ""/usr/lib64/python2.7/site-packages/sqlalche
Feb 12 23:08:24 localhost nova-compute: my/orm/session.py"", line 334, in _prepare_impl\n    self.session.flush()\n', u'  File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (ProgrammingError) (1064, \'You have an error in your SQL syntax; check the manual that corresponds to your MariaDB server version for the right syntax to use near \\\': ""\\\'amd64\\\'"", u\\\'baremetal_driver\\\': ""\\\'nova.virt.baremetal.pxe.PXE\\\'""} WHERE compute\\\' at line 1\') \'UPDATE compute_nodes SET updated_at=%s, vcpus_used=%s, memory_mb_used=%s, local_gb_used=%s, free_ram_mb=%s, free_disk_gb=%s, current_workload=%s, running_vms=%s, stats=%s WHERE compute_nodes.id = %s\' (datetime.datetime(2014, 2, 12, 23, 8, 23, 932601), 0, 0, 0, 4096, 20, 0, 0, {u\'cpu_arch\': u\'amd64\', u\'baremetal_driver\': u\'nova.virt.baremetal.pxe.PXE\'}, 1L)\n'].","Fix baremetal stats type

As of 8a7b95dccdbe449d5235868781b30edebd34bacd the resource
tracker code is expecting stats to be a json string.  Baremetal
was setting it to a dict and it was breaking the db write.

Change-Id: I708fecd623c1a42d002c2b20bfdf06b625c9c287
Closes-Bug: #1279572"
374,310231541661adaa30d92d357f7590c88dc905b3,1400789903,1.0,1.0,65,2,8,8,1,0.773907103,False,,,,,True,,,,,,,,,,,,,,,,,900,1336,1321334,nova,310231541661adaa30d92d357f7590c88dc905b3,1,1,,"Bug #1321334 in OpenStack Compute (nova): ""Non bootable volumes can be used as a bootable source""","The following command results in a successful vm creation:
nova boot --boot-volume <non-bootable-volume-id> --flavor <test-flavor> <test-name>
we should validate that the --boot-volume passed is actually a bootable volume and if it's not we should return an error explaining what went wrong","Check if volume is bootable when creating an instance

Cinder provides a bootable flags for volumes that's not being
checked when creating an instance.

If a user select a non-bootable volume, then the VM became unusable
since nothing will be booted at all.

This check is done only in case that a volume is selected as root
device for a VM.  Boot from snapshot will still work (even that the
volume will be marked as non-bootable by cinder, but will boot the
attached snapshot instead)
If the attribute bootable is not present, we keep the old behaviour
and allow the instance creation.

Change-Id: I8e028d40d6a11beeac4f043afccb17eb61f6ce62
Closes-Bug: #1321334"
375,3146837b7aea1a6c767131ad9aebaac2acb765df,1409664154,,1.0,31,31,19,16,1,0.942169581,False,,,,,True,,,,,,,,,,,,,,,,,1256,1716,1364392,neutron,3146837b7aea1a6c767131ad9aebaac2acb765df,1,1,"“Missing constructor when
“","Bug #1364392 in neutron: ""Missing constructor when raising NoNetworkAvailable""","code in type_tunnel.py type_vlan.py incorrectly throws exception class instead of exception class instance.
Some other places need to be fixed as well.","Throw exception instances instead of classes

Prefer single style of throwing instances and not classes.

Change-Id: If159afcd092de5381309cbe217d64145ed5b45b4
Closes-Bug: #1364392"
376,31569827c749dca6e0affd4646360b839f321758,1398859257,,1.0,72,1,2,2,1,0.847107491,True,2.0,367576.0,9.0,3.0,True,,,,,,,,,,,,,,,,,687,1113,1298042,cinder,31569827c749dca6e0affd4646360b839f321758,1,1,Bug. Wrong state,"Bug #1298042 in Cinder: ""Image stuck in ""Queued"" state on upload to image failure""","When uploading a volume to an image with an unsupported disk type (anything other than vmdk), an orphan image stuck in ""Queued"" state gets left behind.
Steps to reproduce:
1. cinder create 1
2. cinder upload-to-image --disk raw <volume id> <name>
The result is an orphan image in Glance that gets left behind. This image should not exist.","Delete image on upload-to-image failure

On upload-to-image failure before initiating the data transfer or during
data transfer, the source volume status is restored properly whereas the
image created remains in queued or saving state. This change deletes the
image during such failures.

Change-Id: I0aa64798d2bc5bf19b79dd3b88dcd107ff369c42
Closes-Bug: #1298042"
377,3190e5b672ac21f9c82110d319473f2041bf64f7,1402090161,,1.0,30,5,3,2,1,0.916006941,False,,,,,True,,,,,,,,,,,,,,,,,889,1325,1319892,neutron,3190e5b672ac21f9c82110d319473f2041bf64f7,1,1,,"Bug #1319892 in neutron: ""Change of default netpartition behavior in nuage plugin""","Current behavior of nuage plugin assumes a clean slate on the VSD (back-end controller) as part of the neutron startup. There are use-cases where this assumption is invalid. Also, there could exist non default net-partitions on the VSD for which behavior is not the same as for default net-partition. This needs to be fixed.","Change default netpartition behavior in nuage plugin

Allows the nuage plugin handle the case where default netpartition
already exists on the VSD (back-end controller)

Change-Id: Ia5785c1c313cf30b8bad6cda1d0e9819784e06d8
Closes-Bug: #1319892"
378,31bcc97214358482e9ccc035189ef70ced1afc21,1391166519,,1.0,62,33,1,1,1,0.0,True,1.0,5493175.0,12.0,4.0,False,13.0,1451526.0,13.0,5.0,201.0,950.0,957.0,200.0,763.0,770.0,169.0,610.0,610.0,0.149516271,0.537379068,0.537379068,402,816,1279753,glance,31bcc97214358482e9ccc035189ef70ced1afc21,0,0,tests,"Bug #1279753 in Glance: ""The race condition caused by image created_at break registry v2 api test cases""","For now there are two cases I got:
1. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_status_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/2b0dea1/console.html#_2014-02-13_06_27_19_371
2. glance.tests.unit.v2.test_registry_api.TestRegistryRPC.test_get_index_sort_default_created_at_desc
http://logs.openstack.org/79/67079/3/gate/gate-glance-python27/348ad19/console.html#_2014-02-11_18_42_13_276
It is the same reason as https://bugs.launchpad.net/glance/+bug/1272136 fixed.","Provide explicit image create value in Registry v2 API test

Assign an explicit created_at datetime value to image db fixtures,
it be used to fixes race condition in Registry v2 API test cases.

Closes-Bug: #1279753
Change-Id: Ia1957e8c024f81eeaa3c0bc094dd62db1be52d07
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
379,31d67cfdee5b9f1ed77df2c9cda1c849fd7d6181,1405633918,3.0,1.0,319,332,5,3,1,0.82543611,False,,,,,True,,,,,,,,,,,,,,,,,1046,1490,1338853,neutron,31d67cfdee5b9f1ed77df2c9cda1c849fd7d6181,1,1,,"Bug #1338853 in neutron: ""Remove tables that store the mapping between neutron and Nuage VSD resources""","Nuage plugin stores a mapping of neutron and VSD id's for every neutron resource.
This bug is to remove the mapping to avoid storing redundant data and also avoid the
upgrade and out of sync issues.","Changes to remove the use of mapping tables from Nuage plugin

Nuage plugin maintains a mapping of Openstack and Nuage resources.
With this change Nuage VSD can itself maintain this mapping, so the
plugin doesn't have to store the information in the neutron DB.
This eliminates potential out of sync and upgrade issues.

Closes-Bug: #1338853

Change-Id: I03b32a83d792c742962e0195383a622c1967ae4f"
380,31def3ccf425a8f1a617a674a30d29f89d733432,1382076569,,2.0,24,17,4,3,1,0.818003111,True,4.0,526864.0,30.0,7.0,False,10.0,3764955.0,14.0,2.0,362.0,595.0,874.0,341.0,534.0,797.0,106.0,108.0,191.0,0.231101512,0.235421166,0.414686825,1599,1230083,1230083,neutron,31def3ccf425a8f1a617a674a30d29f89d733432,0,0,Bug in test files,"Bug #1230083 in neutron: ""OVS sg_rpc test does not load OVS plugin""","When I ran
    OS_DEBUG=1 python setup.py testr --testr-args='neutron.tests.unit.openvswitch.test_ovs_security_group'
I got the following result in testrepository log: http://paste.openstack.org/show/47468/
According to the log, neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin is loaded instead of OVS plugin. This means OVS plugin is not well tested and DB plugin with security group is tested again.
This comes from the behavior of Python multiple inheritance.
If a class (C1) inherits multiple classes (P1, P2) which have a common parent class (GP),
for example, C1.setUp() calls super(), P1.setUp is called first, then P2.setUp is called and finally GP.setUp is called.
The behavior is tricky and such usage should be avoided.
In Neutron tests multiple inheritance is used in many places.
I am afraid OVS plugin security group rcp test case is just one of them.","Ensure OVS plugin is loaded in OVS plugin test

In TestOpenvswitchSGServerRpcCallBack and NetworkBindingsTest
in OVS plugin tests, OVS plugin was not loaded properly.

* For NetworkBindingsTest, it can be fixed by changing setUp()
  to take 'plugin' arguments in the base test classes
  SecurityGroupDBTestCase and SGServerRpcCallBackMixinTestCase.
  This fixes bug 1242510 in ML2 unit tests.

* For NetworkBindingsTest, it can be fixed by passing the plugin
  class to super.setUp(). The test itself needs to be updated
  because network binding is added when a network is created.

* NetworkBindingsTest in Linux Bridge plugin has the same issue,
  so it is also fixed in this patch.

Closes-Bug: #1230083
Closes-Bug: #1242510
Change-Id: I914876225480585d822748c188e9b69d1adf46f3"
381,31def3ccf425a8f1a617a674a30d29f89d733432,1382076569,,2.0,24,17,4,3,1,0.818003111,True,4.0,526864.0,30.0,7.0,False,10.0,3764955.0,14.0,2.0,362.0,595.0,874.0,341.0,534.0,797.0,106.0,108.0,191.0,0.231101512,0.235421166,0.414686825,1717,1242510,1242510,neutron,31def3ccf425a8f1a617a674a30d29f89d733432,0,0,Bug in test files,"Bug #1242510 in neutron: ""ML2 plugin is not loaded properly in TestMl2SGServerRpcCallBack""","ML2 plugin is not loaded properly in TestMl2SGServerRpcCallBack.
By grepping a result in .testrepository, test_extension_security_group.SecurityGroupTestPlugin is loaded as a core plugin in ML2 test. It means Ml2 plugin is not tested in this case.
$ OS_DEBUG=1 python setup.py testr --slowest --testr-args='neutron.tests.unit.ml2'
$ grep ""Loading Plugin"" .testrepository/235 | cut -d ']' -f 2 | cut -d : -f 2 | sort | uniq -c
    835  neutron.plugins.ml2.plugin.Ml2Plugin
     35  neutron.services.l3_router.l3_router_plugin.L3RouterPlugin
     14  neutron.tests.unit.test_extension_security_group.SecurityGroupTestPlugin
    137  neutron.tests.unit.test_l3_plugin.TestL3NatServicePlugin","Ensure OVS plugin is loaded in OVS plugin test

In TestOpenvswitchSGServerRpcCallBack and NetworkBindingsTest
in OVS plugin tests, OVS plugin was not loaded properly.

* For NetworkBindingsTest, it can be fixed by changing setUp()
  to take 'plugin' arguments in the base test classes
  SecurityGroupDBTestCase and SGServerRpcCallBackMixinTestCase.
  This fixes bug 1242510 in ML2 unit tests.

* For NetworkBindingsTest, it can be fixed by passing the plugin
  class to super.setUp(). The test itself needs to be updated
  because network binding is added when a network is created.

* NetworkBindingsTest in Linux Bridge plugin has the same issue,
  so it is also fixed in this patch.

Closes-Bug: #1230083
Closes-Bug: #1242510
Change-Id: I914876225480585d822748c188e9b69d1adf46f3"
382,31f051a0349a40fe629a7bb5ce0ea683aa28a659,1394484178,,1.0,3,3,1,1,1,0.0,True,4.0,118158.0,73.0,11.0,False,11.0,586838.0,16.0,2.0,631.0,1529.0,1793.0,487.0,1312.0,1485.0,280.0,718.0,784.0,0.299573561,0.76652452,0.836886994,582,1003,1290549,neutron,31f051a0349a40fe629a7bb5ce0ea683aa28a659,0,0,Refactoring code,"Bug #1290549 in neutron: ""update floatingip status tracebacks""","Logs are showing plenty of tracebacks like the following: http://logs.openstack.org/96/66796/15/check/check-tempest-dsvm-neutron-full/d940e56/logs/screen-q-svc.txt.gz?level=DEBUG#_2014-03-06_10_14_46_352
An operation (either update_floatingip_status or get_floating_ips) in update_floatingip_statuses (l3_rpc_base.py) triggers this traceback. Apparently this happens because if a floatingIP is removed, the transaction is aborted because of the exception.
Optimizing as suggested here: https://github.com/openstack/neutron/blob/master/neutron/db/l3_db.py#L680 might solve the issue.","Optimize floating IP status update

Hopefully also avoid tracebacks due to nested transactions
being rolled back

Change-Id: I1e74fb8d90de09b53d330f499b93073f19a5b9bf
Closes-Bug: #1290549"
383,322cc9336fe6f6fe9b3f0da33c6b26a3e5ea9b0c,1389775351,1.0,1.0,202,44,3,3,1,0.488867965,False,,,,,True,,,,,,,,,,,,,,,,,1431,1187244,1187244,nova,322cc9336fe6f6fe9b3f0da33c6b26a3e5ea9b0c,1,1,“there is a NetworkDuplicated erro”,"Bug #1187244 in OpenStack Compute (nova): ""quantum NetworkDuplicated error ,when boot a vm with two port id which belongs to the same network . ""","in /var/log/nova/nova-api.log
```
013-06-04 12:01:17.998 ERROR nova.api.openstack [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] Caught error: Network 5332f0f7-3156-4961-aa67-0b8507265fa5 is duplicated.
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack Traceback (most recent call last):
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/__init__.py"", line 81, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return req.get_response(self.application)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/keystoneclient/middleware/auth_token.py"", line 450, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return self.app(env, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return resp(environ, start_response)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 890, in __call__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     content_type, body, accept)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 942, in _process_stack
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 1022, in dispatch
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/servers.py"", line 898, in create
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     scheduler_hints=scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/hooks.py"", line 85, in inner
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     rv = f(*args, **kwargs)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 962, in create
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     scheduler_hints=scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 676, in _create_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     reservation_id, scheduler_hints)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 634, in _validate_and_provision_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     QUOTAS.rollback(context, quota_reservations)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self.gen.next()
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 522, in _validate_and_provision_instance
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self._check_requested_networks(context, requested_networks)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 358, in _check_requested_networks
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     self.network_api.validate_networks(context, requested_networks)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/network/quantumv2/api.py"", line 454, in validate_networks
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack     raise exception.NetworkDuplicated(network_id=net_id)
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack NetworkDuplicated: Network 5332f0f7-3156-4961-aa67-0b8507265fa5 is duplicated.
2013-06-04 12:01:17.998 2741 TRACE nova.api.openstack
2013-06-04 12:01:18.126 INFO nova.api.openstack [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] http://172.16.136.111:8774/v2/b082fcb819db4104bb6d3dc18bcc4f17/os-volumes_boot returned with HTTP 500
2013-06-04 12:01:18.129 INFO nova.osapi_compute.wsgi.server [req-ea58ab37-aea6-40a4-b85f-2ab19285101e 5d45600dae844064b514f1549fe0dab9 b082fcb819db4104bb6d3dc18bcc4f17] 172.16.136.111 ""POST /v2/b082fcb819db4104bb6d3dc18bcc4f17/os-volumes_boot HTTP/1.1"" status: 500 len: 335 time: 0.4138958
```
how to reproduce this error
we need two subnets which belongs to one network
in my environment,
network id is 5332f0f7-3156-4961-aa67-0b8507265fa5
subnet1 id is e8a9be74-2f39-4d7e-9287-c5b85b573cca
subnet2 id is dca45033-e506-42e4-bf05-aaccd0591c55
1. create ports  from subnet1 and subnet2
quantum port-create --fixed-ip subnet_id=e8a9be74-2f39-4d7e-9287-c5b85b573cca  5332f0f7-3156-4961-aa67-0b8507265fa5
port id is :  dca1f741-a1fc-444c-b764-7aa74ed29d1f
quantum port-create --fixed-ip subnet_id=dca45033-e506-42e4-bf05-aaccd0591c55  5332f0f7-3156-4961-aa67-0b8507265fa5
port id is : 72acaba1-6e84-468b-b3f7-52eb89972eb7
2. boot with two port id
nova boot --flavor 1 --nic port-id=dca1f741-a1fc-444c-b764-7aa74ed29d1f  --nic port-id=72acaba1-6e84-468b-b3f7-52eb89972eb7 --block_device_mapping vda=1cf2a0b0-8c7f-4de1-8dba-91bae1f1856c:::0 m1
ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-ea58ab37-aea6-40a4-b85f-2ab19285101e)
it returns an error , and there is a NetworkDuplicated error  in /var/log/nova/nova-api.log
i refered this bug https://bugs.launchpad.net/nova/+bug/1165088 , but it no helps .","Boot an instance with multiple vnics on same network

If the same L2 network is requested multiple times for the same
instance then creating ports on same network and attaching them
to the same instance raises a DuplicateNetworks exception.
Similarly, attaching multiple existent ports on same L2 network
to the same instance raises a DuplicateNetworks exception. This
is the default behavior that is defaulted by a newly introduced
nova flag ""allow_duplicate_networks"" which is set to
False by default.

Not raising a DuplicateNetwork exception and allowing an instance
to have multiple vnics on same network is useful for NfV service
instances and in that case this newly introduced nova flag should
be set to True.

DocImpact: New neutron.allow_duplicate_networks configuration option
Implements blueprint multiple-if-1-net
Change-Id: Id4d633162c785c9b56b9c8426c0445770bc1352e
Closes-Bug: #1187244"
384,3267d3ad92274303b6339e8a8d237ad8a3bb7bb9,1410959050,,1.0,51,10,2,2,1,0.999806133,False,,,,,True,,,,,,,,,,,,,,,,,1450,1204169,1204169,nova,3267d3ad92274303b6339e8a8d237ad8a3bb7bb9,1,1,"“The solution would be a refactor”,”Whatever refactor we do come up with”","Bug #1204169 in OpenStack Compute (nova): ""compute instance.update messages sometimes have the wrong values for instance state ""","Compute instance.update messages that are not triggered by a state change (e.g. setting the host in the resource tracker) have default (None) values for task_state, old_vm_state and old_ task_state.
This can make the instance state sequence look wrong to anything consuming the messages (e.g stacktach)
 compute.instance.update  None(None) -> Building(none)
 scheduler.run_instance.scheduled
 compute.instance.update  building(None) ->  building(scheduling)
 compute.instance.create.start
 compute.instance.update  building(None) ->  building(None)
 compute.instance.update  building(None) ->  building(networking)
 compute.instance.update  building(networking) -> building(block_device_mapping)","Be less confusing about notification states

Depending on the way we snake through send_update we might end up in
_send_instance_update_notification without the vm or task states. The
issue being that we always send those states to the notification
payload. So we are actually producing notifications with None(None) =>
which is clearly not right.

This whole notification path should probably be refactored to prevent
this threading from happening, but in the mean time this will at least
mean that we aren't creating blatantly wrong notifications.

Add unit tests for the new behavior.

Change-Id: I5092880647a4ac826376396610599dab99efb08e
Closes-Bug: #1204169"
385,326b85dd6154048b1671732d2d7df8deb8b99207,1392851334,3.0,1.0,36,55,30,25,1,0.947706302,True,7.0,185781.0,155.0,20.0,False,65.0,53325.13333,257.0,6.0,666.0,1334.0,1334.0,581.0,1167.0,1167.0,202.0,591.0,591.0,0.242822967,0.708133971,0.708133971,435,850,1282303,neutron,326b85dd6154048b1671732d2d7df8deb8b99207,0,0,"'the only reason why I'm making this change is that I want to add
something to the __init__() class’","Bug #1282303 in neutron: ""all plugins should call __init__ in db_base_plugin for db.configure""","Currently each plugin calls db.configure() within the plugin's __init__ class or defines an initialize() method that's sole job is to call this method. Instead we should just call the super method of the db_base_plugin so that it calls this for us automatically.
Note: the only reason why I'm making this change is that I want to add something to the __init__() class of the db_base_plugin that's needed for the nova-event-callback blueprint and adding it in the base class of init looks to be the best place.","Plugins should call __init__ of db_base_plugin for db.configure

Currently each plugin calls db.configure() within the plugin's __init__
class or defines an initialize() method that's sole job is to call this
method. Instead we should just call the super method of a plugin so that
db.configure() is called for us out of the db_base_plugin class.

Note: the only reason why I'm making this change is that I want to add
something to the __init__() class of the db_base_plugin that's needed for
the nova-event-callback blueprint and adding it in the base class of init
looks to be the best place.

Change-Id: Iec3c912735021ceb90f657108aad3a57460d66e7
Closes-bug: #1282303"
386,328beb0433c04a0888d38cbc2927248fdd963318,1396285948,,1.0,19,6,2,2,1,0.634309555,True,5.0,3619048.0,65.0,22.0,False,202.0,70310.0,1028.0,1.0,1.0,944.0,944.0,1.0,943.0,943.0,1.0,939.0,939.0,0.000259336,0.121887967,0.121887967,349,760,1274924,nova,328beb0433c04a0888d38cbc2927248fdd963318,1,1,Problem updating. Deletes some metadata,"Bug #1274924 in OpenStack Compute (nova): ""Settings the availability zone deletes all attached metadata""","I'm using latest git with devstack on Ubuntu 12.04.
When I update the availability zone the attached metadata gets deleted. Steps to reproduce the problem:
 1) nova aggregate-create testagg   #assuming that this creates a new metadata entry with the Id 26
 2) nova aggregate-set-metadata 26 x=y
 3) nova aggregate-update 26 testagg zone1
Now the availability zone is set, but the x=y metadata is lost.","Do not replace the aggregate metadata when updating az

When the aggregate availability zone is updated, the
existing metadata should not be replaced

Change-Id: Ife856af9e7dca836ebe8a1ec050effaad421e17d
Closes-Bug: #1274924"
387,3292103d80676fc789615465759de966dba4e9f7,1385189068,,1.0,2,11,2,2,1,0.89049164,True,3.0,953592.0,18.0,8.0,False,30.0,265260.0,41.0,5.0,7.0,2077.0,2083.0,7.0,1608.0,1614.0,6.0,1787.0,1792.0,0.001050263,0.268267067,0.269017254,1476,1214162,1214162,nova,3292103d80676fc789615465759de966dba4e9f7,0,0,Bug in the future,"Bug #1214162 in OpenStack Compute (nova): ""dhcp_options_enabled option be removed when neutron supports""","The dhcp_options_enabled is currently set to False because the neutron extra_dhcp_opts patches have not landed yet. This should land in the Havana release of Neutron, as such dhcp_options_enabled option can then be defaulted to True or the conditional in the code can be removed (the desired approach). This is explained in https://review.openstack.org/#/c/31061/, and in Neutron: https://review.openstack.org/#/c/30441/ and https://review.openstack.org/#/c/30447/, for the python-neutronclient. So for Icehouse release of Nova this option should be removed and the code modified to support the dhcp_options.","Removes check CONF.dhcp_options_enabled from nova

* Neutron supports extra_dhcp_opts since Havana
* Hence remove boolean check dhcp_options_enabled

Closes-Bug: #1214162
DocImpact
Change-Id: I9af217a87a237e078e70332cbed1c80ad4d5db0a"
388,3299eb3dfebde39f0bec73008c99976c0b852c89,1382982190,0.0,1.0,281,133,3,2,1,0.676371452,True,13.0,2131649.0,100.0,24.0,False,2.0,751490.0,3.0,4.0,1.0,1928.0,1928.0,1.0,1780.0,1780.0,1.0,437.0,437.0,0.003952569,0.865612648,0.865612648,1649,1236741,1236741,neutron,3299eb3dfebde39f0bec73008c99976c0b852c89,0,0,"feature3, “Adding more tests for Radware LBaaS driver.”","Bug #1236741 in neutron: ""Radware LBaaS driver unit-testing enhancement""",More Radware LBaaS driver unit-testing should be added,"Adds tests, fixes Radware LBaaS driver as a result

Adding more tests for Radware LBaaS driver.
Adding new exception module for the Radware lbaas driver.
The base radware lbaas exception, called RadwareLBaasException,
Several specific exceptions for different failures.
Driver was changed for using new exceptions as well.
Changing the way OperationsHandler obtains context.
Always waiting 1 second before handling the operation
next time, to prevent busy-wait requests on vDirect.
Several code optimizations were done as well.

Change-Id: I15f7845fc2575eedb62c47d15ee6c1cea08e22f5
Closes-Bug: #1236741"
389,32b0adb591f80ad2c5c19519b4ffc2b55dbea672,1402493778,1.0,1.0,19,1,3,2,1,0.843342654,False,,,,,True,,,,,,,,,,,,,,,,,1410,883320,883320,nova,32b0adb591f80ad2c5c19519b4ffc2b55dbea672,1,1,“This is a bad pattern” Code does not fulfil policy,"Bug #883320 in OpenStack Compute (nova): ""nova.crypto. revoke_certs_by_user should handle ProcessExecutionError""","revoke_cert may throw ProcessExecutionError.
https://github.com/openstack/nova/blob/master/nova/crypto.py#L164
This is bad pattern.
(See Policy of Exception Handling
https://blueprints.launchpad.net/openstack-qa/+spec/nova-exception-policy)","Catch ProcessExecutionError in revoke_cert

Catch processExecutionError if revoking the
certificate fails.

This change has been abandoned by Chuck Short, so
I am continuing where he left off (mattoliverau).

Continues abandoned change: 17741

Change-Id: I9714ea8cece87256ff5f9a936286c1da3d628af9
Closes-Bug: #883320
Co-Authored-By: chuck.short@canonical.com"
390,32bac00ea003015add0d33be262cb3002e4c43af,1398720647,1.0,1.0,35,7,2,2,1,0.863120569,True,1.0,73383.0,8.0,2.0,False,18.0,603105.0,78.0,2.0,226.0,643.0,803.0,210.0,537.0,690.0,216.0,617.0,768.0,0.135709819,0.386491557,0.480925578,842,1271,1313894,cinder,32bac00ea003015add0d33be262cb3002e4c43af,1,1,,"Bug #1313894 in Cinder: ""3PAR vlun could be wrong for multi-attach""","When the 3PAR driver creates a vlun and then has to fetch it, the fetching could return the wrong vlun.
When a volume is exported to the same host more than once, we will have multiple LUN ids associated with that volume.
We have to make sure we get the correct LUN for the VLUN we just created.   This is especially important
 in a multi-attach scenario.","Fixed 3PAR driver issue finding correct vlun

This patch fixes an issue that happens when
a volume is attached to the same host
more than once.  The driver wasn't looking
for the correct VLUN on the 3par after the
VLUN was created.  We now get the VLUN
information in the return of the creation call.

Change-Id: Id570094f29900c0adcffe6c9d4145c9ad5e3ce4e
Closes-Bug: #1313894"
391,32cc13365be9baba7a1d50fae397f1b597ff505b,1403070245,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,984,1423,1331317,nova,32cc13365be9baba7a1d50fae397f1b597ff505b,0,0,Bug in test,"Bug #1331317 in OpenStack Compute (nova): ""test_create_instance_with_neutronv2_fixed_ip_already_in_use does not test the purposes.""","In test_create_instance_with_neutronv2_fixed_ip_already_in_use of both
v2 and v3, these tests pass wrong parameters ""fixed-ip"" instead of
valid parameter ""fixed_ip"".
The purposes of these tests are to check the behaviors when passing
in-use addresses, but current tests contain two negative factors and
we cannot test the purposes now.","Fix ""fixed_ip"" parameters in unit tests

In test_create_instance_with_neutronv2_fixed_ip_already_in_use of both
v2 and v3, these tests pass wrong parameters ""fixed-ip"" instead of
valid parameter ""fixed_ip"".
The purposes of these tests are to check the behaviors when passing
in-use addresses, but current tests contain two negative factors and
we cannot test the purposes now.

This patch fixes the parameters. This problem was detected through the
API validation works and Ic17d2227909c9180ca86c9d578dad4c73ec57dfc needs
this fixing.

Change-Id: I4d54a1522983c996431b995ed157b32d13c3bc88
Closes-Bug: #1331317"
392,32d948124adeb4615ba141164a1c191eed11598d,1395695569,,1.0,16,7,4,4,1,0.871038186,True,4.0,157897.0,49.0,21.0,False,123.0,1116016.0,363.0,6.0,413.0,2288.0,2482.0,412.0,1980.0,2173.0,394.0,2131.0,2311.0,0.051472505,0.277821214,0.301277039,678,1104,1296940,nova,32d948124adeb4615ba141164a1c191eed11598d,1,1,"Bug!  Missing exception handling. I think this is not en evolutionary error.. in my opinion, the original author did not consider that a variable could be empty (0722a8fb230697026c54086fd81ae82690530cb7)","Bug #1296940 in OpenStack Compute (nova): ""Potential AttributeError in _get_servers if FlavorNotFound""","In the Nova servers API (v2 and v3), this line could fail with an AtributeError if there is a FlavorNotFound exception above it:
https://github.com/openstack/nova/blob/2014.1.b3/nova/api/openstack/compute/servers.py#L611
That code should either check if instance_list is empty first or set instance_list to an empty InstanceList object if FlavorNotFound is hit.","Don't attempt to fill faults for instance_list if FlavorNotFound

When getting a list of servers with details, if a FlavorNotFound is
raised from the compute API the local instance_list variable is set to
an empty list, but later instance_list.fill_faults() is called on it
assuming it is an InstanceList object, which will cause an
AttributeError.

This patch just checks to see if instance_list is empty before calling
fill_faults since InstanceList.fill_faults assumes the list is not empty
when called, so it's not trivial to simply create an empty InstanceList
here and set it to the instance_list variable.

The new test cases use testtools.matchers.HasLength and while seeing
if testtools was imported I noticed it (along with mox) was in the
wrong import group, so also fixed the order there.

This patch also fixes the test_get_server_details_empty unit test to
actually call the detail method rather than index.

Closes-Bug: #1296940

Change-Id: I9f85c2a0b70f41e5bba2da05a2339caf2a8efc3e"
393,32ea2e349decd750e25cb00a8c907b8f73f795f3,1411405417,,1.0,20,2,2,2,1,0.976020648,False,,,,,True,,,,,,,,,,,,,,,,,1345,1813,1372570,neutron,32ea2e349decd750e25cb00a8c907b8f73f795f3,1,1,,"Bug #1372570 in neutron: ""Booting multiple instances causes race with port security groups""","On a fresh installation, without spinning up any instance, if one boots a number of instances via nova there are a number of default security groups created for the admin user:
mysql> select * from securitygroups;
+----------------------------------+--------------------------------------+---------+-------------+
| tenant_id                        | id                                   | name    | description |
+----------------------------------+--------------------------------------+---------+-------------+
| 67e87d2ac4944c3eb93283637b6e4000 | 0fe7f583-7d9e-4be0-9ade-6a4ee9a07daa | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 1a237a3c-39c3-4a92-bd7b-55a3bc5ef7e9 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 51c83ff8-97f9-426d-836e-10bf631219e4 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 8882d943-4309-410c-bd56-981da8d5d095 | default | default     |
| 67e87d2ac4944c3eb93283637b6e4000 | 893b2462-800a-4936-9b35-fc47822237e9 | default | default     |
| d639ee58de4f4db7b29f30a7b142a047 | baf7ac95-efaf-427d-8fd9-f8210d23cab1 | default | default     |
+----------------------------------+--------------------------------------+---------+-------------+
This should never happen as the admin user is not even spinning up the vm's....","Security groups: prevent race for default security group creation

When a VM is booted via the Nova the client connection is created
with an admin user. This causes problems when creating the neutron
port. That is, there may be a race for the creation of the default
security group for the tenant.
The problem was introduced by commit acf44dba26ca8dca47bfb5fb2916807f9f4e2060

Change-Id: Ie0199c71231a322704f1f49995facde09c92da25
Closes-bug: #1372570"
394,3305f9dd061e70736fc4b890a207ecd4d30c3544,1398209262,,1.0,18,1,2,2,1,0.485460761,True,2.0,841729.0,11.0,5.0,False,83.0,1060192.0,207.0,5.0,440.0,1663.0,1707.0,366.0,1369.0,1409.0,426.0,1493.0,1534.0,0.268553459,0.939622642,0.965408805,807,1235,1308819,cinder,3305f9dd061e70736fc4b890a207ecd4d30c3544,1,1,,"Bug #1308819 in Cinder: ""The status change to retyping when cinder return error for the volumes reach maximum number""","Do the below test:
1. Set the maximum number of volumes to 10 for user admin
2. Create type-1 on IBM-svc
3. Create type-7 on IBM-v7k
4. Create 10 volumes on type-1
5. Retype the volume from type-1 to type-7
The cinder will return error:
stack@ubuntu1310:/etc/cinder$ cinder retype --migration-policy on-demand 82cbafa7-949f-4c90-bde9-170507baa32f type-7
ERROR: VolumeLimitExceeded: Maximum number of volumes allowed (12) exceeded (HTTP 413) (Request-ID: req-15e2c0bb-e493-4ceb-a3f1-59d3c5282eff)
6. Check the volume status after that, it was found the status of the volume change to retyping , and keep on retyping after that.
|                  ID                  |   Status  |          Name         | Size | Volume Type | Bootable |
| 82cbafa7-949f-4c90-bde9-170507baa32f |  retyping |          111          |  1   |    type-1   |  false   |
It may because when retyping for migrate, it will have a template volume creation.
If the volumes has reached the maximum number, it can't create the template volume.
But if the retype command failed,   the status for the volume should not change to retyping.
It is better keeping on available.","Keep volume available if retype fails due to quota

If we're over quota for a volume type that we're retyping a volume to,
keep the volume available.

Closes-Bug: #1308819
Change-Id: I7e66e95bf12b7e5a6df04eeb5a501badb5f2941f"
395,330a476f8a82c672d636d7da78b81cc46db1e9dd,1393241959,,1.0,1,1,1,1,1,0.0,True,1.0,19591.0,5.0,1.0,False,13.0,3285892.0,15.0,1.0,44.0,606.0,650.0,43.0,578.0,621.0,3.0,589.0,592.0,0.002741604,0.404386566,0.406442769,458,873,1283872,cinder,330a476f8a82c672d636d7da78b81cc46db1e9dd,1,1,wrong msg,"Bug #1283872 in Cinder: ""webob.exc.HTTPForbidden can't show correct message""","In nova/api/ec2/__init__.py there are codes like:
154     def __call__(self, req):
155         access_key = str(req.params['AWSAccessKeyId'])
156         failures_key = ""authfailures-%s"" % access_key
157         failures = int(self.mc.get(failures_key) or 0)
158         if failures >= CONF.lockout_attempts:
159             detail = _(""Too many failed authentications."")
160             raise webob.exc.HTTPForbidden(detail=detail)
But webob.exc.HTTPForbidden should use the 'explanation' parameter to show the error message.
The source can be referred to
https://github.com/Pylons/webob/blob/master/webob/exc.py#L666","Fix webob.exc.HTTPForbidden parameter miss

HTTPForbidden should use the parameter 'explanation'
instead of 'detail'.
This patch fixes this bug.

Change-Id: I688145f7ea942277c7d3ed3221d6ff2bc7a852ad
Closes-Bug: #1283872"
396,338171c114743c0e03def04ef551231f0d9c93f8,1407345212,,1.0,9,4,2,2,1,0.779349837,False,,,,,True,,,,,,,,,,,,,,,,,1155,1612,1353271,neutron,338171c114743c0e03def04ef551231f0d9c93f8,1,1,,"Bug #1353271 in neutron: ""404 Error when retrieving metadata using DVR routers""","This happens on master.
Steps to reproduce:
- usual devstack config will do
- boot a VM
- ssh to the VM
- curl http://169.254.169.254 from the console
- 404 Not Found error is reported
I would expect the metadata server response to come across correctly.","Fix 404 error fetching metadata when using DVR

The metadata agent was unable to find networks
attached to the DVR router because it was only
filtering ports for 'centralized' routers.

To fix the issue, this patch expands the search
filters to include DVR router interfaces during
the network lookup operation.

The extra filter cause no evident performance
loss while serving the request; a different
approach would require to pass the router type
around to narrow down the search filter, but it
sounds like an overkill.

Closes-bug: #1353271

Change-Id: Iefbefa1ff300adad48ab9fc472d5eb1913fbe488"
397,338569c7c30509bce8565272023dfd791f4e9609,1382608750,,1.0,2,1,2,2,1,0.918295834,True,1.0,2423964.0,9.0,4.0,False,5.0,159010.0,20.0,3.0,139.0,738.0,792.0,139.0,637.0,691.0,98.0,267.0,297.0,0.202040816,0.546938776,0.608163265,1610,1231915,1231915,neutron,338569c7c30509bce8565272023dfd791f4e9609,1,1, ,"Bug #1231915 in neutron: ""session_persistence attr of VIP is not returned when no session persistence is used""","During the investigation of bug 1231704,
I noticed it is valid that there is no session persistence attribute
and it means no session persistence mechanism is used in load balancing.
It seems an intentional behavior according to the code:
https://github.com/openstack/neutron/blob/master/neutron/db/loadbalancer/loadbalancer_db.py#L241
IMHO, it is confusing that it is determined whether session_persisntece is returned or not by session persistence mode. I would suggest that returning ""session_persistence: null"" (or {}) when no session_persistence is used.
In addition, if we send {'session_persistence': null} in lb-vip-update, session_persistence is cleared.
It looks reasonable to me that ""session_persistence = null"" means no session_persistence is used.
From the point of view of API, I believe all defined attributes in an extension should be returned in a response.
I would like to ask opinions from the community.","LBaaS: when returning VIP include session_persistence even if None

Closes-Bug: #1231915
Change-Id: I3dc5e17118995fb49dd381d08c9d92cb60b80abb"
398,339a97d0f2d17f531cfc79e09cd8b8bc75ce6e2a,1407161849,,1.0,56,35,7,4,1,0.896166384,False,,,,,True,,,,,,,,,,,,,,,,,1115,1570,1349888,nova,339a97d0f2d17f531cfc79e09cd8b8bc75ce6e2a,1,1,,"Bug #1349888 in OpenStack Compute (nova): ""[SRU] Attempting to attach the same volume multiple times can cause bdm record for existing attachment to be deleted.""","[Impact]
 * Ensure attching already attached volume to second instance does not
   interfere with attached instance volume record.
[Test Case]
 * Create cinder volume vol1 and two instances vm1 and vm2
 * Attach vol1 to vm1 and check that attach was successful by doing:
   - cinder list
   - nova show <vm1>
   e.g. http://paste.ubuntu.com/12314443/
 * Attach vol1 to vm2 and check that attach fails and, crucially, that the
   first attach is unaffected (as above). You can also check the Nova db as
   follows:
   select * from block_device_mapping where source_type='volume' and \
       (instance_uuid='<vm1>' or instance_uuid='<vm2>');
   from which you would expect e.g. http://paste.ubuntu.com/12314416/ which
   shows that vol1 is attached to vm1 and vm2 attach failed.
 * finally detach vol1 from vm1 and ensure that it succeeds.
[Regression Potential]
 * none
---- ---- ---- ----
nova assumes there is only ever one bdm per volume. When an attach is initiated a new bdm is created, if the attach fails a bdm for the volume is deleted however it is not necessarily the one that was just created. The following steps show how a volume can get stuck detaching because of this.
$ nova list
c+--------------------------------------+--------+--------+------------+-------------+------------------+
| ID                                   | Name   | Status | Task State | Power State | Networks         |
+--------------------------------------+--------+--------+------------+-------------+------------------+
| cb5188f8-3fe1-4461-8a9d-3902f7cc8296 | test13 | ACTIVE | -          | Running     | private=10.0.0.2 |
+--------------------------------------+--------+--------+------------+-------------+------------------+
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
|                  ID                  |   Status  |  Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | available | test10 |  1   |     lvm1    |  false   |             |
+--------------------------------------+-----------+--------+------+-------------+----------+-------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
+----------+--------------------------------------+
| Property | Value                                |
+----------+--------------------------------------+
| device   | /dev/vdb                             |
| id       | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
| serverId | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
| volumeId | c1e38e93-d566-4c99-bfc3-42e77a428cc4 |
+----------+--------------------------------------+
$ cinder list
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
|                  ID                  | Status |  Name  | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | in-use | test10 |  1   |     lvm1    |  false   | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+--------+--------+------+-------------+----------+--------------------------------------+
$ nova volume-attach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
ERROR (BadRequest): Invalid volume: status must be 'available' (HTTP 400) (Request-ID: req-1fa34b54-25b5-4296-9134-b63321b0015d)
$ nova volume-detach test13 c1e38e93-d566-4c99-bfc3-42e77a428cc4
$ cinder list
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  |  Name  | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
| c1e38e93-d566-4c99-bfc3-42e77a428cc4 | detaching | test10 |  1   |     lvm1    |  false   | cb5188f8-3fe1-4461-8a9d-3902f7cc8296 |
+--------------------------------------+-----------+--------+------+-------------+----------+--------------------------------------+
2014-07-29 14:47:13.952 ERROR oslo.messaging.rpc.dispatcher [req-134dfd17-14da-4de0-93fc-5d8d7bbf65a5 admin admin] Exception during message handling: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 406, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 277, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 319, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 307, in decorated_function
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4363, in detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     self._detach_volume(context, instance, bdm)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4309, in _detach_volume
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     connection_info = jsonutils.loads(bdm.connection_info)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/jsonutils.py"", line 176, in loads
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     return json.loads(strutils.safe_decode(s, encoding), **kwargs)
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/strutils.py"", line 134, in safe_decode
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher     raise TypeError(""%s can't be decoded"" % type(text))
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher TypeError: <type 'NoneType'> can't be decoded
2014-07-29 14:47:13.952 31588 TRACE oslo.messaging.rpc.dispatcher","Fix creating bdm for failed volume attachment

This commit modifies the reserve_block_device_name method to return the
bdm object, when the corresponding keyword argument is True. This
ensures the correct bdm is destroyed if the attach fails. Currently the
code assumes only one bdm per volume and so retrieving it can cause the
incorrect db entry to be returned.

Change-Id: I22a6db76d2044331d1a846eb4b6d7338c50270e2
Closes-Bug: #1349888"
399,33cc64fb81773f0c246073d23c525357c9aa3b08,1390259427,,1.0,8,6,2,2,1,0.863120569,True,7.0,3596095.0,120.0,45.0,False,69.0,224679.0,166.0,10.0,627.0,3941.0,4272.0,570.0,3210.0,3515.0,190.0,2899.0,2968.0,0.026818309,0.407188992,0.416877282,310,719,1270973,nova,33cc64fb81773f0c246073d23c525357c9aa3b08,1,1,,"Bug #1270973 in OpenStack Compute (nova): ""Remove and recreate interface if already exists""","If a nova-compute machine restarts when openvswitch comes up it logs the following warning messages for all tap interfaces that do not exist:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the compute-node  starts it recreates the interfaces and re-adds them to the
ovs-bridge. Unfortunately, ovs does not reinitialize the interfaces as they
are already in ovsdb and does not assign them a ofport number.
This situation corrects itself though the next time a port is added to the
ovs-bridge which is why no one has probably noticed this issue till now.
In order to correct this we should first remove interface that exist and
then readd them.
same bug on neutron-side:  https://bugs.launchpad.net/neutron/+bug/1268762","Remove and recreate interface if already exists

If a nova-compute machine restarts when openvswitch comes up it logs the
following warning messages for all tap interfaces that do not exist:

bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)

Once the compute-node starts it recreates the interfaces and re-adds them
to the ovs-bridge. Unfortunately, ovs does not reinitialize the interfaces
as they are already in ovsdb and does not assign them a ofport number.

This situation corrects itself though the next time a port is added to the
ovs-bridge which is why no one has probably noticed this issue till now.

In order to correct this we should first remove interface that exist and
then readd them.

Change-Id: I1dc01604c5857941d9f35eb1e0c4d5103a56d57b
Closes-bug: #1270973
Related-bug: #1268762"
400,341b9866ce337034b6d8ca768d03581b4bf40a5c,1378429659,,1.0,2,2,1,1,1,0.0,True,4.0,1558481.0,33.0,19.0,False,99.0,69716.0,253.0,9.0,4.0,3299.0,3303.0,4.0,3053.0,3057.0,0.0,2395.0,2395.0,0.000168095,0.402756766,0.402756766,1520,1221493,1221493,nova,341b9866ce337034b6d8ca768d03581b4bf40a5c,1,1,"“the message ""An unknown exception occurred."" was obtained.”","Bug #1221493 in OpenStack Compute (nova): ""Exception message that is different from the expected""","When an exception PciConfigInvalidWhitelist occurs, the message ""An unknown exception occurred."" was obtained.","Fixes unexpected exception message in PciConfigInvalidWhitelist

When an exception PciConfigInvalidWhitelist occurs, the message ""An
unknown exception occurred."" was obtained. This is due to the content
of the variable not being overriden correctly. The same typo applies
to PciTrackerInvalidNodeId.

Closes-Bug: #1221493
Change-Id: I589cb8d0a1426eb84bbbc4a819b5585e4e746843"
401,3445715f98e3aaa967f1661e403e45ad85392909,1403156040,,1.0,9,8,2,2,1,0.522559375,False,,,,,True,,,,,,,,,,,,,,,,,608,1033,1292105,neutron,3445715f98e3aaa967f1661e403e45ad85392909,1,1,Bug after a change.,"Bug #1292105 in neutron: ""ovs tunnel state not syncing (failure pinging overcloud instance)""","I saw this in a recent CI overcloud run: http://logs.openstack.org/66/74866/8/check-tripleo/check-tripleo-overcloud-precise/aa490f1/console.html
2014-03-12 20:01:46.509 | Timing out after 300 seconds:
2014-03-12 20:01:46.509 | COMMAND=ping -c 1 192.0.2.46
2014-03-12 20:01:46.509 | OUTPUT=PING 192.0.2.46 (192.0.2.46) 56(84) bytes of data.
2014-03-12 20:01:46.509 | From 192.0.2.46 icmp_seq=1 Destination Host Unreachable
It appears as though everything ran fine up until it tried to ping the booted overcloud instance.  I'm fairly certain it has nothing to do with my change, so I wanted to open a bug to track it in case anyone else runs into a similar problem.","OVS agent: Correct bridge setup ordering

This patch fixes three issues that combined to break tunnel networks.

The first issue was the the failure mode was being set on the
integration bridge after the canary flow was installed. This
immediately wiped out the canary flow.

The second problem was that the main loop would sync tunnel networks
right before resetting the tunnel bridge when the canary flow was
missing. This meant that it would sync all of the tunnels via RPC and
then wipe them out.

The final issue was that after resetting the tunnel bridge in the
main loop, the tunnel_sync variable was not set to True, so the
tunnels would never be resynchronized.

This patch addresses the three issues in the following ways:
1. Set the failure mode on the bridge before the canary flow is
installed.
2. Run the OVS restart logic before the tunnel synchronization.
3. If the restart logic is triggered, set tunnel_sync to True to
trigger synchronization.

Closes-Bug: #1292105
Change-Id: I6381e3fee49910127c420dd2e3205c64cdb9e185"
402,34558259058e19f9c4b2a96cbb97770883460ded,1407227459,,1.0,3,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1148,1603,1352768,nova,34558259058e19f9c4b2a96cbb97770883460ded,1,1,"""this code review, https://review.openstack.org/#/c/104262/ brings an error in log because of the line at 137:”","Bug #1352768 in OpenStack Compute (nova): ""virt: error in log when log exception in guestfs.py""","In this code review, https://review.openstack.org/#/c/104262/  brings an error in log because of the line at 137:
+ LOG.info(_LI(""Unable to force TCG mode, libguestfs too old?""),
+ ex)
Error is:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 685, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 649, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: not all arguments converted during string formatting
Logged from file guestfs.py, line 139
To fix this issue, we just need to add %s","Fix error in log when log exception in guestfs.py

This code review: https://review.openstack.org/#/c/104262/
brings an issue when log exception in guestfs.py at
LOG.info(_LI(""Unable to force TCG mode, libguestfs too old?"")
To fix this issue we just need to add %s in it.

Change-Id: Ic33a6b7c2ce49e21e96a087984aba6656065d8fc
Closes-bug: #1352768"
403,345f8e1fdc65e0cf23369488d4318c0755d6b65d,1381309984,,1.0,13,2,3,3,1,0.901897236,True,3.0,635011.0,19.0,8.0,False,8.0,215951.3333,10.0,5.0,777.0,1057.0,1059.0,735.0,956.0,957.0,323.0,352.0,352.0,0.744827586,0.811494253,0.811494253,1653,1237082,1237082,neutron,345f8e1fdc65e0cf23369488d4318c0755d6b65d,1,1, ,"Bug #1237082 in neutron: ""ml2 vxlan interface with linuxbridge use proxy mode""","when a vxlan interface is created in linuxbridge agent, it is set with the argument ""proxy"" (ie  ""ip link add vxlan1 type vxlan id 1 dev wlan0 proxy group 224.0.0.1""), which means that it uses an arp-proxy. This should only be done when L2-population is activated, otherwise it leads to unaccessibility between vm that are note hosted on the same host.","use proxy mode on vxlan interface only when l2-population is activated

the proxy mode was set by default on every vxlan interfaces
which leads to inaccessibility between VM that are not hosted
on the same host in a vxlan network

Closes-Bug: #1237082

Change-Id: I34028ee0bdfdccda61c6a29f58759259da060b68"
404,34e14fbbb5293c52ef5dbfeb1172390f15d79e1f,1382783714,,1.0,7,2,2,2,1,0.764204507,True,8.0,12028483.0,93.0,37.0,False,9.0,647545.0,18.0,4.0,13.0,3045.0,3055.0,13.0,2545.0,2555.0,3.0,2002.0,2002.0,0.000620732,0.310831782,0.310831782,34,346,1244918,nova,34e14fbbb5293c52ef5dbfeb1172390f15d79e1f,1,0,I think is a VMware ESX specific error,"Bug #1244918 in OpenStack Compute (nova): ""VMware ESX: Boot from volume errors out due to relocate""","When trying to perform boot instance from volume using the VMwareESXDriver, the operation errors out.
Command:
$ nova boot --flavor 1  --block-device-mapping vda=222e8ece-8723-4930-803c-8ae5cf233a87:::0 vm1
Log messages
d3-59fce43903e8] Root volume attach. Driver type: vmdk attach_root_volume /opt/stack/nova/nova/virt/vmwareapi/volumeops.py:458
2013-10-26 14:49:13.393 30706 WARNING nova.virt.vmwareapi.driver [-] Task [RelocateVM_Task] (returnval){
   value = ""haTask-162-vim.VirtualMachine.relocate-327302855""
   _type = ""Task""
 } status: error The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 ERROR nova.compute.manager [req-e95b7262-a70c-436b-a9d5-0b8045cbf3f5 4471d6567a6b4dd29affbc849f3814d9 256df8ea370d4de2b40edfe9b0ea4063] [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Instance failed to spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] Traceback (most recent call last):
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1410, in _spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 178, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     admin_password, network_info, block_device_info)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 538, in spawn
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     data_store_ref)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 467, in attach_root_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     self._relocate_vmdk_volume(volume_ref, res_pool, datastore)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/volumeops.py"", line 295, in _relocate_vmdk_volume
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     self._session._wait_for_task(task.value, task)
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 901, in _wait_for_task
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     ret_val = done.wait()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     return hubs.get_hub().switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]     return self.greenlet.switch()
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8] NovaException: The operation is not supported on the object.
2013-10-26 14:49:13.394 30706 TRACE nova.compute.manager [instance: 6b190d4d-231d-43ec-86d3-59fce43903e8]","VMware ESX: Boot from volume must not relocate vol

ESX does not support relocate API. Moreover when working with ESX the boot
from volume logic need not perform relocation since the volume's datastore
is always visible to the ESX host.

The fix is to ignore relocation when working with ESX.

Change-Id: I227cf2ea5b6f681150ede0bf6257eb238f1452d2
Closes-bug: #1244918"
405,34f30a1d16d81be86a117690ccc5bdfc5bdd0659,1383039798,,1.0,72,13,2,2,1,0.672294817,True,3.0,2945369.0,43.0,23.0,False,7.0,867182.0,7.0,10.0,29.0,2038.0,2055.0,25.0,1874.0,1887.0,9.0,474.0,474.0,0.019646365,0.933202358,0.933202358,1730,1245799,1245799,neutron,34f30a1d16d81be86a117690ccc5bdfc5bdd0659,1,1,“It's possible to set '@' or ':' character in Linux device name.”,"Bug #1245799 in neutron: ""IP lib fails when int name has '@' character and VLAN interfaces""","IP lib can not distinguish between interfaces with an '@' in their name to a VLAN interfaces.
And an interface name can have more than one '@' in their name.","Linux device name can have '@' or ':' characters

It's possible to set '@' or ':' character in Linux device name. But
Neutron IP lib doesn't handle correctly these names.

To distinguish VLAN devices from others devices with '@' in their name,
the details option in iproute2 is used.

Change-Id: I7ed24ec00f481207d97bdef052f86388f24d9b21
Closes-Bug: #1245799"
406,35008c0d16e41fdae6e868626477f361392aa923,1410265676,,1.0,6,6,5,5,1,0.9697239,False,,,,,True,,,,,,,,,,,,,,,,,1281,1744,1366982,nova,35008c0d16e41fdae6e868626477f361392aa923,1,1,,"Bug #1366982 in OpenStack Compute (nova): ""Exception NoMoreFixedIps doesn't show which network is out of IPs""","The exception NoMoreFixedIps in nova/exception.py has a very generic error message:
""Zero fixed ips available.""
When performing a deploy with multiple networks, it can become difficult to determine which network has been exhausted.  Slight modification to this error message will help simplify the debug process for operators.","Update NoMoreFixedIps message description

The exception NoMoreFixedIps within Nova is a generic exception.
It is used to indicate that the subnets within the network no longer
have any available fixed IPs for use.

The original message was non specific.  This change set provides an
updated message that indicates which network has exhausted its fixed
IP set.

This is important for operators that are deploying instances with
multiple networks.

Change-Id: Ib8b854381b616f83e82bc573f85e109f019897dc
Closes-Bug: #1366982"
407,355ff10f6e5e4111f14c79b1bdfc48854c81f8b3,1382444465,,1.0,126,141,13,1,1,0.807580884,True,11.0,17023819.0,186.0,34.0,False,9.0,1177555.385,11.0,2.0,24.0,171.0,191.0,24.0,171.0,191.0,6.0,109.0,111.0,0.014644351,0.230125523,0.234309623,14,326,1243213,neutron,355ff10f6e5e4111f14c79b1bdfc48854c81f8b3,1,0,postgre specific,"Bug #1243213 in neutron: ""Enums are not deleted after dropping tables in some migrations""","For downgrade with postgres it is not enough to just drop_table if table contains enums. They should be removed separately after
deleting the table.
This problem takes place in migrations like 52ff27f7567a_support_for_vpnaas, 39cf3f799352_fwaas_havana_2_model, 569e98a8132b_metering, f489cf14a79c_lbaas_havana.
Also if some migration use the same enums in postgres work with
them should be organized differently. In migration 3c6e57a23db4_add_multiprovider if does not use create_type=False downgrade fails  with this error http://paste.openstack.org/show/84486/","Fix enums usage for postgres in migrations

For downgrade with postgres it is not enough to just drop_table if
table contains enums. The enums should be removed separately after
deleting the table.

Also, if a migration uses an existing enum then postgresql needs some
specific code to re-use this enum.

Closes-Bug: #1243213

Change-Id: Ie5bb8282f7b289c917679192a0d722eb29b8d30f"
408,3568a9cac73a2da19e86d82f561be10ae9dbe9a0,1380120904,,1.0,32,7,2,2,1,0.976634911,True,6.0,1079571.0,28.0,6.0,False,5.0,22294.0,9.0,3.0,731.0,1819.0,1819.0,695.0,1642.0,1642.0,283.0,307.0,307.0,0.743455497,0.806282723,0.806282723,1609,1231914,1231914,neutron,3568a9cac73a2da19e86d82f561be10ae9dbe9a0,1,1, ,"Bug #1231914 in neutron: ""Midonet plug, add dhcp opt121 route when a new subnet is created""",When a subnet is created (except the first one) another ip is added to the dhcp port. Midonet plugin should react and add the correct route opt121,"Add a route to reach the MD server when a subnet is created

When the first subnet is created, the dhcp port is created and
midonet plugin correctly adds the static route to reach the MD
server in create_port. When a second or following subnets are
created, a new ip is added to the dhcp port. This patch takes
care of adding the static route to correcly reach the MD server
in update_port. This fixes the problem of VMs not being able to
reach the MD if assigned to the second subnet

Closes-bug: #1231914
Change-Id: Ifc95f901d4222b76a4254e21295829ac8d82493b"
409,35f64f8026f7607c749727ac6b3ee93c956bdfd2,1385655431,1.0,1.0,20,5,2,2,1,0.40217919,True,2.0,8398423.0,75.0,25.0,False,2.0,10060991.0,2.0,6.0,44.0,1035.0,1040.0,44.0,957.0,962.0,37.0,458.0,461.0,0.06462585,0.780612245,0.785714286,103,502,1256036,neutron,35f64f8026f7607c749727ac6b3ee93c956bdfd2,1,1,,"Bug #1256036 in neutron: ""Metering iptables driver doesn't read the right root_helper param""","The metering iptables driver doesn't read the root_helper param, thus it uses the default value.","Fix metering iptables driver doesn't read root_helper param

With this patch the iptables driver instantiates the
iptables_manager with the correct root_helper value.

Change-Id: I9a37bae92dc12a8a78ea2d1b9fc5e995321ca322
Closes-bug: #1256036"
410,362e998e89a1f1d9dce3379c4610b205ebeec7c0,1394813205,,1.0,105,15,2,2,1,0.881290899,True,5.0,1646391.0,65.0,19.0,False,37.0,1407731.0,87.0,6.0,7.0,3128.0,3129.0,7.0,2362.0,2363.0,5.0,2906.0,2906.0,0.000788851,0.382198265,0.382198265,620,1045,1292494,nova,362e998e89a1f1d9dce3379c4610b205ebeec7c0,1,1,Strange values when updating quota,"Bug #1292494 in OpenStack Compute (nova): ""Invalid validation for unlimited values when updating user quota""","There is a possible scenario that is not correctly handled when validating quota limit. If the remaining quota is unlimited (-1) and new quota value is unlimited (-1) then both values are summed resulting in a -2 value that then causes the following error: ""Quota limit must be greater than 0"".","Handling unlimited values when updating quota

Changes:
- Adding a general approach to handle unlimited values when performing
calculations with quota values.
- New test that fails without the fix.
- Updated test cases to support the fix.

Change-Id: I762a8e67261c0e0286307832cd13ad12e6d96504
Closes-Bug: #1292494"
411,3666dcf76dd8cf7a22743e407343b528331130f7,1394599076,,1.0,8,3,1,1,1,0.0,True,5.0,1071880.0,92.0,25.0,False,22.0,221033.0,32.0,6.0,1.0,1217.0,1217.0,1.0,1059.0,1059.0,1.0,736.0,736.0,0.002109705,0.77742616,0.77742616,656,1082,1294892,neutron,3666dcf76dd8cf7a22743e407343b528331130f7,0,0,add debugs,"Bug #1294892 in neutron: ""Debug logging for DHCP agent config files""","We've been seeing things that appear to be races between the hosts files being written out for dnsmasq and dhcp requests coming in. We will get occasional errors from dnsmasq saying ""no address available"", ""duplicate IP address"" but by the time you look, the corresponding host file has long since been replaced.
Outputting the dnsmsaq config file in the debug logs would help in establishing what the DHCP server state was at the time of the problem","Log dnsmasq host file generation

We've been seeing things that appear to be races between the hosts
files being written out for dnsmasq and dhcp requests coming in.  We
will get occasional errors from dnsmasq saying ""no address available"",
""duplicate IP address"" but by the time you look, the corresponding
host file has long since been replaced.

If we had some debugging like this, we could at least correlate what
neutron thought dnsmasq knew at the time the requests were coming in.

We store the filename to avoid multiple lookups, and also
s/name/filename/ to avoid ambiguity and be more consistent with the
rest of the code.

Closes-Bug: #1294892
Change-Id: Ifa92ca71419ce0155b28d2015eff9e82557a0825"
412,36b5782dea6d0e627c65c73dbfee39d55dbee318,1394799627,,2.0,70,43,2,2,1,0.979509158,True,1.0,1809427.0,7.0,2.0,False,6.0,5025442.5,13.0,2.0,5.0,1843.0,1847.0,5.0,1538.0,1542.0,5.0,17.0,21.0,0.003921569,0.011764706,0.014379085,594,1018,1291291,cinder,36b5782dea6d0e627c65c73dbfee39d55dbee318,0,0,it should ignore datastores,"Bug #1291291 in Cinder: ""vmware: vmdk driver should ignore datastores in maintenance mode""","The vmdk driver ignores the maintenanceMode property of a datastore and considers it as a potential candidate for volume creation. Rather, it should ignore datastores in maintenance mode for volume creation.","vmware:Ignore inaccessible/inMaintenance datastore

The vmdk driver uses the number of hosts connected to a datastore as one
of the criteria for selecting a datastore for volume creation. It might
incorrectly consider a host to which the datastore is inaccessible while
computing the number of connected hosts. This is because, the driver
sometimes uses the aggregated accessible status of a datastore while
determining its accessibility to a host. Also, the driver is ignoring
the maintenance status of the datastore and might create a volume in a
datastore which is entering maintenance or already in maintenance. This
change fixes these issues in datastore filtering.

Change-Id: Ibd5c1865d42638a735fd588e9c10e8f714c9c030
Closes-Bug: #1291346
Closes-Bug: #1291291"
413,36b5782dea6d0e627c65c73dbfee39d55dbee318,1394799627,,2.0,70,43,2,2,1,0.979509158,True,1.0,1809427.0,7.0,2.0,False,6.0,5025442.5,13.0,2.0,5.0,1843.0,1847.0,5.0,1538.0,1542.0,5.0,17.0,21.0,0.003921569,0.011764706,0.014379085,595,1019,1291346,cinder,36b5782dea6d0e627c65c73dbfee39d55dbee318,1,1,There is a bug because they chose a bad critery,"Bug #1291346 in Cinder: ""vmware: vmdk driver might consider a datastore as accessible to a host when it is not""","The vmdk driver uses the number of hosts connected to a datastore as one of the criteria for selecting a datastore for volume creation. It might incorrectly consider a host to which the datastore is inaccessible while computing the number of connected hosts.
if hasattr(mount_info, ""accessible""):
            accessible = mount_info.accessible
else:
           # If accessible attribute is not set, we look at summary
            summary = self.get_summary(datastore)
            accessible = summary.accessible
If a datastore is accessible through a subset of hosts, then the value of summary.accessible will be true.","vmware:Ignore inaccessible/inMaintenance datastore

The vmdk driver uses the number of hosts connected to a datastore as one
of the criteria for selecting a datastore for volume creation. It might
incorrectly consider a host to which the datastore is inaccessible while
computing the number of connected hosts. This is because, the driver
sometimes uses the aggregated accessible status of a datastore while
determining its accessibility to a host. Also, the driver is ignoring
the maintenance status of the datastore and might create a volume in a
datastore which is entering maintenance or already in maintenance. This
change fixes these issues in datastore filtering.

Change-Id: Ibd5c1865d42638a735fd588e9c10e8f714c9c030
Closes-Bug: #1291346
Closes-Bug: #1291291"
414,36c420157674e8ed34d407f19062b06a657e6490,1380449871,1.0,1.0,28,2,2,2,1,0.881290899,True,1.0,300468.0,14.0,6.0,False,12.0,1766158.0,19.0,4.0,384.0,1102.0,1181.0,379.0,979.0,1057.0,371.0,948.0,1020.0,0.35394862,0.902949572,0.971455756,1615,1232698,1232698,cinder,36c420157674e8ed34d407f19062b06a657e6490,1,1, ,"Bug #1232698 in Cinder: ""migrate does not convert string to bool for force_host_copy""",Volume migration API does not convert string to bool for the force_host_copy parameter.,"Validate force_host_copy API param for migration

For the force_host_copy parameter of volume migration, make sure that we
get a boolean or string that we can convert to boolean.

Change-Id: I7a77ca1780a4ef80bc351aa89df0efaaea0d7cf4
Closes-Bug: #1232698"
415,36e8cbb34e78ff367cb501b8c494d9a02228251d,1412047283,,1.0,17,1,2,2,1,0.503258335,False,,,,,True,,,,,,,,,,,,,,,,,1362,1831,1374573,neutron,36e8cbb34e78ff367cb501b8c494d9a02228251d,1,1,"""This looks like a regression caused by commit: b1677dcb80ce8b83aadb2180efad3527a96bd3bc”","Bug #1374573 in neutron: ""Server hang on external network deletion with FIPs""","This happens on master:
Follow these steps:
1) neutron net-create test --router:external=True
2) neutron subnet-create test 200.0.0.0/22 --name test
3) neutron floatingip-create test
4) neutron net-delete test
Watch command 4) hang (the server never comes back). Expected behavior would be for the command to succeed and delete the network successfully.
This looks like a regression caused by commit: b1677dcb80ce8b83aadb2180efad3527a96bd3bc (https://review.openstack.org/#/c/82945/)","ML2: move L3 cleanup out of network transaction

Move _process_l3_delete out of the delete_network
transaction to eliminate the semaphore deadlock that
occurs when it tries to delete the ports associated
with existing floating IPs.

It makes more sense to live outside of the transaction
anyway because the operations it performs cannot be
rolled back only in the database if the L3 plugin makes
external calls for floating IP creation/deletion.
e.g. if delete_floatingip is successful, it may have
deleted external resources and restoring the DB records
would make things inconsistent.

If a failure to delete the network does occur, any cleanup
done by _process_l3_delete will not be reversed.

Closes-Bug: #1374573
Change-Id: I3ae7bb269df9b9dcef94f48f13f1bde1e4106a80"
416,3709a1be92257daccba3f2506bea6d63f732564a,1403895000,,1.0,27,2,4,3,1,0.887784828,False,,,,,True,,,,,,,,,,,,,,,,,1013,1455,1334708,neutron,3709a1be92257daccba3f2506bea6d63f732564a,1,1,,"Bug #1334708 in neutron: ""NSX - Floating IP status does not transition correctly""","This patch verifies Floating IP status is updated correctly according to blue print.
https://review.openstack.org/#/c/102700/
https://blueprints.launchpad.net/neutron/+spec/fip-op-status
VMWWare-Minesweeper fails consistently. The Jenkins gates pass ok
Please check","NSX: properly handle floating ip status

Ensure the floating IP status is put ACTIVE or DOWN when it
is associated or disassociated.
This is done directly on the plugin class as the NSX backend
applies relevant NAT rules synchronously.

Change-Id: Ie747c4eae2ac33016fdabaade2335f7d2d24eec9
Closes-Bug: #1334708"
417,371a58fd1d3fb1c2db7e1234f9c51f22b1b8787e,1401909135,,1.0,99,157,5,2,1,0.710737605,False,,,,,True,,,,,,,,,,,,,,,,,610,1035,1292119,cinder,371a58fd1d3fb1c2db7e1234f9c51f22b1b8787e,1,1,performance bug,"Bug #1292119 in Cinder: ""Performance issue with brcd_fc_zone_client_cli.py""","Performance issue with brcd_fc_zone_client_cli.py
Every ssh command creates a new switch connection login and teardown which needs to be optimized
The more fiber channel zones the longer it takes, we added logging to capture this.
Add logging which tracks zoning on attach cinder/volume/manager.py
           vol_type = conn_info.get('driver_volume_type', None)
           if (vol_type == 'fibre_channel'):
               import uuid
               myid = uuid.uuid4()
               LOG.warn(""START ZONING!!!! %s"" % myid)
               self._add_or_delete_fc_connection(conn_info)
               LOG.warn(""END ZONING!!!! %s"" % myid)
           return conn_info
21 seconds to zone Brocade switch with no zones on the switch
2014-02-07 15:24:08.334 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING!!!!
<mdenny> 2014-02-07 15:24:29.391 WARNING cinder.volume.manager [req-db7b03f8-be1e-4778-a319-c964fe8fbed8 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
48 seconds to zone Brocade switch with 230 zones on the switch
<mdenny> 2014-02-08 15:22:05.219 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] START ZONING !!!!
<mdenny> 2014-02-08 15:22:53.258 WARNING cinder.volume.manager [req-8d8dfdae-e4e7-400c-973a-e461e5e3e5c6 3837affc52684a7c9419fe17822bd536 49d8fc4dec104897b32ec4330a29a620] END ZONING!!!!
This occurred on FOS v6.4.3 and v7.0.2
Re-factor connection by creating one login connection with locking and performing all or a group of ssh commands might be a solution.","Fix performance issues with brocade zone driver.

Removed firmware checks from common path as the ssh invoke_shell
was taking 5 seconds to complete for each firmware check.
Refactored to only do cfgsave in non-activate case.
Removed extra calls to switch to get the active zoneset.

Closes-Bug: #1292119

Change-Id: Ie1c2c0c603f6dd68439f2c2db74b21ea2dd0fc11"
418,3720fd17b060de6f14e537e6c003e923c67c65a0,1391630843,,1.0,22,17,2,2,1,0.291818257,True,4.0,821812.0,40.0,15.0,False,14.0,1596095.0,20.0,5.0,648.0,2243.0,2608.0,604.0,1761.0,2096.0,636.0,2162.0,2521.0,0.087548103,0.297278725,0.346619021,370,782,1276728,nova,3720fd17b060de6f14e537e6c003e923c67c65a0,0,0,'can be wrong’ not a bug still,"Bug #1276728 in OpenStack Compute (nova): ""Cells capacity reports can be wrong when multiple flavors have the same disk/ram size""","mysql> select memory_mb,memory_mb_used,free_ram_mb,free_disk_gb from compute_nodes where free_ram_mb > 8000;
+-----------+----------------+-------------+--------------+
| memory_mb | memory_mb_used | free_ram_mb | free_disk_gb |
+-----------+----------------+-------------+--------------+
|    131026 |         121640 |        9386 |          810 |
|    131026 |         121621 |        9405 |          990 |
|    131026 |         121636 |        9390 |          790 |
+-----------+----------------+-------------+--------------+
3 rows in set (0.00 sec)
which indicates three hosts that can handle an 8GB instance, but the capacity reported in the cell log shows:
capacities: {'ram_free': {'units_by_mb': {'8192': 6,
What's happening is that two flavors have memory_mb set to 8192 so the slots are being counted twice.","Count memory and disk slots once in cells state manager

If multiple instance types in a cell had the same memory_mb value or
root_gb + ephemeral_gb added to the same value, then capacity counts
would be inaccurate since slots would get counted multiple times.
Rather than counting based on instance type it is now based on the set
of memory or disk values pulled from instance types.

Change-Id: Iaded7fa69dadb4aa0335ef1569975bdfcb2767c6
Closes-bug: #1276728"
419,37520a7dc14971b2d244f37febdb9fb13edbfd2f,1403086758,1.0,1.0,26,3,2,2,1,0.992266639,False,,,,,True,,,,,,,,,,,,,,,,,897,1333,1321220,nova,37520a7dc14971b2d244f37febdb9fb13edbfd2f,1,1,,"Bug #1321220 in OpenStack Compute (nova): ""[EC2] StartInstance response missing instanceset info""","Startinstance response elements shown as below:
""<StartInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-5970ccd7-c763-456c-89f0-5b55ea18880b</requestId>
  <return>true</return>
</StartInstancesResponse>
""
But as per the AWS API reference doc, the response elements shown be as below:
==
<StartInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>0</code>
          <name>pending</name>
      </currentState>
      <previousState>
          <code>80</code>
          <name>stopped</name>
      </previousState>
    </item>
  </instancesSet>
</StartInstancesResponse>
===
here, <instanceSet> information missing in the response elements.","Add instanceset info to StartInstance response

Currently startinstance response doesn't have the
instanceset information with InstanceID, current state
and previous state. It just returns the ""True"".
As per the AWS API reference document, the StartInstance
response elements should include the instanceset information
as below:
<StartInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>req-a7326465-5ce2-4ed6-ab89-394b38cca85f</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>16</code>
        <name>running</name>
      </currentState>
      <previousState>
        <code>80</code>
        <name>stopped</name>
      </previousState>
    </item>
  </instancesSet>
</StartInstancesResponse>

Included the instanceset in to startinstance response elements
and updated the test cases for startinstance response elements
in nova/tests/api/ec2/test_cloud.py file.

Change-Id: I08ef7ed88f983b66a30c76d6b7b754222097a03b
Closes-bug: #1321220"
420,3758f25c3ca83d9465f7a0041f286156504c7065,1399054089,,1.0,2,2,2,2,1,1.0,True,5.0,1490785.0,66.0,14.0,False,6.0,1527766.5,6.0,2.0,0.0,253.0,253.0,0.0,248.0,248.0,0.0,217.0,217.0,0.000832639,0.181515404,0.181515404,854,1287,1315475,neutron,3758f25c3ca83d9465f7a0041f286156504c7065,1,0,"“The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type.”","Bug #1315475 in neutron: ""OpenDaylight plugin does not allow vlan network type""","The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type.","Allow vlan type usage for OpenDaylight ml2

The initial OpenDaylight integration with Openstack did not support
vlan isolation so it was not included as a valid type. This change
adds the vlan type as allowed.

Also modified tests to include vlan as supported in the
check_segment().

DocImpact:
Use of VLANs with ML2 and the OpenDaylight mechanism driver
requires OpenDaylight Helium or newer to be installed.

Closes-Bug: #1315475
Change-Id: I52e3c9bfdc93b8786c58954beca105e7498e3f40"
421,3764cecfc3b0a5b35634b15a4b049f433a8a22de,1389973532,1.0,1.0,21,5,1,1,1,0.0,True,3.0,3821607.0,38.0,19.0,False,14.0,51966.0,31.0,3.0,99.0,1275.0,1346.0,99.0,1101.0,1172.0,34.0,1112.0,1118.0,0.026041667,0.828125,0.832589286,303,712,1270192,cinder,3764cecfc3b0a5b35634b15a4b049f433a8a22de,1,1, cinder volume hanging,"Bug #1270192 in Cinder: ""cinder volume hanging on removing snapshots""","On any lvm2 version without lvmetad running, I can get cinder-volume to hang on issuing lvm related commands after deleting snapshots (that are non-thin provisioned LVM snapshots with clear_volume set to zero).
the issue is that lvm locks up due to trying to access suspended device mapper entries, and at some point cinder-volume does a lvm related command and hangs on that. a setting of ignore_suspended_devices = 1 in lvm.conf helps with that, as lvremove hangs on scanning the device state (which it
needs to do because it doesn't have current information available via lvmetad).
I can use this script to trigger the issue:
=== cut hang.sh ===
enable_fix=1
#enable_fix=0
vg=cinder-volumes
v=testvol.$$
lvcreate --name $v $vg -L 1g
sleep 2
lvcreate --name snap-$v --snapshot $vg/$v -L 1g
vgp=/dev/mapper/${vg/-/--}-snap--${v/-/--}
sleep 2
( sleep 10 < $vgp-cow ) &
test ""$enable_fix"" -eq ""1"" && lvchange -y -an $vg/snap-$v
lvremove -f $vg/snap-$v
sleep 1
lvremove -f $vg/$v
=== cut hang.sh ===
vg needs to be set to a lvm VG that exists and can take a few gig of space. whenever enable_fix is set to 0, lvremove -f ends with :
  Unable to deactivate open cinder--volumes-snap--testvol.27700-cow (252:5)
  Failed to resume snap-testvol.27700.
  libdevmapper exiting with 1 device(s) still suspended.
this is because the sleep command before keeps a fd open on the -cow. The script then also never finishes and any other lvm command hangs as well.
apparently in real-life this is either udev or the dd command still having the fd open for some reason I have not yet understood.
The deactivation before removing seems to help.","Deactivate LV before removing

With certain versions of LVM2, removing an active LV can end up with

  Unable to deactivate open XXX
  libdevmapper exiting with 1 device(s) still suspended.

which causes any lvm command afterwards to hang endlessly on
trying to access the suspended volume. This seems to be caused
by a race with udev, so lets be conservative and do the deactivation,
then wait for udev and then finish the removal.

Closes-Bug: #1270192

Change-Id: I4703133180567090878ea5047dd29d9f97ad85ab"
422,3783c50dce3917257060dd977f69785238f58078,1410855256,,1.0,10,1,2,2,1,0.994030211,False,,,,,True,,,,,,,,,,,,,,,,,1264,1724,1365392,cinder,3783c50dce3917257060dd977f69785238f58078,1,1,,"Bug #1365392 in Cinder: ""c-vol.log reports ""Error checking replication status""  for non-replication volumes every 1 minutes""","Test Steps:
1. Run Cinder CI with ibm storwize storage, it was found much more ERRORs than before:
10:11:45 *** Not Whitelisted *** 2014-09-04 14:49:54.821 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 438e620d-8b04-49c0-9968-820c2bf8590f
10:11:45 *** Not Whitelisted *** 2014-09-04 14:50:54.769 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume 3c9e9a38-e606-47f7-bbea-c3e84205bd79
10:11:45 *** Not Whitelisted *** 2014-09-04 14:52:54.799 29428 ERROR cinder.volume.manager [-] Error checking replication status for volume fedb9af6-d7dc-4f0d-ad7b-eb28127ac507
2. Manually Create volumes without replication  on IBM storwize v7000u, the volume creation successfully.
3. Check c-vol.log, it was found the below logs every 1 minutes:
2014-09-04 17:07:11.515 ERROR cinder.volume.manager [-] Error checking replication status for volume d2ffa93d-4140-4f1e-bcb7-c70af51781a3
2014-09-04 17:07:11.515 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.515 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.515 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.515 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume d9791267-bb7a-448c-938e-c98dff38bcf5
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
2014-09-04 17:07:11.516 ERROR cinder.volume.manager [-] Error checking replication status for volume fedd3b21-d60e-497e-b309-b125d81b1bcd
2014-09-04 17:07:11.516 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1597, in _update_replication_relationship_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     ctxt, vol)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 756, in get_replication_status
2014-09-04 17:07:11.516 TRACE cinder.volume.manager     return self.replication.get_replication_status(volume)
2014-09-04 17:07:11.516 TRACE cinder.volume.manager AttributeError: 'NoneType' object has no attribute 'get_replication_status'
2014-09-04 17:07:11.516 TRACE cinder.volume.manager
3. Expected result:
There is no replication status update Error for those volumes without replication.","Check replication status failed for non-replication

Need to check self.replication whether is None before the
function get_replication_status be called.

Closes-bug: #1365392

Change-Id: I6f6939a98091e40537f2355bb716b4b3205845e2"
423,378c28a8c05d3943324b5a39edef947ed7a9f52a,1391456542,,1.0,17,3,3,2,1,0.745311576,True,11.0,7736360.0,120.0,44.0,False,43.0,539950.0,88.0,4.0,5.0,4151.0,4153.0,5.0,3434.0,3436.0,3.0,3171.0,3171.0,0.000552486,0.438121547,0.438121547,360,771,1275772,nova,378c28a8c05d3943324b5a39edef947ed7a9f52a,1,1,missing raise statement,"Bug #1275772 in OpenStack Compute (nova): ""Missing ""raise"" statement in the Hyper-V driver when checking the config driver format""","There's a missing raise statement when checking the ConfigDrive format:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L283","Added missing raise statement when checking the config driver format

Closes-bug: #1275772

Change-Id: I636525d0272511d4192c43d18c56e867fb7514d8"
424,379273925517e8b9bf971e018a1dc41f809c4814,1393937947,,1.0,18,2,2,2,1,0.811278124,True,13.0,5947515.0,66.0,15.0,False,17.0,2790992.0,41.0,1.0,0.0,1180.0,1180.0,0.0,948.0,948.0,0.0,1035.0,1035.0,0.000668449,0.692513369,0.692513369,456,871,1283338,cinder,379273925517e8b9bf971e018a1dc41f809c4814,1,1,unstable state,"Bug #1283338 in Cinder: ""create snapshot success by error status volume""","I create a LVM volume with too large size, my cinder-volumes VG's free space is insufficient. so, the volume's status is ""error"".
But, when I try to create a snapshot by this error volume, it' ok, and the snapshot' status is ""available"", progress is ""100%"".
I find that  in cinder.volume.manager.py, create_shapshot, self.driver.create_snapshot return false, but only exception would be hanlde and set to error status.So, after that, the snapshot is ok in db.
some logs show that:
""""""
2014-02-22 10:55:07.439 29737 ERROR cinder.brick.local_dev.lvm [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] Unable to find LV: volume-8cb6eb05-6bfa-4422-a683-2cceb4c4ff1f
2014-02-22 10:55:07.498 29737 INFO cinder.volume.manager [req-6d37df28-1cb6-444c-a1e9-26654905acb9 ca7015d9004a41528498c114e0a6ebf1 55a50a99a42441e585b7a5c34214ecef] snapshot 9e2e363b-a0ed-4be8-9874-ced1c8705fde: created successfully
""""""","Can't force-create snapshot by an non-exist error volume

If we create an LVM volume whose size larger than
rest space in VG, the volume would not exist with
error status.
But then we still can force create a snapshot
by this volume, and the snapshot's status is available.
In the code,I found in create_lv_snapshot,
it return False when fail to get the volume.
However, raising an exception is only way to be
handle outside.
So, we should raise exception instead of return False.

Closes-Bug: #1283338

Change-Id: I80256f19d66da460c95ff23834abb79a557763bf"
425,37b78c5d48a9a5da1e9b0187b2004d5352e7c877,1379449352,1.0,1.0,7,7,1,1,1,0.0,True,2.0,36540.0,10.0,3.0,False,8.0,1086231.0,13.0,3.0,16.0,647.0,647.0,16.0,544.0,544.0,8.0,155.0,155.0,0.025787966,0.446991404,0.446991404,1568,1226803,1226803,neutron,37b78c5d48a9a5da1e9b0187b2004d5352e7c877,1,1, ,"Bug #1226803 in neutron: ""bigswitch plugin doesn't pass context to all network updates""",There are several places in the bigswitch plugin where the db context is not correctly passed down to update network calls so the required data isn't available in the database during certain operations (e.g. floating IP update).,"BigSwitch plugin: passes context to all update_network calls

There were a few calls to update the network on the backend
that were not having the context passed to them so they were
incorrectly using the admin context. This patch corrects that
by passing the context to all network update calls. It also
disallows calls to _send_update_network that don't provide
the context because there is not a use case for that condition.

Closes-Bug: #1226803
Change-Id: I7f0cbb797df9f565d0a00a7c67278cd96301f244"
426,37e775f12592824de17aee73216549bcb182c7cc,1379079813,,1.0,4,3,1,1,1,0.0,True,3.0,216691.0,18.0,7.0,False,14.0,1564432.0,18.0,3.0,126.0,1083.0,1097.0,125.0,985.0,999.0,113.0,939.0,946.0,0.114803625,0.946626385,0.95367573,1488,1217552,1217552,cinder,37e775f12592824de17aee73216549bcb182c7cc,1,1,,"Bug #1217552 in Cinder: ""/tmp space near full causes failure of create from image""","when a cinder create using an --image-id as source for the volume happens in conjunction with inadequate space free on /tmp the create fails due to the conversion to bare format running out of disk space.
converting in /tmp seems to me to be a reasonable choice at first but not when you consider that image files in glance can be quite large. Doesn't cinder need some way to allow defining the conversion space to be somewhere other than /tmp. Many conversions will happen on VM's with small memory and disk footprints. Perhaps the conversion would better take place somewhere defined by the admin?","Use $state_path/conversion for image_conversion_dir default

image_conversion_dir currently defaults to /tmp.  Using
$state_path/conversion seems to be a more appropriate default.

Closes-Bug: #1217552
Change-Id: Iaaa462449a257b1b48fc349399bf409d7301a1e6"
427,38018d32f9db5b9222436d0f860244ce38ba97fb,1383251137,,1.0,34,1,3,3,1,0.878525848,True,14.0,18105764.0,70.0,39.0,False,86.0,251995.0,237.0,10.0,23.0,3395.0,3411.0,23.0,2910.0,2926.0,5.0,2344.0,2347.0,0.000923503,0.360935817,0.361397568,15,327,1243301,nova,38018d32f9db5b9222436d0f860244ce38ba97fb,1,1,"The status should be deleted, but was error","Bug #1243301 in OpenStack Compute (nova): ""Changes-since not returning deleted servers""","Based on the OpenStack documentation at http://docs.openstack.org/api/openstack-compute/2/content/ChangesSince.html...""To allow clients to keep track of changes, the changes-since filter displays items that have been recently deleted. Both images and servers contain a DELETED status that indicates that the resource has been removed.""  This allows OpenStack consumers to determine when images and servers are deleted when using the changes-since support.  OpenStack works as documented for images.  However, changes-since support in OpenStack Havana may not return deleted servers with a DELETED status.
The recreate scenario is as follows:
1) Boot a new server and wait for the boot to complete.
2) Save the current time
3) Use changes-since to get all servers changed since the time saved in step 2.  It should be none.
4) Delete the server created in step 1 and wait for the delete to complete.
5) Use changes-since to get all servers changed since the time saved in step 2.  It should include the server deleted in step 4 and the server's status should be DELETED.  The actual result is that the server is included but its status is ERROR.
For the above recreate scenario, the boot failed because no valid host was found.  As a result, the server is in ERROR status after step 1.  I have not tested with a successful boot.","Ensure deleted instances' status is always DELETED

Currently when deleting failed instances that were never assigned a
host, their vm_state is left in 'error' instead of 'deleted'. This is a
problem when invoking the Index Instances REST API with a Changes-Since
query parameter since that API returns both deleted and non-deleted
instances, but currently the failed deleted instances are incorrectly
marked as status:ERROR instead of status:DELETED.

This patch modifies the server ViewBuilder to ensure the status is
correctly set.

Change-Id: I614714676bab4986564b236d74f2c0d77400e48c
Closes-bug: #1243301"
428,3824051b1e5618388a17c88867a3037397bc96b7,1393590699,,1.0,31,8,2,2,1,0.858230793,True,7.0,6565310.0,99.0,27.0,False,205.0,3847.0,1137.0,2.0,82.0,713.0,735.0,82.0,713.0,735.0,82.0,705.0,727.0,0.011105165,0.094460797,0.097404335,539,960,1288574,nova,3824051b1e5618388a17c88867a3037397bc96b7,0,0,no bug yet. it will be an issue if too many backup failed,"Bug #1288574 in OpenStack Compute (nova): ""backup operation should delete image if snapshot failed""","when we snapshot an instance, we will use @delete_image_on_error to delete any failed snapshot
however, the image will not be removed by backup code flow, it will be an issue if too many backup failed
at last ,all useful image will be removed and we have only 'error' image left in host","Delete image when backup operation failed on snapshot step

when we snapshot an instance, we will use @delete_image_on_error to
delete any failed snapshot, however, the image will not be removed
by backup code flow, it will be an issue if too many backup failed
at last ,all useful image will be removed and we have only 'error'
image left in host

Change-Id: Ib527ade86aef52a8445b57fa03fed434578c4204
Closes-Bug: #1288574"
429,384cce84fde784f9e2f39db49502c66e20ff0b4c,1388981714,,1.0,44,4,2,2,1,0.979868757,True,2.0,3803403.0,29.0,13.0,False,145.0,1589717.0,564.0,5.0,555.0,3041.0,3113.0,513.0,2342.0,2401.0,525.0,2766.0,2830.0,0.075520459,0.397272075,0.406460876,1799,1254174,1254174,nova,384cce84fde784f9e2f39db49502c66e20ff0b4c,1,1, ,"Bug #1254174 in OpenStack Compute (nova): ""bulk-delete-floating-ip does not free used quota""","The bulk-create-floating-ip and bulk-delete-floating-ip commands do not interact with floating_ip quotas.  This is by design, since they're for admins rather than tenants.
However, in one case this causes a bug.  If a tenant initially allocates the floating IP with create-floating-ip and consumed quota, and the admin later deletes the floating Ip with bulk-delete-floating-ip, the floating IP is freed but the quota is still consumed.
So we should change bulk-delete-floating-ip to release any quota that was associated with those floating IP addresses.  (In many cases there will not be any so we need to check.)
This is https://bugzilla.redhat.com/show_bug.cgi?id=1029756 (but that bug is mostly private so won't people outside Red Hat much good).","Make floating_ip_bulk_destroy deallocate quota if not auto_assigned

The expected use case for floating_ip_bulk_destroy is to destroy
auto_assigned floating IPs, for which no quota is used so no quota
needs to be recovered.  But it can also be used to destroy other
(not auto_assigned) floating IPs, in which case the previously used
quota was not recovered.  This fixes the quota leak in that unusual
case.

Closes-Bug: #1254174

Change-Id: Ibff80d9ebac8d4422e401909033da12d9ec0b593"
430,3867174bc82b7fd85dd79bc0cc5625a15df2d8fb,1403096562,,1.0,52,19,2,2,1,0.89301081,False,,,,,True,,,,,,,,,,,,,,,,,985,1424,1331456,neutron,3867174bc82b7fd85dd79bc0cc5625a15df2d8fb,0,0,feature. “Need to add the same ability to auto_schedule_networks.”,"Bug #1331456 in neutron: ""Fix dhcp agent scheduler to be resistant to race coditions""","DHCP ChanceScheduler has _schedule_bind_network method which can detect duplicate entry.
Need to add the same ability to auto_schedule_networks.
That might be important for the case when api server and rpc_server run in different processes.","Fix auto_schedule_networks to resist DBDuplicateEntry

This exception may happen if API and RPC workers are in different
processes.
Also make minor refactoring of auto_schedule_networks method
to avoid unnecessary db queries.
Add missing unit tests and adjust unit test naming style

Change-Id: I6460744e2cffec0b9f009da071597374d8c027f6
Closes-Bug: #1331456"
431,38ba5790fb527967c2fcbaf094e76a73f4b94d38,1410981335,,1.0,126,8,5,5,2,0.859743462,False,,,,,True,,,,,,,,,,,,,,,,,1330,1798,1370680,swift,38ba5790fb527967c2fcbaf094e76a73f4b94d38,1,1,,"Bug #1370680 in OpenStack Object Storage (swift): ""swift ring builder shows nasty stacktrace if builder file is empty  or invalid""","swift-ring-builder command shows nasty stacktraces if the builder file is empty or invalid.
If, its empty file, throws EOF error. if a file is corrupted, throws unpickling error.
Also, during these cases throws an error code of 1, which is wrong since the chosen values for swift-ring-builder is
Exit codes: 0 = operation successful
            1 = operation completed with warnings
            2 = error
-- empty file--
[keshava@Kbook tmp]$ >object.builder
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
EOFError
[keshava@Kbook tmp]$ echo $?
1
-- a corrupted file --
[keshava@Kbook tmp]$ swift-ring-builder object.builder
Traceback (most recent call last):
  File ""/usr/local/bin/swift-ring-builder"", line 6, in <module>
    exec(compile(open(__file__).read(), __file__, 'exec'))
  File ""/opt/stack/swift/bin/swift-ring-builder"", line 24, in <module>
    sys.exit(main())
  File ""/opt/stack/swift/swift/cli/ringbuilder.py"", line 834, in main
    builder = RingBuilder.load(builder_file)
  File ""/opt/stack/swift/swift/common/ring/builder.py"", line 991, in load
    builder = pickle.load(open(builder_file, 'rb'))
cPickle.UnpicklingError: invalid load key, 'n'.
[keshava@Kbook tmp]$ echo $?
1
The error codes needs to be corrected, and also a meaningful user information should be provided if a file is empty or corrupted","Provides proper error handling on builder unpickle

This patch provides the necessary error handling while unpickling
a builder file. Earlier if a builder file is empty/invalid/corrupted,
the stacktrace was shown to user with an exit code of 1. This fixes it
to show a user-friendly message and also returns the exit code of 2,
indicating there was a failure.

Change-Id: I51eb24702c422299629f8053d4591dd10f5863f8
Closes-Bug: #1370680"
432,38e192afecd0d08921d4fb4d7166fa156addb6a4,1392817181,,1.0,1,2,1,1,1,0.0,True,1.0,601103.0,14.0,4.0,False,3.0,488634.0,3.0,2.0,11.0,580.0,591.0,10.0,550.0,560.0,0.0,546.0,546.0,0.000692521,0.378808864,0.378808864,433,848,1282084,cinder,38e192afecd0d08921d4fb4d7166fa156addb6a4,0,0,More pythonic,"Bug #1282084 in Cinder: ""Use len instead of for loop to get the end index""","diff --git a/cinder/tests/test_backup_tsm.py b/cinder/tests/test_backup_tsm.py
index 37d528f..2548cbd 100644
--- a/cinder/tests/test_backup_tsm.py
+++ b/cinder/tests/test_backup_tsm.py
@@ -109,8 +109,7 @@ class TSMBackupSimulator:
             ret_msg += ('Total number of objects deleted:  1\n'
                         'Total number of objects failed:  0')
             retcode = 0
-            for idx, backup in enumerate(self._backup_list[path]):
-                index = idx
+            index = len(self._backup_list[path]) - 1
             del self._backup_list[path][index]
             if not len(self._backup_list[path]):
                 del self._backup_list[path]","Use len instead of for-loop to get the end index

Use len() instead of for-loop to get the end index in file
cinder/tests/test_backup_tsm.py

Change-Id: I3a7b0e418f8af4881e21eb5273a03dbcd317c27d
Closes-Bug: #1282084"
433,38ed0523125efeaf8383b36d1db169b42e2f5eb6,1405522177,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,836,1265,1312718,cinder,38ed0523125efeaf8383b36d1db169b42e2f5eb6,1,1,,"Bug #1312718 in Cinder: ""Temporary snapshot may remain during creating volume by source_volid""","When creating LVM volume by source volume, cinder create source volume's snapshot first, then destination volume and active it, at last copy the volume.
Step:
1. self.create_snapshot
2.self._create_volume
3.self.vg.activate_lv
4.volutils.copy_volume
But If we create or active target volume fail since insufficient free space or other system error for example, the source volume's snapshot wouldn't be remove.
Temporary snapshot would remain, and what is worse we can't delete the source volume successfully.","Don't leave snapshots on the floor

The current cloning process in the LVM
driver uses a snapshot to do the data
copy in the background.  Trouble here
is that if the creation or activation
of the new volume fails, we don't
cleanup the snapshot we created.

Just move the create_volume and activate
calls down into the try block so the finally
clause can do the cleanup for us.

Change-Id: If0fbab9d9f39102cdc7d287527be22c4a2b35934
Closes-Bug: #1312718"
434,3903090a60ecb0170c070a68e2f1df4a01b0aca1,1398799814,,1.0,27,91,3,3,1,0.92917684,True,4.0,684790.0,44.0,9.0,False,21.0,1560163.0,26.0,4.0,20.0,2283.0,2293.0,20.0,2004.0,2014.0,9.0,2202.0,2203.0,0.001264382,0.278543432,0.27866987,840,1269,1313119,nova,3903090a60ecb0170c070a68e2f1df4a01b0aca1,1,0,,"Bug #1313119 in OpenStack Compute (nova): ""nova: links in versions response do not work""","The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2011-01-21T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.compute+xml;version=2""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=2""
         }
      ],
      ""id"":""v2.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/os-compute-devguide-2.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/2/wadl/os-compute-2.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}
The links in the GET /v3 response do not work either:
{
   ""version"":{
      ""status"":""EXPERIMENTAL"",
      ""updated"":""2013-07-23T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.compute+json;version=3""
         }
      ],
      ""id"":""v3.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8774/v3/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/os-compute-devguide-3.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.openstack.org/api/openstack-compute/3/wadl/os-compute-3.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}","Fix broken version responses

The version responses link to non-existing PDF and WADL files.
Remove the WADL link and replace the PDF link with a HTML link to
docs.openstack.org.

Change-Id: I200e0fa268c4357403ca6c158fbbe682080c13d1
Closes-Bug: #1313119"
435,39081787963298be87c1941ab6cba5310f6261c6,1400496436,,1.0,43,2,2,2,1,0.894451885,False,,,,,True,,,,,,,,,,,,,,,,,892,1328,1320056,cinder,39081787963298be87c1941ab6cba5310f6261c6,0,0,Feature “Cinder utils SSHPool should allow customized ssh host keys and missing policy”,"Bug #1320056 in Cinder: ""Cinder utils SSHPool should allow customized ssh host keys and missing policy""","In cinder/utils.py, SSHPool is using paramiko.AutoAddPolicy() as default. This may lead security issue without being notified. The utility should allow customized usage when create the pool or session. Also the host_keys file should be allowed to be customized so that any driver utilizing the SSHPool should have their customized security setting or delegate to customer's scenario & configuration to determine the policy and key files.","SSHPool in utils should allow customized host key missing policy

The cinder/utils SSHPool should allow  missing key policy and host key
file being customized so that any caller can determine by their own
scenario if the host key file can be customized, or if an 'AutoAdd' is
appropriate, or just reject the key when mismatch. This will give more
flexible customization and also prevent any security issue as a middle
man.

Closes-Bug: #1320056
Change-Id: I3c72b0d042de719ecd45429d376bd88d0aefb2cc"
436,396a0e42755af96d8b96d42ae1bcf4ae4604443d,1379646938,,1.0,20,3,2,2,1,0.828055725,True,2.0,295947.0,13.0,6.0,False,1.0,6467082.0,2.0,3.0,128.0,948.0,985.0,128.0,900.0,937.0,69.0,303.0,316.0,0.192307692,0.835164835,0.870879121,1579,1227971,1227971,neutron,396a0e42755af96d8b96d42ae1bcf4ae4604443d,1,1, ,"Bug #1227971 in neutron: ""Incorrect usage of _fields method in provider_configuration.py""","When fields filtering is applied, neutron server prints a trace:
 ERROR neutron.api.v2.resource [-] index failed
 TRACE neutron.api.v2.resource Traceback (most recent call last):
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
 TRACE neutron.api.v2.resource     result = method(request=request, **args)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 273, in index
 TRACE neutron.api.v2.resource     return self._items(request, True, parent_id)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 227, in _items
 TRACE neutron.api.v2.resource     obj_list = obj_getter(request.context, **kwargs)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/servicetype_db.py"", line 63, in get_service_providers
 TRACE neutron.api.v2.resource     return self.conf.get_service_providers(filters, fields)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 162, in get_service_providers
 TRACE neutron.api.v2.resource     return self._fields(res, fields)
 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/services/provider_configuration.py"", line 151, in _fields
 TRACE neutron.api.v2.resource     return dict(((key, item) for key, item in resource.items()
 TRACE neutron.api.v2.resource AttributeError: 'list' object has no attribute 'items'","Fix usage of _fields method in provider_configuration.py

Apply _fields() method to each dict in the list.

Change-Id: I9357212e203843ffa4a05216d9c79fe8831971a5
Closes-Bug: #1227971"
437,39b8bddb1124d16eae15f667f8d921e8ddf5701d,1381298540,,1.0,9,5,1,1,1,0.0,True,7.0,141256.0,24.0,14.0,False,8.0,427388.0,15.0,4.0,780.0,944.0,1425.0,739.0,824.0,1279.0,324.0,179.0,381.0,0.748847926,0.414746544,0.880184332,1656,1237209,1237209,neutron,39b8bddb1124d16eae15f667f8d921e8ddf5701d,1,1, ,"Bug #1237209 in neutron: ""BigSwitch: disassociate floating IP references wrong network""","In the code to disassociate floating IPs in the BigSwitch plugin, it incorrectly updates the tenant's network on the backend controller rather than the external network.","BigSwitch: correct net to backend on floating IP disassociation

The corrects the network that is updated on the backendw when a
floating IP is disassociated. It was incorrectly sending the
tenant's network when the update is to the external network the
floating address belongs to.

Closes-Bug: #1237209
Change-Id: I55211ba3b0062e167843830bb318eb8e46249160"
438,39c04fdc2c94b8ee48600f3148c5f850645b7c4e,1395101206,0.0,1.0,25,16,4,3,1,0.662180572,True,3.0,307015.0,72.0,11.0,False,38.0,30007.0,70.0,3.0,51.0,1221.0,1255.0,51.0,1037.0,1071.0,43.0,535.0,562.0,0.044489383,0.541961577,0.569261881,1451,1204173,1204173,neutron,39c04fdc2c94b8ee48600f3148c5f850645b7c4e,1,1,,"Bug #1204173 in neutron: ""subnet CIDR validation broken for non-root CIDRs again""","Earlier this month a fix was released to allow non-root CIDRs to be entered.  The fix was for bug 1188845.  It was approved and merged.  Now, however, the functionality has been reversed by commit 53a66b299f18a7184972502b43441a5ad7b050fd (bug 1195974) which checks the input earlier in the path and rejects anything but the root CIDR.
I am unable to find documentation that says you cannot use a non-root (I.E. X.X.X.254) IP to specify a subnet.  With an IP and subnet mask it is possible to create a network and determine the root for the subnet.","Allow CIDRs with non-zero masked portions

Allow users to specify CIDRs with bits other
than zeros in the masked portion of the subnet.
e.g. 192.168.1.5/24 is accepted and converted
to 192.168.1.0/24.

Closes-Bug: #1204173
Change-Id: I7ddff41e6988feb6e2a87e40a4d99db7174415b1"
439,3a5919fd4af6c3b772397a5e7d90eebdf9b371af,1407506115,,1.0,8,5,2,2,1,0.619382195,False,,,,,True,,,,,,,,,,,,,,,,,1166,1623,1354448,nova,3a5919fd4af6c3b772397a5e7d90eebdf9b371af,1,0,“The Hyper-V driver does not support resize down and is currently”,"Bug #1354448 in OpenStack Compute (nova): ""The Hyper-V driver should raise a InstanceFaultRollback in case of resize down requests""","The Hyper-V driver does not support resize down and is currently rising an exception if the user attempts to do that, causing the instance to go in ERROR state.
The driver should use the recently introduced instance faults ""exception.InstanceFaultRollback"" instead, which will leave the instance in ACTIVE state as expected.","Fixes Hyper-V resize down exception

The Hyper-V driver does not support resize down and is currently
raising an exception if the user attempts to do that, causing the
instance to go in ERROR state.

The driver should use the recently introduced instance faults
""exception.InstanceFaultRollback"" instead, which will leave the
instance in ACTIVE state as expected.

Closes-Bug: #1354448

Change-Id: Ibaf8482562094cd2b3165dc62a907fa9e0e56e19"
440,3a5e1faee04671f2e88b28d805b191b480054254,1380654733,,1.0,48,2,2,2,1,0.904381458,True,2.0,822666.0,27.0,14.0,False,36.0,39683.0,138.0,6.0,801.0,2237.0,2701.0,787.0,1856.0,2306.0,782.0,2025.0,2480.0,0.126883811,0.328309836,0.402041808,1624,1233789,1233789,nova,3a5e1faee04671f2e88b28d805b191b480054254,0,0,"“will result in verbose exception” bug in the future, refactoring","Bug #1233789 in OpenStack Compute (nova): ""Object actions via conductor will result in verbose exception logging""",See  http://logs.openstack.org/87/44287/9/check/gate-tempest-devstack-vm-full/c3a07eb/logs/screen-n-cond.txt.gz,"Avoid spamming conductor logs with object exceptions

Conductor's logs should include tracebacks only when something
unexpected happened, which is why we have the client_exceptions()
decorator. The object_action() and object_class_action() methods
are used for direct remoting of object methods, and thus really
should forward *any* exception to the client.

This patch does that, and also adds missing tests for these two
methods to verify the normal and exception-wrapped behavior.

Closes-bug: #1233789
Closes-bug: #1084706
Change-Id: I505462fa429a6aa68e7b8a08ec2b704bf18d029c"
441,3a915546507e9a5617ada90546342a9239735ff8,1397490977,,1.0,19,17,2,2,1,0.918295834,True,1.0,68518.0,10.0,3.0,False,38.0,343809.0,84.0,3.0,654.0,2221.0,2784.0,368.0,2014.0,2304.0,481.0,2175.0,2575.0,0.061723652,0.278652836,0.329875784,794,1221,1307582,nova,3a915546507e9a5617ada90546342a9239735ff8,1,1,""" This reverts commit 79ab96e34ba5b8dd3e4e542dd3a7f65624b13367.”","Bug #1307582 in OpenStack Compute (nova): ""FixedIp object does not support Neutron""","We recently landed a patch to make the metadata API's base.py use the FixedIP object for address lookups.
This causes problems when using the metadata API with neutron because the Nova FixedIP object class does not yet appear to support neutron.
When using the metadata server today (with Neutron) you'll see the following:
Apr 14 15:36:40 localhost nova-api[3373]: 2014-04-14 15:36:40.035 3505 ERROR nova.api.metadata.handler [-] Failed to get metadata for ip: 172.19.0.5","Revert ""Fix network-api direct database hits in metadata server""

This reverts commit 79ab96e34ba5b8dd3e4e542dd3a7f65624b13367.

This commit broke the Nova metadata API when using neutron.

Change-Id: I6855b03f00492fdf51e18c0dcb0a73590a71b374
Closes-bug: #1307582"
442,3ad414597043979b8d73c17c05e666cff33a9b88,1395202475,,1.0,35,4,2,2,1,0.73206669,True,3.0,4374032.0,33.0,10.0,False,14.0,540079.0,16.0,4.0,860.0,2931.0,3006.0,724.0,2579.0,2633.0,325.0,2369.0,2436.0,0.042686919,0.310331282,0.31910436,645,1071,1294346,nova,3ad414597043979b8d73c17c05e666cff33a9b88,1,1,keyerror,"Bug #1294346 in OpenStack Compute (nova): ""When creating Neutron Security Group Rules with a Protocol other than TCP/UDP/ICMP, breaks nova secgroup-* calls ""","With the following set in /etc/nova/nova.conf:
security_group_api=neutron
You can view security groups and rules that have been created in Neutron with nova secgroup-* commands.
If you create a Neutron Security Group rule with a different protocol though, nova secgroup-* calls fail with a 500 and a lot of stack trace in /var/log/nova/nova-api-os-compute.log:
<snip>
014-03-18 20:23:46.599 25278 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/security_groups.py"", line 215, in _format_security_group_rule
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack     sg_rule['from_port'] = rule['from_port']
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack KeyError: 'from_port'
2014-03-18 20:23:46.599 25278 TRACE nova.api.openstack
2014-03-18 20:23:46.600 25278 INFO nova.api.openstack [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] http://10.240.0.100:8774/v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups returned with HTTP 500
2014-03-18 20:23:46.601 25278 INFO nova.osapi_compute.wsgi.server [req-507402d7-788e-413a-a005-b852e1b7efa2 3d0524290859416f886f49a2973ab616 1be2c0f9589d4822856a9ac2e16f0406] 10.240.0.100 ""GET /v2/1be2c0f9589d4822856a9ac2e16f0406/os-security-groups HTTP/1.1"" status: 500 len: 335 time: 0.0474379
To recreate:
# Test nova secgroup-list works
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id                                   | Name        | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default     | default     |
+--------------------------------------+-------------+-------------+
# Add rule with a different protocol
neutron security-group-rule-create --direction ingress --protocol 50 --remote-ip-prefix 0.0.0.0/0 ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9
Created a new security_group_rule:
+-------------------+--------------------------------------+
| Field             | Value                                |
+-------------------+--------------------------------------+
| direction         | ingress                              |
| ethertype         | IPv4                                 |
| id                | d98e83cf-2aab-4eec-89ed-f9aa4d00d57b |
| port_range_max    |                                      |
| port_range_min    |                                      |
| protocol          | 50                                   |
| remote_group_id   |                                      |
| remote_ip_prefix  | 0.0.0.0/0                            |
| security_group_id | ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 |
| tenant_id         | 1be2c0f9589d4822856a9ac2e16f0406     |
+-------------------+--------------------------------------+
# Test
neutron security-group-list # works
nova secgroup-list # now errors
# Delete rule
neutron security-group-rule-delete d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
Deleted security_group_rule: d98e83cf-2aab-4eec-89ed-f9aa4d00d57b
# Test nova again
nova secgroup-list
+--------------------------------------+-------------+-------------+
| Id                                   | Name        | Description |
+--------------------------------------+-------------+-------------+
| ebfd4f04-00f7-459e-8f5b-e6f03aa2fec9 | default     | default     |
+--------------------------------------+-------------+-------------+","Fix KeyError if neutron security group is not TCP/UDP/ICMP and no ports

Previously, if a neutron security group rule was created that was not
TCP/UDP/ICMP and did not contain a port_range_min/max retrieving it from
nova-api would result in a KeyError in nova-api. This patch resolves
this issue.

Change-Id: I6284a7a8690aec9509b63f7cbd18812e09ef3fdd
Closes-bug: #1294346"
443,3ae0171834521a2152516e4bd7b0f8e46547bfda,1381854955,,1.0,165,117,2,2,1,0.689602498,True,9.0,881961.0,83.0,25.0,False,2.0,4325000.0,2.0,4.0,9.0,1068.0,1068.0,9.0,1008.0,1008.0,2.0,393.0,393.0,0.006622517,0.869757174,0.869757174,1681,1239288,1239288,neutron,3ae0171834521a2152516e4bd7b0f8e46547bfda,0,0,Refactoring “Removing workflows from the Radware driver code”,"Bug #1239288 in neutron: ""Radware LBaaS driver - remove ""workflows"" from code base""","In order to gain more flexibility we decide to remove the ""workflows"" from the driver ""side"" and to load them on the ""vDirect side"".
This will give us more flexibility when we need to customize our solution.
With this change the driver will no longer updload the workflows into vDirect server.
It will only make sure that thw workflows are loaded on the vDirect server side.","Removing workflows from the Radware driver code

Removing workflows handling from the driver code.
Removing workflow related files
Modifying tests to support new behavior

Change-Id: Icbb6106db07e5b33c37192aa53d088e67bd4a795
Closes-bug: #1239288"
444,3b0ac61dea7178e858c6dede44a13818c5162283,1410442338,,1.0,7,7,4,2,1,0.83224889,False,,,,,True,,,,,,,,,,,,,,,,,1302,1768,1368234,neutron,3b0ac61dea7178e858c6dede44a13818c5162283,0,0,Bug in test,"Bug #1368234 in neutron: ""faulty way of checking if mock wasn't called""",Some UT use <mock>.assert_has_calls([]) as a way to check if mock wasn't called - this doesn't work because assert_has_calls only checks if passed calls are present in mock_calls and hence it is always true regardless of whether mock was called or not. This can lead to falsely passed tests.,"Remove faulty .assert_has_calls([])

Some UT use <mock>.assert_has_calls([]) as a way to check if mock
wasn't called - this doesn't work because assert_has_calls only checks
if passed calls are present in mock_calls and hence it is always true
regardless of whether mock was called or not. This can lead to falsely
passed tests.

Change-Id: I1be5327854cc3dc2f5b3733f2bad78200cfbbfd2
Closes-Bug: #1368234"
445,3b2842bf2df46daf4e4d26695c06963499968181,1386058757,,1.0,8,8,8,3,1,1.0,True,2.0,156233.0,14.0,6.0,False,14.0,6107117.75,23.0,3.0,38.0,681.0,698.0,38.0,638.0,655.0,27.0,641.0,647.0,0.02306425,0.528830313,0.533772652,127,526,1257198,cinder,3b2842bf2df46daf4e4d26695c06963499968181,0,0,it's necessary to unify all ,"Bug #1257198 in Cinder: ""All Cinder Public API controllers should inherit from wsgi.Controller""","Most of the Cinder Public API Controllers  inherit from wsgi.Controller, but still some Cinder Public APIs controllers directly inherit from object. So it's necessary to unify all the Cinder Public API Controllers inherit from wsgi.Controller.","All API controllers inherit from wsgi.Controller

Most of the Cinder Public API Controllers inherit from wsgi.Controller,
but still some Cinder Public APIs controllers directly inherit from
object. So it's necessary to unify all the Cinder Public API Controllers
inherit from wsgi.Controller.
1. For code cleanup.
2. When we try to do body valid check in api's put/post method later, we
can use the helper function is_valid_body instead of using try...catch
block.

Change-Id: I88812224e0b182939cfccce63982cb8f95792891
Closes-Bug: #1257198"
446,3ba80960075bf7873e96914f1be54ae176069a8d,1377812132,1.0,1.0,21,21,1,1,1,0.0,True,1.0,8067.0,6.0,2.0,False,3.0,2629121.0,3.0,2.0,9.0,920.0,922.0,9.0,845.0,847.0,9.0,225.0,227.0,0.03875969,0.875968992,0.88372093,1497,1218621,1218621,neutron,3ba80960075bf7873e96914f1be54ae176069a8d,0,0,Test files,"Bug #1218621 in neutron: ""Fix unsuitable assertTrue/assertFalse in api unittest""","Some usages of assertTrue()/assertFalse() are incorrect, improve them to more explicit assert  from the unit test suite (like assertIsNotNone or assertIn).","Fix unsuitable assertTrue/assertFalse in api ut

Some usages of assertTrue()/assertFalse() are incorrect, improve
them to more explicit assert from the unit test suite.

Closes-Bug: #1218621
Change-Id: I22be2af05f06e91020dbaa4cf08bf395d68f563b"
447,3bbdc70d4e1d0a8c960d7d0a7634f10b4b3163fe,1394853241,2.0,1.0,40,4,2,2,1,0.811278124,True,4.0,801122.0,33.0,5.0,False,7.0,190513.0,11.0,2.0,69.0,333.0,341.0,69.0,328.0,336.0,62.0,294.0,297.0,0.052719665,0.246861925,0.249372385,625,1050,1292764,glance,3bbdc70d4e1d0a8c960d7d0a7634f10b4b3163fe,1,1,,"Bug #1292764 in Glance: ""Tempest failure: tempest.api.image.v1.test_images.CreateRegisterImagesTest.test_register_then_upload""","This failure happens with the vmware backend when the size provided to the store is zero but the actual data size is larger than zero.
2014-03-14 17:22:53 | [10.36.11.112] out: Traceback (most recent call last):
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""tempest/api/image/v1/test_images.py"", line 50, in test_register_then_upload
2014-03-14 17:22:53 | [10.36.11.112] out:     self.assertEqual(1024, body.get('size'))
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 321, in assertEqual
2014-03-14 17:22:53 | [10.36.11.112] out:     self.assertThat(observed, matcher, message)
2014-03-14 17:22:53 | [10.36.11.112] out:   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-14 17:22:53 | [10.36.11.112] out:     raise mismatch_error
2014-03-14 17:22:53 | [10.36.11.112] out: MismatchError: 1024 != 0","VMware store.add to return the image size uploaded

This patch fixes a tempest test failing for the VMware datastore.
When the size provided to add() is zero but the image size is actually
larger than zero, add() should return the size of the image uploaded
and not zero.

Tempest failure:
api.image.v1.test_images.CreateRegisterImagesTest.test_register_then_upload

Change-Id: I16524c0f25aa4124c24ca25ac014819267dd72c3
Closes-Bug: #1292764"
448,3be1d7a75c5ec754825e99e0a8d95b4e1521ae4b,1396457613,,1.0,56,7,2,2,1,0.969063253,True,9.0,3558911.0,126.0,12.0,False,2.0,168585.0,3.0,6.0,111.0,1105.0,1147.0,111.0,966.0,1008.0,104.0,545.0,585.0,0.097132285,0.505087882,0.542090657,716,1142,1301035,neutron,3be1d7a75c5ec754825e99e0a8d95b4e1521ae4b,1,1,,"Bug #1301035 in neutron: ""Nova notifier thread does not run in rpc_worker sub-processes""","This reported to me today by Maru.  When an rpc worker is spawned as a sub-process, that happens after the nova notifier thread has already started.
eventlet.hubs.use_hub() is the call in neutron/openstack/common/service.py that causes all thread execution to stop.
From the event let documentation:  ""Make sure to do this before the application starts doing any I/O! Calling use_hub completely eliminates the old hub, and any file descriptors or timers that it had been managing will be forgotten.""
Maru's observation is that this means that thread should not spawn before forking the process if they need to run in the child process.  I agree.
The reason that threads spawn is that the plugin gets loaded prior to forking and the thread for the nova notifier is started in the __init__ method of a sub-class of the plugin.","Replace loopingcall in notifier with a delayed send

The loopingcall thread here was started before processes fork and so
the thread stops working after the fork call.  This is a problem that
will probably need to be worked out in the long run.

To ensure that this notifier works correctly in all processes, this
change replaces the persistent loopingcall thread with a thread
created on demand to delay and batch up notifications.  The first
notification will trigger spawning the thread to wait to send it.  Any
notifications that come in the meantime will notice that there is
already a thread waiting to send and will return without spawning.

Change-Id: I519d4e89b8cee341c0e1cfffbce3e77151e8202a
Closes-Bug: #1301035"
449,3be220539a529572f30f3345fe80f2aa572730a3,1380123064,,1.0,11,7,2,2,1,0.503258335,True,4.0,509933.0,34.0,10.0,False,2.0,77039.0,3.0,3.0,23.0,652.0,654.0,23.0,636.0,638.0,23.0,617.0,619.0,0.023166023,0.596525097,0.598455598,1604,1230296,1230296,cinder,3be220539a529572f30f3345fe80f2aa572730a3,1,0,“errors occur when attaching volumes with Huawei HVS iSCSI driver.”,"Bug #1230296 in Cinder: ""HTTPS error occurs when attaching a volume with Huawei HVS driver""","HTTPS errors occur when attaching volumes with Huawei HVS iSCSI driver.
If the iSCSI initiator is not added to a host, the error will occurs.
But if the iSCSI initiator was added to a host already, no error occurs when attaching volumes to this host.","Fix Huawei HVS driver attaching volume error

If iSCSI initiator is not added to host, we will get https errors
for we find the initiator info by sending url with this initiator name.

This patch fixes the way of getting the initiator info.
First, get the all initiator info.
Then, find the one we need by name.

Closes-bug: #1230296
Change-Id: I92620374923fa136ee71fe6eb3af6e4c78a3d66b"
450,3c0025abf4581dc3561637e802de7bba1434d2b3,1392215114,0.0,1.0,53,35,15,12,1,0.932063701,True,4.0,3152984.0,78.0,14.0,False,36.0,1726727.067,76.0,4.0,459.0,1546.0,1717.0,387.0,1369.0,1499.0,202.0,688.0,719.0,0.257614213,0.874365482,0.913705584,404,818,1279813,neutron,3c0025abf4581dc3561637e802de7bba1434d2b3,0,0,'I don't see any issues due to this but it is better to be fixed.’,"Bug #1279813 in neutron: ""excutils.save_and_reraise_exception should be used when reraising an exception""","excutils.save_and_reraise_exception should be used when reraising an exception, as described in openstack.common.excutils.
I don't see any issues due to this but it is better to be fixed.","Use save_and_reraise_exception when reraise exception

This fixes reraise in pluign codes.

Closes-Bug: #1279813
Change-Id: Iee174d94c0ce69eb01eb86eea1a903eceb7569d5"
451,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1385475777,,2.0,112,77,2,2,1,0.989290885,True,19.0,9905029.0,273.0,55.0,False,11.0,2000266.0,11.0,3.0,0.0,510.0,510.0,0.0,474.0,474.0,0.0,223.0,223.0,0.001751313,0.392294221,0.392294221,86,483,1255058,neutron,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1,1,  Bugfix and refactoring,"Bug #1255058 in neutron: ""ovs_lib.OVSBridge mod_flow() method doesn't work""","During my OVS-related prototyping I spotted this. For bug verification I used 'ovs_lib' as a standalone python package (without Openstack runnig).
OVS info:
    ovs-ofctl (Open vSwitch) 1.4.0+build0
    Compiled Feb 18 2013 13:13:22
    OpenFlow versions 0x1:0x1
I can define flow with no problems:
    from neutron.agent.linux import ovs_lib
    int_br = ovs_lib.OVSBridge('br-int', root_helper='sudo')
    int_br.add_flow(priority=777,dl_type=2048, actions='normal')
Run 'ovs-ofctl dump-flows br-int'
    'cookie=0x0, duration=115.956s, table=0, n_packets=0, n_bytes=0, priority=777,ip actions=NORMAL'
However when I try to modify this flow:
    int_br.mod_flow(dl_type=2048, actions='drop')
I see
    'Command: ['sudo', 'ovs-ofctl', 'mod-flows', 'br-int', 'hard_timeout=0,idle_timeout=0,priority=0,dl_type=2048,actions=drop']
    Exit code: 1
    Stdout: ''
    Stderr: 'ovs-ofctl: unknown keyword hard_timeout\n'","Bugfix and refactoring for ovs_lib flow methods

Remove hardcoded flow parameters from
'_build_flow_expr_str' method, so we can
define any flows we want and can rely on 'ovs-ofctl'
command to verify flow arguments correctness.
When building flow string inside _build_flow_expr_str
use the following approach:
1. Build prefix and remove prefix params from flow_dict.
2. Build postfix (actions) and remove 'actions' from
flow dict.
3. Inside the loop build flow array from everything
what's left in flow_dict.
4. Append postfix (actions) to the flow array.
5. 'Join' flow array into flow string.

Change _build_flow_expr_str() to be a function
instead of an object method because 'self'
parameter  wasn't used.

Remove 'add_or_mod_flow_str' method because
we have to use separate logic when bulding flow
strings for 'add_flow' and 'mod_flow' methods.

Add more unit tests for OVSBridge class.

Closes-Bug: #1255058
Closes-Bug: #1240572

Change-Id: Ic89221d006a626aa2fc40314a9acffc0ea6fd61c"
452,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1385475777,,2.0,112,77,2,2,1,0.989290885,True,19.0,9905029.0,273.0,55.0,False,11.0,2000266.0,11.0,3.0,0.0,510.0,510.0,0.0,474.0,474.0,0.0,223.0,223.0,0.001751313,0.392294221,0.392294221,1699,1240572,1240572,neutron,3c00dd43f613c838f713a7cbf3cedb6767a8c52a,1,1, ,"Bug #1240572 in neutron: ""ovs_lib.OVSBridge flow managment methods poor design""","Unlike 'set_db_attribute()' and 'clear_db_attribute' methods which don't restrict our choice of desired attributes, flow managment methods such as 'add_flow', 'mod_flow' and 'delete_flows' internally make use of '_build_flow_expr_arr()' method, which validates and limits flow parameters. Beside from obvious inconsistency I see the following problems:
- If we misspell flow parameter it just will be ignored, and instead of exception (this is what we would expect) wrong flow will be defined.
- To add more flow options we have to keep hardcoding them in '_build_flow_expr_arr()'
My proposition is to use similar to 'set_db_attribute()' method approach - don't restrict which options we can use, just pack them and execute the command.","Bugfix and refactoring for ovs_lib flow methods

Remove hardcoded flow parameters from
'_build_flow_expr_str' method, so we can
define any flows we want and can rely on 'ovs-ofctl'
command to verify flow arguments correctness.
When building flow string inside _build_flow_expr_str
use the following approach:
1. Build prefix and remove prefix params from flow_dict.
2. Build postfix (actions) and remove 'actions' from
flow dict.
3. Inside the loop build flow array from everything
what's left in flow_dict.
4. Append postfix (actions) to the flow array.
5. 'Join' flow array into flow string.

Change _build_flow_expr_str() to be a function
instead of an object method because 'self'
parameter  wasn't used.

Remove 'add_or_mod_flow_str' method because
we have to use separate logic when bulding flow
strings for 'add_flow' and 'mod_flow' methods.

Add more unit tests for OVSBridge class.

Closes-Bug: #1255058
Closes-Bug: #1240572

Change-Id: Ic89221d006a626aa2fc40314a9acffc0ea6fd61c"
453,3c3e6eef90c87440c87803dba3325e7cc8c79a17,1383231330,0.0,1.0,4,3,1,1,1,0.0,True,4.0,103290.0,38.0,17.0,False,14.0,1312874.0,20.0,2.0,510.0,933.0,933.0,421.0,821.0,821.0,160.0,240.0,240.0,0.312621359,0.467961165,0.467961165,43,371,1246788,neutron,3c3e6eef90c87440c87803dba3325e7cc8c79a17,1,0,postgres specific,"Bug #1246788 in neutron: ""NotSupportedError on postgres db backend""","Change I1e8a87d7dc1a1cb9309aeefd41619e20f49f95a6 has introduced another of those SELECT FOR UPDATE NOT SUPPORTED errors in postgres. Even though this error occurs, that does not seem to upset the gate. This is one of the logs where it occurs:
http://logs.openstack.org/52/54752/1/check/check-tempest-devstack-vm-neutron-pg-isolated/8c288a4/logs/screen-q-svc.txt.gz
And one of the runs where it passes:
https://review.openstack.org/#/c/54752/1","Fix query error on dhcp release port for postgresql

This is achieved by disabling eager loading and
hence by avoiding incurring in left outer joins
that cannot be applied because of a nullable
side.

Closes-bug: #1246788

Change-Id: I16bcc738a43609f715c2561413c6387f443bd99c"
454,3c503adcf703db7b772208389b43ee09cee66185,1409727154,,1.0,59,5,2,2,1,0.337290067,False,,,,,True,,,,,,,,,,,,,,,,,1263,1723,1365352,neutron,3c503adcf703db7b772208389b43ee09cee66185,1,1,,"Bug #1365352 in neutron: ""metadata agent does not cache auth info""","metadata agent had tried to cache auth info by the means of ""self.auth_info = qclient.get_auth_info()"" in _get_instance_and_tenant_id(), however this qclient is not the exact one which would be used in inner methods,  In short, metadata agent does not implement auth info caching correctly but still retrieves new token from keystone every time.","Fix metadata agent's auth info caching

metadata agent does not implement auth info cache correctly but
retrieves from keystone every time

Change-Id: Ifc1f580185d7600b48aaf80d112fc80e0c4253f2
Closes-bug: #1365352"
455,3c876c67eb32ecffb58a2234c828e5cfee70ec90,1388739110,,1.0,9,9,1,1,1,0.0,True,2.0,141141.0,11.0,4.0,False,44.0,792886.0,100.0,2.0,45.0,628.0,648.0,45.0,589.0,609.0,34.0,597.0,606.0,0.027047913,0.462132921,0.469088099,249,653,1265725,cinder,3c876c67eb32ecffb58a2234c828e5cfee70ec90,0,0,Remoe unused parameters,"Bug #1265725 in Cinder: ""remove unused context parameter in check_*** methods of cinder/volume/api.py ""","Input parameter 'context' of Check_*** methods(check_attach, check_detach, _check_metadata_properties, _check_volume_availability) in cinder/volume/api.py are not being used, so remove this 'context' parameter.","removed unused context in check_*** methods

Input parameter 'context' of Check_*** methods(check_attach,
check_detach, _check_metadata_properties, _check_volume_availability)
in cinder/volume/api.py are not being used, so remove this 'context'
parameter.

Change-Id: I314f662f2fe54a6b073181ff5c0f0214e9ef822a
Closes-bug: #1265725"
456,3c88fce604959a68f48d71274e0d93b74da17e34,1398096956,,1.0,32,14,2,2,1,0.713146749,True,1.0,98463.0,5.0,1.0,False,50.0,1565669.0,106.0,1.0,37.0,1069.0,1090.0,33.0,1068.0,1085.0,37.0,1063.0,1084.0,0.004848176,0.135748916,0.13842817,1537,1223452,1223452,nova,3c88fce604959a68f48d71274e0d93b74da17e34,1,1, ,"Bug #1223452 in OpenStack Compute (nova): ""TrustedFilter checks compute trust level, not hypervisors""","The TrustedFilter uses host_state.host as the name that will be checked against the remote attestation service.
This works for the KVM case because the compute node and the hypervisor are the same; however we must be checking host_state.nodename which is the hostname for the hypervisor which will be registered with the attestation server.","Use hypervisor hostname for compute trust level

In XenAPI, service hostname and compute node hostname is different
because the Nova compute service may run in a separated VM and is
different with the hostname of the compute node.

The remote attestation service use the compute node's hostname because
it's the compute node that will run the servers.

Closes-Bug: #1223452

Change-Id: I9a7ce74d595531196804615a8947e253b0bd3f1a"
457,3ca85460a178a18dea60399fd369ed84b17721dc,1408977872,,1.0,14,2,3,3,1,0.70767653,False,,,,,True,,,,,,,,,,,,,,,,,1044,1488,1338479,neutron,3ca85460a178a18dea60399fd369ed84b17721dc,1,0,"“ In SQLite Integer can be stored in 1, 2, 3, 4, 6, or 8 bytes depending
    on the magnitude of the value. “","Bug #1338479 in neutron: ""Unhelpful error message when updating quota""","When updating network quota using the following command:
neutron quota-update --network 10000000000
the client ouputs:
""Request Failed: internal server error while processing your request.""
This request fails since the parameter exceeds the integer range. An error message like ""Request Failed: quota limit exceeds integer range"" would be more friendly to users than just raising a single internal server error.","Fix quota limit range validator

In Quota model limit is defined with Integer type which in case of
MySQL and PostgreSQL is always stored in 4 bytes. At the same time,
the current validator checks that the value does not exceed sys.maxsize
which depends on whether the system is 32-bit or 64-bit based.

In SQLite Integer can be stored in 1, 2, 3, 4, 6, or 8 bytes depending
on the magnitude of the value. Nevertheless, assume that it can not
exceed 4 bytes for consistency.

Limited the upper bound of the validator with 2**31 - 1.

Added a unit test.

Closes-Bug: #1338479
Change-Id: Icefa2fc228e4255a022d586cab4590607953d1ee"
458,3cb9168822a0805a9b8904ce54577f3e3f05d9eb,1385591075,,1.0,72,230,4,3,1,0.572194795,True,1.0,647200.0,10.0,4.0,False,11.0,534635.5,18.0,4.0,18.0,991.0,992.0,18.0,919.0,920.0,13.0,479.0,479.0,0.023931624,0.820512821,0.820512821,1435,1190621,1190621,neutron,3cb9168822a0805a9b8904ce54577f3e3f05d9eb,0,0,Test files,"Bug #1190621 in neutron: ""Improve unit test coverage for Cisco plugin base code""","Improve unit test coverage for ...
quantum/plugins/cisco/network_plugin	199	131	0	17	2	34%","Improve unit test coverage for Cisco plugin base code

Closes-Bug: #1190621

This fix improves test code coverage for the file:
neutron/plugins/cisco/network_plugin.py
from 34% to 98%.

Some of the changes made to realize this code coverage improvements
are the following:
- Core API methods (create_network, update_network, etc.) have been
  removed from the core plugin (network_plugin.py). These methods
  are currently unused. Before this change, we relied on the underlying
  model layer to inform the core plugin whether or not the model layer
  was capable of handling these core API calls itself via a 'MANAGE_STATE'
  attribute. However, there is only one existing model layer
  implementation (i.e. for Nexus plugin; the N1KV plugin does not use
  network_plugin.py), and that model layer supports the core API calls.
  Given that it is unlikely that another model layer for the Cisco Nexus
  plugin will ever be developed (esp. with the availability of the ML2
  plugin), so it makes more sense to delete this untested code.
- Exception raising for non-existent credentials has been removed from
  get_credential_details and rename_credential methods since
  exceptions are already raised for this condition in lower-level
  credential database code.
- The schedule_host, associate_port, and detach_port methods are
  deleted because these essentially do nothing useful (log a debug
  message and then return None), since these methods look for
  the same-named methods in the model layer, but these methods are
  not defined in the model layer.
- The helper functions _invoke_device_plugins and _func_name are
  deleted because they are no longer used.
- In unit test code for the Cisco QoS and credentials database, calls
  are now made indirectly through the core plugin (network_plugin.py)
  extension API methods, as a quick-and-easy way to provide code
  coverage for these core plugin methods.

Change-Id: I0c0d9e2b251a7c0355d83e495912302c5cc4d032"
459,3cc149a802346bef063bcb865182e4e21ec8b086,1379487568,,1.0,1,2,1,1,1,0.0,True,2.0,21946.0,18.0,9.0,False,13.0,660782.0,16.0,5.0,10.0,1139.0,1139.0,10.0,1038.0,1038.0,7.0,944.0,944.0,0.007920792,0.935643564,0.935643564,1572,1226959,1226959,cinder,3cc149a802346bef063bcb865182e4e21ec8b086,1,1, ,"Bug #1226959 in Cinder: ""Argument of exception.GlanceConnectionFailed is incorrect in glance.py""","cinder/cinder/image/glance.py is used as follows.
  raise exception.GlanceConnectionFailed(netloc=netloc,
                                         reason=str(e))
'netloc' is always ignored.","Fixes call GlanceConnectionFailed in invalid ARG

A mapping key of 'netloc' is always ignored. Thus information in
'netloc' is lost. But because it is included in 'error_msg', I remove
'netloc=netloc'.

Change-Id: I937044bcca0cb196fcd96cfa421de777713d76f5
Closes-Bug: #1226959"
460,3ccb676dffea46b4ecfab35e16c0e96642353ee7,1388316939,,1.0,2,2,1,1,1,0.0,True,3.0,169715.0,32.0,17.0,False,38.0,1036062.0,66.0,6.0,1453.0,3764.0,4607.0,1139.0,3118.0,3735.0,1405.0,2732.0,3538.0,0.202535292,0.393690579,0.509795448,210,613,1262858,nova,3ccb676dffea46b4ecfab35e16c0e96642353ee7,1,1,"It says deprecated, but its a typo of a deprecated typo","Bug #1262858 in OpenStack Compute (nova): ""Wrong deprecated name for images_rbd options""","In nova/virt/libvirt/imagebackend.py, the images_rbd_pool and images_rbd_ceph_conf have deprecated_name set to libvirt_images_rdb_pool and libvirt_images_rdb_ceph_conf, respectively, but the actual option names prior to commit 25a7de2054ba6ae5eb318c86fe165f4302fbfff8 are libvirt_images_rbd_pool and libvirt_images_rbd_ceph_conf. (Note the transposition of the d and b in rbd.)","Fix typo'ed deprecated flag names in libvirt.imagebackend

In nova/virt/libvirt/imagebackend.py, the images_rbd_pool
and images_rbd_ceph_conf have deprecated_name set to
libvirt_images_rdb_pool and libvirt_images_rdb_ceph_conf,
respectively, but the actual option names prior to commit
25a7de2054ba6ae5eb318c86fe165f4302fbfff8 are
libvirt_images_rbd_pool and libvirt_images_rbd_ceph_conf.
(Note the transposition of the d and b in rbd.)

Closes-bug: #1262858

Change-Id: I47f4ff6e03674c825336d26b610eeff89e48d21e"
461,3cd2163d5105faad389bee5175ef446f0bb90289,1411464316,,1.0,24,20,3,3,1,0.889896768,False,,,,,True,,,,,,,,,,,,,,,,,1343,1811,1372438,neutron,3cd2163d5105faad389bee5175ef446f0bb90289,1,1,,"Bug #1372438 in neutron: ""Race condition in l2pop drops tunnels""","The issue was originally raised by a Red Hat performance engineer (Joe Talerico)  here: https://bugzilla.redhat.com/show_bug.cgi?id=1136969 (see starting from comment 4).
Joe created a Fedora instance in his OS cloud based on RHEL7-OSP5 (Icehouse), where he installed Rally client to run benchmarks against that cloud itself. He assigned a floating IP to that instance to be able to access API endpoints from inside the Rally machine. Then he ran a scenario which basically started up 100+ new instances in parallel, tried to access each of them via ssh, and once it succeeded, clean up each created instance (with its ports). Once in a while, his Rally instance lost connection to outside world. This was because VXLAN tunnel to the compute node hosting the Rally machine was dropped on networker node where DHCP, L3, Metadata agents were running. Once we restarted OVS agent, the tunnel was recreated properly.
The scenario failed only if L2POP mechanism was enabled.
I've looked thru the OVS agent logs and found out that the tunnel was dropped due to a legitimate fdb entry removal request coming from neutron-server side. So the fault is probably on neutron-server side, in l2pop mechanism driver.
I've then looked thru the patches in Juno to see whether there is something related to the issue already merged, and found the patch that gets rid of _precommit step when cleaning up fdb entries. Once we've applied the patch on the neutron-server node, we stopped to experience those connectivity failures.
After discussion with Vivekanandan Narasimhan, we came up with the following race condition that may result in tunnels being dropped while legitimate resources are still using them:
(quoting Vivek below)
'''
- - port1 delete request comes in;
- - port1 delete request acquires lock
- - port2 create/update request comes in;
- - port2 create/update waits on due to unavailability of lock
- - precommit phase for port1 determines that the port is the last one, so we should drop the FLOODING_ENTRY;
- - port1 delete applied to db;
- - port1 transaction releases the lock
- - port2 create/update acquires the lock
- - precommit phase for port2 determines that the port is the first one, so request FLOODING_ENTRY + MAC-specific flow creation;
- - port2 create/update request applied to db;
- - port2 transaction releases the lock
Now at this point postcommit of either of them could happen, because code-pieces operate outside the
locked zone.
If it happens, this way, tunnel would retain:
- - postcommit phase for port1 requests FLOODING_ENTRY deletion due to port1 deletion
- - postcommit phase requests FLOODING_ENTRY + MAC-specific flow creation for port2;
If it happens the below way, tunnel would break:
- - postcommit phase for create por2 requests FLOODING_ENTRY + MAC-specific flow
- - postcommit phase for delete port1 requests FLOODING_ENTRY deletion
'''
We considered the patch to get rid of precommit for backport to Icehouse [1] that seems to eliminate the race, but we're concerned that we reverted that to previous behaviour in Juno as part of DVR work [2], though we haven't done any testing to check whether the issue is present in Juno (though brief analysis of the code shows that it should fail there too).
Ideally, the fix for Juno should be easily backportable because the issue is currently present in Icehouse, and we would like to have the same fix for both branches (Icehouse and Juno) instead of backporting patch [1] to Icehouse and implementing another patch for Juno.
[1]: https://review.openstack.org/#/c/95165/
[2]: https://review.openstack.org/#/c/102398/","Race for l2pop when ports go up/down on same host

With l2pop enabled, race exists in delete_port_postcommit
when both create/update_port and delete_port deal with
different ports on the same host, where such ports are
either the first (or) last on same network for that host.
This race happens outside the DB locking zones in
the respective methods of ML2 plugin.

To fix this, we have moved determination of
fdb_entries back to delete_port_postcommit and removed
delete_port_precommit altogether from l2pop mechanism
driver.  In order to accomodate dvr interfaces, we
are storing and re-using the mechanism-driver context
which hold dvr-port-binding information while
invoking delete_port_postcommit.  We loop through
dvr interface bindings invoking delete_port_postcommit
similar to delete_port_precommit.

Closes-Bug: #1372438
Change-Id: If0502f57382441fdb4510c81a89794f57a38e696"
462,3d03291df88a2a95e0cfb9ae8bb0b7a56ce1b846,1389783513,0.0,1.0,38,40,13,3,1,0.875274698,True,1.0,324150.0,10.0,6.0,False,64.0,68520.0,219.0,3.0,5.0,254.0,255.0,5.0,253.0,254.0,5.0,220.0,221.0,0.005376344,0.198028674,0.198924731,1815,1268480,1268480,Glance,3d03291df88a2a95e0cfb9ae8bb0b7a56ce1b846,0,0,Bug in test files,"Bug #1268480 in Glance: ""assertTrue(isinstance()) in tests should be replace with assertIsInstance() ""","some of tests use different method of assertTrue(isinstance(A, B)) or assertEqual(type(A), B). The correct way is to use assertIsInstance(A, B) provided by testtools","Change assertTrue(isinstance()) by optimal assert

Some of tests use different method of assertTrue(isinstance(A, B)) or
assertEqual(type(A), B). The correct way is to use assertIsInstance(A, B)
provided by testtools.

Change-Id: Ia8d38f73c159c7ef943a8f6cfe72b945cc493947
Closes-bug: #1268480"
463,3d24fe5710cbea6d7d1f88c3476f4a856347ab5e,1392594454,2.0,1.0,49,58,2,2,1,0.985777009,True,1.0,280427.0,31.0,8.0,False,19.0,580960.0,22.0,5.0,606.0,605.0,1021.0,476.0,550.0,844.0,255.0,417.0,515.0,0.315270936,0.514778325,0.63546798,420,835,1280827,neutron,3d24fe5710cbea6d7d1f88c3476f4a856347ab5e,0,0,No bug but regex parsing is tedious and error prone,"Bug #1280827 in neutron: ""Regex parsing in get_vif_port_by_id is tedious and error prone""","ovs-vsctl has a switch for returning json output, which is arguably better for machine processing, and therefore more suitable for neutron's OVS agent.
Indeed get_vif_port_set in neutron/agent/linux/ovs_lib.py already uses json output.
However, get_vif_port_by_id performs match on the text output from ovs_vsctl.
While it is true that regular expressions are extremely powerful, apparently trivial errors in the regular expression itself can lead to errors like [1].
In [1] the evaluation of a regular expression is failing because the name of a VIF was not wrapped in double quotes.
Even if the could be worth looking into ovs-vsctl output to understand in which cases VIF names are wrapped in quotes and in which not, it is probably better to just switch to JSON output as parsing JSON is easier and therefore less likely to cause errors.
It is also worth noting that the error in regex parsing [1] causes the VIF to not be wired triggering the same failure as bug 1253896.
[1]  http://logs.openstack.org/49/63449/16/experimental/check-tempest-dsvm-neutron-isolated/de067c3/logs/screen-q-agt.txt.gz#_2014-02-16_11_46_14_830","Parse JSON in ovs_lib.get_vif_port_by_id

This patch replaces regex matching of text output with parsing
of JSON output in ovs_lib.get_vif_port_by_id.
This makes the code more reliable as subtle, possibly even
cosmetic, changes in ovs-vsctl output format could cause the
regular expression match to fail.

Also, this makes the code consistent with ovs_lib.get_vif_port_set
which already uses JSON output.

Finally this patch slightly changes the behaviour of
ovs_lib.get_vif_port_by_id returning None if elements such as
mac address or ofport were not available.

Change-Id: Ia985a130739c72b5b88414a79b2c6083ca6a0a00
Closes-Bug: #1280827"
464,3d2f3cbde7bb99ed1371bca835a8e63ddc6323d9,1395246973,,1.0,19,7,4,2,1,0.484959217,True,1.0,22104.0,17.0,3.0,False,5.0,5610888.75,8.0,3.0,512.0,1639.0,1853.0,416.0,1408.0,1564.0,255.0,810.0,874.0,0.254220457,0.805362463,0.868917577,668,1094,1295802,neutron,3d2f3cbde7bb99ed1371bca835a8e63ddc6323d9,0,0,No bug. Allow to add prefix to,"Bug #1295802 in neutron: ""nec plugin: cannot use URI with prefix in OpenFlow controller""","At now NEC plugin assumes REST API of OpenFlow controller starts with /, but NEC OpenFlow controller supports a prefix for REST API. NEC plugin allows to use URI prefix when talking with OpenFlow controller.","NEC plugin: Allow to add prefix to OFC REST URL

Closes-Bug: #1295802
Change-Id: Ieaa3bb7c601fad98506168de1f8ac191849c6569"
465,3d301346a7100e52fe0c319c1c1b5b81a3bc1660,1411148781,,1.0,25,6,2,2,1,0.99323382,False,,,,,True,,,,,,,,,,,,,,,,,1287,1750,1367454,cinder,3d301346a7100e52fe0c319c1c1b5b81a3bc1660,1,0,Bug in API  “There was a bug in WSGIService “,"Bug #1367454 in Cinder: ""osapi_volume_workers warning when unspecified""","Seeing the following warning in cinder logs (juno):
cinder.service [-] Value of config option osapi_volume_workers must be integer greater than 1.  Input value ignored.
We're not specifying this in cinder.conf, so the following OpenStack code from cinder/service.py attempts to get the default:
        self.workers = getattr(CONF, '%s_workers' % name,
                               processutils.get_worker_count())
        setup_profiler(name, self.host)
        if self.workers < 1:
            LOG.warn(_(""Value of config option %(name)s_workers must be ""
                       ""integer greater than 1.  Input value ignored."") %
                     {'name': name})
            # Reset workers to default
            self.workers = processutils.get_worker_count()
processutils.get_worker_count() looks like this:
    try:
        return multiprocessing.cpu_count()
    except NotImplementedError:
        return 1
and multiprocessing.cpu_count() returns this:
>>> import multiprocessing
>>> multiprocessing.cpu_count()
8
It looks to me like getattr is reading a value for osapi_volume_workers from the conf file even though there is no default set and we didn't specify this in the conf, preventing the default processutils.get_worker_count() from being returned by getattr when it should have been. It still works because processutils.get_worker_count() gets called again, but that shouldn't have been necessary, and neither should the warning (in this case).","Fix unnecessary WSGI worker warning at API startup

There was a bug in WSGIService in the way that it was
checking the osapi_volume_workers option.  It was using
getattr() to see if the option was set, if not it was supposed
to set the value to processutils.get_worker_count().  This,
however, never happened because getattr interpreted the default
'None' value to be a value.  So, on any system with no value set
the self.workers < 1 check would be hit and a warning would be
output.

Nova had changed their approach to this option to avoid this
problem.  This patch pulls Nova's approach into Cinder for
consistency.  Cinder will now use processutils.get_worker_count()
if no option is set in /etc/cinder/cinder.conf and when the user sets
osapi_volume_workers to 0.  A negative value will cause an
InvalidInput exception to be thrown.

Unittests have been added for this functionality.

Change-Id: I4ec2fdd0d19195cccffd63cdd1af1b9ca9884c7d
Closes-bug: #1367454"
466,3d4e421204145bec16117ef3b9d5053ae27e836b,1407944168,,1.0,16,4,3,3,1,0.919717324,False,,,,,True,,,,,,,,,,,,,,,,,1180,1637,1355777,nova,3d4e421204145bec16117ef3b9d5053ae27e836b,0,0,“Current git version of nova does not fully support ipv6 nameservers despite being able to set them during subnet creation.”,"Bug #1355777 in OpenStack Compute (nova): ""support for ipv6 nameservers""","Current git version of nova does not fully support ipv6 nameservers despite being able to set them during subnet creation.
This patch adds this support in nova (git) and its interfaces.template. It is currently deployed and used in our infrastructure based on icehouse (Nova 2.17.0).","Add support for ipv6 nameservers

Update interfaces.template to support ipv6 based dns servers. Make
sure we validate the version of the ip addresses before we
fill them in.

Closes-Bug: #1355777

Change-Id: I7229656bfa5ff5a29c63befc3b9ce91c46e04723"
467,3d8ee7eb633e251c3e343239b0e0ee6234df9393,1394602573,,1.0,0,73,41,18,1,0.949079335,True,2.0,647704.0,52.0,9.0,False,86.0,83545.92683,225.0,5.0,190.0,781.0,855.0,188.0,681.0,754.0,154.0,466.0,517.0,0.163329821,0.492096944,0.545837724,592,1015,1291144,neutron,3d8ee7eb633e251c3e343239b0e0ee6234df9393,0,0,tests,"Bug #1291144 in neutron: ""Calling cfg.CONF.reset not necessary in individual unit tests""",oslo.config.cfg.CONF.reset is added to cleanup in BaseTestCase.setUp(). No need for individual test classes to do it.,"Remove individual cfg.CONF.resets from tests

oslo.config.CONF.reset is added to cleanup in BaseTestCase, so it does
not need to be done by individual test cases.

Change-Id: I5fced5c2d480e78e5bb7cc150f0b653313884456
Closes-Bug: #1291144"
468,3d9e183d596c806527008f6eb15edc9d249cb3c0,1395748619,,1.0,16,1,2,2,1,0.522559375,True,1.0,981659.0,27.0,6.0,False,8.0,1158209.0,13.0,5.0,0.0,1654.0,1654.0,0.0,1415.0,1415.0,0.0,825.0,825.0,0.000968992,0.800387597,0.800387597,462,877,1283990,neutron,3d9e183d596c806527008f6eb15edc9d249cb3c0,1,1,Small bug,"Bug #1283990 in neutron: ""Incorrect instantiation of MlnxException exception""",MlnxException exceptions expect a keyword argument with the err_msg key. This is not the case in the code: https://github.com/openstack/neutron/blob/8a70bfd97a6f27dcae41e0b895d84ce5c19238ad/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py#L61,"Fixed TypeError when creating MlnxException

MlnxException expect a 'err_msg' keyword argument.

Change-Id: I4570219c0a6a466391b43cbdaa5372b85566c421
Closes-Bug: #1283990
Signe-off-by: Roey Chen <roeyc@mellanox.com>"
469,3da0d898f487fbc4ca668e57133a2b3f102f73be,1394427928,0.0,1.0,34,7,2,2,1,0.600608575,True,3.0,1242405.0,43.0,13.0,False,159.0,325441.0,496.0,6.0,2.0,3528.0,3528.0,2.0,2859.0,2859.0,2.0,2555.0,2555.0,0.000397509,0.33867762,0.33867762,575,996,1290362,nova,3da0d898f487fbc4ca668e57133a2b3f102f73be,1,0,x86 specific,"Bug #1290362 in OpenStack Compute (nova): ""HPET timer not supported on non-x86 targets ""","High Precision Event Timer is x86 specific hardware design to replace older PIT and RTC.
Also, '-no-hpet' option makes qemu to fail on non x86 targets.
he libvirt's xml generated has the following:
<timer name=""hpet"" present=""no""/>
The error produced...
libvirtError: internal error: process exited while connecting to monitor: Option no-hpet not supported for this target
For non x86 arch, this bug is affecting test_server_basicops test in Tempest:
tempest.scenario.test_server_basic_ops.TestServerBasicOps.test_server_basicops
No valid host was found.","Do not add HPET timer config to non x86 targets

HPET is a harware timer for x86 arch. Also, qemu option '-no-hpet'
is not supported on non x86 targets. This change does not add hpet
in the guest's timer config when the arch is not x86_64 or i686.

Closes-Bug: #1290362

Change-Id: I1b81d1c5ed4f900ed87d1ecbbb97d4fdb22405fa"
470,3dad53ce1d0263786c3f9ff585dc446a0a9dbecf,1378284773,,1.0,9,6,2,2,1,0.918295834,True,2.0,2636521.0,18.0,10.0,False,131.0,96030.0,515.0,4.0,7.0,2464.0,2468.0,7.0,1972.0,1976.0,6.0,2212.0,2215.0,0.001181435,0.37350211,0.374008439,1600,1230102,1230102,nova,3dad53ce1d0263786c3f9ff585dc446a0a9dbecf,1,1," “Remove REGEXP_LIKE operator for oracle db backend since it's not suitable,”","Bug #1230102 in OpenStack Compute (nova): ""For oracle database, the usage of REGEXP is not correct for List Servers by filter.""","The REGEXP_LIKE operator for oracle db backend is not correct.
For the MySql database, the following SQL syntax is right:
---
select xxx from xxx where column REGEXP pattern
---
But for Oracle database, the following SQL syntax is not right:
---
select xxx from xxx where column REGEXP_LIKE pattern
---
It should be:
---
select xxx from xxx where REGEXP_LIKE (column, pattern)
---","Code change for regex filter matching

Remove REGEXP_LIKE operator for oracle db backend since it's not suitable,
Such as:
Oracle: 'select xxx from xxx where REGEXP_LIKE (column, pattern)'
Mysql: 'select xxx from xxx where column REGEXP pattern'

For the simple pattern matching(LIKE operator) used for unsupported backend db,
add '%' wildcard character on both sides of the value.

Change-Id: I4682d38086a06032bb5650a14b253f6b2b859613
Closes-Bug: #1230102"
471,3de7da12d1098ef777305099e5f4a039e536bf99,1385232615,,1.0,6,1,1,1,1,0.0,True,1.0,619653.0,13.0,9.0,False,22.0,2195185.0,44.0,7.0,322.0,1356.0,1371.0,297.0,1201.0,1214.0,309.0,1157.0,1170.0,0.261603376,0.97721519,0.988185654,1802,1254318,1254318,cinder,3de7da12d1098ef777305099e5f4a039e536bf99,1,1, ,"Bug #1254318 in Cinder: ""Properly handle volume cleanup with RBD driver""","If a volume gets deleted in the Ceph backend, and Cinder still knows about it in a 'deleting' state, cinder-volume on startup will try to clean it up. rbd.Image() will however raise a rbd.ImageNotFound exception in these cases. These should be disregarded so cinder can continue the delete if it's no longer in the backend.
Stack trace:
http://paste.openstack.org/show/53858/","Continue to delete volumes that DNE in rbd backend

If a volume has already been deleted in the RBD backend, catch the
exception and let Cinder continue removing it in the database.

Closes-Bug: #1254318
Change-Id: I99b591ce78d82c6eaabff0459d9c523e85c8e65f"
472,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1379526824,,2.0,11,1,2,2,1,0.650022422,True,2.0,726888.0,16.0,8.0,False,8.0,3319900.0,11.0,2.0,22.0,997.0,1000.0,22.0,894.0,897.0,22.0,299.0,302.0,0.065155807,0.849858357,0.858356941,1464,1210877,1210877,neutron,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1,1, ,"Bug #1210877 in neutron: ""Sync router fails with db exception ""","While investigating https://bugs.launchpad.net/neutron/+bug/1210664, salvatore-orlando discovered that a db exception was being raised during router syncing:
Traceback (most recent call last):
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
    **args)
  File ""/opt/stack/neutron/neutron/common/rpc.py"", line 44, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
    plugin.auto_schedule_routers(context, host, router_ids)
  File ""/opt/stack/neutron/neutron/db/agentschedulers_db.py"", line 303, in auto_schedule_routers
    self, context, host, router_ids)
  File ""/opt/stack/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
    context.session.add(binding)
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
    self.commit()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
    self._prepare_impl()
  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
    self.session.flush()
  File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('2df68c3d-f3c9-43d2-bf45-e2e57e84b054', 'c4502f1f-a093-4c7c-b161-929b6342509b', '85d6b60f-f3ff-4437-8f5a-af165087f3ea')
This can be reproduced by running the quantum smoke test in tempest (nosetests tempest/scenario/test_network_basic_ops.py).  The smoke test passes - the exception only occurs at test cleanup.  It may be that the router syncing code is working with stale state after router deletion.","Ensure router exists when auto_schedule_routers

Currently, the auto_schedule_routers() accepts parameter router_ids,
which may contain invalid router ids, since we've already filtered
them via plugin.get_routers(), we can directly use that safe object.

Closes-Bug: #1217998
Closes-Bug: #1210877

Change-Id: I6196f16cca65fee4e848173d0a0a10fde967195d"
473,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1379526824,,2.0,11,1,2,2,1,0.650022422,True,2.0,726888.0,16.0,8.0,False,8.0,3319900.0,11.0,2.0,22.0,997.0,1000.0,22.0,894.0,897.0,22.0,299.0,302.0,0.065155807,0.849858357,0.858356941,1491,1217998,1217998,neutron,3e1116eb0f1d94530707cd6ef4b37f17e9a13918,1,1,Duplicated of  1210877,"Bug #1217998 in neutron: ""CONSTRAINT routerl3agentbindings failure during gate tests""","http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/logs/screen-q-svc.txt.gz
Shows a constraint failure, which is almost always evidence of some kind of race/poor error handling:
2013-08-27 18:45:06.141 29478 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     plugin.auto_schedule_routers(context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self, context, host, router_ids)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     context.session.add(binding)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')
2013-08-27 18:45:06.141 29478 TRACE neutron.openstack.common.rpc.amqp
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] Returning exception (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a') to caller
2013-08-27 18:45:06.142 29478 ERROR neutron.openstack.common.rpc.common [-] ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 424, in _process_data\n    **args)\n', '  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 44, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 47, in sync_routers\n    plugin.auto_schedule_routers(context, host, router_ids)\n', '  File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 302, in auto_schedule_routers\n    self, context, host, router_ids)\n', '  File ""/opt/stack/new/neutron/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers\n    context.session.add(binding)\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__\n    self.commit()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit\n    self._prepare_impl()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl\n    self.session.flush()\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap\n    raise exception.DBError(e)\n', ""DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_2` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('c0771426-7e66-430e-beb9-4c3334e43039', '4dae479a-33a1-4d60-9a82-f0ee09cb9491', '039d8fe8-8993-4c9e-89a4-196dccb2878a')\n""]
Full gate logs are here:
http://logs.openstack.org/60/43760/3/check/gate-tempest-devstack-vm-neutron/dd1e380/","Ensure router exists when auto_schedule_routers

Currently, the auto_schedule_routers() accepts parameter router_ids,
which may contain invalid router ids, since we've already filtered
them via plugin.get_routers(), we can directly use that safe object.

Closes-Bug: #1217998
Closes-Bug: #1210877

Change-Id: I6196f16cca65fee4e848173d0a0a10fde967195d"
474,3e11db0e3b832703feafe8317c0c12fb0a149e53,1410595452,,1.0,54,19,2,2,1,0.675863572,False,,,,,True,,,,,,,,,,,,,,,,,1255,1715,1364344,nova,3e11db0e3b832703feafe8317c0c12fb0a149e53,1,1,,"Bug #1364344 in OpenStack Compute (nova): ""Nova client produces a wrong exception when user tries to boot an instance without specific network UUID""","Description of problem:
=======================
pyhton-novaclient produces a wrong exception when user tries to boot an instance without specific network UUID.
The issue will only reproduce when an external network is shared with the tenant, but not created from within it (I created it in admin tenant).
Version-Release:
================
python-novaclient-2.17.0-2
How reproducible:
=================
Always
Steps to Reproduce:
===================
1. Have 2 tenants (admin + additional tenant would do).
2. In tenant A (admin), Create a network and mark it as both shared and external.
3. In tenant B, Create a network which is not shared or external.
4. Boot an instance within tenant B (I tested this via CLI), do not use the --nic option.
Actual results:
===============
DEBUG (shell:783) It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-caacfa72-82f8-492a-8ce2-9476be8f3e0c)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
Forbidden: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
ERROR: It is not allowed to create an interface on external network 49d0cb8a-2631-4308-89c4-cac502ef0bad (HTTP 403) (Request-ID: req-afce2569-6902-4b25-a9b8-9ebf1a6ce1b9)
Expected results:
=================
This is what happens if:
1. The shared network is no longer marked as external.
2. The tenant itself has two networks.
(+ no network UUID is speficied in the 'nova boot' command)
DEBUG (shell:783) Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 780, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/usr/lib/python2.7/site-packages/novaclient/shell.py"", line 716, in main
    args.func(self.cs, args)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/shell.py"", line 433, in do_boot
    server = cs.servers.create(*boot_args, **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 871, in create
    **boot_kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/v1_1/servers.py"", line 534, in _boot
    return_raw=return_raw, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/base.py"", line 152, in _create
    _resp, body = self.api.client.post(url, body=body)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 312, in post
    return self._cs_request(url, 'POST', **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 286, in _cs_request
    **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 268, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/usr/lib/python2.7/site-packages/novaclient/client.py"", line 262, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)
ERROR: Multiple possible networks found, use a Network ID to be more specific. (HTTP 400) (Request-ID: req-a4e90abd-2ad7-4342-aa3c-1a9aa9f5e2a0)","check network ambiguity before external network auth

This change refactors the logic in the neutronv2 api to check for
network ambiguity before checking permission to attach external
network. In the scenario when there is a network available in the
client's tenant and an external shared network available in a
different tenant, the client was receiving a Forbidden error
because the external network was in the list of available networks
even though the client did have access to the other network in the
list.

With this change, the client receives a NetworkAmbiguous error instead
the same scenario, as they could succeed in booting an instance while
specifying --nic with the net-id of the network they can access.

Closes-Bug: #1364344

Change-Id: Ia94c25ff0ffd6a368da272fc6d883986156a0362"
475,3e1e2448bf162bd2750416efe5d7c1010b51b52a,1378757678,,1.0,33,3,2,2,1,0.99107606,True,3.0,1224682.0,33.0,14.0,False,68.0,679735.0,119.0,3.0,1608.0,1715.0,3053.0,1411.0,1416.0,2578.0,1518.0,1581.0,2840.0,0.254311066,0.26485853,0.475640382,1447,1202177,1202177,Nova,3e1e2448bf162bd2750416efe5d7c1010b51b52a,1,1,"""After catching an exception, it is required to save off the exception details""","Bug #1202177 in OpenStack Compute (nova): ""Missing exception data for exceptions raised in floating ip assignment when ebtables is missing""","When I tried to assign one of my successfully-allocated floating IPs to one of my instances, my nova-network.log showed this rather unhelpful traceback: http://paste.openstack.org/show/40674/
I'm running nova-network 1:2013.1.2-0ubuntu1~cloud0 from the cloud archive, on Ubuntu 12.04 LTS.
My searches through bugs while trying to find the *real* error turned up bug#1119817 which seems to be of a type with this one (although apparently more thoroughly researched).","Don't lose exception info

Using eventlet introduces some extra fun into exception handling.  After
catching an exception, it is required to save off the exception details
if you want to be able to re-raise it later after doing other operations
which may do a greenthread switch.  That's what this patch does.  It
saves off the original exception that we intend to re-raise later and
ensures that nothing stomps on it in the meantime.

A test case was also added to exercise this code path to make sure the
exception saving and re-raising worked as expected.

Change-Id: Ibf5f06a30839b93ece500ce7dbb714f6509f85af
Closes-bug: #1202177"
476,3e263ba6c58c43f71c02417f8948300e8cb5c462,1386406758,1.0,1.0,77,19,3,3,1,0.249017599,True,3.0,5416095.0,44.0,19.0,False,11.0,438099.0,16.0,7.0,326.0,1352.0,1500.0,256.0,1179.0,1289.0,51.0,561.0,567.0,0.084690554,0.915309446,0.925081433,158,560,1258379,neutron,3e263ba6c58c43f71c02417f8948300e8cb5c462,0,0,Evolution: router must have gateway interface set ,"Bug #1258379 in neutron: ""vpnservice's router must have gateway interface set""","at line
https://github.com/openstack/neutron/blob/master/neutron/services/vpn/service_drivers/ipsec.py#L172
it is obvious the router must have gateway interface set  then it can be used as vpnservce router.","validate if the router has external gateway interface set

If the router wants to work with vpn service, we must
make sure external gateway interface is set.

This patch cannot prevent user from clearing the gateway interface
of the router after the vpnservice is created.

Change-Id: If0f00def949b31c1e3da7a2cd055454567201e4c
Closes-Bug: #1258379"
477,3e4f554f614c8cb6d5f014c72f3c635184a4dec2,1389046565,,1.0,56,25,2,2,1,0.999890052,True,2.0,1634287.0,16.0,8.0,False,18.0,1100341.0,36.0,5.0,1089.0,1312.0,1312.0,924.0,1134.0,1134.0,947.0,1152.0,1152.0,0.726993865,0.884202454,0.884202454,251,655,1266048,cinder,3e4f554f614c8cb6d5f014c72f3c635184a4dec2,1,1,,"Bug #1266048 in Cinder: ""Copy image to volume iSCSI multipath doesn't work properly if there are different targets associated with different portals for a mulitpath device.""","The connect_volume and disconnect_volume code in brick assumes that the targets for different portals are the same for the same multipath device.  This is true for some arrays but not for others.  When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during copy image to volume and copy volume to image operations.","Fixed a problem in iSCSI multipath

Multipathing during copy image to volume and copy volume to image
operations doesn't work properly if there are different targets
associated with different portals for a mulitpath device.

Change-Id: I65c93f3788020c944db0d3a55063a6415554ff11
Closes-Bug: #1266048"
478,3ea14e8a70a946dbb162ecafa848e4f2fa29772a,1402215816,,1.0,42,16,7,7,1,0.915253784,False,,,,,True,,,,,,,,,,,,,,,,,956,1394,1327497,nova,3ea14e8a70a946dbb162ecafa848e4f2fa29772a,1,1,,"Bug #1327497 in OpenStack Compute (nova): ""live-migration fails when FC multipath is used""","I tried live-migration against VM with multipath access to FC bootable volume and FC data volume.
After checking the code, I found the reason is that
1. /dev/dm-<NUM> is used, which is subject to change in the destination Compute Node since it is not unique across nodes
2. multipath_id in connnection_info is not maintained properly and may be lost during connection refreshing
The fix would be
1. Like iSCSI multipath, use /dev/mapper/<multipath_id> instead of /dev/dm-<NUM>
2. Since multipath_id is unique for a volume no matter where it is attached, add logic to preserve this information.","Fix live-migration failure in FC multipath case

Currently, /dev/dm-<NUM> instead of /dev/mapper/<multipath_id> is
used to access multipath FC volumes by Compute Node and
multipath_id in connection_info is not maintained properly and
may be lost during connection refreshing.

This implementation will make source Compute Node and destination
Compute Node fail to disconnect/connect to volumes properly and
result in live-migration failure.

To fix it, /dev/mapper<multipath_id> will be used instead of
/dev/dm-<NUM> to access multipath devices, just like iSCSI multipath
implementation, and logic to preserve the unique (across Compute
Nodes) multipath_id is also added.

Change-Id: I17f15852c098af88afd270084c62eb87693c60d4
Closes-Bug: #1327497"
479,3eee50510fed29a7a8d97d4193a1c3c0a209a712,1406603344,1.0,1.0,34,1,2,2,1,0.591672779,False,,,,,True,,,,,,,,,,,,,,,,,1111,1565,1349638,neutron,3eee50510fed29a7a8d97d4193a1c3c0a209a712,1,0,"""With the vendor's version of L3 Router Service plugins that assumption may not be true and hence the invocation of this method throws an exception.”","Bug #1349638 in neutron: ""DVR vmarp table update causes exception in L3 service plugin""","DVR implementation assumes that dvr_vmarp_table_update() method is supported by L3 Router service plugin. With the vendor's version of L3 Router Service plugins that assumption may not be true and hence the invocation of this method throws an exception.
I noticed this during the testing of Arista's  L3 router plugin. I notice that other similar plugins from other vendors are on their way, and will hit this issue as well.","Make dvr_vmarp_table_update call conditional to dvr extension

Without making this call conditional, every l3plugin that
integrates with the ML2 plugin will need to implement this
method and this must not be necessary.

Closes-bug: #1349638

Change-Id: Ie9ba3bad4152810f5bfa530be54be70139cebc0c"
480,3eef1ae80fcd7d9fe82b5c7485702991992b2766,1402758864,,1.0,8,2,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,969,1408,1329099,neutron,3eef1ae80fcd7d9fe82b5c7485702991992b2766,1,1,"“Add a config parameter to Cisco N1kv plugin to determine the total
    number of active REST calls that can be made to the VSM.”","Bug #1329099 in neutron: ""Control active number of REST calls to the controller in Cisco N1kv plugin""",Add a config parameter to Cisco N1kv neutron plugin to determine and control the number of active REST calls to the VSM (controller),"Control active number of REST calls from Cisco N1kv plugin to VSM

Add a config parameter to Cisco N1kv plugin to determine the total
number of active REST calls that can be made to the VSM.

Change-Id: I66433ef5673d35badcd3adc2defa43e578d4094f
Closes-Bug: #1329099"
481,3f110c9a571249c6626ebd1ef7dbf67b71273fa5,1385968659,,1.0,8,3,3,3,1,0.971306738,True,1.0,48494.0,7.0,4.0,False,10.0,2349999.0,15.0,4.0,319.0,747.0,952.0,250.0,672.0,825.0,44.0,318.0,338.0,0.075503356,0.535234899,0.568791946,115,514,1256766,neutron,3f110c9a571249c6626ebd1ef7dbf67b71273fa5,1,1, confusing message,"Bug #1256766 in neutron: ""vpnstateinvalid has confusing message""","after  I create an vpnservice without a name, and then I want to update it with a name, the following msg come out:
2013-12-02 12:09:18.691 3623 ERROR neutron.api.v2.resource [req-aab03b48-08a4-415f-9cd1-7ee4f5a16b19 8fb8b278e48e4d0cbde162e5626032a1 c3b9072bc0f741aa98f72e794dba7ea6] update failed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/api/v2/base.py"", line 492, in update
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     obj = obj_updater(request.context, id, **kwargs)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 579, in update_vpnservice
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     self.assert_update_allowed(vpns_db)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource   File ""/mnt/data/opt/stack/neutron/neutron/db/vpn/vpn_db.py"", line 198, in assert_update_allowed
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource     raise vpnaas.VPNStateInvalid(id=id, state=status)
2013-12-02 12:09:18.691 3623 TRACE neutron.api.v2.resource VPNStateInvalid: Invalid state PENDING_CREATE of vpnaas resource <built-in function id>
There are two problems in above msg:
1. built-in function id
2. the msg itself: I think we can use: the vpn service cannot be updated when it is in state PENDING_CREATE
but I think we should be able to modify the vpn service when it is not used by IPsecSiteConnection","update error msg for invalid state to update vpn resources

Change-Id: I6719fa359d0c14accfa8b9b440c2b7fbabbdcfdb
Closes-Bug: #1256766"
482,3f29c06d54de32f0a0859f53e942e1f18326b4d4,1389855749,,1.0,27,92,2,2,1,0.92909503,True,5.0,5230216.0,53.0,5.0,False,1.0,4137416.0,2.0,2.0,13.0,1002.0,1004.0,13.0,902.0,904.0,12.0,523.0,525.0,0.018492176,0.745376956,0.748221906,72,469,1254639,neutron,3f29c06d54de32f0a0859f53e942e1f18326b4d4,0,0,"'suggests’, ‘should also’","Bug #1254639 in neutron: ""NVP advanced LBaaS should change operation on deleting healthmonitor since Bug #1243129""","Bug #1243129 suggests to check for associations before deleting health monitor, so NVP advanced LBaaS should also change its operation when deleting a healthmonitor.","NVP LBaaS: check for association before deleting health monitor

    Bug #1243129 suggests to check for associations before deleting
health monitor in DB logic, so NVP advanced LBaaS should also Change
its operations when deleting a healthmonitor.

Change-Id: I57677cc4b65a13df9f72ec689f34771c7af6a57e
Closes-Bug: #1254639"
483,3f4a0e40af7705b037db1a2ccc63de94547f6dd7,1410974013,,1.0,77,54,2,2,1,0.941664684,False,,,,,True,,,,,,,,,,,,,,,,,1336,1804,1371072,nova,3f4a0e40af7705b037db1a2ccc63de94547f6dd7,0,0,cleanup. “it would be good to clean up older snapshots”,"Bug #1371072 in OpenStack Compute (nova): ""xenapi: should clean up old snapshots before creating a new one""","When nova-compute gets forcably restarted, or fails, we get left over snapshots.
We have some clean up code for after nova-compute comes back up, but it would be good to clean up older snapshots, and generally try to minimize the size of the snapshot that goes to glance.","XenAPI: clean up old snapshots before create new

Before taking a new snapshot, we now inspect the VDI chain to check for
old snapshots.

Those old snapshots can be left behind from resize, migrates, and
snapshots that get interrupted if you kill the nova-compute process.

If you leave these snapshots behind, the hypervisor disk will get too
full, as VDI chains grow much bigger than you would expect. In addition
snapshots start to grow in size, causing issues copying the snapshot
into storage.

Closes-Bug: #1371072

Change-Id: Ice3741213438c7b4a7fda1e3e0a6dd7faca0fd04"
484,3f67de92cfcfc61ca26156961e1f2d4d2ebded66,1394780697,,1.0,161,5,4,2,1,0.613152882,True,3.0,4878.0,23.0,5.0,False,17.0,938677.0,45.0,2.0,76.0,585.0,623.0,76.0,509.0,547.0,66.0,550.0,578.0,0.043848168,0.360602094,0.378926702,618,1043,1292380,cinder,3f67de92cfcfc61ca26156961e1f2d4d2ebded66,1,1,The info of the first exception is lost,"Bug #1292380 in Cinder: ""In some case lose information of the first exception in SwiftBackupDriver""","When an exception occurs during exception handling, it lose the
information of the first exception.
In SwiftBackupDriver.backup(), in some cases, before re-transmission
of the exception, the exception is rewritten.","Fix unsaved exception in backup/drivers

When an exception occurs during exception handling, it loses the
information of the first exception.

We should use the excutils.save_and_reraise_exception() in some cases.

Change-Id: I5d0ea53ba6c52138c71cca61aedbdf06338f2a7d
Closes-Bug: #1292380"
485,3f9003270efd9ac036f3c229b36baa0bb05203bf,1412770471,,1.0,5,3,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1373,1843,1376368,nova,3f9003270efd9ac036f3c229b36baa0bb05203bf,1,1,"“Cert revocation was broken by
    32b0adb591f80ad2c5c19519b4ffc2b55dbea672.”","Bug #1376368 in OpenStack Compute (nova): ""nova.crypto.revoke_cert always raises ProjectNotFound""","(Marked this as a security issue for now, since cert revocation not working is pretty serious)
https://github.com/openstack/nova/blob/master/nova/crypto.py#L277-L278
os.chdir *always* returns None, which means that path is always taken and the cert is never revoked","Fix broken cert revocation

Cert revocation was broken by
32b0adb591f80ad2c5c19519b4ffc2b55dbea672.  os.chdir() never returns
anything, so this method would always raise an exception.  The proper
way to handle an error from os.chdir() is to catch OSError.

There were existing tests for this code, but they conveniently mocked
os.chdir() to return values that are never actually returned.  The
tests were fixed to match the real behavior.

Change-Id: I7549bb60a7d43d53d6f81eecea31cbb9720cc8b6
Closes-bug: #1376368
(cherry picked from commit c8538208da00c3b0d0646629c9d668aa69944b85)"
486,3f9658dcd9b2cccdc0f03088723305c980130cd3,1400094405,,1.0,9,8,2,2,1,0.322756959,True,5.0,641740.0,84.0,18.0,False,37.0,1899411.0,54.0,1.0,0.0,80.0,80.0,0.0,78.0,78.0,0.0,73.0,73.0,0.000811688,0.060064935,0.060064935,882,1317,1318261,neutron,3f9658dcd9b2cccdc0f03088723305c980130cd3,1,1,,"Bug #1318261 in neutron: ""OVS Agent: With l2-pop ON, cleaned up tunnel ports appear in rules for table 0 in br-tun""","When neutron Openswitch agent runs with l2pop ON, it is not removing flows from table 0 of br-tun, those flow sthat carry cleaned up tunnel ports by l2-pop.
Impact of this bug:  Explosion of flow rules in br-tun table 0 as tunnel ports come-in and go-away in a scaled environment","When l2-pop ON, clean stale ports in table0 br-tun

When l2-pop is turned ON, the tunnels towards a
specific node are created and torn down by other nodes,
based on availability of ports on that specific node in
specific networks. This generally reclaims the resources
used for such tunnels in all the nodes.

Under such conditions, in the current code (without this
fix), the cleaned up ports continue to be present in
rules of table0 of br-tun, resulting in flow explosion.

This fix adds cleanup-logic to remove such rules that
continue to use l2-pop cleaned up ports, from table0 of
br-tun.

Change-Id: I2639ff6432a13320adcadbcc0841319a99ce8c24
Closes-Bug: #1318261"
487,3faea81c6029033c85cefd6e98d7a3e719e858f5,1391092022,2.0,1.0,172,25,2,2,1,0.809260548,True,34.0,9387014.0,461.0,94.0,False,8.0,6273.0,9.0,1.0,14.0,949.0,952.0,14.0,829.0,832.0,14.0,494.0,497.0,0.020134228,0.66442953,0.668456376,367,779,1276440,neutron,3faea81c6029033c85cefd6e98d7a3e719e858f5,1,1,dont not send duplicate msg. High load of traffic,"Bug #1276440 in neutron: ""Metadata-agent sends a lot of same requests to neutron-server during cloud-init""","When cloud-init is running metadata-agent sends a lot of same requests to neutron-server. These requests serves for obtaining instance_id and tenant_id of running instance. When higher amount of instances is started at once mentioned requests cause high load for neutron-server.
Requests could be cached with small ttl.","Metadata agent caches networks for routers

During cloud-init there are several calls that asks neutron API for the
same data which will not be most likely changed. Specifically router's
networks are cached.

Closes-bug: #1276440
Change-Id: Ic5eedb8057c7f4934eed08869ebf55c91e6edfc9"
488,3fff55681739872ab125aa7f0677f7d836c10615,1408865789,,1.0,35,29,3,3,1,0.382209178,False,,,,,True,,,,,,,,,,,,,,,,,1283,1746,1367238,cinder,3fff55681739872ab125aa7f0677f7d836c10615,1,0,"""IBM NAS cinder driver sets 'rw' permissions to all during volume creat” external artefact","Bug #1367238 in Cinder: ""IBM NAS cinder driver sets 'rw' permissions to all during volume create operation, which is security issue""","IBM NAS cinder driver sets 'rw' permissions to all during volume create operation from a volume snapshot or from an existing volume (volume clone operation).
This is not required as 'rw' permissions to the user only should be sufficient.
This also helps resolve the security issue setting 'rw' permissions to all.","IBMNAS: Remove call to set r/w permissions to all

During cinder volume create operation from a volume snapshot or
from an existing volume (volume clone operation), the ibmnas
driver sets 'rw' permissions to all, which is unnecessary and
also poses security concerns.
Fixing this issue, removing the calls to set rw permissions to
all during these operations and adding a call to set 'rw'
permissions only to the owner to make sure even if umask is set
at the filesystem level, which might deny 'rw' access to the owner
we explicitely set the required permissions on the volume file.

Change-Id: I0e5ba9262a298e088f7724ddeda3537afa4b023e
Closes-Bug: #1367238"
489,402175a787c2064404eaf7c260a16ea05e9a99a3,1394671648,,1.0,2,3,2,2,1,0.970950594,True,1.0,658677.0,46.0,5.0,False,1.0,451003.0,11.0,4.0,45.0,976.0,980.0,45.0,808.0,812.0,37.0,353.0,357.0,0.039915966,0.371848739,0.37605042,603,1027,1291695,neutron,402175a787c2064404eaf7c260a16ea05e9a99a3,1,1, should call eventlet sleep in watchdog ,"Bug #1291695 in neutron: ""BigSwitch: should call eventlet sleep in watchdog""","The consistency watchdog in eventlet currently calls time.sleep which will block other greenthreads who are members of the same pool.
https://github.com/openstack/neutron/blob/288e3127440158f177beaae1972236def4916251/neutron/plugins/bigswitch/servermanager.py#L554
It should use eventlet.sleep so it yields to other members of the same pool.","BigSwitch: Use eventlet.sleep in watchdog

Changes the consistency watchdog that runs
in the background to use eventlet.sleep instead
of time.sleep to avoid blocking other members of
the same pool.

Closes-Bug: #1291695
Change-Id: I5ec842cca1063cef761c2cd09d63617baf27d191"
490,40390598c5a440d1bbfa4f229130eeedf5cd4dba,1393907095,1.0,1.0,7,40,6,3,1,0.938081141,True,2.0,154054.0,36.0,7.0,False,41.0,1008723.667,77.0,4.0,90.0,1179.0,1199.0,90.0,1019.0,1039.0,84.0,692.0,708.0,0.094972067,0.774301676,0.792178771,518,939,1287524,neutron,40390598c5a440d1bbfa4f229130eeedf5cd4dba,0,0,'This code could be simplified’,"Bug #1287524 in neutron: ""ip_lib netns.execute should work with or without namespace""","There are a number of places in the neutron code that run an ip command like this:
        if self.network.namespace:
            ip_wrapper = ip_lib.IPWrapper(self.root_helper,
                                          self.network.namespace)
            ip_wrapper.netns.execute(cmd)
        else:
            utils.execute(cmd, self.root_helper)
This code could be simplified if netns.execute simply checked if there was a namespace defined or not.","Refactor netns.execute so that it is not necessary to check namespace

I saw some code in a couple of reviews today that check whether a
namespace is set and run it under ""ip netns exec ..."" if it is.
Otherwise, it runs the command without it in the default namespace.

Change-Id: I55e8f4f3523ec7a7c5a6f082addf918952a05741
Closes-Bug: #1287524"
491,40417e67820b38dc5492211554ac2ac1cd272f09,1405065521,,1.0,4,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1057,1503,1340570,neutron,40417e67820b38dc5492211554ac2ac1cd272f09,0,0,Bug in test,"Bug #1340570 in neutron: ""ovs agent test is blocking to sleep""",The tests for the daemon loop in the ovs tunnel are not mocking out the polling call so they take 30+ seconds each.,"Mock out tunnel_sync in test to avoid sleeping

The tunnel_sync call in the daemon loop for the ovs tunnel
tests blocks for 30 seconds. This patch mocks it out to stop
that.

Closes-Bug: #1340570
Change-Id: I58c3907ed564413a8251bc1bb7922e25f673db18"
492,4054cc4a22a1fea997dec76afb5646fd6c6ea6b9,1377011172,,1.0,41,6,4,4,1,0.742909115,True,2.0,354013.0,10.0,7.0,False,101.0,87382.0,286.0,3.0,1549.0,624.0,1994.0,1365.0,574.0,1768.0,1460.0,514.0,1804.0,0.253822099,0.089471855,0.313585823,1467,1212179,1212179,nova,4054cc4a22a1fea997dec76afb5646fd6c6ea6b9,1,1,“CVE References”,"Bug #1212179 in OpenStack Compute (nova): ""[OSSA 2013-024] nova should check the is_public of flavor when creating an instance""","If creating a flavor with is_public ""false"", the flavor should be accessible only by admin or user who is granted to access.
Now ""get flavor details"" API checks the is_public of flavor but ""create an instance"" API does not check.
In the following case, a user (not admin) cannot access non-public flavor through ""get flavor details"" API, this is right behavior.
However, he can access non-public flavor through ""create an instance"" API.
=== admin ==============================================================
$ nova flavor-create --is-public false private-flavor 6 512 0 1
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name           | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
| 6  | private-flavor | 512       | 0    | 0         |      | 1     | 1.0         | False     |
+----+----------------+-----------+------+-----------+------+-------+-------------+-----------+
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5 -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]""
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 428
X-Compute-Request-Id: req-53ab8206-f458-441c-a0ca-d17e333f4247
Date: Wed, 14 Aug 2013 09:38:10 GMT
{""flavor"": {""name"": ""m1.xlarge"", ""links"": [{""href"": ""http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5"", ""rel"": ""self""}, {""href"": ""http://192.168.0.30:8774/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/5"", ""rel"": ""bookmark""}], ""ram"": 16384, ""OS-FLV-DISABLED:disabled"": false, ""vcpus"": 8, ""swap"": """", ""os-flavor-access:is_public"": true, ""rxtx_factor"": 1.0, ""OS-FLV-EXT-DATA:ephemeral"": 0, ""disk"": 160, ""id"": ""5""}}
$
=== user(""demo"" user on devstack) ==============================================================
$ nova flavor-list
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| ID | Name      | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
| 1  | m1.tiny   | 512       | 1    | 0         |      | 1     | 1.0         | True      |
| 2  | m1.small  | 2048      | 20   | 0         |      | 1     | 1.0         | True      |
| 3  | m1.medium | 4096      | 40   | 0         |      | 2     | 1.0         | True      |
| 4  | m1.large  | 8192      | 80   | 0         |      | 4     | 1.0         | True      |
| 42 | m1.nano   | 64        | 0    | 0         |      | 1     | 1.0         | True      |
| 5  | m1.xlarge | 16384     | 160  | 0         |      | 8     | 1.0         | True      |
| 84 | m1.micro  | 128       | 0    | 0         |      | 1     | 1.0         | True      |
+----+-----------+-----------+------+-----------+------+-------+-------------+-----------+
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/flavors/6 -X GET -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]""
HTTP/1.1 404 Not Found
Content-Length: 78
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-a7ac7e99-6d29-4893-97a7-6705083739df
Date: Wed, 14 Aug 2013 09:36:52 GMT
{""itemNotFound"": {""message"": ""The resource could not be found."", ""code"": 404}}
$
$ curl -i http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers -X POST -H ""X-Auth-Project-Id: demo"" -H ""User-Agent: python-novaclient"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""X-Auth-Token: [..]"" -d '{""server"": {""min_count"": 1, ""flavorRef"": ""6"", ""name"": ""test-not-public"", ""imageRef"": ""428f795d-01b0-44c8-a162-9ad86d1fea35"", ""max_count"": 1}}'
HTTP/1.1 202 Accepted
Location: http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20
Content-Type: application/json
Content-Length: 440
X-Compute-Request-Id: req-7e561044-100a-4d70-8b83-cebd21dca8e2
Date: Wed, 14 Aug 2013 09:41:50 GMT
{""server"": {""security_groups"": [{""name"": ""default""}], ""OS-DCF:diskConfig"": ""MANUAL"", ""id"": ""91407b32-7ed1-4108-9e62-192b3312ff20"", ""links"": [{""href"": ""http://192.168.0.30:8774/v2/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20"", ""rel"": ""self""}, {""href"": ""http://192.168.0.30:8774/7a5c62d3cadb40d28e3c25acf7a05b05/servers/91407b32-7ed1-4108-9e62-192b3312ff20"", ""rel"": ""bookmark""}], ""adminPass"": ""xvFhTwd3yAzE""}}
$ nova list
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
| ID                                   | Name            | Status | Task State | Power State | Networks         |
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
| 91407b32-7ed1-4108-9e62-192b3312ff20 | test-not-public | ACTIVE | None       | Running     | private=10.0.0.3 |
+--------------------------------------+-----------------+--------+------------+-------------+------------------+
$","Enforce flavor access during instance boot

The code in the servers API did not pass the context when retrieving
flavor details.  That means it would use an admin context instead,
bypassing all flavor access control checks.

This patch includes the fix, and the corresponding unit test, for both
the v2 and v3 APIs.

Closes-bug: #1212179

Change-Id: I681ae9965e19767df22fa74c3315e4e03a459d3b"
493,408ef55d4ef1a9d246571511203ab337ba5346c6,1396031516,,1.0,2,2,2,1,1,0.0,True,2.0,629746.0,57.0,11.0,False,15.0,1706546.5,40.0,4.0,1145.0,1586.0,1586.0,974.0,1291.0,1291.0,684.0,842.0,842.0,0.654875717,0.805927342,0.805927342,700,1126,1299145,neutron,408ef55d4ef1a9d246571511203ab337ba5346c6,1,1, Documentation Bug,"Bug #1299145 in neutron: ""Documentation Bug, BigSwitch should be ""Big Switch""""","BigSwitch references should be changed to ""Big Switch"".","Update BigSwitch Name to its correct name

Update Neutron documentation and code comments to replace
""BigSwitch"" name to ""Big Switch""

Closes-Bug: #1299145
Change-Id: I7c14f6170871b361c8929898ee6af5d1a1f41714"
494,40a790c32ee4ea7b9d79e259545f07409fd618fa,1399481007,,1.0,10,6,1,1,1,0.0,True,5.0,930205.0,76.0,16.0,False,10.0,618786.0,18.0,4.0,13.0,3485.0,3493.0,13.0,2840.0,2848.0,6.0,2889.0,2892.0,0.000880946,0.363705009,0.364082557,871,1305,1317180,nova,40a790c32ee4ea7b9d79e259545f07409fd618fa,1,1,“The following patch https://github.com/openstack/nova/commit/4c2f36bfe006cb0ef89ca7a706223f30488a182e#diff-5c6ee11140977e63b54542e2ff5763d3R22 caused a”,"Bug #1317180 in OpenStack Compute (nova): ""Hyper-v fails to attach volumes when using v1 volume utilites""","The following patch https://github.com/openstack/nova/commit/4c2f36bfe006cb0ef89ca7a706223f30488a182e#diff-5c6ee11140977e63b54542e2ff5763d3R22 caused a regression by changing the eventlet.subprocess.Popen with the builtin subprocess.Popen (by using the nova.utils execute method) without changing the way the args were parsed.
In this module, the execution args were parsed separated by whitespaces, which is not allowed by the builtin subprocess.Popen, causing a ""not found"" error. This error is returned for example when attaching a volume, at the point where iscsicli tool is used to login the iSCSI target or portal.
Trace:
http://paste.openstack.org/show/79418/","Fixes arguments parsing when executing command

A regression was caused by changing the eventlet.subprocess.Popen
with the builtin subprocess.Popen (by using the nova.utils execute
method) without changing the way the arguments were parsed.

In the v1 volume utilities module, the execution args were parsed
separated by whitespaces, which is not allowed by the builtin
subprocess.Popen, causing a ""not found"" error.

This error is returned for example when attaching a volume, at the
point where iscsicli tool is used to login the iSCSI target or portal.

This patch fixes the issue by simply splitting the args.

Closes-bug: #1317180

Change-Id: Iee7d5de0dde8b68d8f2bab4214e9b6779ad9f722"
495,411836b5411411a6046043e0264aaa7b6f5760f0,1411148237,1.0,1.0,68,11,2,2,1,0.985969027,False,,,,,True,,,,,,,,,,,,,,,,,1339,1807,1371732,neutron,411836b5411411a6046043e0264aaa7b6f5760f0,1,1,,"Bug #1371732 in neutron: ""create/update_port failure resulting in Lock wait timeout""","create_port can fail with a Lock wait timeout error.
The transaction performed in create_port/update_port makes a call to _process_port_bindings which then calls dvr_update_router_addvm.  The notification made within dvr_update_router_addvm can hang.  The transaction lock is held through the entire hang preventing other create_port  threads from getting the lock and then timing out with a ""Lock wait timeout.""
2014-09-19 04:16:53.332 3391 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ipavailabilityranges.allocation_pool_id AS ipavailabilityranges_allocation_pool_id, ipavailabilityranges.first_ip AS ipavailabilityranges_first_ip, ipavailabilityranges.last_ip AS ipavailabilityranges_last_ip \nFROM ipavailabilityranges INNER JOIN ipallocationpools ON ipallocationpools.id = ipavailabilityranges.allocation_pool_id \nWHERE ipallocationpools.subnet_id = %s \n LIMIT %s FOR UPDATE' ('550ee3a3-6c4d-4a4a-b173-b7603e43356a', 1)","Remove RPC notification from transaction in create/update port

Removing notifications to the L3 agent from within the transaction in
create_port and update_port eliminates many lock wait timeouts in the
dvr check queue job and in scale testing locally.

Since this patch leaves context unused in _process_port_binding, the
argument is removed from the method.

Closes-Bug: #1371732

Change-Id: Ibd86611ad3e7eff085d769bdff777a5870f30c58"
496,412e793780665b43c134ddd59907766549ecadd8,1393685116,,1.0,9,1,2,2,1,0.721928095,True,1.0,827786.0,18.0,6.0,False,13.0,1314633.5,15.0,2.0,18.0,769.0,773.0,17.0,570.0,573.0,16.0,611.0,613.0,0.0144435,0.519966015,0.521665251,521,942,1287760,glance,412e793780665b43c134ddd59907766549ecadd8,0,0,No bug: Task create should return 'Location' header ,"Bug #1287760 in Glance: ""Task create should return 'Location' header""","When creating a new task, the api correctly returns a 201. However per spec it should also return a 'Location' header with the URI to the newly created resource. Currently it does not.","Include Location header in POST /tasks response

In addition to returning a 201, a successful POST to /tasks
should include a Location header with the direct path to the
newly created resource.

Closes-bug: #1287760

Change-Id: I11001e1d6bb5168f0ee80c3d81574e59ddd47147"
497,41d851387cec122f4795d447458fd81e48e256b0,1397028782,,1.0,15,2,2,2,2,0.873981048,True,6.0,411577.0,26.0,4.0,True,,,,,,,,,,,,,,,,,766,1193,1304755,swift,41d851387cec122f4795d447458fd81e48e256b0,1,1,,"Bug #1304755 in OpenStack Object Storage (swift): ""Account-reaper never reaps account when account delete partially fails""","Account-reaper works at account-server with the first account replica, and reaps accounts with ""deleted"" status.
When swift fails to delete some account replicas, account-replicator doesn't replicate ""deleted"" status, but only *_timestamp.
So when swift fails to delete the first account replica, account-reaper will never reap it because its first replica will never have ""deleted"" status.
(If replica count is set as 3, swift returns 204, success return code to delete request in this situation)","Add timestamp checking in AccountBroker.is_status_deleted

Account-reaper works only at account-server with the first replica, and reaps
account with ""deleted"" status.

On the other hand, account-replicator doesn't replicate the status, only
replicates *_timestamp.
When swift fails to delete the first account replica, account-reaper never
reaps the account, because the first replica never gets marked as ""deleted"".

This patch adds a timestamp checking into is_status_deleted method, and
account-reaper will start to reap the account after account-replicator
replicates *_timestamp.

Change-Id: I75e3f15ad217a71b4fd39552cf6db2957597efca
Closes-Bug: #1304755"
498,423f344c59da1eecd7ab8bbb905abc1ff5948b14,1391959332,,1.0,55,47,2,2,1,0.986367607,True,4.0,128227.0,24.0,6.0,False,6.0,1169044.0,7.0,3.0,7.0,3829.0,3830.0,7.0,2503.0,2504.0,7.0,3524.0,3525.0,0.00109499,0.482480153,0.482617027,381,794,1278104,nova,423f344c59da1eecd7ab8bbb905abc1ff5948b14,1,0,Docker version bug,"Bug #1278104 in OpenStack Compute (nova): ""Docker cannot start a new instance because of an internal error""","Cannot create new instances because of an internal error. It seems like that the docker client returns None instead of a empty array if not a single container is started.
Update:
The root cause for the issue is that the docker v1.8 rest API doesn't longer deliver the container list for v1.4 api calls. We must upgrade to >= 1.7 in order to make the docker driver working again.
Additional does docker not set the Content-Type correctly. The docker client expects that the Content-Type is application/json but it is plain/text. The parsing of the content will be skipped in this case.
Please see also: https://github.com/dotcloud/docker/pull/3974
Stacktrace:
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 821, in init_host
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     self._destroy_evacuated_instances(context)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 532, in _destroy_evacuated_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     local_instances = self._get_instances_on_driver(context, filters)
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 511, in _get_instances_on_driver
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     driver_instances = self.driver.list_instances()
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/docker/driver.py"", line 96, in list_instances
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup     for container in self.docker.list_containers():
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
2014-02-09 15:01:44.077 TRACE nova.openstack.common.threadgroup","Docker cannot start a new instance because of an internal error

The root cause for the issue is that the Docker rest API does not
longer deliver the container list for v1.4 api calls.
The Docker API must be upgraded to 1.7.

Docker API 1.7 is a good choice since that is the first docker version
which officially supports rhel/centos.

In addition will docker client only parse json if the response needed
(to_json()) and this method will not catch json parse exception which makes
finding th eroot cause of a problem in the future easier.

Change-Id: I53dfac5f09ee021c6ed4763f7f164206e8ab32ff
Closes-Bug: #1278104"
499,429ac4dedd617f8c1f7c88dd8ece6b7d2f2accd0,1389047248,1.0,1.0,48,15,2,2,1,0.791858353,True,6.0,6046735.0,74.0,24.0,False,47.0,3644651.0,77.0,8.0,27.0,4810.0,4810.0,26.0,3699.0,3699.0,1.0,3590.0,3590.0,0.000286944,0.515208034,0.515208034,252,656,1266051,nova,429ac4dedd617f8c1f7c88dd8ece6b7d2f2accd0,1,1,,"Bug #1266051 in OpenStack Compute (nova): ""Attach/detach volume iSCSI multipath doesn't work properly if there are different targets associated with different portals for a mulitpath device.""","The connect_volume and disconnect_volume code in LibvirtISCSIVolumeDriver assumes that the targets for different portals are the same for the same multipath device. This is true for some arrays but not for others. When there are different targets associated with different portals for the same multipath device, multipath doesn't work properly during attach/detach volume operations.","Fixed a problem in iSCSI multipath

Multipathing for attach volume and detach volume operations
doesn't work properly if there are different targets
associated with different portals for a mulitpath device.

Change-Id: Iab316526cf64de60325d90043f73fa3e83fafb4e
Closes-Bug: #1266051"
500,42c7c7cfc96045930820c37b45f54ba717da117e,1411538046,,1.0,14,14,2,2,1,0.591672779,False,,,,,True,,,,,,,,,,,,,,,,,1350,1819,1373230,nova,42c7c7cfc96045930820c37b45f54ba717da117e,1,1,,"Bug #1373230 in OpenStack Compute (nova): ""start/stop instance in EC2 API shouldn't return active/stopped status immediately""","Always see this error in the gate:
http://logs.openstack.org/73/122873/1/gate/gate-tempest-dsvm-neutron-full/e5a2bf6/logs/screen-n-cpu.txt.gz?level=ERROR#_2014-09-21_05_18_23_709
014-09-21 05:18:23.709 ERROR oslo.messaging.rpc.dispatcher [req-52e7fee5-65ee-4c4d-abcc-099b29352846 InstanceRunTest-2053569555 InstanceRunTest-179702724] Exception during message handling: Unexpected task state: expecting [u'powering-off'] but the actual state is deleting
Checking the EC2 API test in tempest,
    def test_run_stop_terminate_instance(self):
        # EC2 run, stop and terminate instance
        image_ami = self.ec2_client.get_image(self.images[""ami""]
                                              [""image_id""])
        reservation = image_ami.run(kernel_id=self.images[""aki""][""image_id""],
                                    ramdisk_id=self.images[""ari""][""image_id""],
                                    instance_type=self.instance_type)
        rcuk = self.addResourceCleanUp(self.destroy_reservation, reservation)
        for instance in reservation.instances:
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""running"":
                self.assertInstanceStateWait(instance, ""running"")
        for instance in reservation.instances:
            instance.stop()
            LOG.info(""state: %s"", instance.state)
            if instance.state != ""stopped"":
                self.assertInstanceStateWait(instance, ""stopped"")
        self._terminate_reservation(reservation, rcuk)
The test is wait for instance become to stopped. But check the ec2 api code
https://github.com/openstack/nova/blob/master/nova/api/ec2/cloud.py#L1075
it always return stopped status immediately. Actually start/stop action is async call.","Fix start/stop return active/stopped immediately in EC2 API

start/stop action is async. But the EC2 API is hard code the status
as start/stop finished immediately.

This patch fix start action return pending status, and stop action
return stopping status.

Change-Id: I0cf6db1dcb17c48a12fd62b0d707afd124b96b26
Closes-Bug: #1373230"
501,42c882b9e98755b9e119f6c566fc00a62a420af7,1393451943,1.0,1.0,28,4,6,5,1,0.735765597,True,7.0,1975512.0,108.0,6.0,False,7.0,22257.0,13.0,2.0,679.0,772.0,1234.0,586.0,617.0,1008.0,213.0,370.0,458.0,0.246260069,0.426927503,0.528193326,488,906,1285383,neutron,42c882b9e98755b9e119f6c566fc00a62a420af7,0,0, make replication mode configurable ,"Bug #1285383 in neutron: ""NSX: make replication mode configurable""","The replication mode on switches and routers should have been configurable
to use use source replication if one did not want to deploy service node(s).","NSX: Make replication mode configurable

The replication mode on switches and routers should have been configurable
to use source replication if one did not want to deploy service node(s).
This patch fixes that by making this option configurable.

Change-Id: Id9e8043c602b5e9349c10247eab993e59db5a52c
Closes-bug: #1285383"
502,42cec80e6ce1c95326ab5871750401f3a8131f77,1410337278,1.0,1.0,32,25,3,3,1,0.903559228,False,,,,,True,,,,,,,,,,,,,,,,,1875,1367633,1367633,Nova,42cec80e6ce1c95326ab5871750401f3a8131f77,1,0,"""'createImage' server actions was missed for V2.1 API."". Problema!!!!!! Segun el desallorador alguien se olvido de cambiar el nombre a lo que falla (createImage->create_image) Hay algun commit en mi repo responsible de esto33 El que introduce la v33 Yo lo clasificaria como NoBIC","Bug #1367633 in OpenStack Compute (nova): ""Server actions 'createImage' does not work for v2.1 API""","'createImage' server action  does not work for V2.1 API.
This needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs","Fix 'createImage' server actions for V2.1 API

'createImage' server actions was missed for V2.1 API.
This patch converts 'createImage' server action for V2.1 API

The differences between v2 and v3 are described on the wiki page
https://wiki.openstack.org/wiki/NovaAPIv2tov3.

Change-Id: I5490365c72082b1652ddda2fed97c754fceeed22
Closes-Bug: #1367633"
503,42d017c0d498aa4034032104f9cdd56300c866e0,1407994479,,1.0,14,14,2,2,1,0.591672779,False,,,,,True,,,,,,,,,,,,,,,,,1191,1649,1356687,nova,42d017c0d498aa4034032104f9cdd56300c866e0,1,0,"“commit 243879f5c51fc45f03491bcb78765945ddf76be8”, “The jsonutils rule was added in:
    I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3”","Bug #1356687 in OpenStack Compute (nova): ""hacking check for jsonutils produces pep8 traceback""","the new jsonutils hacking check produces a pep8 traceback because it returns a set (column offset and error text) instead of an iterable (as logical line checks, like this check, should).
commit 243879f5c51fc45f03491bcb78765945ddf76be8
Change-Id: I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3
===== 8< ===== TEST CASE ===== 8< =====
$ echo 'foo = json.dumps(bar)' >nova/foobar.py
$ flake8 -vv nova/foobar.py
local configuration: in /home/dev/Desktop/nova-test
  ignore = E121,E122,E123,E124,E125,E126,E127,E128,E129,E131,E251,H405,H803,H904
  exclude = .venv,.git,.tox,dist,doc,*openstack/common*,*lib/python*,*egg,build,tools
checking nova/foobar.py
foo = json.dumps(bar)
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/.venv/bin/flake8"", line 9, in <module>
    load_entry_point('flake8==2.1.0', 'console_scripts', 'flake8')()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/main.py"", line 32, in main
    report = flake8_style.check_files()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1672, in check_files
    runner(path)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/flake8/engine.py"", line 73, in input_file
    return fchecker.check_all(expected=expected, line_offset=line_offset)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1436, in check_all
    self.check_logical()
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/pep8.py"", line 1338, in check_logical
    for offset, text in self.run_check(check, argument_names) or ():
TypeError: 'int' object is not iterable
===== 8< ===== TEST CASE ===== 8< =====
diff --git a/nova/hacking/checks.py b/nova/hacking/checks.py
index a1dd614..7fe7412 100644
--- a/nova/hacking/checks.py
+++ b/nova/hacking/checks.py
@@ -300,7 +300,7 @@ def use_jsonutils(logical_line, filename):
         for f in json_funcs:
             pos = logical_line.find('json.%s' % f)
             if pos != -1:
-                return (pos, msg % {'fun': f})
+                yield (pos, msg % {'fun': f})
 def factory(register):
===== 8< ===== PATCH ===== 8< =====
it's late, so tomorrow, if there hasn't been any activity on this, then i'll submit a patch for review.","Fix hacking check for jsonutils

Hacking checks with logical_lines should use yield not return.

The jsonutils rule was added in:
I86ed6cd3316dd4da5e1b10b36a3ddba3739316d3

Now that the function uses yield, it can trigger twice in dumps (dump
and dump), so look for a '(' at the afterwards. Don't display the '(' at
the end of the error message since its confusing to read.

Change-Id: I277dba08fdd30734409eee36008cebda35886968
Closes-Bug: #1356687"
504,4332bff3f500f062e7e06edccf0da5a9d9379d79,1368300937,,1.0,8,126,3,3,3,0.687450361,True,11.0,8784223.0,57.0,25.0,False,74.0,3450111.333,187.0,2.0,119.0,278.0,397.0,99.0,238.0,337.0,31.0,221.0,252.0,0.052718287,0.365733114,0.416803954,1427,1179007,1179007,Swift,4332bff3f500f062e7e06edccf0da5a9d9379d79,0,0,"""Migrate to pbr for build"",""pbr updates are also part of the upcoming automation around ensuring
global requirements stay in sync.""","Bug #1179007 in OpenStack Object Storage (swift): ""Migrate build system to pbr""","-----BEGIN PGP SIGNED MESSAGE-----
Hash: SHA1
openstack.common.setup and openstack.common.version are now in the
standalone library pbr. Migrating involves moving build config to
setup.cfg, copying in a stub setup.py file, adding pbr and d2to1 to the
build depends, removing openstack.common.(setup|version) from the
filesystem and from openstack-common.conf and making sure *.egg is in
.gitignore.
 affects ceilometer
 affects cinder
 affects git-review
 affects heat-cfntools
 affects heat
 affects keystone
 affects openstack-ci
 affects oslo
 affects python-ceilometerclient
 affects python-cinderclient
 affects python-gear
 affects python-glanceclient
 affects python-heatclient
 affects python-keystoneclient
 affects python-novaclient
 affects python-openstackclient
 affects python-quantumclient
 affects python-swiftclient
 affects reddwarf
 affects swift
 affects zuul
-----BEGIN PGP SIGNATURE-----
Version: GnuPG v1.4.12 (GNU/Linux)
Comment: Using GnuPG with undefined - http://www.enigmail.net/
iEYEARECAAYFAlGObdUACgkQ2Jv7/VK1RgFlkACgzycOW0/rPvnLaXXX9/oqYA7q
kGEAoMaEzGbFEAnsQA6+cEsKIUSMWAPD
=W8F0
-----END PGP SIGNATURE-----","Migrate to pbr for build

pbr is the libification of what was openstack.common.setup. If provides
the build information in a delcarative form, instead of as executable python
code, which works around the chicken and egg problem of needing setup
libraries present to run setup, but needing to run setup to tell if you
need setup libraries.

One of the features that comes along with this is versioning based on
git tags. If the current revision is a signed git tag, then that is the
version of the package. If it is not, the version is equal to the most
recent git tag, plus a commit count, plus a git sha (similar to git
describe, but scrubbed for python version rules compliance)

pbr updates are also part of the upcoming automation around ensuring
global requirements stay in sync.

Closes-Bug: #1179007
Change-Id: Ia473960be7e8aa44f09d48cea72ed3c8845f82fa"
505,43c1f98f074549611db90b6c333d2c54d7e4d4b3,1400758710,,1.0,67,101,9,8,1,0.89481193,False,,,,,True,,,,,,,,,,,,,,,,,924,1360,1323718,neutron,43c1f98f074549611db90b6c333d2c54d7e4d4b3,0,0,"Feature “Additionally, the patch improves”","Bug #1323718 in neutron: ""OVS capabilities test failure masks exception""","neutron/agent/linux/ovs_lib:ofctl_arg_supported executes an ovs-ofctl command and catches the general Exception, concluding that the feature is not supported. However, unexpected exceptions may be raised, swallowed and never logged. So, for example, if an OVS agent starts up and arp_responder is True, then the capabilities test for it may fail in an unexpected way. The result is that arp_responder will not be supported, the agent will start and the exception will not be printed, masking a potential bug.
Additionally, the capability test should be moved to the offline sanity command.","Move ARP responder test to sanity command

Additionally, the patch improves the check itself:
To check if the currently installed OVS supports the ARP responder
feature, we try to add a flow that references an OpenFlow ARP
extension via ofctl. The test may fail due to an (expected)
Runtime error, or due to some other unexpected error.
In such a case the error was previously masked and tossed away.

* Clean up ARP responder unit test
* Extract ARP responder flow actions to be used by the unit
  tests, functional test as well as the ARP responder code itself

After this patch, if the sanity check returned False but the
user never ran it or ignored its results, the OVS agent will
output errors to the log every time an ARP entry is (attempted)
to be added or removed from the flow table.

Closes-Bug: #1323718
Change-Id: I428c954d6561cd398a1e580804a9482969a154af"
506,43e102cc215aabc72a0ef84cf48965c4db2d58d4,1377565106,,1.0,4,4,2,1,1,1.0,True,2.0,815235.0,18.0,8.0,False,28.0,506209.0,63.0,5.0,0.0,2422.0,2422.0,0.0,2209.0,2209.0,0.0,1587.0,1587.0,0.00017138,0.272150814,0.272150814,1481,1215019,1215019,nova,43e102cc215aabc72a0ef84cf48965c4db2d58d4,1,1, “The powervm driver should log ssh stderr as warning rather than debug”,"Bug #1215019 in OpenStack Compute (nova): ""The powervm driver should log ssh stderr as warning rather than debug""","There are 4 methods in the powervm driver that run ssh calls to the backing hypervisor (2 in blockdev, 2 in operator) and check for stderr output but only log it at debug level, which could be masking more serious issues. The stderr results should be logged at warning instead.
https://github.com/openstack/nova/blob/master/nova/virt/powervm/blockdev.py#L563
https://github.com/openstack/nova/blob/master/nova/virt/powervm/blockdev.py#L580
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L694
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L711","Powervm driver now logs ssh stderr to warning

The run_vios_command, run_vios_command_as_root methods in
blockdevice.py and operator.py of powervm driver currently log
stderr at debug level, this commit changed it to log to warning
level rather than debug.

Closes-Bug: #1215019
Change-Id: I903067476049ead01c6c34392342d915917790b3"
507,4431eec1c94c4a353b45e5d873854b3fb1eaa11b,1406556944,0.0,1.0,130,1,2,2,1,0.928139351,False,,,,,True,,,,,,,,,,,,,,,,,1039,1483,1337349,nova,4431eec1c94c4a353b45e5d873854b3fb1eaa11b,1,1,,"Bug #1337349 in OpenStack Compute (nova): ""Nova qemu hypervisor host smbios serial number is leaked to guest""","Erwan Velu from eNovance reported a vulnerability in OpenStack Nova.
The hypervisor is passing host system uuid (-smbios version) to guests, and this happen to be a critical info leak.
The defect have been pinpointed to:
 https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L3054
From a simple virtual machine, this may allow numerous info leak like:
    Allow compute hardware enumeration from guests
    Deduce service tag and get all hardware configuration
    Ability to know if two instances are on the same compute
Dell hardware is particulary impacted as :
    - the uuid encodes the service tag
    - the service tag can be used on support site to determine:
    - detailled hardware configuration
    - date & country where the hw was shipped
    - date & type of support contract
    - amount of servers bought during this shipment
If there is no use case for this, we should scrambled that piece of information.","libvirt: make sysinfo serial number configurable

The 'serial' field in guest SMBIOS tables gets populated
based on the libvirt reported UUID of the host hardware.
The rationale is to allow correlation of guests running
on the same host.

Unfortunately some hardware vendors use a subset of the
host UUID as a key for retrieving hardware support
contract information without requiring any authentication.
So exposing the host UUID to the guest is an information
leak for those vendors. It is possible to override the
use of SMBIOS data by libvirt in /etc/libvirt/libvirtd.conf
by setting the 'host_uuid' parameter.

As a way to reduce the configuration burden though, it is
preferrable to use the /etc/machine-id UUID, instead of
the host hardware UUID. The former is a recent standard
for Linux distros introduced by systemd to provide a UUID
that is unique per operating system install. This means
that even containers will see a separate /etc/machine-id
value. This /etc/machine-id can be expected to be widely
available in current and future distros. If missing, it
is still possible to fallback to the libvirt reported
host UUID.

The host UUID exposed could theoretically be leveraged
by a cloud user to get an approximate count of the number
of unique hosts available to them in the cloud by launching
many short lived VMs. Administrators concerned about this
may wish to disable reporting of any sysinfo serial field
at all.

Introduce a 'sysinfo_serial' config parameter to the libvirt
driver to control behaviour, accepting values:

 - 'auto' - try /etc/machine-id, fallback to
   libvirt reported host UUID (new default)
 - 'hardware' - always use libvirt host UUID (old default)
 - 'os' - always use /etc/machine-id, error if missing
 - 'none' - do not report any value to the guest

DocImpact: new libvirt.sysinfo_serial config parameter
SecurityImpact
Closes-bug: #1337349
Change-Id: I7ba7dbd65e913a66efe35a1d6490a85bec8413da"
508,44545eab5e86d0837f4032759f2d8e8e6f9846b0,1392695754,,1.0,40,4,7,6,1,0.720164528,True,8.0,2721000.0,98.0,26.0,False,197.0,1148003.857,759.0,11.0,14.0,3654.0,3659.0,14.0,2497.0,2502.0,6.0,3347.0,3347.0,0.000949539,0.454150841,0.454150841,280,685,1267685,nova,44545eab5e86d0837f4032759f2d8e8e6f9846b0,0,0,Support IPv6,"Bug #1267685 in OpenStack Compute (nova): ""boot vm don't support ipv6""","when boot vm, it can use '--nic <net-id=net-uuid,v4-fixed-ip=ip-addr, port-id=port-uuid>' to set network info, if it want to use ipv6,it hase to use port-id which hase a ipv6 address, I think it should can use '--nic net-id=net-uuid, fixed-ip=ip-addr' which include ipv4 and ipv6 address.Currently,nova already prevent that:
if address is not None and not utils.is_valid_ipv4(address):
   msg = _(""Invalid fixed IP address (%s)"") % address
    raise exc.HTTPBadRequest(explanation=msg)","Support IPv6 when booting instances

When nova uses neutron, IPv6 addresses may be allocated.  This ensures
those addresses are supported.

Change-Id: Ic904f3a7811b07264128bc62067cdf0bca9efac9
Closes-bug: #1267685"
509,4469591c76888bad263662df424c3f266ded8d48,1404723084,,1.0,7,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1045,1489,1338481,cinder,4469591c76888bad263662df424c3f266ded8d48,1,1,,"Bug #1338481 in Cinder: ""Fix error log level in restore-backup routine""","When doing backup-restore operation, it needs volume's size is not less than backup's size.
But in backup-restore routine, when volume's size is bigger than backup's size, it logs the
the  info of volume's size and backup's size to warn level, which I think it should log the info
to info level.
This bug fix it.","Fix error log level in restore-backup routine

When doing backup-restore operation, it needs volume's size is not less
than backup's size.
But in backup-restore routine, when volume's size is bigger than backup's
size, it logs the the info of volume's size and backup's size to warn
level, which I think it should log the info to info level.
This bug fix it.

Change-Id: Ic7210e3b018053be7a6f567cd0a7b20d51d403b8
Closes-Bug: #1338481"
510,4499b62738e5ad25838eae0398f0d15bf5132387,1406553563,,1.0,64,52,3,1,1,0.643077638,False,,,,,True,,,,,,,,,,,,,,,,,1859,1343544,1343544,Cinder,4499b62738e5ad25838eae0398f0d15bf5132387,1,0,"""It looks like a limitation with logging in python 2.6.""","Bug #1343544 in Cinder: ""AttributeError: ContextAdapter instance has no attribute 'isEnabledFor' with python 2.6""","Running tempest against juno code on RHEL 6.5 (python 2.6), I'm seeing this in the cinder-volume logs:
I'm seeing several warnings from the taskflow code in the cinder volume log:
2014-07-17 02:36:27.703 57746 WARNING taskflow.utils.misc [req-b459108b-be18-4922-94b9-1f0281764bfb 4deacb6f0bb7409cb1bfab8d7080e61f 574d6c38c2fd4ce697071225ecdf2125 - - -] Failure calling callback <bound method DynamicLogListener._task_receiver of <cinder.flow_utils.DynamicLogListener object at 0x9997d4e0>> to notify about event SUCCESS, details: {'task_uuid': '6ef1507b-7749-40ab-8d5e-55bbf9347229', 'result': None, 'task_name': 'cinder.volume.flows.manager.create_volume.CreateVolumeOnFinishTask;volume:create, create.end'}
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc Traceback (most recent call last):
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc   File ""/usr/lib/python2.6/site-packages/taskflow/utils/misc.py"", line 596, in notify
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc     callback(event_type, *args, **kwargs)
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc   File ""/usr/lib/python2.6/site-packages/cinder/flow_utils.py"", line 104, in _task_receiver
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc     if (self._logger.isEnabledFor(base_logging.DEBUG) or
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc AttributeError: ContextAdapter instance has no attribute 'isEnabledFor'
2014-07-17 02:36:27.703 57746 TRACE taskflow.utils.misc
This is basically the same issue as keystone bug 1213284.
It looks like a limitation with logging in python 2.6.
We could just add isEnabledFor to the oslo log ContextAdapter in py26 for now to delegate to logger.isEnabledFor or punt (return False) until this is all moving to the server projects with oslo.log:
https://blueprints.launchpad.net/oslo/+spec/remove-context-adapter","Sync log from oslo-incubator for isEnabledFor fix

Sync log.py to get commit:

726d00a Adjust oslo logging to provide adapter is enabled for

Which is needed to fix an issue with logging in flow_utils
when using python 2.6.

This also brings in dependencies.

Changes:

jsonutils
---------
ef37e03 Added missing jsonutils.dump() function

log
---
726d00a Adjust oslo logging to provide adapter is enabled for
433fa0b Make logging_context_format_string optional in log.set_defaults
ac92c06 Add default log level for websocket
5fd77eb Ability to customize default_log_levels for each project
4d9328c Python 3: enable tests/unit/test_log.py
cb5a804 Move `mask_password` to strutils
3310d8d update new requests logger to default WARN

strutils
--------
cb5a804 Move `mask_password` to strutils

Change-Id: Iea344e4bd3a612cd4110143e4fbb8e2fdfd88165
Closes-Bug: #1343544"
511,44b3783ceb94934bf8d4d62f704d42b49e52866e,1387658928,,1.0,4,2,1,1,1,0.0,True,4.0,1005674.0,38.0,13.0,False,17.0,705080.0,25.0,3.0,197.0,704.0,830.0,196.0,583.0,709.0,138.0,302.0,385.0,0.213846154,0.466153846,0.593846154,193,596,1261652,neutron,44b3783ceb94934bf8d4d62f704d42b49e52866e,1,0,For postgresql specific,"Bug #1261652 in neutron: ""Internal server error when deleting empty network with OVS plugin""","Deleting a network which has no ports and no subnets on it produces the following stack trace in neutron-server logs when using PostgreSql:
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/openvswitch/ovs_neutron_plugin.py"", line 526, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     super(OVSNeutronPluginV2, self).delete_network(context, id)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 999, in delete_network
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     for p in ports)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2227, in __iter__
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     return self._execute_and_instances(context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2242, in _execute_and_instances
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     result = conn.execute(querycontext.statement, self._params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     params)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1584, in _execute_clauseelement
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     compiled_sql, distilled_params
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     context)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource     cursor.execute(statement, parameters)
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource NotSupportedError: (NotSupportedError) SELECT FOR UPDATE/SHARE cannot be applied to the nullable side of an outer join
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource  'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner, ipallocations_1.port_id AS ipallocations_1_port_id, ipallocations_1.ip_address AS ipallocations_1_ip_address, ipallocations_1.subnet_id AS ipallocations_1_subnet_id, ipallocations_1.network_id AS ipallocations_1_network_id, allowedaddresspairs_1.port_id AS allowedaddresspairs_1_port_id, allowedaddresspairs_1.mac_address AS allowedaddresspairs_1_mac_address, allowedaddresspairs_1.ip_address AS allowedaddresspairs_1_ip_address, extradhcpopts_1.id AS extradhcpopts_1_id, extradhcpopts_1.port_id AS extradhcpopts_1_port_id, extradhcpopts_1.opt_name AS extradhcpopts_1_opt_name, extradhcpopts_1.opt_value AS extradhcpopts_1_opt_value, portbindingports_1.port_id AS portbindingports_1_port_id, portbindingports_1.host AS portbindingports_1_host, securitygroupportbindings_1.port_id AS securitygroupportbindings_1_port_id, securitygroupportbindings_1.security_group_id AS securitygroupportbindings_1_security_group_id \nFROM ports LEFT OUTER JOIN portbindingports ON ports.id = portbindingports.port_id LEFT OUTER JOIN ipallocations AS ipallocations_1 ON ports.id = ipallocations_1.port_id LEFT OUTER JOIN allowedaddresspairs AS allowedaddresspairs_1 ON ports.id = allowedaddresspairs_1.port_id LEFT OUTER JOIN extradhcpopts AS extradhcpopts_1 ON ports.id = extradhcpopts_1.port_id LEFT OUTER JOIN portbindingports AS portbindingports_1 ON ports.id = portbindingports_1.port_id LEFT OUTER JOIN securitygroupportbindings AS securitygroupportbindings_1 ON ports.id = securitygroupportbindings_1.port_id \nWHERE ports.tenant_id = %(tenant_id_1)s AND ports.network_id IN (%(network_id_1)s) FOR UPDATE' {'network_id_1': u'f8344378-ed58-4ce3-99ae-408c8591cda9', 'tenant_id_1': u'df3896201a174787b65b2b098aad2968'}
2013-12-17 12:43:49.908 7347 TRACE neutron.api.v2.resource
2013-12-17 12:43:49.920 7347 INFO neutron.wsgi [req-b8b4b523-cf50-4bcb-8356-19bbc4b9d057 8549dfbbb00940b9b3659230186bd26d df3896201a174787b65b2b098aad2968] 192.168.1.51 - - [17/Dec/2013 12:43:49] ""DELETE /v2.0/networks/f8344378-ed58-4ce3-99ae-408c8591cda9.json HTTP/1.1"" 500 230 0.063668","Fix empty network deletion in db_base_plugin for postgresql

Need to disable joined loads for port query in delete_network
as in other methods.

No unit test is being added, because the problem is not reproducible with
sqlite.

Closes-Bug: #1261652

Change-Id: I7a015be6f2f9b8ae43bc29a767d16af30120f13f"
512,44eff5daad81966f6530d84ca3683b2f9c58debf,1408357946,,1.0,6,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1209,1667,1358206,neutron,44eff5daad81966f6530d84ca3683b2f9c58debf,1,1,,"Bug #1358206 in neutron: ""ovsdb_monitor.SimpleInterfaceMonitor throws eventlet.timeout.Timeout(5)""","This is found during functional testing, when .start() is called with
block=True during sightly high load.
This suggest the default timeout needs to be rised to make this module
work in all situations.
https://review.openstack.org/#/c/112798/14/neutron/agent/linux/ovsdb_monitor.py (I will extract patch from here)","Increase ovsdb_monitor.SimpleInterfaceMonitor start timeout

The current timeout fails during functional testing with
slightly higher load. Increasing it will lower the timeout
chances during high load conditions. Changed from 5 seconds
to respawn_interval/2. DEFAULT_OVSDBMON_RESPAWN = 30 , so
the default timeout will be 15 seconds.

Change-Id: I6a9e2977b275e96dcf01c4df90a33169c42287d6
Closes-Bug: #1358206"
513,44fd0f74e5b19593e9d37eaec1f87134003b10f4,1378900016,,1.0,40,18,2,2,1,0.29367631,True,7.0,448971.0,41.0,19.0,False,12.0,1320533.0,18.0,7.0,8.0,1613.0,1613.0,8.0,1499.0,1499.0,1.0,274.0,274.0,0.006430868,0.884244373,0.884244373,1518,1221315,1221315,neutron,44fd0f74e5b19593e9d37eaec1f87134003b10f4,0,0,Feature3 “_validate_network_tenant must be less strict”,"Bug #1221315 in neutron: ""_validate_network_tenant_ownership must be less strict""","Neutron, currently does a strict validation code in https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L618
so that for non-shared network the subnets and ports must belong to the same tenant as the network. In the case of a “service VM” created by an admin user, this function should return thus allowing admin users to create ports and networks in a tenant network.
Original code: https://github.com/openstack/neutron/blob/master/neutron/api/v2/base.py#L604
Proposed Fix:
    def _validate_network_tenant_ownership(self, request, resource_item):
        # TODO(salvatore-orlando): consider whether this check can be folded
        # in the policy engine
        if self._resource not in ('port', 'subnet') or request.context.is_admin:
            return
        network = self._plugin.get_network(
            request.context,
            resource_item['network_id'])
        # do not perform the check on shared networks
        if network.get('shared'):
            return
        network_owner = network['tenant_id']
        if network_owner != resource_item['tenant_id']:
            msg = _(""Tenant %(tenant_id)s not allowed to ""
                    ""create %(resource)s on this network"")
            raise webob.exc.HTTPForbidden(msg % {
                ""tenant_id"": resource_item['tenant_id'],
                ""resource"": self._resource,
            })","_validate_network_tenant_ownership must be less strict

Neutron, currently does a strict validation code
so that for non-shared network the subnets and
ports must belong to the same tenant as the network. In
the case of a ""service VM"" created by
admin user, this function should return thus allowing
admin users to create ports and networks in a tenant
network.

Change-Id: Ied831402d56b98a1323d30eb6a769fd2df5278ee
Closes-Bug: #1221315"
514,4502759243c0f363545f4236b41abaf8bcf9a37b,1408535377,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1206,1664,1357462,glance,4502759243c0f363545f4236b41abaf8bcf9a37b,1,0,“Since the `_upload` method loads the store based on the scheme”,"Bug #1357462 in Glance: ""glance cannot find store for scheme vmware_datastore""","I have python-glance-2014.1.2-1.el7ost.noarch
when configuring
default_store=vmware_datastore
known_stores = glance.store.vmware_datastore.Store
vmware_server_host = 10.34.69.76
vmware_server_username=root
vmware_server_password=qum5net
vmware_datacenter_path=""New Datacenter""
vmware_datastore_name=shared
glance-api doesn't seem to come up at all.
glance image-list
Error communicating with http://172.16.40.9:9292 [Errno 111] Connection refused
there seems to be nothing interesing in the logs. After changing to the
  default_store=file
  glance image-create --disk-format vmdk --container-format bare     --copy-from 'http://str-02.rhev/OpenStack/cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
or
  glance image-create --disk-format vmdk --container-format bare     --file 'cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
the image remains in queued state
I can see log lines
2014-08-15 12:38:55.885 24732 DEBUG glance.store [-] Registering store <class 'glance.store.vmware_datastore.Store'> with schemes ('vsphere',) create_stores /usr/lib/python2.7/site-packages/glance/store/__init__.py:208
2014-08-15 12:39:54.119 24764 DEBUG glance.api.v1.images [-] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057
2014-08-15 12:43:31.408 24764 DEBUG glance.api.v1.images [eac2ff8d-d55a-4e2c-8006-95beef8a0d7b caffabe3f56e4e5cb5cbeb040224fe69 77e18ad8a31e4de2ab26f52fb15b3cc1 - - -] Store for scheme vmware_datastore not found get_store_or_400 /usr/lib/python2.7/site-packages/glance/api/v1/images.py:1057
so it looks like there is inconsistency on the scheme that should be used. After hardcoding
  STORE_SCHEME = 'vmware_datastore'
in the
  /usr/lib/python2.7/site-packages/glance/store/vmware_datastore.py
the behaviour changed, but did not improve very much:
  glance image-create --disk-format vmdk --container-format bare     --file 'cirros-0.3.1-x86_64-disk.vmdk'     --name cirros-0.3.1-x86_64-disk.vmdk --is-public true     --property vmware_disktype=""sparse""     --property vmware_adaptertype=""ide""     --property vmware_ostype=""ubuntu64Guest"" --name prdel --store vmware_datastore
400 Bad Request
Store for image_id not found: 7edc22ae-f229-4f21-8f7d-fa19a03410be
    (HTTP 400)","Check on schemes not stores

Since the `_upload` method loads the store based on the scheme, we need
to check whether the scheme exists in the known schemes instead of
checking in the list of known stores. Without this fix, stores with a
scheme name different than the store itself will not be usable through
the `x-image-meta-store` header.

Change-Id: Ia33afc1629cb47a31982df7932ac0d9aeb201b4f
Closes-bug: #1357462"
515,45381fe1c742c75773d97f1c0bd1f3cb1e7a6468,1399226077,,1.0,4,3,1,1,1,0.0,True,3.0,1547528.0,70.0,25.0,False,27.0,415937.0,52.0,6.0,16.0,1404.0,1418.0,15.0,1137.0,1150.0,3.0,790.0,791.0,0.003314002,0.655343828,0.656172328,853,1286,1315467,neutron,45381fe1c742c75773d97f1c0bd1f3cb1e7a6468,1,1,,"Bug #1315467 in neutron: ""Neutron deletes the router interface instead of adding a floatingip""","After parsing a lot of log files related to check failure, looks like the q-vpn at the time when I would expect to add
floating ip , it destroys the router's qg- and qr-  interfaces.
However after the floating ip deletion request the q-vpn service restores the qg-, qr- interfaces.
tempest.scenario.test_minimum_basic.TestMinimumBasicScenario.test_minimum_basic_scenario[compute,image,network,volume]
failed in http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/console.html.
admin user: admin/9e02f14321454af6bb27587770f27d9b
admin tenant id: admin/413bb1232bca45069f3a3256839effa1
test user: TestMinimumBasicScenario-1306090821/c8dd95056c0b407e8dd168dbf410a66a
test Tenant:  TestMinimumBasicScenario-993819377/2527b8222e3343bca9f70343e608880c
External Net : public/c29040d3-7e73-4a87-9f73-bb5cbe602afb
External subnet: public-subnet/ee754eb6-6194-4a25-a4cc-f9233d366c1e
Network: TestMinimumBasicScenario-1375749858-network/f029d7a8-54e0-484c-a215-cc34066ae830
Subnet: TestMinimumBasicScenario-1375749858-subnet/7edc72be-1207-4571-95d4-911223885ae7  10.100.0.0/28
Router id:TestMinimumBasicScenario-1375749858-router/08216822-5ee2-4313-be7e-dad2d84147db
Expected interfaces in the qrouter-08216822-5ee2-4313-be7e-dad2d84147db:
* lo 127.0.0.1
* qr-529eddd4-2c 10.100.0.1/28 iface_id: 529eddd4-2ca8-43ec-9cab-29c3a6632604, attached-mac: fa:16:3e:2a:f8:ba, ofport 166
* qg-9be8f502-93 172.24.4.85/24 iface-id: 9be8f502-9360-47dc-9eff-33c8743e7c2b  attached-mac: fa:16:3e:be:a1:54, ofport 37
Floating IP: 172.24.4.87 (Never appears in the q-vpn log)
port: (net/subnet/port)(c29040d3-7e73-4a87-9f73-bb5cbe602afb/ee754eb6-6194-4a25-a4cc-f9233d366c1e/013b0b2d-80ed-403d-b380-6b6895ce34f5)
mac?: fa:16:3e:15:f2:57
floating ip uuid: cd84111e-af6a-4c26-af73-3167419c664a
Instance:
Ipv4: 10.100.0.2 mac: FA:16:3E:18:F1:69
Instance uuid: 8a552eda-2fbd-4972-bfcf-cee7e6472871
iface_id/port_id: 4188c532-3265-4294-8b4e-9bbfe5a482e8
ovsdb_interface_uuid: 9d7b858b-745e-482b-b91a-1e9ae34fc545
intbr tag: 49
dhcp server dev: tap4c6c6e06-e4
ns: qdhcp-f029d7a8-54e0-484c-a215-cc34066ae830
ip: 10.100.0.3 mac: fa:16:3e:a2:f1:ea
intbr tag: 49
Host:
eth0: 10.7.16.229/15 mac: 02:16:3e:52:5d:ff
Router + router interface creation in the logs:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_49_46_924
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-svc.txt.gz#_2014-05-01_19_49_46_724
Floating IP create:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_447
Floating IP associate:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_50_18_814
q-vpn starts destroying the router:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_50_20_277
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-c0cd93e7-5cb0-403b-a509-8e07b352b89d', 'ipsec', 'whack', '--ctlbase', '/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto', '--status']
Exit code: 1
Stdout: ''
Stderr: 'whack: Pluto is not running (no ""/opt/stack/data/neutron/ipsec/c0cd93e7-5cb0-403b-a509-8e07b352b89d/var/run/pluto.ctl"")\n' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.277 19279 DEBUG neutron.openstack.common.lockutils [-] Semaphore / lock released ""sync"" inner /opt/stack/new/neutron/neutron/openstack/common/lockutils.py:252
2014-05-01 19:50:20.277 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.516 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-08216822-5ee2-4313-be7e-dad2d84147db', 'ip', '-o', 'link', 'show', 'qr-529eddd4-2c']
Exit code: 0
Stdout: '605: qr-529eddd4-2c: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UNKNOWN \\    link/ether fa:16:3e:2a:f8:ba brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.517 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-int'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:20.533 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['ip', '-o', 'link', 'show', 'br-int']
Exit code: 0
Stdout: '6: br-int: <BROADCAST,MULTICAST> mtu 1500 qdisc noop state DOWN \\    link/ether c6:49:9f:72:d2:4a brd ff:ff:ff:ff:ff:ff\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:74
2014-05-01 19:50:20.534 19279 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:48
2014-05-01 19:50:21.061 19279 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=10', '--', '--if-exists', 'del-port', 'br-int', 'qr-529eddd4-2c']
Exit code: 0
Tempest tries to connect to the VM:
2014-05-01 19:50:19,463 .. 2014-05-01 19:53:48,494 ~ 209 sec
Delete request for the floatingip:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_24_191
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-n-api.txt.gz#_2014-05-01_19_54_25_047
Neutron touches the qr-529eddd4-2c interface + port,
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/screen-q-vpn.txt.gz#_2014-05-01_19_54_37_886
Delete request for router and router interface:
http://logs.openstack.org/79/88579/2/check/check-tempest-dsvm-neutron-full/f76ee0e/logs/tempest.txt.gz#_2014-05-01_19_54_57_423","L3 RPC loop could delete a router on concurrent update

routers_updated does not acquire any lock just updates
a set for future rpc loop processing.

The self.updated_routers can be changed by concurrent update
notification. If this change happens at the time around the
self.plugin_rpc.get_routers call, the additional routers
- by mistake - is considered as admin_state_up=false routers, which
 are safe to delete.

Creating a local copy of the updated_routers and preserve
the fresh updated_routers entries for the next _rpc_loop
operations.

Change-Id: Icc7377f9c29e248c3b34562465e859b15ecc2ec3
Closes-Bug: #1315467
Partial-Bug: #1253896"
516,4539ff26d903174c844d6571533dfe719195e107,1398411466,,1.0,56,1,2,2,1,0.297472249,True,3.0,1150431.0,49.0,8.0,False,19.0,1535259.5,32.0,5.0,5.0,772.0,774.0,5.0,681.0,683.0,4.0,561.0,562.0,0.004226543,0.475063398,0.475908707,829,1258,1311971,neutron,4539ff26d903174c844d6571533dfe719195e107,1,1,,"Bug #1311971 in neutron: ""database exception causes UnboundLocalError in linuxbridge-agent""","When database exception raises in update_device_down, the linuxbridge-agent doesn't deal with them in a proper way that causes accessing not-existed local variables.
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent Traceback (most recent call last):
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 997, in daemon_loop
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     sync = self.process_network_devices(device_info)
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 894, in process_network_devices
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     resync_b = self.treat_devices_removed(device_info['removed'])
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent   File ""/usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 963, in treat_devices_removed
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent     if details['exists']:
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent UnboundLocalError: local variable 'details' referenced before assignment
2014-04-22 20:35:53.436 494 TRACE neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
2014-04-22 20:35:53.437 494 DEBUG neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Loop iteration exceeded interval (2 vs. 51.2715768814)! daemon_loop /usr/lib/python2.6/site-packages/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py:1011
2014-04-22 20:35:53.438 494 INFO neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent [req-ffc712fc-80af-4837-a068-6e1a076e4ebc None] Agent out of sync with plugin!","Database exception causes UnboundLocalError in linuxbridge-agent

When database exception raises from update_device_down,
the linuxbridge-agent doesn't deal with them in a proper way
that causes accessing not-existed local variables.

A straightforward workaround is to set it as None by default
and verify its value then.

Change-Id: I32a9467876a619a7d0ad53ff3d5d95ff977e54f4
Closes-Bug: #1311971"
517,45489f9814fce0f7b3f6c947076529962902f729,1385108479,,1.0,0,1,1,1,1,0.0,True,1.0,180490.0,7.0,4.0,False,13.0,3552.0,19.0,3.0,12.0,656.0,661.0,12.0,649.0,654.0,7.0,623.0,623.0,0.006785411,0.529262087,0.529262087,1795,1253931,1253931,cinder,45489f9814fce0f7b3f6c947076529962902f729,0,0,"“Remove unused code in test_admin_actions.py”, refactoring","Bug #1253931 in Cinder: ""Remove unused code in test_admin_actions.py""","Remove unused code in test_admin_actions.py - in _migrate_volume_comp_exec(), admin_ctx is not used.","Remove unused code in test_admin_actions.py

Change-Id: I66fa378005d0e8b181274a47b1746a1fe787e716
Closes-Bug: #1253931"
518,45553a6dda84ef6be38150071f8bb6fa9850a53e,1410830638,,1.0,37,10,5,5,1,0.852906716,False,,,,,True,,,,,,,,,,,,,,,,,1871,1365751,1365751,Nova,45553a6dda84ef6be38150071f8bb6fa9850a53e,0,0,"The only file that is not a test and has been changed was the hacking file ""Add a new HACKING rule for nova to prevent assert_called_once()
usage from creeping in""","Bug #1365751 in OpenStack Compute (nova): ""Use of assert_called_once() instead of assert_called_once_with()""","mock.assert_called_once() is a noop, it doesn't test anything.
Instead it should be mock.assert_called_once_with()
This occurs in the following places:
  Nova
    nova/tests/virt/hyperv/test_ioutils.py
    nova/tests/virt/libvirt/test_driver.py
  Cliff
    cliff/tests/test_app.py
  Neutron
    neutron/tests/unit/services/l3_router/test_l3_apic_plugin.py
    neutron/tests/unit/services/loadbalancer/drivers/radware/test_plugin_driver.py
    neutron/tests/unit/test_l3_agent.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_sync.py
    neutron/tests/unit/ml2/drivers/cisco/apic/test_cisco_apic_mechanism_driver.py","mock.assert_called_once() is not a valid method

mock.assert_called_once() is a no-op that tests nothing.  Instead
with mock.assert_called_once_with() should be used (or use
assertEqual(1, mock_obj.call_count) if you don't want to check
parameters).

Add a new HACKING rule for nova to prevent assert_called_once()
usage from creeping in.

Closes-Bug: #1365751

Change-Id: I1055093a1c31792b6411a3cd46c80b8bcaaf78c1"
519,45557c43e4f8e42a98c09deb45739bebaa5b8507,1389612820,,1.0,3,1,1,1,1,0.0,True,2.0,233959.0,26.0,7.0,False,8.0,3516617.0,11.0,3.0,568.0,2095.0,2314.0,452.0,1843.0,2024.0,217.0,344.0,461.0,0.319180088,0.505124451,0.676427526,286,692,1268561,neutron,45557c43e4f8e42a98c09deb45739bebaa5b8507,0,0,tests,"Bug #1268561 in neutron: ""test_create_security_group_rule_bad_tenant succeeds for the wrong reason""","https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_extension_security_group.py#L760
When the security group is created, the security group has already been deleted.
This means that the notfound error is raised because the security group has been removed, and not because the rule is being created with the wrong tenant id.","Fix negative unit test for sec group rules

Ensure the test fails because the security group
rule could not be created, and not because the security
group was deleted before creating the rule.

Closes-Bug: #1268561

Change-Id: Ib2ea8c4507862d7bb343ce5181cf018b3cdf4b73"
520,4559a5f381082dba30bb535b69deeb09135da680,1394144954,,1.0,176,1,5,2,1,0.850749571,True,6.0,6451556.0,95.0,16.0,False,14.0,576528.6,18.0,5.0,7.0,1703.0,1704.0,7.0,1471.0,1472.0,4.0,787.0,787.0,0.00547046,0.86214442,0.86214442,549,970,1289007,neutron,4559a5f381082dba30bb535b69deeb09135da680,1,1,This leads to erroneous values ,"Bug #1289007 in neutron: ""Hyper-V agent does not count metrics for individual ports""","The Hyper-V agent is currently obtaining aggregated metrics instead of values for each individual port.
This leads to erroneous values in case instances have multiple ports.","Fixes the Hyper-V agent individual ports metrics

Replaces aggregated metric values with separated values for each port.

Co-Authored-By: Adrian Vladu <avladu@cloudbasesolutions.com>
Change-Id: Ie946dff984ef53f014c6c57f8d1d5bb9c6e7596d
Closes-Bug: #1289007"
521,45a523681f2136f8fefb6c3da44540decd6a0fda,1410831608,,1.0,2,10,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1348,1817,1373100,neutron,45a523681f2136f8fefb6c3da44540decd6a0fda,1,1,“1373100” revert,"Bug #1373100 in neutron: ""New race condition exposed when cleaning up floating ips on router delete""","The patch that cleans up floating ips on router deletion [1] has triggered a race condition that causes spurious failures in the dvr job in the check queue.  Reverting this patch [2] has shown to stabilize it.
[1] https://review.openstack.org/#/c/120885/
[2] https://review.openstack.org/#/c/121729/","Revert ""Cleanup floatingips also on router delete""

This reverts commit c3326996e38cb67f8d4ba3dabd829dc6f327b666.

The patch being reverted here addresses an issue that can no longer be
reproduced, in that under no circumstances, I can make the FIP lie around
before deleting a router (which can only be done after all FIP have been
disassociated or released).

Unless we have more clarity as to what the initial commit was really meant
to fix, there is a strong case for reverting this patch at this point.

Closes-bug: #1373100

Change-Id: I7e0f80e456ff4d9eb57a1d31c6ffc7cdfca5a163"
522,4640542cc502a442d4807822d75d3fa3eff0a33b,1394062620,0.0,1.0,56,22,3,2,1,0.809496442,True,11.0,5617449.0,143.0,40.0,False,58.0,528022.3333,156.0,9.0,2.0,4190.0,4190.0,2.0,3410.0,3410.0,2.0,3199.0,3199.0,0.000398883,0.425475336,0.425475336,511,931,1287292,nova,4640542cc502a442d4807822d75d3fa3eff0a33b,1,1,incorrect use,"Bug #1287292 in OpenStack Compute (nova): ""VMware: vim.get_soap_url improper IPv6 address""","The vim.get_soap_url function incorrectly builds an IPv6 address using hostname/IP and port.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vim.py#L151
The result of this line would create an address as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348:443]/sdk
Ports should be outside the square brackets, not inside, as follows:
https://[2001:db8:85a3:8d3:1319:8a2e:370:7348]:443/sdk
For reference see: http://en.wikipedia.org/wiki/IPv6_address section Literal IPv6 addresses in network resource identifiers","VMware: Fix usage of an alternate ESX/vCenter port

IPv6 addresses use square brackets to delimit address vs. port.
A comment in vim.py file incorrectly states the host and port can be
specified as host:port.  According to nova.conf, port isn't
even part of the host option.

This patch will create a separate port option to nova.conf instead
of allowing the port to be part of the host_ip option.

DocImpact: new host_port option added to nova.conf
Change-Id: Iebd415b5b3a9d2be42861220366135d2cb57b14d
Closes-bug: #1287292"
523,46495f4995b6604cb249b688aaa249ef4e8b18ef,1381315835,,1.0,34,1,2,2,1,0.927526588,True,2.0,249068.0,17.0,8.0,False,8.0,213148.0,9.0,7.0,981.0,2142.0,2176.0,924.0,1907.0,1939.0,353.0,409.0,418.0,0.808219178,0.936073059,0.956621005,1652,1236993,1236993,neutron,46495f4995b6604cb249b688aaa249ef4e8b18ef,1,0,"“This is the side effect of the recent commit: Quota DB driver has been recently enabled by default”""","Bug #1236993 in neutron: ""Plugins without quota extension fail to start due to missing quota table""","As reported in bug 1236970, plugins without quota extensions fail to start due to missing table in the database.
This is the side effect of the recent commit: Quota DB driver has been recently enabled by default""
https://review.openstack.org/#/c/49993
If a plugin has no support of quota extensions, quota_driver should fallback to the config quota driver.
Otherwise we need to add quota extension support to all plugins.","Fallback to Quota Conf Driver if Quotas table is not defined

commit de15e0b9c5 enabled Quota DB driver default considering
production environments, but it breaks plugins without per-tenant
quota extension. In these plugin quotas tables is not loaded.

This commit fallbacks to ConfDriver if Quota model is not loaded by
checking neutron.db.quota_db which defines Quota model is imported.

Change-Id: Idaaaa9810598cfd3e5ce70020f498643b4819d16
Closes-Bug: #1236993"
524,464c307c68fca0353fb7d65acf650001c8de3f73,1392243931,,1.0,5,3,3,2,1,0.669591946,True,6.0,1161485.0,100.0,13.0,False,15.0,2743.0,82.0,5.0,650.0,1716.0,1716.0,574.0,1437.0,1437.0,187.0,681.0,681.0,0.237673831,0.862199747,0.862199747,397,811,1279551,neutron,464c307c68fca0353fb7d65acf650001c8de3f73,0,0,Evolution change (bug?),"Bug #1279551 in neutron: ""NSX: newly created ports status should be DOWN""",NSX: newly created ports status should be DOWN,"NSX: Fix newly created port's status should be DOWN

Previously when creating a port in neutron using the nsx plugin
the port status returned would be ACTIVE even if this was not the
case. Now, DOWN is returned which will be updated by the backend
when the port goes ACTIVE.

Change-Id: I600fb1292ae79146cb14c7e23170262973878de0
Closes-bug: #1279551"
525,46501246c202f1f424f0d2a05769926303a39c89,1406035388,,1.0,19,2,2,2,1,0.791858353,False,,,,,True,,,,,,,,,,,,,,,,,1095,1546,1347452,neutron,46501246c202f1f424f0d2a05769926303a39c89,1,1,,"Bug #1347452 in neutron: ""Start message from OVS agent not received by server""","When OVS agent starts, it will send server a message with ""start_flag"" to tell server it's up, which is done by the _report_state method. Currently _report_state uses cast method to send rpc message, so there's no guarantee that server receives the ""start"" message. In our test, sometimes this message did miss. As a result, server didn't update the start time of the agent so fdb entries didn't send by the l2pop driver.","Use call to report state when ovs_agent starts up

Make sure to report ovs_agent state when starting up, otherwise
ovs_agent start_time would not be updated and thus l2pop does
not send fdb entries.

Change-Id: Idd770a85a9eabff112d9613e75d8bb524020234a
Closes-Bug: #1347452"
526,466e89970f11918a809aafe8a048d138d4664299,1406047807,1.0,1.0,17,1,2,2,1,0.764204507,True,5.0,5607069.0,83.0,23.0,False,8.0,7095044.0,11.0,2.0,0.0,289.0,289.0,0.0,284.0,284.0,0.0,279.0,279.0,0.000805802,0.225624496,0.225624496,531,952,1288358,neutron,466e89970f11918a809aafe8a048d138d4664299,1,0,Architecture/db specific,"Bug #1288358 in neutron: ""mysql engine InnoDB not being used on centos or redhat 6.5""","when using centos or redhat 6.5, it has mysql 5.1, with default engine of myISAM.  Openstack projects require InnoDB.  I see some code, neutron/db/model_base.py#L25,  that looks to try to set the engine type for create tables.  But after an install, tables are instead created with default engine myISAM.
Where seeing this on neutron and ceilometer.","Use storage engine when creating tables in migrations

Although __table_args__ is set correctly in the base model, existing
migration scripts were not generated with the mysql_engine option in
the alembic output, likely due to:

https://bitbucket.org/zzzeek/alembic/issue/110/

This adds the mysql_engine option to each table creation operation.

Change-Id: I990e8d64bcadf2be4e1e319319bc23225123b304
Closes-Bug: #1288358"
527,46922068ac167f492dd303efb359d0c649d69118,1386293297,1.0,2.0,199,108,2,2,1,0.956509498,True,20.0,11267907.0,167.0,61.0,False,31.0,860.0,44.0,5.0,586.0,2477.0,2477.0,546.0,2245.0,2245.0,178.0,1897.0,1897.0,0.026323529,0.279117647,0.279117647,160,562,1258620,nova,46922068ac167f492dd303efb359d0c649d69118,0,0,Evolution: Make network_cache more robust with neutron ,"Bug #1258620 in OpenStack Compute (nova): ""Make network_cache more robust with neutron""","Currently, the network cache assumes neutron is the source of truth for which interfaces are actually attached to an instance. This is not actually correct as nova is really the source of truth here. In order to demonstrate this issue  if one creates multiple ports in neutron that match the same device_id/instance_id as instances in nova those ports will show up in nova list  even though they are not part of the instance.","Make network_cache more robust with neutron

Currently, nova treats neutron as the source of truth for which ports are
attached to an instance which is a false assumption. Because of this
if someone creates a port in neutron with a device_id that matches one
of their existing instance_ids that port will eventually show up in
nova list (through the periodic heal task).

This problem usually manifests it's self when nova-compute
calls to neutron to create a port and the request times out (though
the port is actually created in neutron). When this occurs the instance
can be rescheduled on another compute node which it will call out to
neutron again to create a port. In this case two ports will show
up in the network_cache table (since they have the same instance_id) though
only one port is attached to the instance.

This patch addresses this issue by only adding ports to network_cache
if nova successfully allocated the port (or it was passed in). This
way these ghost ports are avoided. A follow up patch will come later
that garbage collects these ports.

Closes-bug: #1258620
Closes-bug: #1272195

Change-Id: I961c224d95291727c8614174de07805a0d0a9e46"
528,46922068ac167f492dd303efb359d0c649d69118,1386293297,1.0,2.0,199,108,2,2,1,0.956509498,True,20.0,11267907.0,167.0,61.0,False,31.0,860.0,44.0,5.0,586.0,2477.0,2477.0,546.0,2245.0,2245.0,178.0,1897.0,1897.0,0.026323529,0.279117647,0.279117647,320,729,1272195,nova,46922068ac167f492dd303efb359d0c649d69118,0,0,Refactoring: Make network_cache more robust with neutron ,"Bug #1272195 in OpenStack Compute (nova): ""Previous ip show twice after 'interface-attach'""","Hi team,
I encounter this problem in the below situation:
OS : Ubuntu
Version : Icehouse
1. Normally boot a vm
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| ID                                   | Name     | Status  | Task State | Power State | Networks                     |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE  | None       | Running     | private=10.0.0.5             |
+--------------------------------------+----------+---------+------------+-------------+------------------------------+
xianghui@xianghui:/opt/stack/nova$ neutron net-list
+--------------------------------------+---------+----------------------------------------------------+
| id                                   | name    | subnets                                            |
+--------------------------------------+---------+----------------------------------------------------+
| 2d6842e2-b82c-4d5c-8601-7928ab85a8fd | private | 44d8d50a-197d-4f52-90c4-487495fdb8b5 10.0.0.0/24   |
+--------------------------------------+---------+----------------------------------------------------+
2. Attach the instance with an interface
xianghui@xianghui:/opt/stack/nova$ nova interface-attach cirros_3 --net-id=2d6842e2-b82c-4d5c-8601-7928ab85a8fd
xianghui@xianghui:/opt/stack/nova$ nova list
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| ID                                   | Name     | Status  | Task State | Power State | Networks                             |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
| 35cc6c7c-31b1-491a-8e0d-766a098fb8d9 | cirros_3 | ACTIVE  | None       | Running     | private=10.0.0.5, 10.0.0.5, 10.0.0.6 |
| fa79e7a9-a838-484b-b8a2-4447d4f5d6a0 | fedora-1 | SHUTOFF | None       | Shutdown    | private=10.0.0.3, 172.24.0.2         |
| 97a3758f-9777-44cc-9035-ac95e57f8304 | fedora-2 | SHUTOFF | None       | Shutdown    | private=10.0.0.4                     |
+--------------------------------------+----------+---------+------------+-------------+--------------------------------------+
3. Above shows the previous ip twice until next update_info_cache() happens.","Make network_cache more robust with neutron

Currently, nova treats neutron as the source of truth for which ports are
attached to an instance which is a false assumption. Because of this
if someone creates a port in neutron with a device_id that matches one
of their existing instance_ids that port will eventually show up in
nova list (through the periodic heal task).

This problem usually manifests it's self when nova-compute
calls to neutron to create a port and the request times out (though
the port is actually created in neutron). When this occurs the instance
can be rescheduled on another compute node which it will call out to
neutron again to create a port. In this case two ports will show
up in the network_cache table (since they have the same instance_id) though
only one port is attached to the instance.

This patch addresses this issue by only adding ports to network_cache
if nova successfully allocated the port (or it was passed in). This
way these ghost ports are avoided. A follow up patch will come later
that garbage collects these ports.

Closes-bug: #1258620
Closes-bug: #1272195

Change-Id: I961c224d95291727c8614174de07805a0d0a9e46"
529,46d8dcfa5807ef84d6b96b056957a1b9aa0daff7,1395624823,,1.0,88,18,2,2,1,0.67842322,True,5.0,806445.0,43.0,17.0,False,31.0,2340889.5,43.0,4.0,14.0,1303.0,1309.0,14.0,1078.0,1084.0,12.0,1010.0,1014.0,0.001695578,0.131863832,0.132385548,671,1097,1296164,nova,46d8dcfa5807ef84d6b96b056957a1b9aa0daff7,0,0, Missing implementation,"Bug #1296164 in OpenStack Compute (nova): ""Missing implementation to get, delete, and update volume metadata""","There are four methods in nova/nova/volume/cinder.py which are NotImplemented.  They are as follows:
1.  get_volume_metadata
2.  delete_volume_metadata
3.  update_volume_metadata
4.  get_volume_metadata_value
These methods are required in cases where nova needs to modify a cinder volume's metadata, e.g. attach and detach time.
The latest code in nova's master branch shows these methods as NotImplemented.","Implement methods to modify volume metadata.

There are four methods in nova/volume/cinder.py which are
NotImplemented. They are as follows:
  1. get_volume_metadata
  2. delete_volume_metadata
  3. update_volume_metadata
  4. get_volume_metadata_value
These methods are required in cases where nova needs to modify
a cinder volume's metadata, e.g. attach and detach time. It also
completes the cinder.API class.

Change-Id: I360076ae28db43e661466f556425947813c9040e
Closes-Bug: #1296164"
530,46e88320e6e6231550f3e2b40312c51f55e059f5,1403402957,,1.0,22,3,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,954,1391,1327406,nova,46e88320e6e6231550f3e2b40312c51f55e059f5,1,1,,"Bug #1327406 in OpenStack Compute (nova): ""The One And Only network is variously visible""","I am testing with the templates in https://review.openstack.org/#/c/97366/
I can create a stack.  I can use `curl` to hit the webhooks to scale up and down the old-style group and to scale down the new-style group; those all work.  What fails is hitting the webhook to scale up the new-style group.  Here is a typescript showing the failure:
$ curl -X POST 'http://10.10.0.125:8000/v1/signal/arn%3Aopenstack%3Aheat%3A%3A39675672862f4bd08505bfe1283773e0%3Astacks%2Ftest4%2F3cd6160b-d8c5-48f1-a527-4c7df9205fc3%2Fresources%2FNewScaleUpPolicy?Timestamp=2014-06-06T19%3A45%3A27Z&SignatureMethod=HmacSHA256&AWSAccessKeyId=35678396d987432f87cda8e4c6cdbfb5&SignatureVersion=2&Signature=W3aJQ6SR7O5lLOxLEQndbzNB%2FUhefr1W7qO9zNZ%2BHVs%3D'
<ErrorResponse><Error><Message>The request processing has failed due to an internal error:Remote error: ResourceFailure Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)
[u'Traceback (most recent call last):\n', u'  File ""/opt/stack/heat/heat/engine/service.py"", line 61, in wrapped\n    return func(self, ctx, *args, **kwargs)\n', u'  File ""/opt/stack/heat/heat/engine/service.py"", line 911, in resource_signal\n    stack[resource_name].signal(details)\n', u'  File ""/opt/stack/heat/heat/engine/resource.py"", line 879, in signal\n    raise failure\n', u""ResourceFailure: Error: Nested stack UPDATE failed: Error: Resource CREATE failed: NotFound: No Network matching {'label': u'private'}. (HTTP 404)\n""].</Message><Code>InternalFailure</Code><Type>Server</Type></Error></ErrorResponse>
The original sin looks like this in the heat engine log:
2014-06-06 17:39:20.013 28692 DEBUG urllib3.connectionpool [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 ] ""GET /v2/39675672862f4bd08505bfe1283773e0/os-networks HTTP/1.1"" 200 16 _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-06-06 17:39:20.014 28692 ERROR heat.engine.resource [req-2391a9ea-46d6-46f0-9a7b-cf999a8697e9 None] CREATE : Server ""my_instance"" Stack ""test1-new_style-qidqbd5nrk44-43e7l57kqf5w-4t3xdjrfrr7s"" [20523269-0ebb-45b8-ad59-75f55607f3bd]
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource Traceback (most recent call last):
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resource.py"", line 383, in _do_action
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     handle())
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resources/server.py"", line 493, in handle_create
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     nics = self._build_nics(self.properties.get(self.NETWORKS))
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/heat/heat/engine/resources/server.py"", line 597, in _build_nics
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     network = self.nova().networks.find(label=label_or_uuid)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource   File ""/opt/stack/python-novaclient/novaclient/base.py"", line 194, in find
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource     raise exceptions.NotFound(msg)
2014-06-06 17:39:20.014 28692 TRACE heat.engine.resource NotFound: No Network matching {'label': u'private'}. (HTTP 404)
Private debug logging reveals that in the scale-up case, the call to ""GET /v2/{tenant-id}/os-networks HTTP/1.1"" returns with response code 200 and an empty list of networks.  Comparing with the corresponding call when the stack is being created shows no difference in the calls --- because the normal logging omits the headers --- even though the results differ (when the stack is being created, the result contains the correct list of networks).  Turning on HTTP debug logging in the client reveals that the X-Auth-Token headers differ.","Made unassigned networks visible in flat networking

This change fixes a bug in Nova's
GET /v2/{tenant_id}/os-networks

The doc
http://docs.openstack.org/api/openstack-compute/2/content/GET_os-networks-v2_ListNetworks__v2__tenant_id__os-networks_ext-os-networks.html
says that ""Lists networks that are available to the tenant"".

When invoked by a non-admin user it was returning only networks
assigned to the user's tenant.  But in flat and flat DHCP nova
networking, networks CAN NOT be assigned to tenants --- thus a
non-admin user would get zero networks from this operation.

The fix was to make this operation conditionally use an option already
present in the lower-level code to ""allow_none"" when fetching the list
of networks, meaning to include networks whose project_id field in the
DB held ""none"" (meaning the network is not assigned to a tenant).  The
condition under which this is done is that the Nova configuration
option named network_manager contains the string
""'nova.network.manager.Flat"" --- which is true for flat and flat DHCP
nova networking and false for VLAN nova networking.

Change-Id: I64c5a3f31c912cca6b5b9987152ba7a9b3f5987d
Closes-Bug: #1327406"
531,47079b78b8d8414da9e876d31c6364d6c78389ab,1392805698,0.0,1.0,3,3,1,1,1,0.0,True,1.0,1109930.0,21.0,4.0,False,3.0,2868455.0,8.0,4.0,0.0,1155.0,1155.0,0.0,1014.0,1014.0,0.0,647.0,647.0,0.001201923,0.778846154,0.778846154,431,846,1281921,neutron,47079b78b8d8414da9e876d31c6364d6c78389ab,1,1,typo in code,"Bug #1281921 in neutron: ""Fix class name typo in test_db_rpc_base""","There are typos in the following lines, as TestDhcpRpcCallbackMixin is misspelled.
test_db_rpc_base.py:24:class TestDhcpRpcCallackMixin(base.BaseTestCase):
test_db_rpc_base.py:27:        super(TestDhcpRpcCallackMixin, self).setUp()
test_db_rpc_base.py:39:        super(TestDhcpRpcCallackMixin, self).tearDown()","Fix class name typo in test_db_rpc_base

Change-Id: I23fd11fe4fdd2988463f2cf592d884cba72cdb41
Closes-Bug: #1281921"
532,474e48bf1169a24123e92381f45650b29990d3bf,1380228100,1.0,1.0,44,14,2,2,1,0.968897709,True,8.0,2423687.0,36.0,9.0,False,122.0,57389.0,350.0,2.0,1.0,1502.0,1502.0,1.0,1298.0,1298.0,1.0,1449.0,1449.0,0.000326105,0.236425893,0.236425893,1452,1204424,1204424,nova,474e48bf1169a24123e92381f45650b29990d3bf,1,1,,"Bug #1204424 in OpenStack Compute (nova): ""Unable to create VM with ephemeral block device in lvm""","Hey,
I am running Xubuntu 12.04.2 and I installed OpenStack Grizzly through Ubuntu Cloud Archive.
I configured nova-compute to use LVM as instance storage backend and I put these two lines in nova.conf.
libvirt_images_type=lvm
libvirt_images_volume_group=nova
When I am trying to boot a VM with a self-created flavor like:
nova flavor-create --ephemeral 10 m1.myWithEphSmall 7 4096 20 2
(with ephemeral block device != 0) the VM doesn't boot.
All VM with pre-defined flavors works fine.
here the /var/log/nova/nova-compute.log:
http://paste.openstack.org/show/41529/
some other infos:
http://paste.openstack.org/show/41533/
nova.conf:
http://paste.openstack.org/show/41534/
many thanks...","Fixing ephemeral disk creation.

The patch addresses 2 scenarios, which occur when
CONF.default_ephemeral_format was provided:

1. Creation of ephemeral block devices would fail,
   due to permission issues, during an attempt to format it without sudo,
   in _create_local.
2. The device/image would be formatted twice, while the final
   filesystem will differ from the one provided in
   CONF.default_ephemeral_format.

This patch will format only once, relying on the changed
disk.mkfs command, that takes CONF.default_ephemeral_format
into consideration.
It will also run the command as root only for block devices,
according to the disk_image is_block_dev property.

Closes-Bug: #1204424
Change-Id: I2bf7abc982e20eaaf7bca7cd798b4134b76d10a8"
533,476dc5efcdb11c9859b062dc69a8c205e69be861,1397661795,,1.0,1,1,1,1,1,0.0,True,2.0,3079504.0,58.0,20.0,False,69.0,1378487.0,158.0,4.0,207.0,2643.0,2754.0,189.0,2313.0,2408.0,207.0,1720.0,1831.0,0.026608673,0.220161187,0.234361008,801,1229,1308544,nova,476dc5efcdb11c9859b062dc69a8c205e69be861,1,1,,"Bug #1308544 in OpenStack Compute (nova): ""libvirt: Trying to delete a non-existing vif raises an exception""","If an instance fails during its network creation (for example if the network-vif-plugged event doesn't arrive in time) a subsequent delete will also fail when it tries to delete the vif, leaving the instance in a Error(deleting) state.
This can be avoided by including the ""--if-exists"" option to the ovs=vsctl command.
Example of stack trace:
 2014-04-16 12:28:51.949 AUDIT nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Terminating instance
2014-04-16 12:28:52.309 ERROR nova.virt.libvirt.driver [-] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] During wait destroy, instance disappeared.
2014-04-16 12:28:52.407 ERROR nova.network.linux_net [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] Unable to execute ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']. Exception: Unexpected error while running command.
Command: sudo nova-rootwrap /etc/nova/rootwrap.conf ovs-vsctl --timeout=120 del-port br-int qvo67a96e96-10
Exit code: 1
Stdout: ''
Stderr: 'ovs-vsctl: no port named qvo67a96e96-10\n'
2014-04-16 12:28:52.573 ERROR nova.compute.manager [req-af72c100-5d9b-44f6-b941-3d72529b3401 demo demo] [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Setting instance vm_state to ERROR
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] Traceback (most recent call last):
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2261, in do_terminate_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self._delete_instance(context, instance, bdms, quotas)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/hooks.py"", line 103, in inner
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     rv = f(*args, **kwargs)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2231, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     quotas.rollback()
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2203, in _delete_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self._shutdown_instance(context, db_inst, bdms)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2145, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     requested_networks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/compute/manager.py"", line 2135, in _shutdown_instance
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     block_device_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 955, in destroy
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     destroy_disks)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 991, in cleanup
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_vifs(instance, network_info)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/driver.py"", line 863, in unplug_vifs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.vif_driver.unplug(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_ovs(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     self.unplug_ovs_hybrid(instance, vif)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     v2_name)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1318, in delete_ovs_vif_port
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     _ovs_vsctl(['del-port', bridge, dev])
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]   File ""/mnt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f]     raise exception.AgentError(method=full_args)
2014-04-16 12:28:52.573 TRACE nova.compute.manager [instance: 3b7ac090-1ada-4beb-9e56-1ba3a6445e1f] AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo67a96e96-10']","Ignore errors when deleting non-existing vifs

If an instance fails during its network creation (for example
if the network-vif-plugged event doesn't arrive in time) a
subsequent delete will also fail when it tries to delete the
vif, leaving the instance in a Error(deleting) state.

This can be avoided by including the ""--if-exists"" option to
the ovs-vsctl command so that the command doesn't fail if the
vif doesn't exist.

Change-Id: Ic613c9d6e0619f51fcd931e62f6d6bce404ebe81
Closes-bug: #1308544"
534,477b3d4dec1a815edeae02531db987b75be4194c,1385929068,,1.0,1,1,1,1,1,0.0,True,1.0,88838.0,11.0,6.0,False,63.0,17256.0,143.0,5.0,574.0,2823.0,3144.0,534.0,2271.0,2575.0,170.0,1742.0,1833.0,0.025333333,0.258222222,0.271703704,110,509,1256696,nova,477b3d4dec1a815edeae02531db987b75be4194c,1,1,Typo in code and comment,"Bug #1256696 in OpenStack Compute (nova): ""correct network_device_mtu help string""",mtu setting for network interface not vlan.,"Correct network_device_mtu help string

This patch corrects the network_device_mtu help string. This setting
changes the mtu size for the interface not the vlan.

Change-Id: I2f3bcac42f08b70eb1649e3085c0de9af27b350c
Closes-bug: #1256696"
535,47871570eb7ed351738828bc233343520283de0d,1389733412,,1.0,3,2,1,1,1,0.0,True,1.0,1913893.0,31.0,6.0,False,13.0,721262.0,65.0,4.0,620.0,1108.0,1445.0,566.0,995.0,1298.0,165.0,566.0,590.0,0.23953824,0.818181818,0.852813853,290,696,1269152,neutron,47871570eb7ed351738828bc233343520283de0d,0,0,"Missing LOG,exception to show the error","Bug #1269152 in neutron: ""Missing log.exception in nvp plugin for create_router""",Missing log.exception in nvp plugin for create_router,"NVP: Add LOG.exception to see why router was not created

Add missing LOG.exception so that it exposes what error occurred.

Change-Id: I9194795678a6be59eb1d2555dfa99ca7a035c418
Closes-bug: #1269152"
536,478f447070e2aa11ac14d419ae283e6bb9edeeb3,1393419803,,1.0,55,16,10,9,1,0.863124784,True,5.0,6593977.0,70.0,15.0,False,283.0,54644.0,1690.0,2.0,1.0,1024.0,1025.0,1.0,1006.0,1007.0,0.0,1013.0,1013.0,0.000134156,0.136034344,0.136034344,480,897,1284996,nova,478f447070e2aa11ac14d419ae283e6bb9edeeb3,0,0,No bug. Feature. reduce VM network down time,"Bug #1284996 in OpenStack Compute (nova): ""reduce VM network down time during live (block) migration""","It appears that during live migration, for certain duration network is unavailable and there is a scope to reduce the network downtime.
Please refer http://paste.openstack.org/show/69718/ to check the packet loss during live migration.","reduce network down time during live-migration

Called unplug_vifs() method before post_live_migration_to_destination call
because floating ip address will not work until the vifs are unplugged
from the source compute node.

Added new method post_live_migration_at_source in virt driver and
implemented it in libvirt driver to unplug the vifs at source. Other
drivers will raise NotImplementedError.
Added new keyword argument destroy_vifs to cleanup() method to verify
if unplug_vifs() is already called then do not call it again.

Closes-Bug: #1284996
Change-Id: Ida3ed3bef77239e5a0fdbf8866e8a4421d4f8222"
537,47a26f1ac41100c40b7997bc4df802fa867fbd42,1410514831,,1.0,7,7,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1314,1780,1369418,cinder,47a26f1ac41100c40b7997bc4df802fa867fbd42,1,1,"“This issue is introduced in patch https://review.openstack.org/#/c/106377/""","Bug #1369418 in Cinder: ""Set socket options in correct way""","Currently socket options, socket.SO_REUSEADDR and socket.SO_KEEPALIVE
are set only if SSL is enabled.
Ref: https://github.com/openstack/cinder/blob/master/cinder/wsgi.py#L209
The above socket options should be set no matter SSL is enabled or not.
This issue is introduced in patch https://review.openstack.org/#/c/106377/","Set socket options in correct way

Currently socket options, socket.SO_REUSEADDR and socket.SO_KEEPALIVE
are set only if SSL is enabled.
The above socket options should be set no matter SSL is enabled or not.

Closes-Bug: #1369418

Change-Id: Ia5f6603918bda40be9965387bf464c581449d912"
538,47c95694e93b361e3b0738afac8e724557a36c8a,1383726545,,1.0,4,4,3,3,1,0.94639463,True,7.0,2381184.0,56.0,25.0,False,146.0,14640.0,424.0,3.0,38.0,2048.0,2073.0,31.0,1534.0,1555.0,37.0,1809.0,1833.0,0.005822862,0.27735213,0.281029727,66,394,1248443,nova,47c95694e93b361e3b0738afac8e724557a36c8a,1,1,,"Bug #1248443 in OpenStack Compute (nova): ""Wrong comparisons with ""in"" operator""","In the code there are several comparisons using the ""in"" operator to check if a value occurs in a tuple with only one element, like:
    if a in (""foo""):
        do_something()
This comparison is wrong, since that is not a tuple with one element [1], therefore if the variable ""a"" had a value of ""f"" it will match.
[1] http://docs.python.org/2/tutorial/datastructures.html#tuples-and-sequences","Fix ""in"" comparisons with one element tuples

A single value enclosed in parentheses is not a tuple (it needs a comma
after the element) therefore the comparisons like 'if foo in (""bar"")'
are wrong.

Closes-Bug: #1248443
Change-Id: I29a2227c9d83efc17ecfacc86d507d75edd4eaca"
539,47e51e7521784f6a2edcfbf71a9aac0237e76e42,1398741858,,1.0,6,5,2,2,1,0.945660305,True,1.0,183630.0,17.0,3.0,False,6.0,21972.0,11.0,2.0,325.0,1724.0,1821.0,302.0,1454.0,1530.0,257.0,882.0,935.0,0.216624685,0.741393787,0.785894207,844,1273,1313997,neutron,47e51e7521784f6a2edcfbf71a9aac0237e76e42,1,1,,"Bug #1313997 in neutron: ""NSX: fix migration of networks with no subnets""","Corner case with networks without a subnet leads to a spurious exception during net-migration, because the validation logic is called twice: before migration and after.","NSX: fix migration for networks without a subnet

In case the network is without a subnet, calling the validation
logic during the report phase leads to an error because the LSN
would have been already allocated during the migration phase.

Bypass the issue by calling the plugin directly, which is what
the validation logic does in the first place.

Closes-bug: #1313997

Change-Id: I14f77ae3b0cc147c4ea1c79e56bdd809de7c76a0"
540,480a932173219bda0f7ead61fd406fe4a366832c,1411690523,,1.0,12,1,2,2,1,0.619382195,False,,,,,True,,,,,,,,,,,,,,,,,1359,1828,1374158,nova,480a932173219bda0f7ead61fd406fe4a366832c,1,1,“Typo in call to LibvirtConfigObject's parse_dom() method”,"Bug #1374158 in OpenStack Compute (nova): ""Typo in call to LibvirtConfigObject's parse_dom() method""","In Juno in nova/virt/libvirt/config.py:
LibvirtConfigGuestPUNUMA.parse_dom() calls super with a capital 'D' in parse_dom().
        super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)
LibvirtConfigObject does not have a 'parse_Dom()' method. It has a 'parse_dom()' method. This causes the following exception to be raised.
...
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 1733, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     obj.parse_dom(c)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 542, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     numa.parse_dom(child)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack   File ""/usr/lib/python2.7/site-packages/nova/virt/libvirt/config.py"", line 509, in parse_dom
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack     super(LibvirtConfigGuestCPUNUMA, self).parse_Dom(xmldoc)
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstackAttributeError: 'super' object has no attribute 'parse_Dom'
2014-09-25 15:35:21.546 14344 TRACE nova.api.openstack
2014-09-25 15:35","Fix Typo in method name - parse_Dom

Silly typo

Closes-Bug: #1374158

Change-Id: Icccfb1a606bc9e14188d9f5c5f5667e35f2cb413"
541,482dfeb113f1e0ff814aa1fc980d67e0c8b06d76,1383164755,,1.0,24,15,2,2,1,0.941828535,True,4.0,3444653.0,19.0,6.0,False,179.0,54668.5,883.0,4.0,44.0,3429.0,3435.0,43.0,2613.0,2619.0,40.0,3179.0,3185.0,0.006317411,0.489984592,0.490909091,40,368,1246327,nova,482dfeb113f1e0ff814aa1fc980d67e0c8b06d76,1,0,Evolution of Havana,"Bug #1246327 in OpenStack Compute (nova): ""the snapshot of a volume-backed instance cannot be used to boot a new instance""","After the changes in the block device mappings introduced for Havana, if we try to create an snapshot of a volume-backed instance the resulting image cannot be used to boot a new instance due to conflicts with the bootindex between the block_device_mapping stored in the image properties and the current image.
The steps to reproduce are:
$ glance image-create --name f20 --disk-format qcow2 --container-format bare --min-disk 2 --is-public True --min-ram 512 --copy-from http://download.fedoraproject.org/pub/fedora/linux/releases/test/20-Alpha/Images/x86_64/Fedora-x86_64-20-Alpha-20130918-sda.qcow2
$ cinder create --image-id <uuid of the new image> --display-name f20 2
$ nova boot --boot-volume <uuid of the new volume> --flavor m1.tiny test-instance
$ nova image-create test-instance test-snap
This will create an snapshot of the volume and an image in glance with a block_device_mapping containing the snapshot_id and all the other values from the original block_device_mapping (id, connection_info, instance_uuid, ...):
| Property 'block_device_mapping' | [{""instance_uuid"": ""989f03dc-2736-4884-ab66-97360102d804"", ""virtual_name"": null, ""no_device"": null, ""connection_info"": ""{\""driver_volume_type\"": \""iscsi\"", \""serial\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""data\"": {\""access_mode\"": \""rw\"", \""target_discovered\"": false, \""encrypted\"": false, \""qos_spec\"": null, \""device_path\"": \""/dev/disk/by-path/ip-192.168.122.2:3260-iscsi-iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7-lun-1\"", \""target_iqn\"": \""iqn.2010-10.org.openstack:volume-cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_portal\"": \""192.168.122.2:3260\"", \""volume_id\"": \""cb6d4406-1c66-4f9a-9fd8-7e246a3b93b7\"", \""target_lun\"": 1, \""auth_password\"": \""wh5bWkAjKv7Dy6Ptt4nY\"", \""auth_username\"": \""oPbN9FzbEPQ3iFpPhv5d\"", \""auth_method\"": \""CHAP\""}}"", ""created_at"": ""2013-10-30T13:18:57.000000"", ""snapshot_id"": ""f6a25cc2-b3af-400b-9ef9-519d28239920"", ""updated_at"": ""2013-10-30T13:19:08.000000"", ""device_name"": ""/dev/vda"", ""deleted"": 0, ""volume_size"": null, ""volume_id"": null, ""id"": 3, ""deleted_at"": null, ""delete_on_termination"": false}] |
When we try latter to use this image to boot a new instance, the API won't let us because both, the device in the image bdm and the image (which is empty) are considered to be the boot device:
$ nova boot --image test-snap --flavor m1.nano test-instance2
ERROR: Block Device Mapping is Invalid: Boot sequence for the instance and image/block device mapping combination is not valid. (HTTP 400) (Request-ID: req-3e502a29-9cd3-4c0c-8ddc-a28d315d21ea)
If we check the internal flow we can see that nova considers the image to be the boot device even thought the image itself doesn't define any local disk but only a block_device_mapping pointing to the snapshot.
To be able to generate proper images from volume-backed instances we should:
 1. copy only the relevant keys from the original block_device_mapping to prevent duplicities in DB
 2. prevent nova from adding a new block device for the image if this one doesn't define any local disk","Process image BDM earlier to avoid duplicates

If the block device mapping from the image properties are not handled at
the same time as the ones defined from the API, we might make the false
assumption that the instance has no root and failing or, in the case
we're booting from an image that only defines a BDM and has no disk,
creating a local disk BDM as root and ending up with two root devices
which is forbidden.

This patch moves the handling of the image block device mappings to the
same method were we check the ones provided throught the API. This
allows nova to decide wether the instance needs a local disk from an
image if no root device is defined in any of the block device mappings.

Change-Id: Ide95357895ab4dd1338ab5ee3ec25294af1d010b
Closes-Bug: #1246327"
542,4845d08967cdff22d53d55467245402e10ae9814,1379451707,,1.0,10,5,2,2,1,0.721928095,True,2.0,1174184.0,23.0,13.0,False,38.0,70605.0,85.0,9.0,1.0,3031.0,3031.0,1.0,2710.0,2710.0,1.0,2122.0,2122.0,0.00032987,0.350156688,0.350156688,1570,1226826,1226826,nova,4845d08967cdff22d53d55467245402e10ae9814,1,1,"“The VMwareVCDriver method ""detach_volume"" is missing the “encryption""""","Bug #1226826 in OpenStack Compute (nova): ""VMwareVCDriver: detaching volume  fails with TypeError""","When VMwareVCDriver is being used, detaching a volume will fail. The following error shows up in the n-cpu log:
Traceback (most recent call last):
 File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
   **args)
 File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
   result = getattr(proxyobj, method)(ctxt, **kwargs)
 File ""/opt/stack/nova/nova/exception.py"", line 89, in wrapped
   payload)
 File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
   return f(self, context, *args, **kw)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
   pass
 File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
   e, sys.exc_info())
 File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
   return function(self, context, *args, **kwargs)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3720, in detach_volume
   self._detach_volume(context, instance, bdm)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3692, in _detach_volume
   self.volume_api.roll_detaching(context, volume_id)
 File ""/opt/stack/nova/nova/compute/manager.py"", line 3685, in _detach_volume
   encryption=encryption)
ypeError: detach_volume() got an unexpected keyword argument 'encryption'
Full log of detach operation here:
http://paste.openstack.org/show/47179/
The solution is to update the VMwareVCDriver method ""detach_volume"" to take an ""encryption"" argument.","VMware: Fix volume detach failure

The VMwareVCDriver method ""detach_volume"" is missing the ""encryption""
argument and thus does not conform to the driver API. Adding the missing
argument solves TypeError being thrown when detaching a volume.

Change-Id: Ib785c2b2cf4c25deed272f8e3f0f47abe6139d1c
Closes-Bug: #1226826"
543,485c174fc7e52e697d1d73a96cd924c2e3a04414,1410938534,,1.0,65,67,5,4,2,0.624185852,False,,,,,True,,,,,,,,,,,,,,,,,1297,1763,1368033,neutron,485c174fc7e52e697d1d73a96cd924c2e3a04414,0,0,"Feature ""So the CRD configuration options are to be moved to a separate file to be used with other plugin/drivers”","Bug #1368033 in neutron: ""Separate Configuration from Freescale SDN ML2 mechanism Driver""","In the current implementation, CRD configuration is existing within the code of ML2 mechanism driver.
When any other plugin/driver need to use this configuration needs to duplicate the complete configuration.
So the CRD configuration options are to be moved to a separate file to be used with other plugin/drivers.","Separate Configuration from Freescale SDN ML2 mechanism Driver

- In the current implementation, CRD configuration is existing
  within the code of ML2 mechanism driver.

- When any other plugin/driver (like, Freescale FWaaS Plugin) need
  to use this configuration, it needs to duplicate the complete configuration.

- So the CRD configuration is moved to a separate file for use
  in other plugin/drivers.

- Unit testing of this MD is also updated.

Closes-Bug: #1368033

Change-Id: I488fee47803a494aae9df42f9c59fffa9843e727"
544,487b98a73f6be41697dd4f59a245f1a7f338a72f,1405382657,,1.0,33,2,3,3,1,0.667862766,False,,,,,True,,,,,,,,,,,,,,,,,1061,1507,1341014,neutron,487b98a73f6be41697dd4f59a245f1a7f338a72f,1,1,"I think this has a BIC …. “This patch deletes all VSM credentials on neutron
start up before adding the newer VSM credentials” (b034f9de18ff0fa131a7b8ac77eeef52f23adaf5)","Bug #1341014 in neutron: ""Update VSM credential correctly""","Today if we modify the VSM credential in the cisco_plugins.ini, the
older VSM ip address remains in the db, and all requests are sent to
the older VSM. This patch deletes all VSM credentials on neutron
start up before adding the newer VSM credentials. Hence making sure
that there is only one VSM IP address and credential in the db.","update vsm credential correctly

Today if we modify the n1kv VSM credential in the cisco_plugins.ini, the
older VSM ip address remains in the db, and all requests are sent to
the older VSM. This patch deletes all n1kv VSM credentials on neutron
start up before adding the newer VSM credentials. Hence making sure
that there is only one n1kv VSM IP address and credential in the db.

Change-Id: I772a86bf896a1d3d9c69c545ce6918b0fe3a2e48
Closes-Bug: #1341014"
545,488f88c4d021bf429f62ca46dd8299a70d31505e,1404375559,,1.0,30,9,2,2,1,0.552495114,False,,,,,True,,,,,,,,,,,,,,,,,1038,1482,1337236,nova,488f88c4d021bf429f62ca46dd8299a70d31505e,1,1,,"Bug #1337236 in OpenStack Compute (nova): ""vmware: nova-compute will not start if some instance relying datastore not available""","First use vcenter driver to spawn some instances to one of the datastores that esxi is binding to.  Later this datastore became unavailable due to certain reason(power off or network problem).  Then when restart nova-compute, found that compute service will exit with errors.  This will openstack compute not usable.
2014-07-03 01:38:13.961 3634 DEBUG nova.compute.manager [req-11bc0618-8696-464d-8820-7565db8f44c3 None None] [instance: 9428cf95-5
37f-48f6-b79e-faa981f6066d] NV-AC7AA80 Checking state _get_power_state /usr/lib/python2.6/site-packages/nova/compute/manager.py:10
54
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/lib/python2.6/site-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/usr/lib/python2.6/site-packages/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1037, in init_host
    self._init_instance(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 865, in _init_instance
    try_reboot, reboot_type = self._retry_reboot(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 963, in _retry_reboot
    current_power_state = self._get_power_state(context, instance)
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1056, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/driver.py"", line 862, in get_info
    return _vmops.get_info(instance)
  File ""/usr/lib/python2.6/site-packages/nova/virt/vmwareapi/vmops.py"", line 1376, in get_info
    max_mem = int(query['summary.config.memorySizeMB']) * 1024
KeyError: 'summary.config.memorySizeMB'","VMWare: Fix nova-compute crash when instance datastore not available

During nova-compute service startup, init_host will try to get_info for
instances. If datastore of certain instances become not reachable, some
VM properties like 'summary.config.memorySizeMB' will not be available.
This will cause nova-compute process exit with error.

Change-Id: I6753ce5658b629c216ebfbaab610c8fd0755466a
Closes-Bug: #1337236"
546,48955e56b886c0da51c1555aca62e099761ad99b,1394142626,,1.0,21,27,3,2,1,0.677033339,True,4.0,27165.0,15.0,2.0,False,1.0,1746173.0,1.0,1.0,38.0,1183.0,1186.0,28.0,948.0,951.0,36.0,1038.0,1041.0,0.024584718,0.690365449,0.692358804,523,944,1287944,cinder,48955e56b886c0da51c1555aca62e099761ad99b,1,1,Many bugs in 1 commit,"Bug #1287944 in Cinder: ""EMC VNX Direct Driver needs to be cleaned up""","There are quite a few review comments on the EMC VNX Direct Driver:
https://review.openstack.org/#/c/73672
These issues need to be addressed right after the code is merged.","Clean Up EMC VNX Direct Driver in Cinder

This patch cleans up issues discovered during the review of
EMC VNX Direct Driver.

https://review.openstack.org/#/c/73672/

Implements blueprint emc-vnx-direct-driver
Closes-Bug: #1287944

Change-Id: I4002ef9ea14e2d843dd8cbccffa025997a54c738"
547,48a20d85cb9ef91c48f681c76fc6ca7512eccfbc,1387340371,,1.0,78,1,2,2,1,0.170330576,True,6.0,1425296.0,41.0,18.0,False,49.0,237277.0,107.0,11.0,30.0,3420.0,3435.0,30.0,2920.0,2935.0,14.0,2477.0,2477.0,0.002178333,0.359860587,0.359860587,203,606,1262039,nova,48a20d85cb9ef91c48f681c76fc6ca7512eccfbc,0,0,"We should modify the ""image"" to empty dictionary","Bug #1262039 in OpenStack Compute (nova): ""Inconsistent ""image"" value on _get_image()""","1. Create a server from a image, then show the server details, the ""image"" is a dict, such as:
          {
                ""id"": image_id,
                ""links"": [{
                    ""rel"": ""bookmark"",
                    ""href"": bookmark,
                }],
            }
2. Create a server from a volume, then show the server details, the ""image"" is a dict, such as:
        """"
3. It's inconsistent ""image"" value, i think it's a bug.","Fix inconsistent ""image"" value on _get_image()

The type of ""image"" value is different on showing details of a server,
such as:
1. It's a dict while creating a server from a image
2. It's a empty string while creating a server from a volume

We should modify the type to dict, otherwise someone who call
the api will be confused about the value or can't parse the value
in the same way.

The patch just modifies the v3 api.

Change-Id: I0d7ce59f64adec0933353af58dc5b5170390674b
Closes-Bug: #1262039"
548,48cb82971e0418f9a629e2b39d0433dc2c0e6919,1412962587,,1.0,172,267,5,2,1,0.595884457,False,,,,,True,,,,,,,,,,,,,,,,,1396,1866,1379830,cinder,48cb82971e0418f9a629e2b39d0433dc2c0e6919,1,1,“This is a regression caused by commit 4be8913520f5e9fe4109ade101da9509e4a83360”,"Bug #1379830 in Cinder: ""unable to re-attach a  volume to instance in VMWare deployment""","Attach a volume to instance fails with following exception  :
014-10-10 19:29:54.112 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mThe instance: (Property){
   value = ""vm-2360""
   _type = ""VirtualMachine""
 } for which initialize connection is called, exists.[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:656[00m
2014-10-10 19:29:55.114 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mBacking exists[00m [00;33mfrom (pid=10484) _initialize_connection /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:666[00m
2014-10-10 19:29:57.068 [00;32mDEBUG cinder.volume.drivers.vmware.vmdk [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[00;32m] [01;35m[00;32mDatastore: (ManagedObjectReference){
   value = ""datastore-34""
   _type = ""Datastore""
 }, profile: None[00m [00;33mfrom (pid=10484) _relocate_backing /opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py:1934[00m
2014-10-10 19:29:59.015 [01;31mERROR cinder.volume.manager [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mUnable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
2014-10-10 19:29:59.017 [01;31mERROR oslo.messaging.rpc.dispatcher [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mException during message handling: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'[00m
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mTraceback (most recent call last):
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    incoming.message))
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    return self._do_dispatch(endpoint, method, ctxt, args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    result = getattr(endpoint, method)(ctxt, **new_args)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    return f(*args, **kwargs)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m    raise exception.VolumeBackendAPIException(data=err_msg)
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00mVolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'
[01;31m2014-10-10 19:29:59.017 TRACE oslo.messaging.rpc.dispatcher [01;35m[00m
2014-10-10 19:29:59.021 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31mReturning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value' to caller[00m
2014-10-10 19:29:59.022 [01;31mERROR oslo.messaging._drivers.common [[01;36mreq-90f52202-4720-41ca-aa01-cc1bd2e1a4be [00;36m304c157ff8fe4136a279d5f157e63cfc eb53ada20d274ed8832cf46afb21636b[01;31m] [01;35m[01;31m['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n    return f(*args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 901, in initialize_connection\n    raise exception.VolumeBackendAPIException(data=err_msg)\n', ""VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: 'Text' object has no attribute 'value'\n""][00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._publish_service_capabilities[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.547 [00;32mDEBUG cinder.manager [[00;36m-[00;32m] [01;35m[00;32mNotifying Schedulers of capabilities ...[00m [00;33mfrom (pid=10484) _publish_service_capabilities /opt/stack/cinder/cinder/manager.py:128[00m
2014-10-10 19:30:02.552 [00;32mDEBUG cinder.openstack.common.periodic_task [[00;36m-[00;32m] [01;35m[00;32mRunning periodic task VolumeManager._report_driver_status[00m [00;33mfrom (pid=10484) run_periodic_tasks /opt/stack/cinder/cinder/openstack/common/periodic_task.py:193[00m
2014-10-10 19:30:02.553 [00;36mINFO cinder.volume.manager [[00;36m-[00;36m] [01;35m[00;36mUpdating volume status[00m","Revert ""Relocate volume to compliant datastore""

Commit 4be8913520f5e9fe4109ade101da9509e4a83360 introduced a regression
which causes failures during cinder volume re-attach. This patch reverts
commit 4be8913520f5e9fe4109ade101da9509e4a83360 as an immediate fix.

Closes-Bug: #1379830
Change-Id: I5dfbd45533489c3c81db8d256bbfd2f85614a357"
549,48de2895b9a550a0944b31212349275605a4061d,1406212527,,1.0,25,2,4,4,1,0.691172454,False,,,,,True,,,,,,,,,,,,,,,,,1007,1449,1334164,nova,48de2895b9a550a0944b31212349275605a4061d,1,1,,"Bug #1334164 in OpenStack Compute (nova): ""nova error migrating VMs with floating ips: 'FixedIP' object has no attribute '_sa_instance_state'""","Seeing this in conductor logs when migrating a VM with a floating IP assigned:
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 1019, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/conductor/manager.py"", line 527, in network_migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     self.network_api.migrate_instance_start(context, instance, migration)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 94, in wrapped
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return func(self, context, *args, **kwargs)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 543, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     self.network_rpcapi.migrate_instance_start(context, **args)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/nova/network/rpcapi.py"", line 350, in migrate_instance_start
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     floating_addresses=floating_addresses)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/transport.py"", line 90, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 409, in send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.6/site-packages/oslo/messaging/_drivers/amqpdriver.py"", line 402, in _send
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-06-23 09:22:38.899 20314 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'FixedIP' object has no attribute '_sa_instance_state'","Fix FloatingIP.save() passing FixedIP object to sqlalchemy

This prevents the FloatingIP.save() method from passing the
calculated FixedIP object to the sqlalchemy floating_ip_update()
function, which would expect it to be an SA object. It also
aborts any attempt to save the object with a modified fixed_ip_id
linkage, as associate/disassociate should be used for that.

This also fixes a bug where FloatingIP expects the result of
floating_ip_update() to be a FloatingIp SA object.

Change-Id: I065caedf4d81c8583a3b390934a1d403cf2e87bd
Closes-bug: #1334164"
550,48e36b542904effb9b35c9d62bd0247e25f0dac6,1381396258,,1.0,28,5,2,2,1,0.32984607,True,3.0,1266942.0,27.0,15.0,False,5.0,2250629.5,10.0,7.0,617.0,1366.0,1393.0,587.0,1279.0,1306.0,161.0,410.0,417.0,0.368181818,0.934090909,0.95,1667,1237912,1237912,neutron,48e36b542904effb9b35c9d62bd0247e25f0dac6,1,1, ,"Bug #1237912 in neutron: ""Cannot update IPSec Policy lifetime""","When you try to update IPSec Policy lifetime, you get an error:
(neutron) vpn-ipsecpolicy-update ipsecpolicy --lifetime units=seconds,value=36001
Request Failed: internal server error while processing your request.
Meanwhile updating IKE Policy lifetime works well:
(neutron) vpn-ikepolicy-update ikepolicy --lifetime units=seconds,value=36001
Updated ikepolicy: ikepolicy","Fix access to lifetime dict in update_ipsecpolicy method

Also add corresponding unit test.

Change-Id: Ie1141b5034415ccf3039a71dfedc47cfb2ca7a88
Closes-Bug: #1237912"
551,48e94bf75ce2be50d323e8b883cf3322c4d06c4e,1411658110,,1.0,2,7,3,3,1,0.905712598,False,,,,,True,,,,,,,,,,,,,,,,,1355,1824,1373950,nova,48e94bf75ce2be50d323e8b883cf3322c4d06c4e,1,1,,"Bug #1373950 in OpenStack Compute (nova): ""Serial proxy service  and API broken by design""","As part of the blueprint https://blueprints.launchpad.net/nova/+spec/serial-ports we introduced an API extension and a websocket proxy binary. The problem with the 2 is that a lot of the stuff was copied verbatim from the novnc-proxy API and service which relies heavily on the internal implementation details of NoVNC and python-websockify libraries.
We should not ship a service that will proxy websocket traffic if we do not acutally serve a web-based client for it (in the NoVNC case, it has it's own HTML5 VNC implementation that works over ws://). No similar thing was part of the proposed (and accepted) implementation. The websocket proxy based on websockify that we currently have actually assumes it will serve static content (which we don't do for serial console case) which will then when excuted in the browser initiate a websocket connection that sends the security token in the cookie: field of the request. All of this is specific to the NoVNC implementation (see: https://github.com/kanaka/noVNC/blob/e4e9a9b97fec107b25573b29d2e72a6abf8f0a46/vnc_auto.html#L18) and does not make any sense for serial console functionality.
The proxy service was introduced in https://review.openstack.org/#/c/113963/
In a similar manner - the API that was proposed and implemented (in https://review.openstack.org/#/c/113966/) that gives us back the URL with the security token makes no sense for the same reasons outlined above.
We should revert at least these 2 patches before the final Juno release as we do not want to ship a useless service and commit to a useles API method.
We could then look into providing similar functionality through possibly something like https://github.com/chjj/term.js which will require us to write a different proxy service.","cmd: update the default behavior of serial console

When using serial console we are expecting using a plain websocket
so we should to return the 'ws://' scheme as the default one, also
it makes no sense to handle webserving content since currently
nothing has been implemented in favor of this case.

DocImpact: The 'base_url' option serial_console has been updated
Closes-Bug: #1373950
Change-Id: I0d0e4f7060febec5e0a357cd3e8c05486f2afaa5"
552,48e9c8b79bea9c9a65ceb1c24528a89db6d313d2,1403263746,,1.0,10,7,2,2,1,0.787126586,False,,,,,True,,,,,,,,,,,,,,,,,994,1434,1332412,neutron,48e9c8b79bea9c9a65ceb1c24528a89db6d313d2,1,0,“probably due to oslo.messaging changes.”,"Bug #1332412 in neutron: ""Could not send notification to notifications""","""Could not send notification to notifications"" errors started happening today.
probably due to oslo.messaging changes.
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging Traceback (most recent call last):
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/notify/_impl_messaging.py"", line 47, in notify
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     version=self.version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 96, in _send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     self._driver.send_notification(target, ctxt, message, version)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 394, in send_notification
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     envelope=(version == 2.0), notify=True)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 355, in _send
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     rpc_amqp.pack_context(msg, context)
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqp.py"", line 212, in pack_context
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     context_d = six.iteritems(context.to_dict())
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging   File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 553, in iteritems
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging     return iter(d.iteritems(**kw))
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging AttributeError: 'Context' object has no attribute 'iteritems'
2014-06-20 13:00:54.239 TRACE oslo.messaging.notify._impl_messaging
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkNvdWxkIG5vdCBzZW5kIG5vdGlmaWNhdGlvbiB0byBub3RpZmljYXRpb25zXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjE3MjgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDMyNDIyNjczOTd9","Pass serializer to oslo.messaging Notifier

oslo.messaging has a workaround [1] that requires context to be
transformed to pure dict before passing into amqpdriver.

Renamed serializer class to reflect its broader usage.

Updated FakeNotifier to expect serializer and other keyword arguments
supported by oslo.messaging Notifier class.

[1]: oslo/messaging/_drivers/amqpdriver.py#L337

blueprint oslo-messaging

Closes-Bug: #1332412
Change-Id: I7e7658d03639afae7bf6d3ad71445cb5b6459c09"
553,49502246ef319cf8981b8a867ce2a3b6a163fc9c,1396605311,,1.0,28,38,2,2,1,0.999337504,True,7.0,990717.0,125.0,36.0,False,19.0,2370392.5,28.0,6.0,46.0,834.0,852.0,46.0,734.0,752.0,46.0,536.0,554.0,0.043119266,0.49266055,0.509174312,741,1168,1302463,neutron,49502246ef319cf8981b8a867ce2a3b6a163fc9c,0,0,"Feature, “LinuxBridgeManager is adding a new method for checking”","Bug #1302463 in neutron: ""LinuxBridgeManager is not using device_exist from ip_lib""",LinuxBridgeManager is adding a new method for checking the existence of a device instead of using the one in ip_lib,"Remove device_exists in LinuxBridgeManager

LinuxBridgeManager is adding a device_exists method instead of
using the one in ip_lib as the rest of the codebase

Change-Id: Ibeb95be76a1e1b20c69841e708c2a2cbf1cbe05a
Closes-Bug: #1302463"
554,4963fa0fa0d96d8641339614438d42c81958f728,1389865473,,1.0,78,16,6,6,1,0.934194158,True,46.0,8270463.0,304.0,106.0,False,62.0,3690955.167,204.0,5.0,38.0,2010.0,2016.0,38.0,1753.0,1759.0,37.0,1927.0,1932.0,0.005375584,0.272740133,0.273447447,177,580,1260249,nova,4963fa0fa0d96d8641339614438d42c81958f728,1,1,fdf248652a38eff287d6cabbac1a260151fecd40,"Bug #1260249 in OpenStack Compute (nova): ""migration-list: 'unicode' object has no attribute 'iteritems'""","There is an AttributeError when we try to use the command ""nova migration-list""
Traceback (most recent call last):
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 721, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/opt/stack/python-novaclient/novaclient/shell.py"", line 657, in main
    args.func(self.cs, args)
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 71, in do_migration_list
    args.cell_name))
  File ""/opt/stack/python-novaclient/novaclient/v1_1/contrib/migrations.py"", line 53, in list
    return self._list(""/os-migrations%s"" % query_string, ""migrations"")
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 80, in _list
    for res in data if res]
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 426, in __init__
    self._add_details(info)
  File ""/opt/stack/python-novaclient/novaclient/base.py"", line 449, in _add_details
    for (k, v) in six.iteritems(info):
  File ""/usr/local/lib/python2.7/dist-packages/six.py"", line 439, in iteritems
    return iter(getattr(d, _iteritems)(**kw))
AttributeError: 'unicode' object has no attribute 'iteritems'
ERROR: 'unicode' object has no attribute 'iteritems'","Fix broken API os-migrations

The return is not what we are expecting:
  migration-list: 'unicode' object has no attribute 'iteritems'

The conversion of the object nova.objects.MigrationList into a list
is not applied automatically.

Change-Id: I1579b1baf62275bf448f23be9465d4e9bc11ed85
Closes-Bug: #1260249"
555,497bdfec93d696c7200a71b85a0680d99ce07bb9,1385138825,,1.0,61,10,2,2,1,0.367498495,True,5.0,1568268.0,31.0,17.0,False,44.0,984841.0,95.0,7.0,0.0,2961.0,2961.0,0.0,2491.0,2491.0,0.0,2033.0,2033.0,0.000150105,0.30531372,0.30531372,73,470,1254664,nova,497bdfec93d696c7200a71b85a0680d99ce07bb9,1,1,Something is ignored,"Bug #1254664 in OpenStack Compute (nova): ""Network quotas are ignored by libvirt when Open vSwitch is used""","I created a flavor with quotas on the network bandwidth and created a VM with this flavor: upload limited 1 MB/sec, download limited to 500 kB/sec. The limits are ignored: copy from DevStack to the VM is faster than 13 MB/sec.
I'm using Neutron with Open vSwitch (OVS) for the network.
It looks like a regression in nova/virt/libvirt/vif.py. According to a colleague, it was maybe introduced when the OVS, LinuxBridge and HyperV classes were merged into one LibvirtGenericVIFDriver class. designer.set_vif_bandwidth_config() is only called for bridge types:
- get_config_ovs_hybrid()
- get_config_ivs_hybrid()
- network_model.VIF_TYPE_BRIDGE
Command to create the flavor and create a VM with this flavor:
---
nova flavor-create --ephemeral=1 victor_test_vif 50 256 1 1
nova flavor-key victor_test_vif set quota:vif_inbound_average=1000
nova flavor-key victor_test_vif set quota:vif_inbound_peak=1000
nova flavor-key victor_test_vif set quota:vif_outbound_peak=500
nova flavor-key victor_test_vif set quota:vif_outbound_average=500
nova boot --flavor=victor_test_vif --image=cirros-0.3.1-x86_64-uec victor_test
---
Command to start a small and fast TCP server on DevStack, uploading a file of 10 MB:
---
ip netns
# copy the qrouter-xxx name
sudo ip netns exec qrouter-128db593-a0db-40c3-84c7-e6383d40c75f bash
# following commands are executed in the qrouter namespace to reach the VM network
dd if=/dev/urandom of=random10MB bs=1024 count=10240
nc -l 0.0.0.0 12345 < random
---
Command to download the file on the VM:
---
time nc 10.0.0.1 12345 > /dev/null
---
Current result: timing smaller than 1 second (faster than 10 MB/sec)
Expected result: timing higher than 10 second (1 MB/sec or slower)
The problem is that the <bandwidth> tag is not generated in the libvirt.xml file of the VM.
I will provide a patch.","Fix LibvirtGenericVIFDriver.get_config() for quota

When Nova is used with Neutron and Open vSwitch, network quotas on the
bandwidth are ignored by libvirt: the ""<bandwidth>"" tag is not generated in
libvirt.xml of the virtual machine. This change fixes this issue but also for
the network types: ""bridge"", ""network"" and ""direct"".

Commands to create the flavor with download limited to 1 MB/sec and create a
virtual machine with this flavor:
---
nova flavor-create --ephemeral=1 victor_test_vif 50 256 1 1
nova flavor-key victor_test_vif set quota:vif_inbound_average=1000
nova boot --flavor=victor_test_vif --image=cirros-0.3.1-x86_64-uec victor_test
---

Commands to upload a file to this virtual machine with SCP to check that the
bandwidth is limited to 1 MB/sec (replace 10.0.0.3 with the IP of the virtual
machine):
---
ip netns
sudo ip netns exec qrouter-128db593-a0db-40c3-84c7-e6383d40c75f bash

dd if=/dev/urandom of=random10MB bs=1024 count=10240
scp random10M cirros@10.0.3:
---

Change-Id: I0f0111fc79fe90900e38df022034b208f1129088
Closes-Bug: #1254664"
556,49b54af36a2aacd3d27330636f61d6f996016d65,1407965149,,1.0,4,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1178,1635,1355622,cinder,49b54af36a2aacd3d27330636f61d6f996016d65,1,1,,"Bug #1355622 in Cinder: ""solidfire api commands not encoded""","Lines 132 & 133 of cinder/cinder/volume/drivers/solidfire.py are currently the following:
payload = json.dumps(command, ensure_ascii=False)
payload.encode('utf-8')
The string.encode() method used on line 133 returns an encoded string, but does not modify the payload string itself. Because of this, payload is never actually encoded to UTF-8. A suggested fix might be to modify the two lines to be the following:
payload = json.dumps(command, ensure_ascii=False).encode('utf-8')","Actually encode the SolidFire json dump result

The SolidFire driver intends to do a encode on the json data to utf-8,
unfortunatly it's not really doing anything because the syntax of the
calls is wrong.

This patch changes things to actually perform the encoding.

Change-Id: I2bc94ed1fdc1dd5f9a6371ae5f63a955f213bc15
Closes-Bug: #1355622"
557,4a60c6a655006b2882331844664fac5cf67c5f34,1404790540,,1.0,22,1,3,3,1,0.797551117,False,,,,,True,,,,,,,,,,,,,,,,,936,1372,1325128,nova,4a60c6a655006b2882331844664fac5cf67c5f34,1,1,"“CVE-2014-3517""","Bug #1325128 in OpenStack Compute (nova): ""[OSSA 2014-024] nova metadata does not use a constant time compare for validating an HMAC token (CVE-2014-3517)""","Here:
https://github.com/openstack/nova/blob/HEAD/nova/api/metadata/handler.py#L173
a constant time comparison should be used, more information on this type of attack here: http://codahale.com/a-lesson-in-timing-attacks/
An example constant time comparison in Python can be found here: https://github.com/django/django/blob/master/django/utils/crypto.py#L80 or via the PyCA cryptography library: https://cryptography.io/en/latest/hazmat/primitives/constant-time/","Avoid possible timing attack in metadata api

Introduce a constant time comparison function to
nova utils for comparing authentication tokens.

Change-Id: I7374f2edc6f03c7da59cf73ae91a87147e53d0de
Closes-bug: #1325128"
558,4a83dbb9e6ef9cc69176d8d96de6993ee4b45f77,1380112440,,1.0,3,2,3,3,1,0.960229718,True,1.0,116858.0,12.0,6.0,False,21.0,6442190.0,66.0,4.0,2.0,679.0,679.0,1.0,540.0,540.0,2.0,504.0,504.0,0.003102378,0.522233713,0.522233713,1588,1229366,1229366,glance,4a83dbb9e6ef9cc69176d8d96de6993ee4b45f77,1,1, ,"Bug #1229366 in Glance: ""incorrect response code from v2 delete image member call""","Currently on success, DELETE /v2/image/{imageId}/members/{memberId} returns 200 with no response body.
According to the Glance API contract, a 200 means you should expect a response body.  Success with no response body should be 204.","Change response code for successful delete image member to 204.

The delete image member call currently returns 200 with no response
body.  According to the API contract, 200 means to expect a body.
This patch changes the response code to 204.

Change-Id: Id48187b3c47e39075a352d3027dd3d3451d46905
Closes-Bug: #1229366"
559,4a8ad10c3d5d79e09805c28486bd0f8a5a16c511,1385447958,,1.0,21,10,2,2,1,0.554778163,True,2.0,345646.0,12.0,5.0,False,13.0,68729.0,19.0,4.0,9.0,669.0,671.0,9.0,624.0,626.0,7.0,603.0,603.0,0.006705784,0.506286672,0.506286672,80,477,1254975,cinder,4a8ad10c3d5d79e09805c28486bd0f8a5a16c511,0,0,removes redundant conde,"Bug #1254975 in Cinder: ""The body validation is redundant for os-volume_upload_image""","The body validation is redundant for os-volume_upload_image, should remove the redundant KeyError for 'os-volume_upload_image'. In fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action.","Redundant body validation for volume_upload_image

The body validation is redundant for os-volume_upload_image,should remove
the redundant validation.

Change-Id: Iba7a4d5f917615f8a8d6320b5fe18af66e4011e0
Closes-Bug: #1254975"
560,4ab7b6b7ad8b6047ec108c8f98bb1c0f310b9899,1410484383,,1.0,22,1,2,2,1,0.828055725,False,,,,,True,,,,,,,,,,,,,,,,,803,1231,1308649,nova,4ab7b6b7ad8b6047ec108c8f98bb1c0f310b9899,1,1,,"Bug #1308649 in OpenStack Compute (nova): ""Attaching Nova volumes fails if open-iscsi daemon is not already running""","nova-compute traces on first volume attach if open-iscsi is not running:
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] Traceback (most recent call last):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/compute/manager.py"", line 4142, in _attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     do_check_attach=False, do_driver_attach=True)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     ret_val = method(obj, context, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 248, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     connector)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     six.reraise(self.type_, self.value, self.tb)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/block_device.py"", line 239, in attach
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     device_type=self['device_type'], encryption=encryption)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1224, in attach_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     disk_info)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1183, in volume_driver_method
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return method(connection_info, *args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/contextlib.py"", line 34, in __exit__
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     self.gen.throw(type, value, traceback)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 212, in lock
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     yield sem
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 249, in inner
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     return f(*args, **kwargs)
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]   File ""/usr/lib64/python2.6/site-packages/nova/virt/libvirt/volume.py"", line 285, in connect_volume
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]     for ip, iqn in self._get_target_portals_from_iscsiadm_output(out):
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481] ValueError: too many values to unpack
2014-04-16 13:19:44.359 6079 TRACE nova.compute.manager [instance: ddc07867-67a9-4af7-b7d0-d75dff53b481]
Thats because it runs ""iscsiadm -m discovery -t sendtargets -p"", which prints upon first run:
Loading iscsi modules:                                                                                   done
Starting iSCSI initiator service:                                                                        done
Setting up iSCSI targets:                                                                                unused
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
192.168.204.82:3260,1 iqn.2010-10.org.openstack:volume-f9b12623-6ce3-4dac-a71f-09ad4249bdd3
aka there is some garbage before the actual output that it looks for. this is due to /etc/iscsid.conf having configured to start the daemon:
from /etc/iscsid.conf:
# Default for upstream open-iscsi scripts (uncomment to activate).
iscsid.startup = /usr/sbin/rcopen-iscsi start","Fix parsing sloppiness from iscsiadm discover

As reported by a user, iscsiadm might spew some other random gorp when
it starts up, and if it does, nova explodes poorly. Put a modicum of
sanity testing into the parsing of the output to make this less
likely.

Change-Id: Iaf55438bb3ec412187158e80f9fbc1d8e7071207
Closes-Bug: #1308649"
561,4adf35778b3aae3db99a90a1bbb94e668ec7963d,1382558910,,1.0,50,2,3,3,1,0.88704661,True,7.0,86042.0,27.0,10.0,False,19.0,2548728.0,39.0,2.0,1006.0,1083.0,1083.0,882.0,959.0,959.0,865.0,937.0,937.0,0.775985663,0.840501792,0.840501792,16,328,1243485,cinder,4adf35778b3aae3db99a90a1bbb94e668ec7963d,1,1,Exceptions wasnt handled,"Bug #1243485 in Cinder: ""Volume or Snapshot not found ERROR in c-api log after successful tempest run""","Happens a log, for example in http://logs.openstack.org/81/52181/8/check/check-tempest-devstack-vm-postgres-full/aa99a8c/logs/screen-c-api.txt.gz
2013-10-23 01:20:17.784 22210 ERROR cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return _snapshot_get(context, snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault     raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 7e7b9afe-d872-4ae0-b8d7-42b326dbb978 could not be found.
2013-10-23 01:20:17.784 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:20:17.786 22210 INFO cinder.api.middleware.fault [req-fec0ce89-321d-4cac-9a13-11edb6270993 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes returned with HTTP 404
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Authenticating user token __call__ /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:558
2013-10-23 01:20:17.797 22210 DEBUG keystoneclient.middleware.auth_token [-] Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role _remove_auth_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:617
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Returning cached token ba5f0f940d4cc2f687d5825436378d55 _cache_get /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:1016
2013-10-23 01:20:17.798 22210 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: bcf3db5a242f42d9b2e05d2adf9e4b13 with project_id : 6240d8b99d584ebd87a89b63e39f3a0b and roles: _member_  _build_user_headers /opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Matched POST /6240d8b99d584ebd87a89b63e39f3a0b/volumes __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Route path: '/{project_id}/volumes', defaults: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-10-23 01:20:17.799 22210 DEBUG routes.middleware [-] Match dict: {'action': u'create', 'controller': <cinder.api.openstack.wsgi.Resource object at 0x4809910>, 'project_id': u'6240d8b99d584ebd87a89b63e39f3a0b'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-10-23 01:20:17.799 22210 INFO cinder.api.openstack.wsgi [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] POST http://127.0.0.1:8776/v1/6240d8b99d584ebd87a89b63e39f3a0b/volumes
2013-10-23 01:20:17.800 22210 DEBUG cinder.api.v1.volumes [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Create volume request body: {'volume': {'scheduler_hints': {}, 'metadata': {u'Type': u'work'}, 'display_name': u'Volume--tempest-572743380', 'source_volid': u'e4974f44-969d-4cef-944d-2fd9b2b78458', 'size': u'1'}} create /opt/stack/new/cinder/cinder/api/v1/volumes.py:358
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: gigabytes is set by the default quota flag: quota_gigabytes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.813 22210 WARNING cinder.quota [req-1f98a6d8-005d-4976-a9ee-68c9a0ff4c63 fcd199daea8940918ded6d1a9dd8862a 487a496cac7f4b639ebc6bcb85ce87bf] Deprecated: Default quota for resource: volumes is set by the default quota flag: quota_volumes, it is now deprecated. Please use the the default quota class for default quota.
2013-10-23 01:20:17.817 22210 ERROR cinder.api.middleware.fault [req-2de5058a-06d4-468f-be2a-e50928757e25 bcf3db5a242f42d9b2e05d2adf9e4b13 6240d8b99d584ebd87a89b63e39f3a0b] Caught error: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 390, in create
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     source_volid)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 335, in get_volume
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     rv = self.db.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 213, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return IMPL.volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1149, in volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return _volume_get(context, volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1142, in _volume_get
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault     raise exception.VolumeNotFound(volume_id=volume_id)
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault VolumeNotFound: Volume e4974f44-969d-4cef-944d-2fd9b2b78458 could not be found.
2013-10-23 01:20:17.817 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.139 22210 ERROR cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] Caught error: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/middleware/fault.py"", line 77, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 571, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 827, in __call__
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     content_type, body, accept)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 875, in _process_stack
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/openstack/wsgi.py"", line 951, in dispatch
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/api/v1/volumes.py"", line 383, in create
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/volume/api.py"", line 330, in get_snapshot
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     rv = self.db.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 267, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return IMPL.snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1480, in snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return _snapshot_get(context, snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 138, in wrapper
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 1473, in _snapshot_get
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault     raise exception.SnapshotNotFound(snapshot_id=snapshot_id)
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault SnapshotNotFound: Snapshot 6095dd59-1e8a-46d5-bfee-4d2dccbf7f91 could not be found.
2013-10-23 01:23:50.139 22210 TRACE cinder.api.middleware.fault
2013-10-23 01:23:50.140 22210 INFO cinder.api.middleware.fault [req-3c68c3cd-7647-4717-a5be-5e1a0bffcab9 c562188b400e44a3b9d3900495ecf748 5dada9a2750641eab97cc64931d55961] http://127.0.0.1:8776/v1/5dada9a2750641eab97cc64931d55961/volumes returned with HTTP 404","Handle NotFound exceptions in API

There were a number of calls in the API that
weren't catching NotFound exceptions.  The result is
unhandled exception traces and errors in the logs on good
runs in tempest.

Basicly any negative test that requests a non-existent element
would result in an unhandled exception.  This patch adds try/except
around the volume_api.getxxx calls in cinder/api/... methods to
clean this up.

Closes-bug: #1243485

Change-Id: I902acc7f4fdbc20fdb1a68697679417694c5533e"
562,4af2163bd41648d64cf1c3c838990737955d2133,1386678026,,1.0,50,11,5,4,1,0.512776763,True,8.0,2949705.0,132.0,41.0,False,33.0,482739.0,55.0,4.0,544.0,1726.0,2000.0,442.0,1525.0,1763.0,193.0,254.0,390.0,0.313915858,0.412621359,0.632686084,1803,1254520,1254520,Neutron,4af2163bd41648d64cf1c3c838990737955d2133,1,0,"Enviromental change, this is a bug related with ovs-vsctl","Bug #1254520 in neutron: ""Open vSwitch commands timeout on gate tests""","Error 242 occurs fairly often in gate tests: http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkV4aXQgY29kZTogMjQyXCIgIiwiZmllbGRzIjpbImZpbGVuYW1lIl0sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNDMyMDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzg1MzE3OTg4NTQxLCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
This is actual an ALARM_CLOCK error [142] (rootwrap adds 100 to the error code), and means open vswitch times out; as the default timeout is 2 seconds there is a chance they could occur quite often in a rather stressful scenario as the one represented by parallel tests in tenant isolation.
It might be therefore advisable to allow for a configurable timeout on ovs commands, and increase this timeout for gate tests.
Kernel logs do not provide enough additional information.
It might be also worth adding open vswitch logs to the logs collected by devstack-gate","Make timeout for ovs-vsctl configurable

This patch adds a new configuration variable for the timeout on
ovs-vsctl commands, and sets the default timeout to 10 seconds.
This is aimed at allowing users to tune the agents in order to avoid
timeout errors on their deployments.

Change-Id: I73ea0d0de49a4b4a118bc2d68ad9c093ea122717
Closes-Bug: #1254520"
563,4b2304d6bb1d77db1e411f375a00a93aaabca262,1411697783,,1.0,19,5,2,2,1,0.994984828,False,,,,,True,,,,,,,,,,,,,,,,,1351,1820,1373333,glance,4b2304d6bb1d77db1e411f375a00a93aaabca262,1,1,,"Bug #1373333 in Glance: ""glance-replicator livecopy cannot work due to Bad header: x-image-meta-virtual-size""","I would like to use ""glance-replicator livecopy"" command to replicate the images from one openstack instance to anthor, it goes to error while running the livecopy command:
2014-09-24 17:32:09.082 24953 CRITICAL glance [-] ServerErrorException: 400 Bad Request
The server could not comply with the request since it is either malformed or otherwise incorrect.
 Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance Traceback (most recent call last):
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/bin/glance-replicator"", line 10, in <module>
2014-09-24 17:32:09.082 24953 TRACE glance     sys.exit(main())
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 739, in main
2014-09-24 17:32:09.082 24953 TRACE glance     command(options, args)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 516, in replication_livecopy
2014-09-24 17:32:09.082 24953 TRACE glance     image_response)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 238, in add_image
2014-09-24 17:32:09.082 24953 TRACE glance     response = self._http_request('POST', url, headers, image_data)
2014-09-24 17:32:09.082 24953 TRACE glance   File ""/usr/lib/python2.6/site-packages/glance/cmd/replicator.py"", line 125, in _http_request
2014-09-24 17:32:09.082 24953 TRACE glance     raise ServerErrorException(response.read())
2014-09-24 17:32:09.082 24953 TRACE glance ServerErrorException: 400 Bad Request
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance The server could not comply with the request since it is either malformed or otherwise incorrect.
2014-09-24 17:32:09.082 24953 TRACE glance
2014-09-24 17:32:09.082 24953 TRACE glance  Bad header: x-image-meta-virtual-size
2014-09-24 17:32:09.082 24953 TRACE glance","Fix bad header bug in glance-replicator

When trying to use ""glance-replicator livecopy"" to replicate the
images from one OpenStack glance to another, it encounters ""Bad
header"" error for ""x-image-meta-virtual-size"" header. This is
because this header is not in the whitelist of v1 API headers of
form ""x-image-meta-xxx"".

Another problem after adding this ""x-image-meta-virtual-size"" to
header whitelist is that the sqlalchemy fail due to virual_size is
""None"", which cannot be converted to int. To fix this problem, we
should avoid converting the None value attribute in GET
/v1/images/detail response to the header of POST /v1/images. I
added the None value check, following what the way in
glance/common/utils.py image_meta_to_http_headers() method.

Closes-Bug: #1373333

Change-Id: I13182c4e3212024a9d5663cefd2951c576961f4f"
564,4b4f0d61836e527340511f6198c2f2c0bdf70888,1387764115,,1.0,0,114,5,2,1,0.819523717,True,1.0,3098514.0,10.0,5.0,False,81.0,3293091.0,243.0,5.0,185.0,2661.0,2778.0,185.0,1958.0,2075.0,175.0,2402.0,2510.0,0.025437202,0.347304524,0.362913716,216,619,1263569,nova,4b4f0d61836e527340511f6198c2f2c0bdf70888,0,0,delete obsolete code,"Bug #1263569 in OpenStack Compute (nova): ""Remove update_service_capabilities from nova""","From the comments of update_service_capabilities, it is said that once publish_service_capabilities was removed, then we can begin the process of  its removal.
Now publish_service_capabilities has been removed, so we can remove update_service_capabilities now as no one is calling it
def update_service_capabilities(self, context, service_name,
                                    host, capabilities):
        """"""Process a capability update from a service node.""""""
        #NOTE(jogo) This is deprecated, but is used by the deprecated
        # publish_service_capabilities call. So this can begin its removal
        # process once publish_service_capabilities is removed.
        if not isinstance(capabilities, list):
            capabilities = [capabilities]
        for capability in capabilities:
            if capability is None:
                capability = {}
            self.driver.update_service_capabilities(service_name, host,
                                                    capability)","Remove update_service_capabilities from nova

From the comments of update_service_capabilities, it is said that
once publish_service_capabilities was removed, then we can begin the
process of its removal.

publish_service_capabilities has been removed in patch
I31c21055163e94b712d337568b16b9b7a224b52f, so we can remove
update_service_capabilities now.

Change-Id: I46be33462321f5e3ff0904bdff8dbfe2fbe115cd
Closes-Bug: #1263569"
565,4ba5cbc48f471c39826aa38244f86a2a6252b2e9,1383041989,,1.0,1,1,1,1,1,0.0,True,7.0,5160900.0,44.0,17.0,False,25.0,9535.0,76.0,2.0,8.0,147.0,155.0,7.0,147.0,154.0,0.0,115.0,115.0,0.000998004,0.115768463,0.115768463,25,337,1244092,glance,4ba5cbc48f471c39826aa38244f86a2a6252b2e9,1,1,,"Bug #1244092 in oslo-incubator: ""db connection retrying doesn't work against db2""","When I start Openstack following below steps, Openstack services can't be started without db2 connection:
1, start openstack services;
2, start db2 service.
I checked codes in session.py under nova/openstack/common/db/sqlalchemy, the root cause is db2 connection error code ""-30081"" isn't in conn_err_codes in _is_db_connection_error function, connection retrying codes are skipped against db2, in order to enable connection retrying function against db2, we need add db2 support in _is_db_connection_error function","Add db2 communication error code when check the db connection

Closes-Bug: #1244092

The oslo patch set is 3acd57c2825a8bf6660c3124f12794414622eb26

Change-Id: Ic5244cab4e517f740b398138d4da877affe46ab3"
566,4ba7ee4c1bb89b2fa48c2091df1d7f0bcf10ee02,1411022668,,1.0,28,1,2,2,1,0.479832024,False,,,,,True,,,,,,,,,,,,,,,,,1252,1711,1363326,nova,4ba7ee4c1bb89b2fa48c2091df1d7f0bcf10ee02,1,1,,"Bug #1363326 in OpenStack Compute (nova): ""Error retries in _allocate_network""","In /nova/compute/manager.py/def _allocate_network_async,line 1559.
attempts = retries > 1 and retries + 1 or 1
retry_time = 1
for attempt in range(1, attempts + 1):
Variable attempts wants to determine the retry times of allocate network,but it made a small mistake.
See the Simulation results below:
retries=0,attempts=1
retries=1,attempts=1
retries=2,attempts=3
When retries=1, attempts=1 ,It actually does not retry.","the value of retries is error in _allocate_network

the variable 'retries' stands for the times retried by allocate
network ,but it makes a mistake

Change-Id: Ia4c046641619f619f8bf6205f841b3a332d31260
Closes-Bug: #1363326"
567,4be8913520f5e9fe4109ade101da9509e4a83360,1409151394,1.0,1.0,266,171,5,2,1,0.592203036,False,,,,,True,,,,,,,,,,,,,,,,,723,1149,1301348,cinder,4be8913520f5e9fe4109ade101da9509e4a83360,1,1,,"Bug #1301348 in Cinder: ""vmware: Volume backing not getting relocated to a compliant datastore during re-attach""","Create a volume with volume type mapped to gold profile. [gold profile mapped to datastore1]
Now attach to instance and observe that volume is placed in datastore1.
Now detach above volume .
Now change volume type of above volume to Silver profile   [Silver profile mapped to datastore2]
Now re attach volume to instance and observe that volume is still  placed in datastore1 instead of datastore2.","VMware: Relocate volume to compliant datastore

During attach to a nova instance, the backing VM corresponding to the
volume is relocated only if the nova instance's ESX host cannot access
the backing's current datastore. The storage profile is ignored and
the volume's virtual disk might end up in a non-compliant datastore.
This patch fixes the problem by checking storage profile compliance of
the current datastore.

Change-Id: I3865654e219c05dcec3aaab07c4cee0658fe181e
Closes-Bug: #1301348"
568,4c2b42e21744be56cbf32aeac6f4b4f1c87de24e,1413024167,,1.0,25,7,4,3,1,0.693480147,False,,,,,True,,,,,,,,,,,,,,,,,1377,1847,1377241,neutron,4c2b42e21744be56cbf32aeac6f4b4f1c87de24e,1,1,,"Bug #1377241 in neutron: ""Lock wait timeout on delete port for DVR""","We run a script to configure networks, VMs, Routers and assigin floatingIP to the VM.
After it is created, then we run a script to clean all ports, networks, routers and gateway and FIP.
The issue is seen when there is a back to back call to router-interface-delete and router-gateway-clear.
There are three calls to router-interface-delete and the fourth call to router-gateway-clear.
At this time there is a db lock obtained for port delete and when the other delete comes in, it timeout.
2014-10-03 09:28:39.587 DEBUG neutron.openstack.common.lockutils [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] Got semaphore ""db-access"" from (pid=25888) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-03 09:29:30.777 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54899)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54900)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54901)
2014-10-03 09:29:30.778 INFO neutron.wsgi [-] (25888) accepted ('192.168.15.144', 54902)
2014-10-03 09:29:30.780 ERROR neutron.api.v2.resource [req-a89ee05c-d8b2-438a-a707-699f450d3c41 admin d3bb4e1791814b809672385bc8252688] remove_router_interface failed
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 200, in _handle_action
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 247, in remove_router_interface
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context.elevated(), router, subnet_id=subnet_id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/l3_dvr_db.py"", line 557, in delete_csnat_router_interface_ports
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     l3_port_check=False)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/ml2/plugin.py"", line 983, in delete_port
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     port_db, binding = db.get_locked_port_and_binding(session, id)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/ml2/db.py"", line 135, in get_locked_port_and_binding
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     with_lockmode('update').
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2310, in one
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     ret = list(self)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2353, in __iter__
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     return self._execute_and_instances(context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2368, in _execute_and_instances
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     result = conn.execute(querycontext.statement, self._params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 662, in execute
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     params)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 761, in _execute_clauseelement
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     compiled_sql, distilled_params
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 874, in _execute_context
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 125, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     six.reraise(type(newraise), newraise, sys.exc_info()[2])
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/compat/handle_error.py"", line 102, in _handle_dbapi_exception
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     per_fn = fn(ctx)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 323, in handler
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     context.is_disconnect)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/oslo/db/sqlalchemy/exc_filters.py"", line 254, in _raise_operational_errors_directly_filter
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource     raise operational_error
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') 'SELECT ports.tenant_id AS ports_tenant_id, ports.id AS ports_id, ports.name AS ports_name, ports.network_id AS ports_network_id, ports.mac_address AS ports_mac_address, ports.admin_state_up AS ports_admin_state_up, ports.status AS ports_status, ports.device_id AS ports_device_id, ports.device_owner AS ports_device_owner \nFROM ports \nWHERE ports.id = %s FOR UPDATE' ('bec69266-227d-4482-a346-ef47dd3a7a78',)
2014-10-03 09:29:30.780 TRACE neutron.api.v2.resource","Call DVR VMARP notify outside of transaction

The dvr vmarp table update notification was being called inside
of the delete_port transaction in ML2, which can cause a yield
and lead to the glorious mysql/eventlet deadlock.

This patch moves it outside the transaction and adjusts it to
use an existing port dictionary rather than re-looking it up since
the port is now gone from the DB by the time it is called.

Closes-Bug: #1377241
Change-Id: I0b4dac61e49b2a926353f8478e421cd1a70be038"
569,4c4dc3a6d331426e472e2dd1e9b0513da7cb7450,1407782190,,1.0,4,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1174,1631,1355348,nova,4c4dc3a6d331426e472e2dd1e9b0513da7cb7450,1,1,,"Bug #1355348 in OpenStack Compute (nova): ""Terminating an instance while attaching a volume leads to both actions failing""","This is happening with the xenapi driver, but it's possible that this can happen with others.  The sequence of events I'm witnessing is:
An attach_volume request is made and shortly after a terminate_instance request is made.
From the attach_volume request the block device mapping has been updated, the volume has been connected to the hypervisor, but has not been attached to the instance.  The terminate request begins processing before the volume connection is attached to the instance so when it detaches volumes and their connections it misses the latest one that's still attaching.  This leads to a failure when asking Cinder to clean up the volume, such as:
2014-08-06 20:30:14.324 30737 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://127.0.0.1/volumes/<uuid>/export?force=False returned '409' with 'Volume '<uuid>' is currently attached to '127.0.0.1'' (HTTP 409) (Request-ID: req-)
And in turn, when the attach_volume tries to attach the volume to the instance it finds that the instance no longer exists due to the terminate request.  This leaves the instance undeletable and the volume stuck.
Having attach_volume share the instance lock with terminate_instance should resolve this.  Virt drivers may also want to try to cope with this internally and not rely on a lock.","Lock attach_volume

There are some issues with instance and volume cleanup when the volume
is not in a fully attached state so it will be safer to not attempt a
terminate_instance while there are attachments in progress.

Change-Id: I4347794e51004a881bf4ef5ee30f65ac28773e51
Closes-Bug: #1355348"
570,4c68f19cd8d1f045bde02be7eec50c34c1d87932,1394783952,,1.0,2,1,2,2,1,0.918295834,True,2.0,3260663.0,82.0,20.0,False,201.0,13752.5,1011.0,6.0,288.0,3115.0,3213.0,252.0,2738.0,2830.0,200.0,2174.0,2258.0,0.02644041,0.286108919,0.297158642,475,892,1284709,nova,4c68f19cd8d1f045bde02be7eec50c34c1d87932,1,1,,"Bug #1284709 in OpenStack Compute (nova): ""nova evacuate fails with neutron""","When I deploy nova with a shared storage and with neutron/ML2/linuxbridge, I have an error when I want to to use ""nova evacuate""
command :
# nova evacuate 1fa486f3-259c-4e1e-ae82-8b52606f1efd devstack2 --on-shared-storage
here are the logs on the compute node (devstack2) which will host the VM after the evacuation:
2014-02-25 16:08:39.918 ERROR nova.compute.manager [req-8307b6e1-b6ee-423e-9baf-d05a0ac5e91d admin admin] [ins
tance:
1fa486f3-259c-4e1e-ae82-8b52606f1efd] Setting instance vm_state to ERROR
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/manager.py"", line 5261, in _error_out_instance_on_exception
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     yield
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2267, in rebuild_instance
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     extra_usage_info=extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/api.py"", line 271, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 428, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     extra_usage_info=extra_usage_info_p)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/client.py"", line 150, in call
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     wait_for_reply=True, timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/transport.py"", line 87, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     timeout=timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 393, in send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py"", line 386, in _send
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     raise result
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] Traceback (most recent call last):
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     incoming.message))
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     return self._do_dispatch(endpoint, method, ctxt, args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     result = getattr(endpoint, method)(ctxt, **new_args)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/conductor/manager.py"", line 502, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     system_metadata, extra_usage_info)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/compute/utils.py"", line 276, in notify_usage_exists
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     ignore_missing_network_data)
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]   File ""/opt/stack/nova/nova/notifications.py"", line 288, in bandwidth_usage
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]     macs = [vif['address'] for vif in nw_info]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd] TypeError: 'NoneType' object is not iterable
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]
2014-02-25 16:08:39.918 12410 TRACE nova.compute.manager [instance: 1fa486f3-259c-4e1e-ae82-8b52606f1efd]","Add info_cache as expected attribute when evacuate instance

Nova evacuates instance failed with neutron. The reason is
when nova-conductor try to get network info for instance, it
need instance include 'info_cache', but the instance dict which
passed didn't include it. This patch adds 'info_cache' as the
expected attribute for get instance.

Change-Id: I6f110c607e32b0ab536916741605b25d572331dc
Closes-Bug: #1284709"
571,4cac1bd3227fd8b65744e75f0df018fb34bcb1c1,1392363967,3.0,1.0,185,90,6,3,1,0.579740221,True,6.0,996807.0,35.0,9.0,False,84.0,81332.66667,262.0,3.0,5.0,914.0,914.0,5.0,867.0,867.0,5.0,869.0,869.0,0.004201681,0.609243697,0.609243697,380,793,1278035,cinder,4cac1bd3227fd8b65744e75f0df018fb34bcb1c1,0,0,Feature not a bug,"Bug #1278035 in Cinder: ""storwize-svc migration/retype may leave extra volume copies after cinder crash""","Cinder storage-assisted volume migration and retype work by creating a new copy of the volume, waiting for it to sync, and deleting the original copy.  During the sync time (which may be long), if the Cinder process goes down for any reason, we will end up with a volume that has two copies, which wastes resources.","Storwize/SVC: Change volume copy task to async

If Cinder crashes during a migration or retype (where data is moved and
the operation can take a long time), the storage ends up with multiple
copies of the same volume which requires storage admin intervention.

This patch maintain a list of pending operations which is backed up in
admin metadata, and a periodic task reviews the list and removes volume
copies whose copy operation completed. When Cinder comes up, check the
admin metadata and rebuild the list.

Change-Id: I6549712bb0083996faced89c2207a4c438ae953d
Closes-Bug: #1278035"
572,4caca0dfae45dd6c61ddc1b4a2233d2ce1a11685,1404470477,4.0,1.0,86,59,4,2,1,0.535203379,False,,,,,True,,,,,,,,,,,,,,,,,1041,1485,1337796,cinder,4caca0dfae45dd6c61ddc1b4a2233d2ce1a11685,1,1,,"Bug #1337796 in Cinder: ""Cinder api service outputs error messages when SIGHUP signal is sent""","When SIGHUP signal is send to cinder-api service, it stops all the cinder-api processes and while restarting the cinder-api processes, it throws AttributeError: 'WSGIService' object has no attribute 'reset'.
Steps to reproduce:
1. Run cinder-api service as daemon.
2. Send SIGHUP signal to cinder-api service
   kill -1 <parent_process_id_of_cinder_api>","Cinder-api service throws error on SIGHUP signal

Added reset method in WSGIService class.

After adding reset method when SIGHUP signal is sent to
wsgi service parent process,then it sends SIGHUP signal
to all of its child processes. Each child process handles
SIGHUP signal by first stopping the service and then calls
service start method again. When it stops the service, it
kills the eventlet thread, which internally closes the wsgi
server socket object. This server socket object is now not
usable again and it throws following error, while restarting
the service:

error: [Errno 9] Bad file descriptor

To resolve 'Bad file descriptor' error, creating duplicate
socket object, every time service starts.

Closes-Bug: #1337796

Change-Id: Iab32a3fe230a11692a8cad274304214247d6c2c6"
573,4cb139fb99814f04b9e4595d7e2efa34e7992826,1399468665,,1.0,23,5,3,2,1,0.716620108,True,3.0,1109013.0,41.0,10.0,False,39.0,606444.0,103.0,3.0,2.0,1599.0,1599.0,2.0,1368.0,1368.0,2.0,1297.0,1297.0,0.000377691,0.163414327,0.163414327,867,1300,1316674,nova,4cb139fb99814f04b9e4595d7e2efa34e7992826,1,1,,"Bug #1316674 in OpenStack Compute (nova): ""VCDriver - Stats update does not ignore hosts in maintenance mode""","Cluster has 2 hosts, with each host having 24 cores.
Both hosts are active.
Stats update is correct
Snippet of nova-compute.log
-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free ram (MB): 57582
2014-05-07 03:14:28.001 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 99
2014-05-07 03:14:28.002 AUDIT nova.compute.resource_tracker [-] Free VCPUS: 48
2014-05-07 03:14:28.021 INFO nova.compute.resource_tracker [-] Compute_service record updated for sagar-devstack:domain-c162(Demo-Pulsar-Cluster-DRS)
2014-05-07 03:14:28.021 DEBUG nova.openstack.common.lockutils [-] Semaphore / lock released ""update_available_resource"" from (pid=28072) inner /opt/stack/nova/nova/openstack/common/lockutils.py:252
Put one of the hosts in Maintenance mode, stats update does not ignore host in maintenance mode and the free vCPUS is still 48.
It should be 24","VCDriver - Ignore host in Maintenance mode in stats update

The VCDriver during stats update checks only for connected hosts.
However hosts which are in connected state to the Cluster can
be set to Maintenance mode.
The cluster does not boot instances to hosts which are in
maintenance mode.
Hence the VCDriver should check for hosts which are in
maintenance mode and ignore them for stats update.
VISDK API returns True for the property inMaintenanceMode for
Hosts which are in maintenance mode

Change-Id: Ieaf7530ed2d98c59c5450e8cb75e209823193d49
Closes-Bug: #1316674"
574,4cb3eb2a98246f28f8016cfe32946d365203fbae,1396971780,,1.0,15,1,2,2,1,0.811278124,True,1.0,1141330.0,13.0,3.0,False,6.0,678237.0,14.0,3.0,308.0,1952.0,2004.0,286.0,1603.0,1636.0,240.0,921.0,938.0,0.216337522,0.827648115,0.842908438,757,1184,1304127,neutron,4cb3eb2a98246f28f8016cfe32946d365203fbae,1,1,"""https://review.openstack.org/#/c/69465/""","Bug #1304127 in neutron: ""NSX: dhcp port missing on the metadata network""","The DHCP agent used to have a leg on the metadata network, this is a regression caused by:
https://review.openstack.org/#/c/69465/","NSX: ensure dhcp port is setup on metadata network

Change in scheduling behavior caused a regression
where the dhcp port is no longer provisioned on
the (admin) metadata network created when a subnet
is uplinked to a router.

This change recovers the past behavior and extend UT
coverage to avoid further regression.

Closes-bug: #1304127

Change-Id: I8420203f68a43368f3784adb0c4cbbe55f048662"
575,4cc2366623743393f89a198581fcb69dc04d31cd,1395431163,,1.0,3,4,1,1,1,0.0,True,5.0,3658056.0,41.0,18.0,False,15.0,1197852.0,26.0,8.0,420.0,1643.0,2034.0,419.0,1367.0,1757.0,13.0,1433.0,1433.0,0.009067358,0.928756477,0.928756477,637,1063,1293792,cinder,4cc2366623743393f89a198581fcb69dc04d31cd,0,0,tests,"Bug #1293792 in Cinder: ""test_volume_get_all_filters_limit incorrectly asserts certain metadata ordering""","cinder.tests.test_db_api.DBAPIVolumeTestCase.test_volume_get_all_filters_limit
This test indirectly asserts that the metadata on a volume is returned in a certain order. This is incorrect because python dictionaries to not maintain ordering. This causes spuratic unit test failures with this error:
http://paste.openstack.org/show/73692/","Correct metadata ordering issue in tests

Some test including test_volume_get_all_filters
fails randomly because metadata is in a dict and
the items order is undefined.
The test comparison was changed to avoid unexpected
test results.

Change-Id: Ibb24d21cd05aa1eefb45b61c63de067b34fb1013
Closes-Bug: #1293792"
576,4d43983726f4b5d1d93337be005d9e1c165c4939,1378983007,,1.0,40,11,2,2,1,0.873981048,True,4.0,969769.0,24.0,9.0,False,20.0,156221.0,50.0,4.0,343.0,574.0,722.0,339.0,550.0,694.0,330.0,523.0,662.0,0.335020243,0.530364372,0.671052632,1547,1224334,1224334,cinder,4d43983726f4b5d1d93337be005d9e1c165c4939,1,0,"“The HyperV driver doesn't support CHAP authentication,” DocImpact","Bug #1224334 in Cinder: ""HyperV doesn't work with CHAP in Storwize/SVC driver""","The HyperV driver doesn't support CHAP authentication, which is always enabled by the Storwize/SVC driver.","Storwize/SVC: Optional CHAP authentication

The Storwize/SVC driver doesn't work with Nova drivers that don't
support CHAP (e.g., HyperV). This patch makes CHAP optional.

DocImpact

Closes-Bug: #1224334
Change-Id: I22df4b916b2800a53c1e4968913f7e95965eaf4b"
577,4da02aeabc9bb072bd33b21a1c8fb6e10cc4e262,1393748372,,1.0,0,7,1,1,1,0.0,True,1.0,563907.0,23.0,10.0,False,13.0,63256.0,14.0,5.0,19.0,538.0,548.0,18.0,418.0,427.0,17.0,376.0,384.0,0.015280136,0.320033956,0.326825127,530,951,1288337,glance,4da02aeabc9bb072bd33b21a1c8fb6e10cc4e262,0,0,remove code,"Bug #1288337 in Glance: ""Remove task-specific validation from tasks resource""","Currently there is import-specific validation of the input field hard-coded into the create method of the tasks resource. Since this field is provider specific, validation should be left up to the actual import process. The task resource should only validate the fields common to all tasks (type and input - presence only).","Remove import specific validation from tasks resource

Task-type specific validation should not be placed in the the
task resource as this will vary among providers. Validation of
the input values should occur in the task script and be reported
as part of the task status.

Change-Id: Ifbc1af868359e6bd51b18b10cd5402340438917a
Closes-bug: #1288337"
578,4dcfa79058a407eb7a6ce83f4d804df19ec95026,1405900387,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1122,1577,1350268,nova,4dcfa79058a407eb7a6ce83f4d804df19ec95026,1,1,,"Bug #1350268 in OpenStack Compute (nova): ""allocate_fixed_ip should cleanup with correct param""","in nova-network , when allocate_fixed_ip failed for some unknown reason
it will add
cleanup.append(fip.disassociate)
to cleanup the stuffs it did when handle exception
but the function is following in objects/fixed_ips.py
def disassociate(self, context):
so the cleanup function will not be executed correctly
try:
                        f()
                    except Exception:
                        LOG.warn(_('Error cleaning up fixed ip allocation. '
                                   'Manual cleanup may be required.'),
                                 exc_info=True)","Add context as param to cleanup function

In nova-network, when we start
to cleanup the stuffs it did when handle exception
but the function is following in objects/fixed_ips.py
def disassociate(self, context):
so the context param is lost and it can lead to problem.

Change-Id: I66a1681f47fc69bf0fd92137ee27481659833bef
Closes-Bug: #1350268"
579,4ded12bef837e707c76af87d71155f04534dcb0d,1387354470,0.0,1.0,6,5,1,1,1,0.0,True,1.0,1397967.0,15.0,5.0,False,3.0,2836368.0,3.0,3.0,185.0,853.0,921.0,183.0,718.0,786.0,144.0,344.0,392.0,0.226917058,0.539906103,0.615023474,205,608,1262089,neutron,4ded12bef837e707c76af87d71155f04534dcb0d,0,0,tests,"Bug #1262089 in neutron: ""Tracebacks in py26 unit test console logs related to metadata agent""","After adding state reporting to metadata agent following tracebacks appeared in py26 unit test console logs:
2013-11-22 15:08:33.914 | ERROR:neutron.agent.metadata.agent:Failed reporting state!
2013-11-22 15:08:33.914 | Traceback (most recent call last):
2013-11-22 15:08:33.914 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/metadata/agent.py"", line 258, in _report_state
2013-11-22 15:08:33.914 |     use_call=self.agent_state.get('start_flag'))
2013-11-22 15:08:33.914 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/agent/rpc.py"", line 74, in report_state
2013-11-22 15:08:33.914 |     return self.cast(context, msg, topic=self.topic)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/proxy.py"", line 171, in cast
2013-11-22 15:08:33.915 |     rpc.cast(context, self._get_topic(topic), msg)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/__init__.py"", line 158, in cast
2013-11-22 15:08:33.915 |     return _get_impl().cast(CONF, context, topic, msg)
2013-11-22 15:08:33.915 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 166, in cast
2013-11-22 15:08:33.915 |     check_serialize(msg)
2013-11-22 15:08:33.916 |   File ""/home/jenkins/workspace/gate-neutron-python26/neutron/openstack/common/rpc/impl_fake.py"", line 131, in check_serialize
2013-11-22 15:08:33.916 |     json.dumps(msg)
2013-11-22 15:08:33.916 |   File ""/usr/lib64/python2.6/json/__init__.py"", line 230, in dumps
2013-11-22 15:08:33.916 |     return _default_encoder.encode(obj)
2013-11-22 15:08:33.916 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 367, in encode
2013-11-22 15:08:33.916 |     chunks = list(self.iterencode(o))
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.917 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.917 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.917 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.918 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.918 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.918 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 309, in _iterencode
2013-11-22 15:08:33.919 |     for chunk in self._iterencode_dict(o, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 275, in _iterencode_dict
2013-11-22 15:08:33.919 |     for chunk in self._iterencode(value, markers):
2013-11-22 15:08:33.919 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 317, in _iterencode
2013-11-22 15:08:33.919 |     for chunk in self._iterencode_default(o, markers):
2013-11-22 15:08:33.920 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 323, in _iterencode_default
2013-11-22 15:08:33.920 |     newobj = self.default(o)
2013-11-22 15:08:33.920 |   File ""/usr/lib64/python2.6/json/encoder.py"", line 344, in default
2013-11-22 15:08:33.920 |     raise TypeError(repr(o) + "" is not JSON serializable"")
2013-11-22 15:08:33.920 | TypeError: <MagicMock name='cfg.CONF.host' id='989177040'> is not JSON serializable
2013-11-22 15:08:33.921 | WARNING:neutron.openstack.common.loopingcall:task run outlasted interval by <MagicMock name='cfg.CONF.AGENT.report_interval.__sub__().__neg__()' id='989263056'> sec
this can be observed in jenkins results for any patch on review.
Need to mock loopingcall in metadata agent tests in order to fix this","Mock looping_call in metadata agent tests

Change-Id: Icb5146084730384bac1fb7d3176ce0cfcd565cc5
Closes-Bug: #1262089"
580,4e80524a61a655fe9732ff42dc8f035810214670,1409210761,,1.0,64,167,2,2,1,0.195909271,False,,,,,True,,,,,,,,,,,,,,,,,1245,1704,1362466,neutron,4e80524a61a655fe9732ff42dc8f035810214670,1,1,,"Bug #1362466 in neutron: ""iptables metering removes wrong labels on update""","If a router is removed from the list passed to update_routers(), the iptables_driver removes the labels for the last(?) router passed, not the one removed.","Remove chain for correct router during update_routers()

The existing code incorrectly used the stale value from a previous list
comprehension - and deleted the chains for the wrong router :(
(Found via pylint)

Also: change to using a set() rather than a list(), since it is used for
repeated membership tests.

Also: refactor test cases to remove test case duplication.

Closes-Bug: #1362466
Change-Id: I4df400d57bab5427362db47a715576faa6340173"
581,4e8f4155fc559a5d49a70aee7882d7e0e117563a,1405944794,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1084,1534,1346245,neutron,4e8f4155fc559a5d49a70aee7882d7e0e117563a,1,1,“b7a8863760e_rm_cisco_vlan_bindin”,"Bug #1346245 in neutron: ""Incorrect downgrade in migration b7a8863760e_rm_cisco_vlan_bindin""",Downgrade in migration b7a8863760e_rm_cisco_vlan_bindin fails http://paste.openstack.org/show/87396/,"Fix incorrect downgrade

In downgrade of migration b7a8863760e_rm_cisco_vlan_bindin is used
Integer type with parameter display_width. Integer should be used
without any parameters.

Closes-bug: #1346245

Change-Id: Ic444af81ed3ec25116d2171ab465ce473cba8113"
582,4e9d4824050eedfd1495469a69e28864b6fec757,1396886014,,1.0,23,8,4,2,1,0.881058585,True,10.0,622859.0,150.0,40.0,False,38.0,1130269.0,59.0,7.0,50.0,856.0,869.0,50.0,759.0,772.0,50.0,601.0,614.0,0.046279492,0.546279492,0.558076225,739,1166,1302312,neutron,4e9d4824050eedfd1495469a69e28864b6fec757,1,1,,"Bug #1302312 in neutron: ""dhcp agent sometimes fail to add gateway to interface because interface is down""","2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 217, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     kwargs.get('use_root_namespace', False))
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 70, in _as_root
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     namespace)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/ip_lib.py"", line 81, in _execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     root_helper=root_helper)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/linux/utils.py"", line 75, in execute
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent     raise RuntimeError(m)
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent RuntimeError:
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qdhcp-e1eaed6e-ef91-4741-acb2-62daba3ffede', 'ip', 'route', 'replace', 'default', 'via', '10.2.0.1', 'dev', 'tapb1e4235c-4e']
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Exit code: 2
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stdout: ''
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent Stderr: 'RTNETLINK answers: Network is unreachable\n'
2014-04-03 20:37:02.020 TRACE neutron.agent.dhcp_agent","DHCP agent should check interface is UP before adding route

The DHCP agent should check not only that an interface for
network's DHCP exists but also make sure that is UP before
adding a default route.
For this purpose a method ""ensure_device_is_ready"" was
added to ip_lib.

Change-Id: I9af06aa0f39634fe7b63c064337cd4191db5c026
Closes-bug: #1302312"
583,4ea6dfd0b1140e437703128bf52d65dbd2751751,1378505928,1.0,1.0,6,6,2,2,1,0.918295834,True,3.0,335010.0,12.0,5.0,False,8.0,1904.0,14.0,2.0,24.0,948.0,962.0,21.0,863.0,874.0,13.0,813.0,816.0,0.01443299,0.839175258,0.842268041,1528,1221946,1221946,cinder,4ea6dfd0b1140e437703128bf52d65dbd2751751,1,0,"In the beginning the code only need two args but then it evolved and now it need three args so they change the method. “NMS call nms.folder.create doesn't support third argument, that is why this call changed to nms.folder.create_with_opts.”","Bug #1221946 in Cinder: ""Bug in Nexenta NFS volume driver""","Exception in during volume creation in Nexenta NFS volume driver
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 215, in create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     flow.run(context.elevated())
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/decorators.py"", line 105, in wrapper
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return f(self, *args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 232, in run
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     run_it(r)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 212, in run_it
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.rollback(context, cause)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     result = runner(context, *args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/taskflow/utils.py"", line 260, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self.result = self.task(*args, **kwargs)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1441, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     **volume_spec)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1418, in _create_raw_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     return self.driver.create_volume(volume_ref)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nfs.py"", line 96, in create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     self._do_create_volume(volume)
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nexenta/nfs.py"", line 109, in _do_create_volume
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     {'compression': self.configuration.nexenta_volume_compression}
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/nexenta/jsonrpc.py"", line 82, in __call__
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp     raise NexentaJSONException(response['error'].get('message', ''))
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp NexentaJSONException: Fewer items found in D-Bus signature than in Python arguments
2013-09-03 14:50:58.291 TRACE cinder.openstack.common.rpc.amqp","Fix bug in Nexenta NFS driver _do_create_volume

Fix bug in _do_create_volume method of Nexenta NFS volume driver.
NMS call nms.folder.create doesn't support third argument, that is why
this call changed to nms.folder.create_with_opts.

Closes-Bug: #1221946
Change-Id: I712ee2919f1bd117e7a1691979639689d95adc92"
584,4ecc1c52dbe02d5c4211d3936e2eec57dcc22de3,1394849844,,1.0,0,1,1,1,1,0.0,True,2.0,281438.0,27.0,7.0,False,13.0,1625748.0,16.0,4.0,9.0,3605.0,3607.0,9.0,2728.0,2730.0,7.0,3186.0,3186.0,0.001050972,0.41868103,0.41868103,631,1057,1293298,nova,4ecc1c52dbe02d5c4211d3936e2eec57dcc22de3,0,0,remove import,"Bug #1293298 in OpenStack Compute (nova): ""Don't import guestfs  directly""","Library guestfs is not in requirements file, but is imported directly
in nova/virt/disk/vfs/guestfs.py. That conflicts with global variable
guestfs, leads out one more time import and shadow issue, so remove it
from import group.","Don't import library guestfs directly

Library guestfs is not in requirements file, but is imported directly
in nova/virt/disk/vfs/guestfs.py. That conflicts with global variable
guestfs, leads out one more time import and shadow issue, so remove it
from import group.

Closes-Bug: #1293298

Change-Id: I55530ed682ebf449673759ab6127bcd28cb92594"
585,4ed1c8473f50f7f11082ad1ee389f5339707e212,1385098631,,1.0,4,4,2,2,2,1.0,True,4.0,56248.0,16.0,6.0,False,18.0,2614747.0,29.0,2.0,213.0,498.0,550.0,211.0,433.0,485.0,213.0,480.0,532.0,0.270543616,0.608091024,0.673830594,1793,1253891,1253891,swift,4ed1c8473f50f7f11082ad1ee389f5339707e212,1,1,“UNCAUGHT EXCEPTION”,"Bug #1253891 in OpenStack Object Storage (swift): ""uncaught exception in container-sync from use of verbose option""","See: http://logs.openstack.org/10/56310/4/check/check-grenade-devstack-vm/29a7de6/logs/syslog.txt
UNCAUGHT EXCEPTION
Traceback (most recent call last):
  File ""/usr/local/bin/swift-container-sync"", line 10, in <module>
    execfile(__file__)
  File ""/opt/stack/new/swift/bin/swift-container-sync"", line 20, in <module>
    run_daemon('container-sync')
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 186, in run_daemon
    daemon.run(**options)
  File ""/opt/stack/new/swift/swift/common/daemon.py"", line 75, in run
    self.run_forever(**kwargs)
TypeError: run_forever() got an unexpected keyword argument 'verbose'
Kibana search showing others: http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVU5DQVVHSFQgRVhDRVBUSU9OXCIgYW5kIE5PVCBtZXNzYWdlOlwiL29wdC9zdGFjay9vbGQvc3dpZnRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiY3VzdG9tIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7ImZyb20iOiIyMDEzLTA5LTAxVDE3OjQ3OjEwKzAwOjAwIiwidG8iOiIyMDEzLTExLTIwVDE3OjQ3OjEwKzAwOjAwIiwidXNlcl9pbnRlcnZhbCI6IjAifSwic3RhbXAiOjEzODUwOTUyNTkzMDUsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=","Handle optional arguments for run_forever()

All the other daemons do this, and since the out deamon wrapper
scripts pass all the command line options through directly, seems
simple enough to handle them by ignoring.

This is also applied to run_once().

Change-Id: I1df83bdf78f0dc3d911019f67f78301967b5da72
Closes-Bug: #1253891
Signed-off-by: Peter Portante <peter.portante@redhat.com>"
586,4f0547d4978172e29eb328bceb404335da1b9e0a,1412007348,,1.0,29,1,2,2,1,0.353359335,False,,,,,True,,,,,,,,,,,,,,,,,1364,1834,1375379,nova,4f0547d4978172e29eb328bceb404335da1b9e0a,1,1,,"Bug #1375379 in OpenStack Compute (nova): ""console: wrong check when verify the server response""","When trying to connect to a console with internal_access_path if the server does not respond by 200 we should raise an exception but the current code does not insure this case.
https://github.com/openstack/nova/blob/master/nova/console/websocketproxy.py#L68
The method 'find' return -1 on failure not False or 0","console: fix bug when invalid connection info

Fixes bug when checking response returned from the server during
a connection with internal_access_path, if the response's status
is not 200 we should raise the exception.

Closes-Bug: #1375379
Change-Id: Icafcad52cb4e6d9ec4d270e93b89cf13df6b8f61"
587,4f8185549dfe11eb1ce405711593baa1528045ea,1397279951,,1.0,13,1,2,2,1,0.863120569,True,1.0,1189576.0,11.0,4.0,False,186.0,433045.0,954.0,1.0,293.0,1031.0,1295.0,254.0,1030.0,1255.0,205.0,1025.0,1201.0,0.026410256,0.131538462,0.154102564,784,1211,1306922,nova,4f8185549dfe11eb1ce405711593baa1528045ea,1,1,,"Bug #1306922 in OpenStack Compute (nova): ""binding:host_id isn't updated after unshelve instance""","After unshelve an instance, found the attached port's binding info didn't get updated.
os@cloudcontroller:~$ nova show vm2
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                    | AUTO                                                     |
| OS-EXT-AZ:availability_zone          | nova                                                     |
| OS-EXT-SRV-ATTR:host                 | xuhj-c1                                                  |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | xuhj-c1                                                  |
| OS-EXT-SRV-ATTR:instance_name        | instance-0000000d                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-STS:task_state                | -                                                        |
| OS-EXT-STS:vm_state                  | active                                                   |
| OS-SRV-USG:launched_at               | 2014-04-11T07:15:36.000000                               |
| OS-SRV-USG:terminated_at             | -                                                        |
| accessIPv4                           |                                                          |
| accessIPv6                           |                                                          |
| config_drive                         |                                                          |
| created                              | 2014-04-11T06:51:04Z                                     |
| flavor                               | m1.small (2)                                             |
| hostId                               | 493891ff9d0ff1ab3dddfa99ec9aba8fc30bfb2ccbc34e91f9f5c5b8 |
| id                                   | 6c952df5-8893-47c8-b19d-c3f393c7f688                     |
| image                                | ubuntu-core (0237b107-a492-44ea-817e-d94589943c9c)       |
| key_name                             | -                                                        |
| metadata                             | {}                                                       |
| name                                 | vm2                                                      |
| net1 network                         | 12.0.0.7                                                 |
| os-extended-volumes:volumes_attached | []                                                       |
| progress                             | 0                                                        |
| security_groups                      | default                                                  |
| status                               | ACTIVE                                                   |
| tenant_id                            | fac60a38271e4b3bbc4a7dc97aee4e90                         |
| updated                              | 2014-04-11T07:15:36Z                                     |
| user_id                              | d7f959000a764cccbc0e9c634eb82514                         |
+--------------------------------------+----------------------------------------------------------+
os@cloudcontroller:~$ neutron port-show c514acdd-4038-414f-be3a-d30f76b4cba6
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| binding:host_id       | xuhj-c2                                                                         |
| binding:profile       | {}                                                                              |
| binding:vif_details   | {""port_filter"": true}                                                           |
| binding:vif_type      | bridge                                                                          |
| binding:vnic_type     | normal                                                                          |
| device_id             | 6c952df5-8893-47c8-b19d-c3f393c7f688                                            |
| device_owner          | compute:nova                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""b4bf03ac-5243-404d-bd16-705b4b61643e"", ""ip_address"": ""12.0.0.7""} |
| id                    | c514acdd-4038-414f-be3a-d30f76b4cba6                                            |
| mac_address           | fa:16:3e:49:9b:1f                                                               |
| name                  |                                                                                 |
| network_id            | cea70fce-b954-40da-b325-b0bea5179ca0                                            |
| security_groups       | 83dac46b-fa06-41e6-a028-35f9b7bf4046                                            |
| status                | BUILD                                                                           |
| tenant_id             | fac60a38271e4b3bbc4a7dc97aee4e90                                                |
+-----------------------+---------------------------------------------------------------------------------+
| OS-EXT-SRV-ATTR:host                 | xuhj-c1                                                  |
-------------
| binding:host_id       | xuhj-c2                                                                         |","Update port binding when unshelve instance

When unshelve instance, it should update the attached port also.
This patch invoke network_api.migrate_instance_finish to update
the port's binding info.

Change-Id: Iaa77163dde5494154d67ef392142669d70c3e643
Closes-Bug: #1306922"
588,4f8ccd7b95c27180a1cfe689e3c6f46bde5f803b,1404170972,1.0,1.0,49,21,3,3,1,0.621609745,False,,,,,True,,,,,,,,,,,,,,,,,1003,1445,1333654,nova,4f8ccd7b95c27180a1cfe689e3c6f46bde5f803b,1,1,,"Bug #1333654 in OpenStack Compute (nova): ""Timeout waiting for vif plugging callback for instance""","The neutron full job is exhibiting a rather high number of cases where network-vif-plugged timeout are reported.
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIlRpbWVvdXQgd2FpdGluZyBmb3IgdmlmIHBsdWdnaW5nIGNhbGxiYWNrIGZvciBpbnN0YW5jZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiIxNzI4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDAzNjA5MTk0NDg4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
95.78% of this kind of messages appear for the neutron full job. However, only a fraction of those cause build failures, but that's because the way the tests are executed.
This error is currently being masked by another bug as tempest tries to get the console log of a VM in error state: https://bugs.launchpad.net/tempest/+bug/1332414
This bug will target both neutron and nova pending a better triage.
Fixing this is of paramount importance to get the full job running.
Note: This is different from https://bugs.launchpad.net/nova/+bug/1321872 and https://bugs.launchpad.net/nova/+bug/1329546","Do not process events for instances without host

In some cases Neutron might send events such as 'VIF unplugged'
for instances which are either being deleted or shelved. When
that happens there will be a failure in dispatching the event
to the appropriate compute node - as there is no host for the
instance.

As multiple neutron events can be stashed in a single call
it is important to avoid that this kind of errors will prevent
processing of other events in the same call.

This patch does not process events for instances without a host,
marking them as failed.

When the above condition occurs, the create event request will
return a 207 response code. For specific events, a 422
unprocessable entity code will be set.

This patch also preserve the characteristic that events are
returned in the response in the same order they were found in
the request.

Change-Id: I18062b81e50c722ec96b4296ac39384493683ae3
Closes-Bug: #1333654"
589,4fd4d490ac1e12b2bf835949447d21b64c641616,1385400954,,1.0,10,3,2,2,1,0.995727452,True,5.0,164855.0,49.0,27.0,False,5.0,3480764.0,5.0,5.0,115.0,711.0,800.0,81.0,668.0,724.0,29.0,333.0,345.0,0.052816901,0.588028169,0.60915493,1805,1254530,1254530,neutron,4fd4d490ac1e12b2bf835949447d21b64c641616,1,1, ,"Bug #1254530 in neutron: ""Logging prints traces for q-svc and dhcp-agt""","q-svc and q-dhcp services are printing the following stack traces in their screen tabs:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 850, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 566, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 530, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: u'project_name'
As a workaround, colorized logging config could be fixed to remove project_name from the list of fields in neutron.conf and that solves the issue. However new devstack setup brings the problem back.","Fix format errors seen in rpc logging

The previous commit for this bug didn't include the 'project_name'
key in the context dict.  The missing key was causing the amqp
module to generate log formatting exceptions instead of normal
log output.

Separately, the context module itself was generating logging
exceptions in the quantum service when logging was attempted
before the context was fully initialized

Change-Id: I0f4c6f5a6804442932c9b2bd409a258cfc2419ff
Closes-Bug: #1254530
Related-Bug: #1239923"
590,4fe60f6192abdf154cbf7f65021a47d7e339aa76,1378398447,,1.0,4,8,2,1,1,1.0,True,2.0,19984.0,20.0,6.0,False,3.0,12865265.0,3.0,2.0,15.0,953.0,957.0,14.0,875.0,879.0,8.0,816.0,816.0,0.009384776,0.851929093,0.851929093,1505,1220436,1220436,cinder,4fe60f6192abdf154cbf7f65021a47d7e339aa76,1,1, ,"Bug #1220436 in Cinder: ""test_cinder_quota_class_show failes during gate jobs""","http://logs.openstack.org/42/44542/1/gate/gate-tempest-devstack-vm-postgres-full/2d73c40/console.html
2013-09-03 19:29:16.317 | ======================================================================
2013-09-03 19:29:16.318 | FAIL: tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | tempest.cli.simple_read_only.test_cinder.SimpleReadOnlyCinderClientTest.test_cinder_quota_class_show
2013-09-03 19:29:16.318 | ----------------------------------------------------------------------
2013-09-03 19:29:16.319 | _StringException: Empty attachments:
2013-09-03 19:29:16.319 |   stderr
2013-09-03 19:29:16.319 |   stdout
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.320 | pythonlogging:'': {{{2013-09-03 19:23:46,092 running: '/usr/local/bin/cinder --os-username admin --os-tenant-name admin --os-password secret --os-auth-url http://127.0.0.1:5000/v2.0/   quota-class-show abc'}}}
2013-09-03 19:29:16.320 |
2013-09-03 19:29:16.321 | Traceback (most recent call last):
2013-09-03 19:29:16.321 |   File ""tempest/cli/simple_read_only/test_cinder.py"", line 56, in test_cinder_quota_class_show
2013-09-03 19:29:16.321 |     params='abc'))
2013-09-03 19:29:16.322 |   File ""tempest/cli/__init__.py"", line 86, in cinder
2013-09-03 19:29:16.322 |     'cinder', action, flags, params, admin, fail_ok)
2013-09-03 19:29:16.323 |   File ""tempest/cli/__init__.py"", line 102, in cmd_with_auth
2013-09-03 19:29:16.323 |     return self.cmd(cmd, action, flags, params, fail_ok)
2013-09-03 19:29:16.323 |   File ""tempest/cli/__init__.py"", line 123, in cmd
2013-09-03 19:29:16.324 |     result)
2013-09-03 19:29:16.324 | CommandFailed: Command '['/usr/local/bin/cinder', '--os-username', 'admin', '--os-tenant-name', 'admin', '--os-password', 'secret', '--os-auth-url', 'http://127.0.0.1:5000/v2.0/', 'quota-class-show', 'abc']' returned non-zero exit status 1","Remove quota fetch race condition

When displaying quotas, we shouldn't pull the latest resources
just to convert the results to the dict.

Closes-Bug: #1220436

Change-Id: Id01a310481353b272e103643d053957b65cd4ce3"
591,4ffdf53550bd823f18c7a45c094adee18ed320d8,1380547329,1.0,1.0,27,1,2,2,1,0.811278124,True,2.0,113620.0,27.0,5.0,False,17.0,984220.5,36.0,3.0,1250.0,2044.0,3097.0,1133.0,1815.0,2753.0,439.0,1942.0,2193.0,0.071509833,0.31578092,0.356574029,1618,1233161,1233161,nova,4ffdf53550bd823f18c7a45c094adee18ed320d8,1,1, ,"Bug #1233161 in OpenStack Compute (nova): ""VMWare: Exception with periodic task - invalid data access""","2013-09-30 05:40:58.352 ERROR nova.openstack.common.periodic_task [req-126d36dc-15d2-4525-a2cd-9f4015987140 None None] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 4861, in update_available_resource
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     nodenames = set(self.driver.get_available_nodes())
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 599, in get_available_nodes
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     CONF.vmware.cluster_name)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 1008, in get_all_cluster_refs_by_name
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     cls_mor = find_entity_mor(cls, entity_path)
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/vm_util.py"", line 992, in find_entity_mor
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task     return [mor for mor in entity_list if mor.propSet[0].val == entity_name]
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-09-30 05:40:58.352 TRACE nova.openstack.common.periodic_task","VMware: fix bug for invalid data access

Ensure that propSet exists prior to reading the propSet value.

Closes-Bug: #1233161

Change-Id: I667d6fb7618b09f6e43f3b3e7f69d5db914324fc"
592,501213686886baccd3280e10b8856a25d3517519,1393866513,,1.0,43,3,2,2,1,0.666578358,True,24.0,12454203.0,364.0,112.0,False,21.0,691888.0,23.0,8.0,57.0,2407.0,2428.0,48.0,2060.0,2073.0,34.0,576.0,579.0,0.03945885,0.650507328,0.653889515,223,626,1263866,neutron,501213686886baccd3280e10b8856a25d3517519,1,1,I see a race condition. Bug,"Bug #1263866 in neutron: ""OVS lib deferred apply cannot handle concurrency""","OVS lib propose a deferred apply methods to save system calls to 'ovs-ofctl' binaries ('ovs-ofctl' can apply flow from file or stdin if file is '-').
This method use a dict for 'add', mod' or 'del' flow actions that contain a concatenated string flows. This dict is purge after all flows are applied at the end of 'deferred_apply_off' method.
If another call is made on that dict during the 'deferred_apply_off', some flows could be deleted a the end when they have not been applied.
I can see that on ML2 plugin with l2-pop mechanism driver. If I delete more than one port at a time, some flooding flow rules could be not deleted on the br-tun bridge.","OVS lib defer apply doesn't handle concurrency

The OVS lib deferred apply methods use a dict to save flows to add,
modify or delete when deffered apply is switched off.
If another thread adds, modifies or deletes flows on that dict during
another process called deffered_apply_off, its flows could be ignored.

This fix stash reference flows list and point the flows list to a new
cleared flows list. Then, it applies flows from the stashed flows list.

Closes-bug: #1263866
Change-Id: Ia3c6ce181e1599d1474da7eb944feff7d84f1d73"
593,501ceec8b96ba70df4bce0f5aff33a2bb1aabbac,1395652380,1.0,1.0,27,4,2,2,1,0.708835673,True,3.0,4106582.0,63.0,23.0,False,209.0,223320.0,1164.0,7.0,60.0,4224.0,4233.0,59.0,3261.0,3270.0,55.0,2989.0,2995.0,0.007302125,0.38988134,0.390663711,674,1100,1296590,nova,501ceec8b96ba70df4bce0f5aff33a2bb1aabbac,1,1,I think is a bug. They left a snapshot in a state that shouldnt be,"Bug #1296590 in OpenStack Compute (nova): ""[libvirt] snapshots in progress are not cleaned when deleting an instance""","When creating an instance snapshot, if such instance is deleted while in the middle of the process, the snapshot may be left in the SAVING state because the instance disappears in the middle of the process or moves to the deleting task_state.
Steps to reproduce:
$ nova boot --image <image_id> --flavor <flavor> test
$ nova image-create test test-snap
$ nova delete test
The image 'test-snap' will be left in the SAVING state although it should be deleted when we detect the situation.","Delete in-process snapshot when deleting instance

Try to delete the image created to hold the snapshot when the snapshot
creation fails because either the instance doesn't exist or it is in the
deleting task_state.

Closes-Bug: #1296590
Change-Id: I422c4f10c348d189fee217b55e0766e73eb5b16c"
594,502fa4875a3975990cbdf84fc0f846f7ede8fa92,1405133485,1.0,1.0,40,5,2,2,1,0.867281622,False,,,,,True,,,,,,,,,,,,,,,,,898,1334,1321239,nova,502fa4875a3975990cbdf84fc0f846f7ede8fa92,1,1,“The <instanceSet> information missing in the response elements.”,"Bug #1321239 in OpenStack Compute (nova): ""[EC2] StopInstance response missing instanceset info""","Stoptinstance response elements shown as below:
Sample Request to stop the specified instance:
===
https://ec2.amazonaws.com/?Action=StopInstances
&InstanceId.1=i-10a64379
&AUTHPARAMS
==
Response elements are:
==
"":<StopInstancesResponse xmlns=""""http://ec2.amazonaws.com/doc/2013-10-15/"""">
  <requestId>req-30edb813-5802-4fa2-8a83-9dbcb751264e</requestId>
  <return>true</return>
</StopInstancesResponse>
""
But as per the AWS API reference doc, the response elements shown be as below:
==
<StopInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>59dbff89-35bd-4eac-99ed-be587EXAMPLE</requestId>
  <instancesSet>
    <item>
      <instanceId>i-10a64379</instanceId>
      <currentState>
          <code>64</code>
          <name>stopping</name>
      </currentState>
      <previousState>
          <code>16</code>
          <name>running</name>
      </previousState>
  </instancesSet>
</StopInstancesResponse>
===
The <instanceSet> information missing in the response elements.","Add instanceset info to StopInstance response

Currently stopinstance response missing the instanceset
information with InstanceID, current state and previous
state details. It just returns the ""True"".

As per the AWS EC2 API reference document, the StopInstance
response elements should include the instanceset information
as below:
<StopInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2014-02-01/"">
  <requestId>req-a7326465-5ce2-4ed6-ab89-394b38cca85f</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>80</code>
        <name>stopped</name>
      </currentState>
      <previousState>
        <code>16</code>
        <name>running</name>
      </previousState>
    </item>
  </instancesSet>
</StopInstancesResponse>

Included the instanceset into stopinstance response elements
and updated the test cases for stopinstance response elements
in nova/tests/api/ec2/test_cloud.py file.

Closes-bug: #1321239

Change-Id: I4d8a6faf2689a7df71920183682fd1e403ce2a42"
595,504f496d6b4fd6234c3e65ba636ee12b004fae1f,1386746926,,1.0,7,10,2,2,1,0.522559375,True,1.0,7883713.0,35.0,11.0,False,2.0,1091495.0,3.0,5.0,48.0,1499.0,1508.0,48.0,1338.0,1347.0,41.0,547.0,554.0,0.067524116,0.881028939,0.892282958,56,384,1248002,neutron,504f496d6b4fd6234c3e65ba636ee12b004fae1f,0,0,Bug in tests,"Bug #1248002 in neutron: ""bandwidth metering - excluded option doesn't work""","When I using 'meter-label-rule-create' command with neutronclient, excluded option is doesn't work.
The option exclude this cidr from the label.
You have to fix below code:
neutron / neutron / services / metering / drivers / iptables / iptables_driver.py
(line number is 157)
def _process_metering_label_rules(self, rm, rules, label_chain,
                                      rules_chain):
        im = rm.iptables_manager
        ext_dev = self.get_external_device_name(rm.router['gw_port_id'])
        if not ext_dev:
            return
        for rule in rules:
            remote_ip = rule['remote_ip_prefix']
            dir = '-i ' + ext_dev
            if rule['direction'] == 'egress':
                dir = '-o ' + ext_dev
            if rule['excluded'] == 'true':   ------> fix it : True (boolean type)
                ipt_rule = dir + ' -d ' + remote_ip + ' -j RETURN'
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule, wrap=False,
                                           top=True)
            else:
                ipt_rule = dir + ' -d ' + remote_ip + ' -j ' + label_chain
                im.ipv4['filter'].add_rule(rules_chain, ipt_rule,
                                           wrap=False, top=False)","fix --excluded of meter-label-rule-create is not working

rule['excluded'] is boolean type, should not be compared with 'true'

Closes-Bug: #1248002
Change-Id: Ie78c307ede1f7124c58354825a411474ff6c1234"
596,50636a881b60f112027494bcd84af66888db8c1c,1400982635,,1.0,131,8,7,2,1,0.882490802,False,,,,,True,,,,,,,,,,,,,,,,,916,1352,1322926,nova,50636a881b60f112027494bcd84af66888db8c1c,1,1,,"Bug #1322926 in OpenStack Compute (nova): ""Hyper-V driver volumes are attached incorrectly when multiple iSCSI servers are present""",Hyper-V can change the order of the mounted drives when rebooting a host and thus passthrough disks can be assigned to the wrong instance resulting in a critical scenario.,"Fixes Hyper-V volume mapping issue on reboot

Hyper-V does not manage iSCSI mounted passthrough disks
correctly on reboot in some circumstances where disk
number assignment can change, resulting in instances potentially
booting with the wrong volume attached.

Co-Authored-By: Adelina Tuvenie <atuvenie@cloudbasesolutions.com>
Co-Authored-By: Adrian Bora <abora@cloudbasesolutions.com>

Change-Id: I99fa75aaa950de2ada8a03bd23c910c3ff810d69
Closes-Bug: #1322926"
597,5065aeca1b4acad513c07e3832ec0e12de2e6568,1412203582,,1.0,27,12,2,2,1,0.172036949,False,,,,,True,,,,,,,,,,,,,,,,,1374,1844,1376492,nova,5065aeca1b4acad513c07e3832ec0e12de2e6568,1,1,“Patch I7598afbf0dc3c527471af34224003d28e64daaff introduces a tempest failure with Minesweeper”,"Bug #1376492 in OpenStack Compute (nova): ""Minesweeper failure: tempest.api.compute.servers.test_server_actions.ServerActionsTestJSON.test_resize_server_revert""","Patch I7598afbf0dc3c527471af34224003d28e64daaff introduces a tempest failure with Minesweeper due to the fact that the destroy operation can be triggered by both the user and the revert resize operation. In case of a revert resize operation, we do not want to delete the original VM.","Destroy orig VM during resize if triggered by user

Patch I7598afbf0dc3c527471af34224003d28e64daaff introduces a
Minesweeper failure, due to the fact that it doesn't distinguish
between destroy operation triggered by the user and by the revert
resize.

This patch fixes the issue by checking the task state. If the task
state is revert_resize, the original VM doesn't get deleted.

Closes-Bug: #1376492

Change-Id: Idb9ac6c1ec5dcea52ce8e028f5cce08da1779321
(cherry picked from commit e464bc518e8590d59c2741948466777982ca3319)"
598,50c65d16bee8e6e46840f232519e92d9ba9989b4,1409682471,,1.0,47,8,5,4,1,0.884112632,False,,,,,True,,,,,,,,,,,,,,,,,1258,1718,1364696,neutron,50c65d16bee8e6e46840f232519e92d9ba9989b4,0,0,Feature request,"Bug #1364696 in neutron: ""Big Switch: Request context is missing from backend requests""",The request context that comes into Neutron is not included in the request to the backend. This makes it difficult to correlate events in the debug logs on the backend such as what incoming Neutron request resulted in particular REST calls to the backend and if admin privileges were used.,"BSN: Add context to backend request for debugging

Include the request context with calls to the backend
Big Switch controllers to assist with event correlation
and debugging object provenance. The auth token is
stripped since this information is sensitive and
these requests will appear in debug logs.

This also removes mutable objects from default arguments
in some of the server manager function definitions that
were interferring with the new use of the headers dict.

Closes-Bug: #1364696
Change-Id: I5b80b1596cc145742457b3603cbcd67f6e0d9f36"
599,50da460869661ba63971a17c9c1b514bf22874a2,1393255291,,1.0,27,11,4,2,1,0.910557832,True,4.0,542764.0,56.0,11.0,False,14.0,1082781.25,16.0,3.0,0.0,1225.0,1225.0,0.0,1034.0,1034.0,0.0,650.0,650.0,0.001168224,0.760514019,0.760514019,23,335,1243878,neutron,50da460869661ba63971a17c9c1b514bf22874a2,1,1,,"Bug #1243878 in neutron: ""metadata service returns Content-Type: text/html""","When making an http request against the OpenStack metadata service, it returns ""Content-Type: text/html"", despite the returned content being in json format.
For example (using httpie):
$ http get http://169.254.169.254/openstack/latest/meta_data.json
HTTP/1.1 200 OK
Content-Length: 1283
Content-Type: text/html; charset=UTF-8
Date: Wed, 23 Oct 2013 18:55:17 GMT
{""random_seed"": ... }
The returned content type should really be something like:
Content-Type: application/json; charset=UTF-8
Version: grizzly","Include proper Content-Type in the HTTP response headers

Neutron namespace proxy handler and metadata agent were not setting the
Content-Type in its response. Both of them were returning only the response
data which is obtained from the nova-metadata-service. Since they were returning
only the response data, the Content-Type returned to the clients has the default
one which is - ""text/html"". Ideally this should be set to the data type which is
present in the HTTP Response. The fix now includes the Content-Type which is
returned by nova-metadata-service

Closes-Bug: #1243878
Change-Id: If68f0b508fbea4ecd1eb0e58d602b5ba6ccbe263"
600,50edccfad29d3c602eba628fc80e46a0a67de090,1393357026,,1.0,1,1,1,1,1,0.0,True,1.0,512522.0,34.0,7.0,False,1.0,976286.0,4.0,5.0,35.0,981.0,984.0,35.0,798.0,801.0,27.0,457.0,460.0,0.032520325,0.531939605,0.535423926,477,894,1284881,neutron,50edccfad29d3c602eba628fc80e46a0a67de090,0,0,Delete unnecessary code,"Bug #1284881 in neutron: ""BigSwitch plugin unnecessarily uses external locks""","The BigSwitch servermanager uses the synchronized decorators on rest backend calls. It currently sets the external flag to True, which isn't necessary since there aren't multiple independent processes running the plugin on the same host.
This results in the unnecessary creation of a lock file when it can just be handled by the default in-memory locks.","BigSwitch: Stop using external locks

Changes BigSwitch server manager REST lock
to an in-memory lock since the plugin isn't
expected to run in independent processes.

Closes-Bug: #1284881
Change-Id: I2fdd5ba79a0c4b94b2b410db54a63f7c0ca47525"
601,5119d323650f97f12e12fdb6a2a80e46739629fc,1395337316,,1.0,8,1,2,2,1,0.503258335,True,3.0,388887.0,18.0,7.0,False,1.0,3307220.0,1.0,1.0,0.0,1203.0,1203.0,0.0,950.0,950.0,0.0,1058.0,1058.0,0.000649351,0.687662338,0.687662338,544,965,1288816,cinder,5119d323650f97f12e12fdb6a2a80e46739629fc,1,0,Adds xiv_chap to xiv/ds8k driver configuration,"Bug #1288816 in Cinder: ""xiv_ds8k driver fails chap support due to missing parameter""","CHAP encryption is supported in the xiv_ds8k cinder driver, but the value specified in the cinder conf file is not passed and the feature does not work.
The fix is minimal (patch attached).","Adds xiv_chap to xiv/ds8k driver configuration

In order to support CHAP, xiv_chap was added to
the driver configuration. Valid values are disabled
and enabled (default: disabled)

Change-Id: I0c68f581ad78a87a506b00ab308806f4a8fa78a8
DocImpact
Closes-Bug: #1288816"
602,5120c4f7c2670eaa71898fe6941029bbb0081949,1403212518,1.0,1.0,35,3,2,2,1,0.74248757,False,,,,,True,,,,,,,,,,,,,,,,,990,1430,1332198,nova,5120c4f7c2670eaa71898fe6941029bbb0081949,1,1,,"Bug #1332198 in OpenStack Compute (nova): ""Volumes are not detached when a build fails""","When a build fails in the driver spawn method attached volumes are not detached.  If the instance goes to ERROR and is later deleted everything gets cleaned up appropriately.  If the instance is rescheduled then the next compute will fail with:
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Traceback (most recent call last):
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 1786, in _prep_block_device
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     self.driver, self._await_block_device_map_created) +
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 368, in attach_block_devices
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     map(_log_and_attach, block_device_mapping)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 366, in _log_and_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     bdm.attach(*attach_args, **attach_kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 45, in wrapped
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     ret_val = method(obj, context, *args, **kwargs)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/virt/block_device.py"", line 218, in attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     volume_api.check_attach(context, volume, instance=instance)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]   File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 249, in check_attach
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]     raise exception.InvalidVolume(reason=msg)
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] InvalidVolume: Invalid volume: status must be 'available'
2014-06-18 20:09:01.954 11008 TRACE nova.compute.manager [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad]
2014-06-18 20:09:02.002 11008 ERROR nova.compute.manager [req-e76e85f6-0520-4372-b47d-a80744c912a7 None] [instance: be78cd0e-c67f-439c-bf30-885fb135d9ad] Failure prepping block device
which stops the build and properly stops a reschedule.
Cinder volumes need to be detached on a build failure.","Instance and volume cleanup when a build fails

On failed builds the _shutdown_instance method used to get called which
would clean up leftover instance artifacts, volume attachments, and
networking.  This no longer happens which is causing volumes to be left
in an attached state when they're not attached to anything.

Network deallocation is already handled in this code path so it should
not happen in _shutdown_instance.

Change-Id: I899b64ac5941acc282ccae7e1963d6f714c01e8b
Closes-bug: #1332198"
603,513c6bbd36563e57a85d33f9c94f4a20ab7c00f4,1400660693,1.0,1.0,171,7,9,2,1,0.883662617,False,,,,,True,,,,,,,,,,,,,,,,,903,1339,1321640,nova,513c6bbd36563e57a85d33f9c94f4a20ab7c00f4,1,1,,"Bug #1321640 in OpenStack Compute (nova): ""[HyperV]: Config drive is not attached to instance after resized or migrated""","If we use config-drive (whether set --config-drive=true in boot command or set force_config_drive=always in nova.conf), there is bug for config-drive when resize or migrate instances on hyperv.
You can see from current nova codes:
https://github.com/openstack/nova/blob/master/nova/virt/hyperv/migrationops.py#L269
when finished migration, there is no code to attach configdrive.iso or configdrive.vhd to the resized instance. compared to boot instance (https://github.com/openstack/nova/blob/master/nova/virt/hyperv/vmops.py#L226). Although this commit https://review.openstack.org/#/c/55975/ handled coping configdrive to resized or migrated instance, there is no code to attach it after resized or migrated.","Fix attaching config drive issue on Hyper-V when migrate instances

After instance resized or migrated on Hyper-V hypervisor.
The configdrive iso or vhd is copied to resized or migrated
instance, but is not attached to instance.

Because there are configurations for config drive like
config_drive_cdrom, config_drive_format, and the configurations
on different Hyper-V compute node may be different. it will need
to convert configdrive format after resized or migrated.
It is easy to convert from iso9660 or vfat to vhd, but it seems
impossible to convert from vhd to iso9660 or vfat.
So this commit just ignore the target Hyper-V compute node's
config drive configurations, leave the original config drive format.

Change-Id: I349e3b2221fff0ae217a71a91895afd21ff7d18d
Closes-Bug: #1321640"
604,5148c9648fc959c1d807313176afe3fcf84b89cf,1404197261,,1.0,56,42,14,3,1,0.899185784,False,,,,,True,,,,,,,,,,,,,,,,,1853,1336168,1336168,Glance,5148c9648fc959c1d807313176afe3fcf84b89cf,1,1, ,"Bug #1336168 in Glance: ""Chunksize should be configurable when using copy_from""","When using the copy_from option, readers and writers can have different
speeds to respectively read and write.
A reader timeout will happen if the writer is slow and the writer is
being asked to write a lot. This is currently happening when using
the VMware store and copying from an HTTP server. The reader is reading
16MB which takes too long to upload to vCenter which is causing a
timeout from the HTTP server. The writer should be able to control the
size of the chunks being read when using copy_from: this way the writer
will write fast enough to not make the reader timeout.
This can be reproduced with the filesystem store and copying from http by adding sleep of 30/40 seconds in
https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L433 between each chunk read.
The chunk will become smaller and then zero.
This is simulating the effect of a slow writer.","Decouple read chunk size from write chunk size

When using the copy_from option, readers and writers can have different
speeds to respectively read and write.
A reader timeout will happen if the writer is slow and the writer is
being asked to write a lot. This is currently happening when using
the VMware store and copying from an HTTP server. The reader is reading
16MB which takes too long to upload to vCenter which is causing a
timeout from the HTTP server. The writer should be able to control the
size of the chunks being read when using copy_from: this way the writer
will write fast enough to not make the reader timeout.

This patch addresses the issue by introducing the notion of read chunk
size and write chunk size. Each store can have its own value for read
and write. The write chunk size of the destination store will be used
as the read chunk size of the source store in case of an image-create
where the copy_from option is specified.

Closes-Bug: #1336168

Signed-off-by: Arnaud Legendre <alegendre@vmware.com>
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>

Change-Id: I4e0c563b8f3a5ced8f65fcca83d341a97729a5d4"
605,5161d6c0023151d39fb56a85f739063205e676f4,1387208270,,1.0,0,5,1,1,1,0.0,True,1.0,612488.0,8.0,2.0,False,4.0,31908339.0,9.0,2.0,1.0,2594.0,2595.0,1.0,2154.0,2155.0,1.0,2447.0,2448.0,0.000291036,0.356228172,0.35637369,1414,1097999,1097999,nova,5161d6c0023151d39fb56a85f739063205e676f4,1,1, “iptables setup multiple times”,"Bug #1097999 in OpenStack Compute (nova): ""multi-process metadata server runs iptables  setup multiple times""","Using Devstack (latest master 1/9/2012)
If I enable multi process metadata service (metadata_workers=5 in nova.conf)
the iptables are modified multiple times:
2013-01-10 00:25:16.0 25535 INFO nova.wsgi [-] metadata listening on 0.0.0.0:8775
2013-01-10 00:25:16.1 25535 INFO nova.service [-] Starting 5 workers
2013-01-10 00:25:16.3 25535 INFO nova.service [-] Started child 255442013-01-10 00:25:16.6 25535 INFO nova.service [-] Started child 255452013-01-10 00:25:16.8 25535 INFO nova.service [-] Started child 25546
2013-01-10 00:25:16.9 25544 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.10 25544 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.11 25544 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:16.11 25535 INFO nova.service [-] Started child 25547
2013-01-10 00:25:16.18 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.20 25535 INFO nova.service [-] Started child 25548
2013-01-10 00:25:16.16 25546 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.22 25546 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.27 25548 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.17 25545 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.33 25545 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.32 25547 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:185
2013-01-10 00:25:16.33 25547 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.42 25548 DEBUG nova.openstack.common.lockutils [-] Attempting to grab file lock ""iptables"" for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:189
2013-01-10 00:25:16.214 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.216 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.368 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.370 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.535 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.536 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.703 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.704 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:16.855 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:16.856 25544 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.18 25544 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.19 25544 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:17.20 25544 INFO nova.metadata.wsgi.server [-] (25544) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:17.25 25548 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:17.26 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.180 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.182 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.340 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.342 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.510 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.511 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.666 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.667 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.800 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.801 25548 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:17.952 25548 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:17.953 25548 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:17.955 25548 INFO nova.metadata.wsgi.server [-] (25548) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:17.962 25546 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:17.964 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.120 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.121 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.278 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.279 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.439 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.441 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.596 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.598 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.750 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.751 25546 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:18.932 25546 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:18.932 25546 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:18.934 25546 INFO nova.metadata.wsgi.server [-] (25546) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:18.941 25545 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:18.942 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.94 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.95 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.210 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.210 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.328 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.330 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.470 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.471 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.617 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.619 25545 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.735 25545 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.736 25545 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:19.737 25547 DEBUG nova.openstack.common.lockutils [-] Got file lock ""iptables"" at /opt/stack/data/nova/nova-iptables for method ""_apply""... inner /opt/stack/nova/nova/openstack/common/lockutils.py:219
2013-01-10 00:25:19.738 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t filter execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.739 25545 INFO nova.metadata.wsgi.server [-] (25545) wsgi starting up on http://0.0.0.0:8775/
2013-01-10 00:25:19.854 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.855 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:19.971 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:19.972 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t mangle execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.88 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.89 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.205 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.206 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-save -c -t nat execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.327 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.328 25547 DEBUG nova.utils [-] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf iptables-restore -c execute /opt/stack/nova/nova/utils.py:202
2013-01-10 00:25:20.447 25547 DEBUG nova.utils [-] Result was 0 execute /opt/stack/nova/nova/utils.py:226
2013-01-10 00:25:20.447 25547 DEBUG nova.network.linux_net [-] IPTablesManager.apply completed with success _apply /opt/stack/nova/nova/network/linux_net.py:385
2013-01-10 00:25:20.449 25547 INFO nova.metadata.wsgi.server [-] (25547) wsgi starting up on http://0.0.0.0:8775/","Initialize iptables rules on initialization of MetadataManager

To avoid multiple initialization of iptables rules if there are a few
workers for metadata service, perform iptables configuration in
__init__() of MetadataManager.

Change-Id: I674c04f973318f06cbb98693f0a884c824af8748
Closes-Bug: #1097999"
606,51e5f52e4cb60e266ccde71f205c91eb8c97b48b,1382367148,1.0,1.0,20,25,2,2,1,0.836640742,True,4.0,13611778.0,45.0,32.0,False,17.0,3920380.5,20.0,9.0,167.0,1858.0,1858.0,167.0,1658.0,1658.0,153.0,1352.0,1352.0,0.024088847,0.211637729,0.211637729,1705,1241275,1241275,nova,51e5f52e4cb60e266ccde71f205c91eb8c97b48b,1,1, ,"Bug #1241275 in OpenStack Compute (nova): ""Nova / Neutron Client failing upon re-authentication after token expiration""","By default, the token length for clients is 24 hours.  When that token expires (or is invalidated for any reason), nova should obtain a new token.
Currently, when the token expires, it leads to the following fault:
    File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 136, in _get_available_networks
      nets = neutron.list_networks(**search_opts).get('networks', [])
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
      ret = self.function(instance, *args, **kwargs)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 325, in list_networks
      **_params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1197, in list
      for r in self._pagination(collection, path, **params):
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1210, in _pagination
      res = self.get(path, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1183, in get
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1168, in retry_request
      headers=headers, params=params)
    File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1103, in do_request
      resp, replybody = self.httpclient.do_request(action, method, body=body)
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 188, in do_request
      self.authenticate()
    File ""/usr/lib/python2.6/site-packages/neutronclient/client.py"", line 224, in authenticate
      token_url = self.auth_url + ""/tokens""
    TRACE nova.openstack.common.rpc.amqp TypeError: unsupported operand type(s) for +: 'NoneType' and 'str'
This error is occurring because nova/network/neutronv2/__init__.py obtains a token for communication with neutron.  Nova is then authenticating the token (nova/network/neutronv2/__init__.py - _get_auth_token).  Upon authentication, it passes in the token into the neutron client (via the _get_client method).  It should be noted that the token is the main element passed into the neutron client (auth_url, username, password, etc... are not passed in as part of the request)
Since nova is passing the token directly into the neutron client, nova does not validate whether or not the token is authenticated.
After the 24 hour period of time, the token naturally expires.  Therefore, when the neutron client goes to make a request, it catches an exceptions.Unauthorized block.  Upon catching this exception, the neutron client attempts to re-authenticate and then make the request again.
The issue arises in the re-authentication of the token.  The neutron client's authenticate method requires that the following parameters are sent in from its users:
 - username
 - password
 - tenant_id or tenant_name
 - auth_url
 - auth_strategy
Since the nova client is not passing these parameters in, the neutron client is failing with the exception above.
Not all methods from the nova client are exposed to this.  Invocations to nova/network/neutronv2/__init__.py - get_client with an 'admin' value set to True will always get a new token.  However, the clients that invoke the get_client method without specifying the admin flag, or by explicitly setting it to False will be affected by this.  Note that the admin flag IS NOT determined based off the context's admin attribute.
Methods from nova/network/neutronv2/api.py that are currently affected appear to be:
 - _get_available_networks
 - allocate_for_instance
 - deallocate_for_instance
 - deallocate_port_for_instance
 - list_ports
 - show_port
 - add_fixed_ip_to_instance
 - remove_fixed_ip_from_instance
 - validate_networks
 - _get_instance_uuids_by_ip
 - associate_floating_ip
 - get_all
 - get
 - get_floating_ip
 - get_floating_ip_pools
 - get_floating_ip_by_address
 - get_floating_ips_by_project
 - get_instance_id_by_floating_address
 - allocate_floating_ip
 - release_floating_ip
 - disassociate_floating_ip
 - _get_subnets_from_port","Pass thru credentials to allow re-authentication

Nova's network client obtains a token from keystone for authenitcation
to the neutron services.  When the timeout of keystone is set low, it
creates a condition where some of the invocations to neutron start
failing.

This is because the neutron client detects that the token is expired,
but during the re-authorization attempt fails due to not having enough
information to properly reauthorize.  Nova also assumes that within a
given context, the token will always be valid.  It does not perform the
authorization check (and doing so would be double verification as the
neutron client is already validating this).

Since the neutron client has logic in place to re-authorize the token,
this change set will pass thru the credentials to the neutron client
for the re-authorization attempts.  This change also updates the unit
tests.

Note: the change only will pass in the admin credential information
if the context indicates that it is an admin.

Change-Id: I2858562b180f3e058a2da9d67bef02af80927177
Closes-Bug: #1241275"
607,51ef6a1dc30576f31a8a120066d81feedd0bf224,1386140399,,1.0,3,3,1,1,1,0.0,True,2.0,3434114.0,30.0,18.0,False,1.0,1452270.0,1.0,8.0,28.0,1299.0,1318.0,28.0,1168.0,1187.0,10.0,479.0,480.0,0.018242123,0.7960199,0.797678275,138,539,1257607,neutron,51ef6a1dc30576f31a8a120066d81feedd0bf224,1,1,Typo in code,"Bug #1257607 in neutron: ""Mistake in usage drop_constraint parameters""","In miration e197124d4b9_add_unique_constrain mistake in usage drop_constraint parameter type_ and positional agruments name
and table_name.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/e197124d4b9_add_unique_constrain.py"", line 64, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)
The same mistake was already fixed in miration 63afba73813_ovs_tunnelendpoints_id_unique.","Fix mistake in usage drop_constraint parameters

In migration e197124d4b9_add_unique_constrain mistake in usage
drop_constraint parameter type_ and positional arguments name
and table_name.

The same mistake was already fixed in migration
63afba73813_ovs_tunnelendpoints_id_unique.

Change-Id: I6a250e55ea53048bb11afd71fecf94da6f0c7421
Closes-bug: #1257607"
608,51fa32f53d81864e9267fc40ce3fcfe494b62437,1404492505,0.0,,24,1,2,2,2,0.529360865,False,,,,,True,,,,,,,,,,,,,,,,,1873,1367514,1367514,Swift,51fa32f53d81864e9267fc40ce3fcfe494b62437,1,1,When we broke out stats per-policy for some reason we only did two of the three:,"Bug #1367514 in OpenStack Object Storage (swift): ""Accounts don't report per-policy container count""","Accounts have historically reported three numbers:
                container_count INTEGER,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0,
When we broke out stats per-policy for some reason we only did two of the three:
            CREATE TABLE policy_stat (
                storage_policy_index INTEGER PRIMARY KEY,
                object_count INTEGER DEFAULT 0,
                bytes_used INTEGER DEFAULT 0
            );
We should track container_count in POLICY_STAT_TRIGGER_SCRIPT","Increase Account Auditor Test Code Coverage

Some simple additional test code to bring coverage from 82% to 99%

Change-Id: I561bc0a043f3451bb0a84cad274fedc907e61ee8"
609,51fd5edb106e26d65696ce37a70eef6a4f75b1e2,1381409984,,1.0,9,3,2,2,1,1.0,True,5.0,32645.0,20.0,10.0,False,8.0,2140273.0,18.0,5.0,983.0,1405.0,1405.0,865.0,1231.0,1231.0,844.0,1031.0,1031.0,0.780961183,0.953789279,0.953789279,1669,1237994,1237994,cinder,51fd5edb106e26d65696ce37a70eef6a4f75b1e2,1,0,“supports_thin_provisioning now uses a regexp to ensure parsing of lvm version succeeds when the build is customised;”,"Bug #1237994 in Cinder: ""with lvm_type=thin parsing of lvm version fails""","supports_thin_provisioning fails on customised builds; RHEL for instance reports version as:
2.02.100(2)-RHEL6
2013-10-10 14:58:56.928 32460 TRACE cinder.service   File ""/usr/lib/python2.6/site-packages/cinder/brick/local_dev/lvm.py"", line 142, in supports_thin_provisioning
2013-10-10 14:58:56.928 32460 TRACE cinder.service     version_tuple = tuple(map(int, version.split('.')))
2013-10-10 14:58:56.928 32460 TRACE cinder.service ValueError: invalid literal for int() with base 10: '100-RHEL6'","improves lvm version parsing for customised builds

supports_thin_provisioning now uses a regexp to ensure parsing of
lvm version succeeds when the build is customised; also adds a test
for a customised string parsing

Closes-Bug: #1237994
Change-Id: I49049a58bbdb5315b9d2d7c259a9324ca15d78cb"
610,520a5e2e5fa749cc3e505c6ae526171714cd6369,1380866452,,1.0,16,16,1,1,1,0.0,True,1.0,157436.0,16.0,5.0,False,9.0,790580.0,11.0,5.0,48.0,996.0,1001.0,48.0,925.0,930.0,42.0,363.0,365.0,0.102380952,0.866666667,0.871428571,1633,1235082,1235082,neutron,520a5e2e5fa749cc3e505c6ae526171714cd6369,0,0,Test in files,"Bug #1235082 in neutron: ""pythonic method name style not following pep8 in test_ovs_tunnel.py""","currently, most neutron codes name method name following pep8 recommendation while some code in test_ovs_tunnel.py not, this is found while review https://review.openstack.org/#/c/45725/
ref: pep8 recommendation: http://www.python.org/dev/peps/pep-0008/#method-names-and-instance-variables","Pythonic method name for test_ovs_tunnel.py

Change CamelCase to lowercase with words separated by underscores.

Change-Id: Ieab8f8ad9affdb9290d13002e6b4b0d5a0185034
Closes-Bug: #1235082"
611,521a03bb67724ab545430ea891eb6d6d90b992f5,1388723857,,1.0,0,6,3,1,1,1.0,True,2.0,1684632.0,14.0,6.0,False,25.0,13602072.0,33.0,3.0,50.0,2205.0,2254.0,50.0,1812.0,1861.0,5.0,2104.0,2108.0,0.000862193,0.302485989,0.303060785,254,658,1266344,nova,521a03bb67724ab545430ea891eb6d6d90b992f5,0,0,Remove duplicate code,"Bug #1266344 in OpenStack Compute (nova): ""duplicate __init__() in ExtensionResource""","Let's go in codes directly, to get resources from extension, the child class[0] has an __init__() to register itself, and the farther class [1]'s __init__() has already did this. If there are no some specified variables needed by child class, the child's __init__ could be removed. In <nova>/nova/api/openstack/compute/contrib/, nearly all of extensions class don't have such a duplicate __init__(). Removing the __init__() in child class could help keeping codes consistent and clean.
[0] https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/fixed_ips.py#L88
[1] https://github.com/openstack/nova/blob/master/nova/api/openstack/extensions.py#L63","remove redundant __init__() overwriting when getting ExtensionResources

As described in #1266344, some of child class in
<nova>/nova/api/openstack/compute/contrib/ has overwrited __init__()
without specified needs. Removing that could help keeps codes clean and
consistent (nearly all others don't overwrite __init__()).

Change-Id: I87dfd9c1622bc8250f620ef591391a0e98573f78
Closes-Bug: #1266344"
612,5228d01bcde531cc2c141f38a150dbb2daef1c07,1386732094,,1.0,19,3,3,2,1,0.846719854,True,16.0,2116858.0,101.0,66.0,False,141.0,32293.33333,425.0,4.0,9.0,2051.0,2058.0,9.0,1670.0,1677.0,9.0,1928.0,1935.0,0.001464987,0.282595957,0.283621447,170,572,1259796,nova,5228d01bcde531cc2c141f38a150dbb2daef1c07,1,1,,"Bug #1259796 in OpenStack Compute (nova): ""nova-compute failed to start because get_host_capablilities failed""","When starting nova-compute service on node with lxc hypervisor, it failed with the following error:
 2013-12-11 03:04:41.439 ERROR nova.openstack.common.threadgroup [-] can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     x.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 448, in run_service
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     service.start()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 822, in pre_start_hook
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 4971, in update_available_resource
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     nodenames = set(self.driver.get_available_nodes())
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/driver.py"", line 980, in get_available_nodes
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     stats = self.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4569, in get_host_stats
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     return self.host_state.get_host_stats(refresh=refresh)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 429, in host_state
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self._host_state = HostState(self)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4960, in __init__
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.update_status()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 4999, in update_status
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.driver.get_instance_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3702, in get_instance_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     caps = self.get_host_capabilities()
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2739, in get_host_capabilities
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self._caps.host.cpu.parse_str(features)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/virt/libvirt/config.py"", line 61, in parse_str
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup     self.parse_dom(etree.fromstring(xmlstr))
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""lxml.etree.pyx"", line 2993, in lxml.etree.fromstring (src/lxml/lxml.etree.c:62980)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup   File ""parser.pxi"", line 1614, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:92786)
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup ValueError: can only parse strings
2013-12-11 03:04:41.439 TRACE nova.openstack.common.threadgroup
This is because no cpu model info can be gotten from libvirt getCapabilities interface for lxc hypervisor.","lxc: Fix a bug of baselineCPU parse failure

When starting nova-compute on compute node with lxc hypervisor,
a valueError exception will be raised because a libvirt bug of
the incorrect return value of baselineCPU() function, in function
get_host_capabilities, which introduced by commit
3b8c26321bce07b8ad588099c1fdcef858e748a8.

For libvirt interface baselineCPU() in python library, -1 will be
returned if basecpu info is None from version 1.1.2, and the return
value of this interface is corrected to None from version 1.2.0.

So a checking is added to make sure the return value of baselineCPU()
is not -1, and also a test case is added.

Closes-bug: #1259796

Co-authored-by: Wangpan <hzwangpan@corp.netease.com>

Change-Id: I5e4cb4a8d09b660edc4f6c6a4ad4bfe58a4dd5cb"
613,52301e4727091f867c42b18b316d4c4aacffea31,1400620212,,1.0,0,71,7,7,1,0.667724341,False,,,,,True,,,,,,,,,,,,,,,,,905,1341,1321864,neutron,52301e4727091f867c42b18b316d4c4aacffea31,1,1,,"Bug #1321864 in neutron: ""allowed address pairs - unnecessary and incomplete overlap check""","Current code intends to disallow assigning a fixed ip to a port when that ip
overlaps with one of the addresses in the allowed-address-pairs list. However, it is an unnecessary check and also the current code does not enforce it in all cases.
Cases where it enforces:
1) A port-update with allowed-address-pairs list containing an IP address which is *exactly* same as one of the fixed-ips on the port. For example, for a port with fixed-ip of 10.10.8.6, the following fails:
$> neutron port-update  58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
2) A port-update with a fixed-ip which is exactly same as one of the allowed
IP addresses in the allowed-address-pairs list. For example, for a port
with allowed-address-pairs with """", the following fails:
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6"", ""mac_address"": ""fa:16:3e:7f:3c:06""}                    |
$> neutron port-update 58062310-ee5a-4b14-b554-5df699064bc9 -- --fixed-ips type=dict list=true ip_address=10.10.8.6
400-{u'NeutronError': {u'message': u""Port's Fixed IP and Mac Address match an address pair entry."", u'type': u'AddressPairMatchesPortFixedIPAndMac', u'detail': u''}}
However, allowed-address-pairs can work with IP CIDRs and the overlap check
is *not* properly enforced when IP addresses are specified in the CIDR notation.
Case where the current code is incomplete:
Same as case (1) above but IP address specified in cidr notation. In this case, the code does not check for overlaps.
$> neutron port-update  58062310-ee5a-4b14-b554-5df699064bc9 --allowed-address-pairs type=dict list=true ip_address=10.10.8.6/32
Updated port: 58062310-ee5a-4b14-b554-5df699064bc9
$> neutron port-show 58062310-ee5a-4b14-b554-5df699064bc9 | grep allowed
| allowed_address_pairs | {""ip_address"": ""10.10.8.6/32"", ""mac_address"": ""fa:16:3e:7f:3c:06""}                 |
Functionally, it is incorrect to allow overlaps in one type of inputs and not allow in other types of input.
On the other hand, if we fix this bug and check overlaps in all cases, then the API will become hard to use. For example, if a fixed IP 10.10.1.1 exists on a port and we want to allow addresses in 10.10.1.0/24 cidr on that port, then one has to configure a list of 8 cidrs ([10.10.1.0/32, 10.10.1.2/31,
10.10.1.4/30, ..., 10.10.1.128/25]) on the allowed-address-pairs.
In any case, this is an unnecessary check as the overlap does not have any negative effect. Allowed-address-pairs is ADDING on to what is allowed because of the fixed IPs. So, there is no possibility of conflict. The check will probably make sense if we are maintaining denied addresses (instead of allowed addresses).
My suggestion is to remove this check entirely.
https://review.openstack.org/#/c/94508/","Allowed Addresspairs: Removing check for overlap with fixed ips

Current code does not allow assigning a fixed ip to a port when that ip
overlaps with one of the addresses in the allowed-addresspairs list.
This is an unnecessary check as the overlap does not have any negative
effect. Further, such a check actually makes it hard to use this
API. For example, if a fixed IP 10.10.1.1 exists on a port and we
want to allow addresses in 10.10.1.0/24 cidr on that port, then one
has to configure a list of 8 cidrs ([10.10.1.0/32, 10.10.1.2/31,
10.10.1.4/30, ..., 10.10.1.128/25]) on the allowed-addresspairs.
In addition to the above reasons, the current code also does not
check for the overlaps in all cases.

This patch summarily removes this overlap check.

Closes-Bug: #1321864
Change-Id: I5498c4a72b31267644da10a54a9860c1fc3bb250"
614,52389188372b38ef237898e90cc533cf27d3eda1,1393270532,2.0,1.0,49,7,2,2,1,0.999079718,True,1.0,40686.0,11.0,3.0,False,13.0,374968.0,50.0,2.0,207.0,1340.0,1402.0,199.0,1114.0,1175.0,197.0,1186.0,1243.0,0.135430917,0.811901505,0.850889193,466,881,1284275,cinder,52389188372b38ef237898e90cc533cf27d3eda1,0,0,Add a feature,"Bug #1284275 in Cinder: ""3PAR FC driver doesn't return initiator_target_map""","In order for the Fibre Channel Zone Manager to automatically zone up the endpoints (initiator and target) for the Fibre Channel fabric, the 3PAR driver needs to return the initiator_target_map in initialize_connection and terminate_connection","Added 3par initiator target map for FCZM

This patch adds the required initiator target map
for the Fibre Channel Zone Manager.   The
FCZM looks for the initiator_target_map to try and
automatically zone up the endpoints.

Change-Id: I61aaf31752f8a5eebb672887c9a943584df66995
Closes-Bug: #1284275"
615,523e52f2f754f35b1e2453190f935d1a990464fb,1399094522,,1.0,6,6,6,2,1,0.871049064,True,5.0,523336.0,69.0,11.0,False,26.0,5529296.667,49.0,4.0,296.0,2643.0,2687.0,293.0,2124.0,2166.0,287.0,2285.0,2320.0,0.036331525,0.288381481,0.292796771,858,1291,1315597,nova,523e52f2f754f35b1e2453190f935d1a990464fb,1,0,"“api-version 3 list-extensions"" with a patch https://review.openstack.org/#/c/91942/ ,
the output is the following.
So 'extensions', 'flavors' and 'ips' are not camelcase. This seems inconsistent.”","Bug #1315597 in OpenStack Compute (nova): ""Some v3 API extension names are not camelcase""","When operating ""nova --os-compute-api-version 3 list-extensions"" with a patch https://review.openstack.org/#/c/91942/ ,
the output is the following.
So 'extensions', 'flavors' and 'ips' are not camelcase. This seems inconsistent.
$ nova --os-compute-api-version 3 list-extensions
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+
| Name                     | Summary                                                               | Alias                         | Version |
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+
| Consoles                 | Consoles.                                                             | consoles                      | 1       |
| extensions               | Extension information.                                                | extensions                    | 1       |
| FlavorAccess             | Flavor access support.                                                | flavor-access                 | 1       |
| FlavorsExtraSpecs        | Flavors Extension.                                                    | flavor-extra-specs            | 1       |
| FlavorManage             | Flavor create/delete API support.                                     | flavor-manage                 | 1       |
| flavors                  | Flavors Extension.                                                    | flavors                       | 1       |
| ips                      | Server addresses.                                                     | ips                           | 1       |
| Keypairs                 | Keypair Support.                                                      | keypairs                      | 1       |
| AccessIPs                | Access IPs support.                                                   | os-access-ips                 | 1       |
| AdminActions             | Enable admin-only server actions...                                   | os-admin-actions              | 1       |
| AdminPassword            | Admin password management support.                                    | os-admin-password             | 1       |
| Agents                   | Agents support.                                                       | os-agents                     | 1       |
| Aggregates               | Admin-only aggregate administration.                                  | os-aggregates                 | 1       |
| AttachInterfaces         | Attach interface support.                                             | os-attach-interfaces          | 1       |
| AvailabilityZone         | 1. Add availability_zone to the Create Server API....                 | os-availability-zone          | 1       |
| BlockDeviceMapping       | Block device mapping boot support.                                    | os-block-device-mapping       | 1       |
| Cells                    | Enables cells-related functionality such as adding neighbor cells,... | os-cells                      | 1       |
| Certificates             | Certificates support.                                                 | os-certificates               | 1       |
| ConfigDrive              | Config Drive Extension.                                               | os-config-drive               | 1       |
| ConsoleAuthTokens        | Console token authentication support.                                 | os-console-auth-tokens        | 1       |
| ConsoleOutput            | Console log output support, with tailing ability.                     | os-console-output             | 1       |
| CreateBackup             | Create a backup of a server.                                          | os-create-backup              | 1       |
| DeferredDelete           | Instance deferred delete.                                             | os-deferred-delete            | 1       |
| Evacuate                 | Enables server evacuation.                                            | os-evacuate                   | 1       |
| ExtendedAvailabilityZone | Extended Server Attributes support.                                   | os-extended-availability-zone | 1       |
| ExtendedServerAttributes | Extended Server Attributes support.                                   | os-extended-server-attributes | 1       |
| ExtendedStatus           | Extended Status support.                                              | os-extended-status            | 1       |
| ExtendedVolumes          | Extended Volumes support.                                             | os-extended-volumes           | 1       |
| FlavorRxtx               | Support to show the rxtx status of a flavor.                          | os-flavor-rxtx                | 1       |
| HideServerAddresses      | Support hiding server addresses in certain states.                    | os-hide-server-addresses      | 1       |
| Hosts                    | Admin-only host administration.                                       | os-hosts                      | 1       |
| Hypervisors              | Admin-only hypervisor administration.                                 | os-hypervisors                | 1       |
| LockServer               | Enable lock/unlock server actions.                                    | os-lock-server                | 1       |
| MigrateServer            | Enable migrate and live-migrate server actions.                       | os-migrate-server             | 1       |
| Migrations               | Provide data on migrations.                                           | os-migrations                 | 1       |
| Multinic                 | Multiple network support.                                             | os-multinic                   | 1       |
| MultipleCreate           | Allow multiple create in the Create Server v3 API.                    | os-multiple-create            | 1       |
| PauseServer              | Enable pause/unpause server actions.                                  | os-pause-server               | 1       |
| PCIAccess                | Pci access support.                                                   | os-pci                        | 1       |
| Quotas                   | Quotas management support.                                            | os-quota-sets                 | 1       |
| RemoteConsoles           | Interactive Console support.                                          | os-remote-consoles            | 1       |
| Rescue                   | Instance rescue mode.                                                 | os-rescue                     | 1       |
| SchedulerHints           | Pass arbitrary key/value pairs to the scheduler.                      | os-scheduler-hints            | 1       |
| SecurityGroups           | Security group support.                                               | os-security-groups            | 1       |
| ServerActions            | View a log of actions and events taken on an instance.                | os-server-actions             | 1       |
| ServerDiagnostics        | Allow Admins to view server diagnostics through server action.        | os-server-diagnostics         | 1       |
| ServerExternalEvents     | Server External Event Triggers.                                       | os-server-external-events     | 1       |
| ServerPassword           | Server password support.                                              | os-server-password            | 1       |
| ServerUsage              | Adds launched_at and terminated_at on Servers.                        | os-server-usage               | 1       |
| Services                 | Services support.                                                     | os-services                   | 1       |
| Shelve                   | Instance shelve mode.                                                 | os-shelve                     | 1       |
| SuspendServer            | Enable suspend/resume server actions.                                 | os-suspend-server             | 1       |
| Server Metadata          | Server Metadata API.                                                  | server-metadata               | 1       |
| Servers                  | Servers.                                                              | servers                       | 1       |
| Versions                 | API Version information.                                              | versions                      | 1       |
+--------------------------+-----------------------------------------------------------------------+-------------------------------+---------+","Fix v3 API extension names for camelcase

Most v3 API names are camelcase, but few APIs are not.
This patch changes them to camelcase for the consistency.

Change-Id: I0dc6c23ffcdcf97fc52d3575d521692805ffd9e3
Closes-Bug: #1315597"
616,5247f5cdf15bad4c62bbf854e30716fcf00a1d2a,1392809959,,1.0,267,4,3,3,1,0.165828543,True,14.0,1176536.0,230.0,13.0,True,,,,,,,,,,,,,,,,,1086,1536,1346444,neutron,5247f5cdf15bad4c62bbf854e30716fcf00a1d2a,0,0,Bug in test. “ Implement ModelsMigrationsSync test from oslo.db”,"Bug #1346444 in neutron: ""DB migrations need unit tests""","Now that the DB healing https://review.openstack.org/96438 is merged, the DB migrations need unit tests.","Implement ModelsMigrationsSync test from oslo.db

Add tests to verify that database migrations produce
the same schema as the database models.

Also for MySQL, check that all tables are configured to use InnoDB
as the storage engine.
These tests make use of the ModelsMigrationsSync test class from
oslo.db and the load_tests protocol from Python unittest.

Closes-bug: #1346444

Change-Id: Ic0e7eb37c30cc5e94cbdbddf07a6dc1ebf377c17"
617,524981cce05a9b365036c0a1e9810036936d3d5b,1409904067,,1.0,66,10,2,2,1,0.995499385,False,,,,,True,,,,,,,,,,,,,,,,,1269,1731,1365806,neutron,524981cce05a9b365036c0a1e9810036936d3d5b,1,1,“Add missing methods to NoopFirewallDriver”,"Bug #1365806 in neutron: ""Noopfirewall driver or security group disabled should avoid impose security group related calls to Neutron server""","With openvswitch neutron agent, during the daemon loop, the phase for setup_port_filters will try to grab/call method 'security_group_rules_for_devices'  to Neutron Server.
And this operation will be very time consuming and have big performance bottleneck as it include ports query,  rules query, network query as well as reconstruct the huge Security groups Dict Message.  This message size is very large and for processing it, it will occupy a lot of CPU of Neutron Server. In cases like VM/perhost arrive to 700, the Neutron server will be busy doing the message and couldn't to do other thing and this could lead to message queue connection timeout and make queue disconnect the consumers. As a result the Neutron server is crashed and not function either for deployments or for API calls.
For the Noopfirewall or security group disabled situation, this operation should be avoided. Because eventually these reply message would not be used by Noopfirewall driver.  (There methods are pass).
 with self.firewall.defer_apply():
            for device in devices.values():
                LOG.debug(_(""Update port filter for %s""), device['device'])
                self.firewall.update_port_filter(device)","Reduce security group db calls to neutron server

Within ovs agent daemon loop, prepare_devices_filter will impose heavy workloads
to neutron server in order to retrieve the security groups message to apply
firewall rules. If agent is configured to use Noopfirewall driver or security
groups are disabled, there is no need for loading the rules from server and
refreshing the firewalls. This will reduce the number of db calls and improve
performance for neutron server in this case.

Change-Id: Id244aab3cac37fc6ed3dc05cbee91cdf9e34d9cc
Closes-Bug: #1365806"
618,524ee4913d9015c2893cca69cefbc80fbe6878bb,1399515449,,1.0,21,7,2,2,1,0.371232327,True,3.0,146151.0,34.0,9.0,False,214.0,140993.5,1214.0,5.0,35.0,2756.0,2766.0,35.0,2089.0,2099.0,28.0,2346.0,2349.0,0.003648258,0.295257265,0.295634671,874,1308,1317321,nova,524ee4913d9015c2893cca69cefbc80fbe6878bb,0,0,Bug in test,"Bug #1317321 in OpenStack Compute (nova): ""Add extra unit test case for more than 1 ephemeral disk in BDM""","There was a comment on https://review.openstack.org/#/c/90583/ (after it was approved for merge) to add an extra test case to handle more than 1 ephemeral disk, as well as correct a comment in the code to be more accurate.  The purpose of this bug is to add the extra test case for _get_instance_block_device_info to test more than 1 ephemeral disk and correct one of the comments in _get_instance_block_device_info.","Unit test case for more than 1 ephemeral disks in BDM

The following patch modifies the test case for
_get_instance_block_device_info to handle more than 1 ephemeral
disks in the BDM.  This is to address one of the comments made
in https://review.openstack.org/#/c/90583/ after it was already
approved for merger.  This patch also corrects a comment in
_get_instance_block_device_info to be more accurate.

Change-Id: I11e97f2e2ef999fa08af1693483d56bf8ad0e556
Closes-Bug: #1317321"
619,527d9e532d1c2692fdd367314d98c37237ecc834,1394199842,,1.0,8,1,2,2,1,0.99107606,True,2.0,306297.0,25.0,9.0,False,82.0,2189595.5,306.0,4.0,410.0,2583.0,2794.0,391.0,1994.0,2192.0,402.0,2491.0,2698.0,0.053483743,0.330723291,0.35819509,564,985,1289361,nova,527d9e532d1c2692fdd367314d98c37237ecc834,1,1,,"Bug #1289361 in OpenStack Compute (nova): ""xenapi: unable to create instances with ephemeral disks""",The resize ephemeral disk blueprint has regressed the ability to spawn instances with ephemeral disks.,"xenapi: fix spawn servers with ephemeral disks

The new resize ephemeral disk code uses vdis[""ephemerals""] to pass the
list of ephemeral vdis to the resize disk code. This is shared between
spawn and resize. As spawn wants to resize the root disk, resize=True
during the spawn operation.

The code in _attach_disks correctly deals with empty ephemeral disks,
but the code in _resize_up_vdis did not, and failed due to
ephemeral_vdis being None.

This is a quick fix for the issue. We need some refactoring in this area
so there is, ideally, only one place that generates ephemeral disks.

Change-Id: If058c5c17c2ed76569fd98d4a3d8e5274721fbd0
Closes-Bug: #1289361"
620,529e7d14a716fcc1264b2e8053be3c842afd2153,1396947439,,1.0,52,9,3,2,1,0.714546732,True,11.0,2725883.0,143.0,49.0,False,57.0,44972.0,164.0,3.0,0.0,2637.0,2637.0,0.0,2192.0,2192.0,0.0,2564.0,2564.0,0.000128866,0.330541237,0.330541237,742,1169,1302482,nova,529e7d14a716fcc1264b2e8053be3c842afd2153,1,1,,"Bug #1302482 in OpenStack Compute (nova): ""VMware driver: Nova boot fails when there is a datacenter with no datastore associated with it.""","when there is a Datacenter in the vCenter with no datastore associated with it, nova boot fails  even though there are data-centers configured properly.
The log error trace
Error from last host: devstack (node domain-c162(Demo-1)): [u'Traceback (most recent call last):\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1322, in _build_instance\n    set_access_ip=set_access_ip)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 399, in decorated_function\n    return function(self, context, *args, **kwargs)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1734, in _spawn\n    LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', u'  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File ""/opt/stack/nova/nova/compute/manager.py"", line 1731, in _spawn\n    block_device_info)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 619, in spawn\n    admin_password, network_info, block_device_info)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 211, in spawn\n    dc_info = self.get_datacenter_ref_and_name(data_store_ref)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1715, in get_datacenter_ref_and_name\n    self._update_datacenter_cache_from_objects(dcs)\n', u'  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1693, in _update_datacenter_cache_from_objects\n    datastore_refs = p.val.ManagedObjectReference\n', u""AttributeError: 'Text' object has no attribute 'ManagedObjectReference'\n""]
2014-04-04 03:05:41.629 WARNING nova.scheduler.driver [req-cc690e5a-2bf3-4566-a697-30ca882df815 nova service] [instance: f0abb23a-943a-475d-ac63-69d2563362cb] Setting instance to ERROR state.","VMware: Add check for datacenter with no datastore

When there is a datacenter with no datastore in it,
nova boot fails.

This patch addresses the issue by adding a check
and ignoring the empty datacenter.

Change-Id: I3c8242767f5991d4aeffab09c763a21493fc363e
closes-bug: #1302482"
621,52a9137d18369848feb1f496497523098d08ceff,1377176229,,1.0,6,4,1,1,1,0.0,True,2.0,7981033.0,41.0,13.0,False,61.0,1186401.0,163.0,3.0,1.0,1865.0,1865.0,1.0,1546.0,1546.0,1.0,1611.0,1611.0,0.000345185,0.278218847,0.278218847,1484,1215390,1215390,nova,52a9137d18369848feb1f496497523098d08ceff,1,1,"“we rename the 'stats' attribute of the class to 'compute_node' (due to the fact that the corresponding relation actually maps to ComputeNode class, not to ComputeNodeStat)”","Bug #1215390 in OpenStack Compute (nova): ""Incorrect relationship declaration in models.ComputeNodeStat""","db.sqlalchemy.models.ComputeNodeStat.stats field is not properly named: since it is a relation that maps to db.sqlalchemy.models.ComputeNode, it should be named 'compute_node', not 'stats'. Besides, the call to the relationship() method lacks the foreign_keys parameter.","Stylistic improvement of models.ComputeNodeStat

The patch is aimed at improving the readability of models.ComputeNodeStat.

Here, we rename the 'stats' attribute of the class to 'compute_node' (due to
the fact that the corresponding relation actually maps to ComputeNode class,
not to ComputeNodeStat). We also rewrite the relationship() method call to
make it stylistically conforming with the analogous 'service' relationship
in the ComputeNode class: we add the 'foreign_keys' parameter, wrap the
'backref' parameter into a method call and remove the auxiliary 'primary_join'
variable.

Change-Id: I2b1f27b4c1d06f364af01fc82d3490fd3bbf4ac0
Closes-Bug: #1215390"
622,52de9395e5fe4f328f6dab0b35d660a700787c76,1405687396,,1.0,60,14,5,2,1,0.775614424,False,,,,,True,,,,,,,,,,,,,,,,,1079,1529,1344036,nova,52de9395e5fe4f328f6dab0b35d660a700787c76,1,0,“Fixes Hyper-V agent force_hyperv_utils_v1 flag issue”,"Bug #1344036 in OpenStack Compute (nova): ""Hyper-V agent generates exception when force_hyperv_utils_v1 is True on Windows Server / Hyper-V Server 2012 R2""","WMI root\virtualization namespace v1 (in Hyper-V) has been removed from Windows Server / Hyper-V Server 2012 R2, according to:
http://technet.microsoft.com/en-us/library/dn303411.aspx
Because of this, setting the force_hyperv_utils_v1 option on the Windows Server 2012 R2 nova compute agent's nova.conf will cause exceptions, since it will try to use the removed root\virtualization namespace v1.
Logs:
http://paste.openstack.org/show/87125/","Fixes Hyper-V agent force_hyperv_utils_v1 flag issue

WMI root\virtualization namespace v1 (in Hyper-V) has been removed
from Windows Server / Hyper-V Server 2012 R2.

Hyper-V compute agent now creates instances which uses
root\virtualization\v2 namespace if the agent's OS is
Windows Server / Hyper-V Server 2012 R2 or newer.

Closes-Bug: #1344036

Change-Id: I874ade4456b92a63959a765c7851bcd001befa32"
623,52f0750d80e8742efc253b48c6bc121ae9853cc1,1385718333,,1.0,192,19,6,3,1,0.656196779,True,12.0,9548114.0,145.0,14.0,False,1.0,1049836.5,1.0,2.0,11.0,702.0,707.0,11.0,649.0,654.0,10.0,221.0,225.0,0.018612521,0.375634518,0.382402707,106,505,1256243,neutron,52f0750d80e8742efc253b48c6bc121ae9853cc1,0,0,Evolution: Add Session Persistence support ,"Bug #1256243 in neutron: ""Add Session Persistence support for NVP advanced LBaaS""","Need to add session persistence support for NVP advanced LBaaS, as the present session persistence implementation on Edge is set by default.","Add session persistence support for NVP advanced LBaaS

Change-Id: I2042894755cdaf54b2bc39e58028746aa7c1e8ea
Closes-Bug: #1256243"
624,52f9994aab8b60de2ba22579f923beb050e92bca,1387666347,,1.0,25,1,2,2,1,0.995727452,True,6.0,1280596.0,37.0,19.0,False,22.0,1318504.0,67.0,3.0,34.0,2430.0,2445.0,34.0,1985.0,2000.0,34.0,2276.0,2291.0,0.005060729,0.329236553,0.331405437,189,592,1261442,nova,52f9994aab8b60de2ba22579f923beb050e92bca,1,1,Fix image cache,"Bug #1261442 in OpenStack Compute (nova): ""Spurious error from ComputeManager._run_image_cache_manager_pass""","I got the following errors in a tempest test at http://logs.openstack.org/97/62397/1/check/check-tempest-dsvm-full/3f7b8c3:
2013-12-16 16:07:52.189 27621 DEBUG nova.openstack.common.processutils [-] Result was 1 execute /opt/stack/new/nova/nova/openstack/common/processutils.py:172
2013-12-16 16:07:52.189 27621 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager._run_image_cache_manager_pass: Unexpected error while running command.
Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
Exit code: 1
Stdout: ''
Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5210, in _run_image_cache_manager_pass
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     self.driver.manage_image_cache(context, filtered_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 4650, in manage_image_cache
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     self.image_cache_manager.verify_base_images(context, all_instances)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 603, in verify_base_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     inuse_backing_images = self._list_backing_images()
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/imagecache.py"", line 345, in _list_backing_images
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     backing_file = virtutils.get_disk_backing_file(disk_path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 442, in get_disk_backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     backing_file = images.qemu_img_info(path).backing_file
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/images.py"", line 56, in qemu_img_info
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     'qemu-img', 'info', path)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/utils.py"", line 175, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     return processutils.execute(*cmd, **kwargs)
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task     cmd=' '.join(cmd))
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task ProcessExecutionError: Unexpected error while running command.
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Command: env LC_ALL=C LANG=C qemu-img info /opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Exit code: 1
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stdout: ''
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task Stderr: ""qemu-img: Could not open '/opt/stack/data/nova/instances/95dbf14f-3abb-42c7-94c5-dd7355ecd78a/disk': No such file or directory\n""
2013-12-16 16:07:52.189 27621 TRACE nova.openstack.common.periodic_task","Fix image cache periodic task concurrent access bug

This fix catch the exception and ignore a disk file absent
exception. It will defer the image cache clean to next round
image cache periodic task because it's not harmful.
Refer to detailed info, log analysis and root cause in defect.

Change-Id: I3a431bf389658f976edf5c847c04b7e0aac496fd
Closes-Bug: #1261442"
625,53345da70e3c969168e7192c3098540ba642ea09,1374616491,1.0,,120,74,6,4,2,0.691571492,True,5.0,605281.0,25.0,6.0,False,43.0,277827.5,128.0,2.0,87.0,329.0,381.0,83.0,267.0,318.0,86.0,325.0,376.0,0.152097902,0.56993007,0.659090909,1459,1208396,1208396,Swift,53345da70e3c969168e7192c3098540ba642ea09,0,0,“/opt/stack/swift/swift/common/utils.py:docstring of swift.common.utils.parse_content_type:7: ERROR: Unexpected indentation.”,"Bug #1208396 in OpenStack Object Storage (swift): ""error in swift.comm.utils when building doc""",/opt/stack/swift/swift/common/utils.py:docstring of swift.common.utils.parse_content_type:7: ERROR: Unexpected indentation.,"some container serialization cleanup

Have json and xml use common record cleanup code.
Do a somewhat better job of parsing extensions from content-types.
Use a real XML serializer.

Change-Id: I10e14dffd1da590b4fd180b4d33ab5de862e2b55"
626,534ce5dde117a61abbab49e8b1bc34b5fcbfc095,1388041741,,1.0,1,1,1,1,1,0.0,True,4.0,1119671.0,37.0,19.0,False,3.0,95517.0,3.0,6.0,53.0,877.0,888.0,53.0,838.0,849.0,48.0,589.0,596.0,0.038311181,0.461297889,0.466770915,230,633,1264223,cinder,534ce5dde117a61abbab49e8b1bc34b5fcbfc095,1,1,typo in code,"Bug #1264223 in Cinder: ""webob.exc.HTTPBadRequest doesn't accept parameter 'reason'""","If the specific error message is required, we must set explanation='message' to initialize subclass of webob.exc.WSGIHTTPException(). There is some code specify the message to 'reason' incorrectly.
Ex.
OK: raise webob.exc.HTTPBadRequest(explanation=_('What?'))
WRONG: raise webob.exc.HTTPBadRequest(reason=_(""What's up?""))","Fix the invalid argument of webob.exc.HTTPBadRequest

webob.exc.HTTPBadRequest() does not have a parameter 'reason'.

Change-Id: I65103ce72849e921150c4d098993fa523f53fe3a
Closes-Bug: #1264223"
627,536bcbd2ce0822bec757f7ec949b9f70df20c966,1406089826,1.0,1.0,28,6,4,4,1,0.893563293,False,,,,,True,,,,,,,,,,,,,,,,,1094,1545,1347355,nova,536bcbd2ce0822bec757f7ec949b9f70df20c966,1,1,,"Bug #1347355 in OpenStack Compute (nova): ""Extra image metadata didn't assgin to volume based instance snapshot image""","curl -i 'http://cloudcontroller:8774/v2/fdbb1e8f23eb40c89f3a677e2621b95c/servers/e2461ba7-3624-4d43-a456-acb87a0fb6f9/action' -X POST -H ""X-Auth-Project-Id: admin"" -H ""User-Agent: python-novaclient"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""X-Auth-Token: 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-2CM0-JKKjNf-NG9X3NpXPDt1mvaP1brXvzoq0W9IhxvE5THBLyrVrzEk5s+cZlDfo6QqyPtRqGgs80WNdOe3UQ8pL14E+SPoc3QIv66G6on6wOL9JK7KAtVbcfE9ucCgZmLI9hQGzn7J5GyXHzP0dRNRxEw+39P4pKOdTLj7dIQA7-PGEHdajFkMVWIIkD--c+G8pojKzZjESrGWpJh8wzmH8Awkcr5qEvvkEqEXqHKsf4ILdX2d90C4nfYYRDYpggVaVn5H8eK2z4dYkuUdRAf03c7kUzE9n12PabhSxssFNtdF-Dm8VSM0eF1BkBYByA"" -d '{""createImage"": {""name"": ""snapx2"", ""metadata"": {""a"": ""b""}}}'
os@os2:~/devstack$ glance image-show 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59
+---------------------------------+----------------------------------------------------------------------------------+
| Property                        | Value                                                                            |
+---------------------------------+----------------------------------------------------------------------------------+
| Property 'bdm_v2'               | True                                                                             |
| Property 'block_device_mapping' | [{""guest_format"": null, ""boot_index"": 0, ""no_device"": null, ""snapshot_id"":       |
|                                 | ""2fb5111f-b618-4d8b-b47f-142ed4762064"", ""delete_on_termination"": null,           |
|                                 | ""disk_bus"": ""virtio"", ""image_id"": null, ""source_type"": ""snapshot"",               |
|                                 | ""device_type"": ""disk"", ""volume_id"": null, ""destination_type"": ""volume"",          |
|                                 | ""volume_size"": null}]                                                            |
| Property 'checksum'             | 4eada48c2843d2a262c814ddc92ecf2c                                                 |
| Property 'container_format'     | ami                                                                              |
| Property 'disk_format'          | ami                                                                              |
| Property 'image_id'             | da82a342-aeac-407a-bf9d-cf28bf68dc6b                                             |
| Property 'image_name'           | cirros-0.3.2-x86_64-uec                                                          |
| Property 'kernel_id'            | 3b6a8424-1454-4394-810f-2adc95c6e326                                             |
| Property 'min_disk'             | 0                                                                                |
| Property 'min_ram'              | 0                                                                                |
| Property 'ramdisk_id'           | aa76f1bd-8167-4586-8185-4347dc4951e0                                             |
| Property 'root_device_name'     | /dev/vda                                                                         |
| Property 'size'                 | 25165824                                                                         |
| created_at                      | 2014-07-23T03:16:18                                                              |
| deleted                         | False                                                                            |
| id                              | 6c2ba6d6-e906-41b6-8b28-3b4c9fa80d59                                             |
| is_public                       | False                                                                            |
| min_disk                        | 0                                                                                |
| min_ram                         | 0                                                                                |
| name                            | snapx2                                                                           |
| owner                           | fdbb1e8f23eb40c89f3a677e2621b95c                                                 |
| protected                       | False                                                                            |
| size                            | 0                                                                                |
| status                          | active                                                                           |
| updated_at                      | 2014-07-23T03:16:18                                                              |
+---------------------------------+----------------------------------------------------------------------------------+","Fix extra metadata didn't assign into snapshot image

create_image action API accept metadata parameter to add extra
metadata into snapshot image. But the current code doesn't work
for snapshot of volume based instance. This error is because the
code use var 'props' to store extra metadatas, and the code use
same var to store root bdm metadata later, then the extra metadata
is overwritten by root bdm metadata. This patch use another var
'properties' to store root bdm metadata to avoid overwrite the
extra metadata.

Change-Id: I2224905f596690e22aa71dc6541284f005ebba9d
Closes-Bug: #1347355"
628,53886fc7374bc4f0bda1f8f6b80f815d718c81e6,1384391007,,1.0,0,6,2,2,1,0.918295834,True,2.0,1033643.0,24.0,12.0,False,7.0,3473735.5,15.0,7.0,188.0,2233.0,2273.0,173.0,2041.0,2067.0,125.0,506.0,511.0,0.232902033,0.93715342,0.946395564,1760,1251086,1251086,neutron,53886fc7374bc4f0bda1f8f6b80f815d718c81e6,0,0,deprecated code,"Bug #1251086 in neutron: ""nvp_cluster_uuid is no longer used in nvp.ini""",remove it!,"Removes unused nvp plugin config param

nvp_cluster_uuid is no longer used; it was missed
during the configuration file tweaks that was done
during Havana.

Change-Id: Id6dd684833c99453cd97d88584be04d99fd21550
Closes-bug: #1251086"
629,539eaaab89eea42f3f92a4ce069183c7730034af,1383787625,0.0,1.0,16,1,2,2,1,0.522559375,True,2.0,532312.0,14.0,8.0,False,11.0,4040227.0,14.0,7.0,1.0,790.0,790.0,1.0,766.0,766.0,0.0,719.0,719.0,0.000873362,0.628820961,0.628820961,65,393,1248434,cinder,539eaaab89eea42f3f92a4ce069183c7730034af,1,1,,"Bug #1248434 in Cinder: ""RequestContext initialization failed in cinder.""","RequestContext initialization failed in cinder because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call cinder api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown.","RequestContext initialization failed in cinder.

RequestContext initialization failed in cinder because of the following
error:
""TypeError: 'in <string>' requires string as left operand, not NoneType""

It must traverses in tuple not in string when find the ""compute""
service_catalog.

Change-Id: I46f4bbd0ffb9d1db8bdcb0254ca95551fea08baf
Closes-Bug: #1248434"
630,53ad788af6ced83bd9d6e58a25196a325d60fc4e,1397060949,,1.0,1,1,1,1,1,0.0,True,1.0,45194.0,13.0,6.0,False,183.0,79811.0,937.0,3.0,1188.0,2295.0,3131.0,986.0,1766.0,2446.0,1169.0,2067.0,2892.0,0.150579151,0.266151866,0.372329472,769,1196,1304968,nova,53ad788af6ced83bd9d6e58a25196a325d60fc4e,1,1,,"Bug #1304968 in OpenStack Compute (nova): ""Nova cpu full of instance_info_cache stack traces due to attempting to send events about deleted instances""","The bulk of the stack traces in n-cpu is because emit_event is getting triggered on a VM delete, however by the time we get to emit_event the instance is deleted (we see this exception 183 times in this log - which means it's happening on *every* compute terminate) so when we try to look up the instance we hit the exception found here:
    @base.remotable_classmethod
    def get_by_instance_uuid(cls, context, instance_uuid):
        db_obj = db.instance_info_cache_get(context, instance_uuid)
        if not db_obj:
            raise exception.InstanceInfoCacheNotFound(
                    instance_uuid=instance_uuid)
        return InstanceInfoCache._from_db_object(context, cls(), db_obj)
A log trace of this interaction looks like this:
2014-04-08 11:14:25.475 DEBUG nova.openstack.common.lockutils [req-fe9db989-416e-4da0-986c-e68336e3c602 TenantUsagesTestJSON-153098759 TenantUsagesTestJSON-953946497] Semaphore / lock released ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""75da98d7-bbd5-42a2-ad6f-7a66e38977fa"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""do_terminate_instance"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.907 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore ""<function _lock_name at 0x41635f0>"" lock /opt/stack/new/nova/nova/openstack/common/lockutils.py:168
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Got semaphore / lock ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:248
2014-04-08 11:14:25.908 DEBUG nova.openstack.common.lockutils [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Semaphore / lock released ""_clear_events"" inner /opt/stack/new/nova/nova/openstack/common/lockutils.py:252
2014-04-08 11:14:25.928 AUDIT nova.compute.manager [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Terminating instance
2014-04-08 11:14:25.989 DEBUG nova.objects.instance [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Lazy-loading `system_metadata' on Instance uuid 75da98d7-bbd5-42a2-ad6f-7a66e38977fa obj_load_attr /opt/stack/new/nova/nova/objects/instance.py:519
2014-04-08 11:14:26.209 DEBUG nova.network.api [req-687de0cf-67fc-434f-927f-4c37665ad5d8 FixedIPsTestJson-234831436 FixedIPsTestJson-1960919997] Updating cache with info: [VIF({'ovs_interfaceid': None, 'network': Network({'bridge': u'br100', 'subnets': [Subnet({'ips': [FixedIP({'meta': {}, 'version': 4, 'type': u'fixed', 'floating_ips': [], 'address': u'10.1.0.2'})], 'version': 4, 'meta': {u'dhcp_server': u'10.1.0.1'}, 'dns': [IP({'meta': {}, 'version': 4, 'type': u'dns', 'address': u'8.8.4.4'})], 'routes': [], 'cidr': u'10.1.0.0/24', 'gateway': IP({'meta': {}, 'version': 4, 'type': u'gateway', 'address': u'10.1.0.1'})}), Subnet({'ips': [], 'version': None, 'meta': {u'dhcp_server': None}, 'dns': [], 'routes': [], 'cidr': None, 'gateway': IP({'meta': {}, 'version': None, 'type': u'gateway', 'address': None})})], 'meta': {u'tenant_id': None, u'should_create_bridge': True, u'bridge_interface': u'eth0'}, 'id': u'9751787e-f41c-4299-be13-941c901f6d18', 'label': u'private'}), 'devname': None, 'qbh_params': None, 'meta': {}, 'details': {}, 'address': u'fa:16:3e:d8:87:38', 'active': False, 'type': u'bridge', 'id': u'db1ac48d-805a-45d3-9bb9-786bb5855673', 'qbg_params': None})] update_instance_cache_with_nw_info /opt/stack/new/nova/nova/network/api.py:74
2014-04-08 11:14:27.661 2894 DEBUG nova.virt.driver [-] Emitting event <nova.virt.event.LifecycleEvent object at 0x4932e50> emit_event /opt/stack/new/nova/nova/virt/driver.py:1207
2014-04-08 11:14:27.661 2894 INFO nova.compute.manager [-] Lifecycle event 1 on VM 75da98d7-bbd5-42a2-ad6f-7a66e38977fa
2014-04-08 11:14:27.773 2894 ERROR nova.virt.driver [-] Exception dispatching event <nova.virt.event.LifecycleEvent object at 0x4932e50>: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 597, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 500, in refresh
    self.info_cache.refresh()
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 151, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 103, in refresh
    self.instance_uuid)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 112, in wrapper
    result = fn(cls, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_info_cache.py"", line 70, in get_by_instance_uuid
    instance_uuid=instance_uuid)
InstanceInfoCacheNotFound: Info cache for instance 75da98d7-bbd5-42a2-ad6f-7a66e38977fa could not be found.
2014-04-08 11:14:27.840 2894 INFO nova.virt.libvirt.driver [-] [instance: 75da98d7-bbd5-42a2-ad6f-7a66e38977fa] Instance destroyed successfully.
Raw logs for a failure: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=TRACE
Specific failure point: http://logs.openstack.org/38/62038/14/check/check-tempest-dsvm-full/86cde16/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-04-08_11_14_25_928","Read deleted instances during lifecycle events

This prevents us from failing to find the associated instance if
the lifecycle event happens after the instance has been deleted,
as in the case of a ... delete.

Change-Id: I819ddf8ff68edc407d7932bf43771c440f514f26
Closes-bug: #1304968"
631,53d055d5b0924f1233ca370d94542e918eda095e,1386293511,,1.0,6,2,1,1,1,0.0,True,2.0,962704.0,17.0,6.0,False,5.0,7299278.0,12.0,3.0,17.0,677.0,679.0,17.0,508.0,510.0,15.0,508.0,509.0,0.015281757,0.486150907,0.487106017,1746,1250228,1250228,glance,53d055d5b0924f1233ca370d94542e918eda095e,1,1,"“On an forbidden update, the message returned to the user is not tied to the type of object modified by the operation.""","Bug #1250228 in Glance: ""Forbidden update to image member says 'image' instead of 'image member' in error message""","Incorrectly says 'image' instead of 'image member'
curl -i -X PUT -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" https://localhost/v2/images/ef4570bf-2e26-4921-810a-5f8499e9822f/members/5855250 -d '{""status"": ""accepted""}'
HTTP/1.1 403 Forbidden
Content-Type: text/html;charset=UTF-8
Via: 1.1 Repose (Repose/2.12)
Content-Length: 177
Date: Fri, 08 Nov 2013 19:13:37 GMT
x-openstack-request-id: req-11c02e0c-d5bd-4f17-ab03-b474b071b3f0
Server: Jetty(8.0.y.z-SNAPSHOT)
<html>
<head>
<title>403 Forbidden</title>
</head>
<body>
<h1>403 Forbidden</h1>
You are not permitted to modify 'status' on this image.<br /><br /> <------------------this should say 'image member'
The issue is in ImmutableMemberProxy in glance/api/authorization.py where it uses the  _immutable_attr function.","Forbidden update message diffs images/tasks/member

On an forbidden update, the message returned to the user
is not tied to the type of object modified by the operation.
For example: a forbidden image member update will return
`You are not permitted to modify 'status' on this image.`
instead of image_member.

This patch fixes the issue for images, image members and tasks.

Change-Id: Ie9774807f0190a517a619aeb56c1398741ac4407
Closes-Bug: #1250228"
632,53e784d27eafe848593467d4d598305f34030848,1396886086,,1.0,0,10,2,2,1,0.970950594,True,2.0,833896.0,61.0,9.0,False,4.0,7239823.0,3.0,4.0,16.0,2470.0,2486.0,8.0,2078.0,2086.0,0.0,578.0,578.0,0.000906618,0.524932004,0.524932004,792,1219,1307472,neutron,53e784d27eafe848593467d4d598305f34030848,0,0,"Obsolete code “Remove workaround for bug #1219530""","Bug #1307472 in neutron: ""workaround for bug #1219530 is obsolete""",Bug #1219530 has been fixed and workaround for it stands in a way of putting together support for rootwrap daemon mode.,"Remove workaround for bug #1219530

Bug #1219530 has been fixed before Havana. We can remove this workaround.

Closes-Bug: #1307472
Change-Id: Ib0b1abf7d11627045be922f79aff6b80448ccbf4"
633,53fe8696314fb73ca9943fce998d96fa6d0414b4,1411972264,,1.0,10,1,2,2,1,0.945660305,False,,,,,True,,,,,,,,,,,,,,,,,392,805,1279172,nova,53fe8696314fb73ca9943fce998d96fa6d0414b4,1,1,Bug encoding text,"Bug #1279172 in OpenStack Compute (nova): ""Unicode encoding error exists in extended Nova API, when the data contain unicode""","We have developed an extended Nova API, the API query disks at first, then add a disk to an instance.
After querying, if disk has non-english disk name, unicode will be converted to str in nova/api/openstack/wsgi.py line 451
""node = doc.createTextNode(str(data))"", then unicode encoding error exists.","Fix XML UnicodeEncode serialization error

The generic Nova XMLSerializer code will currently attempt
to cast to str the value for all leaf nodes. This patch
ensures that no attempt is made to convert unicode which
can cause a UnicodeEncode error. We don't need to convert
unicode for XML text and regardless we encode to UTF-8 at
a later point.

Change-Id: I8135d2b9a67db62b0eafdd301b7fdb67a5dd72cc
Closes-Bug: #1279172"
634,541cc9d53ac999535edcb7a02d1a76a883cfb02b,1399324428,,1.0,66,41,6,2,1,0.730418555,True,5.0,501126.0,42.0,10.0,False,18.0,169720.6667,87.0,2.0,2.0,1576.0,1576.0,2.0,1278.0,1278.0,2.0,1418.0,1418.0,0.001867995,0.883561644,0.883561644,1840,1315195,1315195,Cinder,541cc9d53ac999535edcb7a02d1a76a883cfb02b,0,0,"""In order to eliminate the need to have the hp3parclient in the global-requirements""","Bug #1315195 in Cinder: ""remove hp3parclient requirement from unit tests""","In order to eliminate the need to have the hp3parclient in the global-requirements project, we need to remove the hp3parclient from being imported in all 3par driver unit tests in cinder.
It should _at least_ be optional for the tests.","eliminate the need for hp3parclient in tests

In order to eliminate the need to have the hp3parclient in the
global-requirements project, we need to remove the hp3parclient
from being imported in all 3par driver unit tests in cinder.

Closes-Bug: #1315195
Change-Id: Ife5c70871e742be5970be8f0284e12554f93cab4"
635,5427630f735381569066815605c75bccd278def8,1397700190,,1.0,2,2,1,1,1,0.0,True,1.0,23945.0,12.0,3.0,False,12.0,14059.0,50.0,3.0,27.0,3403.0,3414.0,20.0,2590.0,2599.0,21.0,3326.0,3331.0,0.002813299,0.42544757,0.426086957,806,1234,1308811,nova,5427630f735381569066815605c75bccd278def8,1,1,,"Bug #1308811 in OpenStack Compute (nova): ""nova.objects.base imports conductor wrong""","in nova.objects.base it imports conductor
from nova.conductor import api as conductor_api
self._conductor = conductor_api.API()
This bypasses the logic to detemin whether to use conductor RPC service or not.
Should do
from nova import conductor
self._conductor = conductor.API()","Fix up import of conductor

Need to import API from nova.conductor not from nova.conductor.api
directly. This is needed so the is_local flag can be used to
determine the conductor API to use (local or rpc)

Change-Id: I9a45a705a39050b296936edc486db5c2f0aae4dc
Closes-Bug: #1308811"
636,544745f9da3245a71d771cff26dc6d0255bb0470,1392751958,,1.0,4,4,4,4,1,1.0,True,3.0,3240846.0,43.0,14.0,False,122.0,311449.5,356.0,5.0,38.0,3102.0,3110.0,37.0,2509.0,2517.0,33.0,2946.0,2949.0,0.00460892,0.399484885,0.399891555,429,844,1281904,nova,544745f9da3245a71d771cff26dc6d0255bb0470,1,1,Wrong exception type raised,"Bug #1281904 in OpenStack Compute (nova): ""Wrong exception type HTTPBadRequest is raised""","When user without admin permission wants to get a list of servers
which are in 'deleted' state, currently it raises HTTPBadRequest.
The code is:
 231         if search_opts.get(""vm_state"") == ['deleted']:
 232             if context.is_admin:
 233                 search_opts['deleted'] = True
 234             else:
 235                 msg = _(""Only administrators may list deleted instances"")
 236                 raise exc.HTTPBadRequest(explanation=msg)
This should be changed to HTTPForbidden exception.","Change exception type from HTTPBadRequest to HTTPForbidden

When user without admin permission wants to get a list of servers
which are in 'deleted' state, currently it raises  HTTPBadRequest.
This should be changed to HTTPForbidden exception.

Change-Id: I29da5f055c69e8aba66ffb068a3601ae69b2fc94
Closes-Bug: #1281904"
637,545a4976fc7896e8afee8c70661d109c464c8f44,1385458312,,1.0,14,3,2,2,1,0.672294817,True,10.0,2478676.0,43.0,8.0,False,41.0,6683.0,67.0,3.0,15.0,1236.0,1248.0,15.0,1064.0,1076.0,5.0,1180.0,1182.0,0.000895656,0.176294969,0.176593521,83,480,1255001,nova,545a4976fc7896e8afee8c70661d109c464c8f44,1,1,Bad code,"Bug #1255001 in OpenStack Compute (nova): ""Fix exception for os-migrateLive ""","Several exception is not correct in os-migrateLive action:
1.If the server state conflict to live-migrate, it's raises 400 exception.I think we should raise HTTPConflict instead of HTTPBadRequest.
2.the error msg is not accurate while several exception such as :
                exception.NoValidHost,
                exception.InvalidLocalStorage,
                exception.InvalidSharedStorage,
                exception.MigrationPreCheckError","Fix incorrect exception on os-migrateLive

If the server state is conflict to live-migrate, it raises
400(HTTPBadRequest)exception.We should raise HTTPConflict instead of
HTTPBadRequest.

Change-Id: I40ccb0f268716da20ae4b3de9d295a41a2dba18d
Closes-Bug: #1255001"
638,54d79acaab50993f08b50f21019f82f17d38caa3,1383263611,2.0,1.0,26,16,2,2,1,0.591672779,True,13.0,2321137.0,64.0,14.0,False,24.0,32281.0,40.0,4.0,184.0,1244.0,1305.0,169.0,1148.0,1195.0,121.0,432.0,449.0,0.236434109,0.839147287,0.872093023,1682,1239637,1239637,neutron,54d79acaab50993f08b50f21019f82f17d38caa3,1,1, ,"Bug #1239637 in neutron: ""internal neutron server error on tempest VolumesActionsTest""","Logstash query:
@message:""DBError: (IntegrityError) null value in column \""network_id\"" violates not-null constraint"" AND @fields.filename:""logs/screen-q-svc.txt""
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/logs/screen-q-svc.txt.gz#_2013-10-14_10_13_01_431
http://logs.openstack.org/22/51522/2/check/check-tempest-devstack-vm-neutron-pg-isolated/015b3d9/console.html
2013-10-14 10:16:28.034 | ======================================================================
2013-10-14 10:16:28.034 | FAIL: tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | tearDownClass (tempest.api.volume.test_volumes_actions.VolumesActionsTest)
2013-10-14 10:16:28.035 | ----------------------------------------------------------------------
2013-10-14 10:16:28.035 | _StringException: Traceback (most recent call last):
2013-10-14 10:16:28.035 |   File ""tempest/api/volume/test_volumes_actions.py"", line 55, in tearDownClass
2013-10-14 10:16:28.036 |     super(VolumesActionsTest, cls).tearDownClass()
2013-10-14 10:16:28.036 |   File ""tempest/api/volume/base.py"", line 72, in tearDownClass
2013-10-14 10:16:28.036 |     cls.isolated_creds.clear_isolated_creds()
2013-10-14 10:16:28.037 |   File ""tempest/common/isolated_creds.py"", line 453, in clear_isolated_creds
2013-10-14 10:16:28.037 |     self._clear_isolated_net_resources()
2013-10-14 10:16:28.037 |   File ""tempest/common/isolated_creds.py"", line 445, in _clear_isolated_net_resources
2013-10-14 10:16:28.038 |     self._clear_isolated_network(network['id'], network['name'])
2013-10-14 10:16:28.038 |   File ""tempest/common/isolated_creds.py"", line 399, in _clear_isolated_network
2013-10-14 10:16:28.038 |     net_client.delete_network(network_id)
2013-10-14 10:16:28.038 |   File ""tempest/services/network/json/network_client.py"", line 76, in delete_network
2013-10-14 10:16:28.039 |     resp, body = self.delete(uri, self.headers)
2013-10-14 10:16:28.039 |   File ""tempest/common/rest_client.py"", line 308, in delete
2013-10-14 10:16:28.039 |     return self.request('DELETE', url, headers)
2013-10-14 10:16:28.040 |   File ""tempest/common/rest_client.py"", line 436, in request
2013-10-14 10:16:28.040 |     resp, resp_body)
2013-10-14 10:16:28.040 |   File ""tempest/common/rest_client.py"", line 522, in _error_checker
2013-10-14 10:16:28.041 |     raise exceptions.ComputeFault(message)
2013-10-14 10:16:28.041 | ComputeFault: Got compute fault
2013-10-14 10:16:28.041 | Details: {""NeutronError"": ""Request Failed: internal server error while processing your request.""}","Fix DB integrity issues when using postgres

The IntegrityError faced on network deletion that
spooks Tempest tests is most likely caused by the
fact that the ml2 plugin does a dirty read. By
adding a 'select for update' we should be able to
address the issue once and for all.

Also, there's a chance that the dhcp port gets
reallocated while we are deleting the network. To
this aim, catch the integrity error and attempt
to recover from it.

This patch also removes the handling of errors
on concurrent reads that can no longer occur when
deleting the network.

(Hopefully) Closes-bug: #1239637

PS: This fix may be relevant for bugs #1243726 and #1246737

Change-Id: Iefae7fc8a903cc35d8bf80fc4cee533b7f64032c"
639,54ffa26ce8e13ce0f77baae7b29bb793eb98f845,1403835118,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1015,1458,1334922,neutron,54ffa26ce8e13ce0f77baae7b29bb793eb98f845,0,0,Bug in test,"Bug #1334922 in neutron: ""agent functional tests reference incorrect MAX_LEN constant""","File ""neutron/neutron/tests/functional/agent/linux/base.py"", line 59, in create_resource
    name = self.get_rand_name(n_const.DEV_NAME_MAX_LEN, name_prefix)
AttributeError: 'module' object has no attribute 'DEV_NAME_MAX_LEN'","Use correct MAX_LEN constant in agent functional tests

Closes-bug: #1334922

Change-Id: I62bd12da21e778f5cca97d6280c107575d912a81"
640,551cad4c7bf7043c8942319fc46e58d851051b2c,1394662728,1.0,1.0,47,15,2,2,1,0.823811633,True,6.0,328155.0,35.0,8.0,False,4.0,760083.0,7.0,2.0,66.0,328.0,336.0,66.0,323.0,331.0,59.0,289.0,292.0,0.050420168,0.243697479,0.246218487,601,1025,1291646,glance,551cad4c7bf7043c8942319fc46e58d851051b2c,0,0,Refactoring code,"Bug #1291646 in Glance: ""Make the VMware datastore backend more robust""","Several issues to address:
- need better error handling for the add,get,get_size,delete operations: need to catch exception when httplib call fails, also need to log when the response is not expected.
- need to handle cases where the store_image_dir contains non expected characters. It should support the following use cases:
/openstack_glance
openstack_glance
openstack_glance/
openstack glance  -> this one should fail with logging
openstack+glance
etc.
- need to quote special characters","Make the VMware datastore backend more robust

This patch contains several fixes related to invalid configurations
and corner cases:
- better error handling for the store operations
- better logging when response is not successful
- better support for various store_image_dir formats

Change-Id: I6c0003e29d621c1f71490cb9fce163dc840a113f
Closes-Bug: #1291646"
641,5529071bf1393d0d448bc495cc906a68bc30a820,1385571476,,1.0,93,2,3,3,1,0.919683038,True,6.0,7218163.0,36.0,23.0,False,4.0,1483622.667,5.0,9.0,71.0,1741.0,1780.0,71.0,1568.0,1607.0,12.0,485.0,486.0,0.02233677,0.835051546,0.836769759,1801,1254246,1254246,neutron,5529071bf1393d0d448bc495cc906a68bc30a820,1,1, ,"Bug #1254246 in neutron: ""somehow getting duplicate openvswitch agents for the same host""","While investigating spurious failures in our TripleO continous deployment, I had this problem:
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| id                                   | agent_type         | host                                | alive | admin_state_up |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
| 3a9c6aca-e91f-49c9-850a-67db219fdf58 | L3 agent           | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| 3fb9f6cf-b545-4a34-a490-dda834973d1e | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx   | True           |
| 855349b2-b0fc-4270-bb96-385b61aa5a6c | DHCP agent         | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| 8b8a4128-9716-42ee-b886-f053db166ce3 | Metadata agent     | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| c8297e0d-8575-47f0-ae65-499c1e0319b3 | Open vSwitch agent | overcloud-notcompute-wjo2jbvvd2sm   | :-)   | True           |
| f746fc1d-9083-46f4-a922-739c5d332d7c | Open vSwitch agent | overcloud-novacompute0-ubrjpv4jz64a | xxx   | True           |
+--------------------------------------+--------------------+-------------------------------------+-------+----------------+
Note that overcloud-novacompute0-ubrjpv4jz64a has _two_ Open vSwitch agents.
This caused many 'vif_type=binding_failed' errors when booting nova instances.
Deleting f746fc1d-9083-46f4-a922-739c5d332d7c resulted in the problem going away.
Seems like there might be a race if the agent restarts quickly, thus not seeing its own agent record and sending a second RPC to create one. I think, I am not entirely sure how this works, that is just a hypothesis.","Fix a race condition in agents status update code

Code handling agents status updates coming via RPC checks,
if a corresponding entry for the given (agent_type, host)
pair already exists in DB and updates it. And if it doesn't
exist, a new entry is created.

Without a unique constraint this can cause a race condition
resulting in adding of two agent entries having the same value
of (agent_type, host) pair.

Note, that it's already not allowed to have multiple agents of
the same type having the same host value, but currently it's
enforced only at code level, not at DB schema level, which
effectively makes race conditions possible.

Closes-Bug: #1254246

Change-Id: I1ebaa111154b3d6b34074705b579097ab730594c"
642,55296828a7d7a03736a21d846da15571ca22ec9e,1388562029,0.0,1.0,120,10,6,2,1,0.706156529,True,22.0,2483531.0,121.0,54.0,False,56.0,890979.6667,144.0,9.0,1595.0,3810.0,3810.0,1386.0,3156.0,3156.0,734.0,2783.0,2783.0,0.105801065,0.400748525,0.400748525,245,649,1265512,nova,55296828a7d7a03736a21d846da15571ca22ec9e,1,1,,"Bug #1265512 in OpenStack Compute (nova): ""VMware: unnecesary session termination""","In some cases, the session with the VC is terminated and restarted again. This can happen for example when the user does:
nova list (and there are no running VMs)
In addition to the restart of the session the operation also waits 2 seconds.","VMware: unnecessary session reconnection

An error FAULT_NOT_AUTHENTICATED could imply that there is a problem
with the backend or that no values were found (for example there are
no running VMS). In the latter case the connection should not
be restarted.

The patch also ensures that the session is restarted if there are
connection issues.

This issue occasionally causes CI issues with minesweeper.

Closes-bug: #1265512

Co-authored-by: Shawn Harsock <hartsocks@vmware.com>

Change-Id: Ibfa83d0b5b93680890aad8c88c041fa344765448"
643,55937308c2020cbc185f656779f5876eae6ca29c,1386820811,,1.0,11,5,5,5,1,0.936202523,True,4.0,516172.0,23.0,9.0,False,49.0,106557.8,175.0,3.0,3.0,208.0,208.0,3.0,208.0,208.0,3.0,175.0,175.0,0.003755869,0.165258216,0.165258216,1759,1251055,1251055,glance,55937308c2020cbc185f656779f5876eae6ca29c,1,1, ,"Bug #1251055 in Glance: ""A GET on imagedata in glance v2 returns 404 instead of 204 when data does not exist.""","A GET on imagedata in glance v2 returns 404 instead of 204 when data does not exist.
See http://docs.openstack.org/api/openstack-image-service/2.0/content/get-image-file.html
curl -i -X GET -H ""Content-Type: application/json"" -H ""X-Auth-Token: $AUTH_TOKEN"" http://localhost:9292/v2/images/e3a8964d-b320-450d-a729-1735dac05ba9/file
HTTP/1.1 404 Not Found
Content-Length: 150
Content-Type: text/html; charset=UTF-8
X-Openstack-Request-Id: req-fcc598de-6012-4576-a74b-833bfd6e1299
Date: Wed, 13 Nov 2013 20:51:31 GMT
<html>
 <head>
  <title>404 Not Found</title>
 </head>
 <body>
  <h1>404 Not Found</h1>
  No image data could be found<br /><br />
 </body>
</html>","Return 204 when image data does not exist

Currently a GET on image-data in glance v2 returns 404 when data does not
exist. But according to the API docs the HTTP status code returned in this
case should be 204.

This fix makes Glance return the right HTTP status code.

Change-Id: I34cf68e96f631f05cff349329245f74aee42397c
Closes-Bug: #1251055"
644,55bfb412f1732e97da76f1b34e581d2429df38e6,1377778086,1.0,1.0,2,1,1,1,1,0.0,True,1.0,57584.0,8.0,4.0,False,3.0,1900335.0,3.0,4.0,295.0,770.0,933.0,286.0,732.0,890.0,43.0,222.0,228.0,0.17254902,0.874509804,0.898039216,1494,1218338,1218338,neutron,55bfb412f1732e97da76f1b34e581d2429df38e6,1,1, ,"Bug #1218338 in neutron: ""alembic autogenerate detects table deletion of service plugins""","When I run neutron-db-manage revision --autogenerate, it detects lbaas table deletion
even when loadbalancer service plugins are declared in neutron.conf.
The reason is table definitions of service plugins are not loaded when ""revision --autogenerate"" is run.","Load tables of service plugins when migration auto generation

When we run ""neutron-db-manage revision --autogenerate"", it detects
table deletion of service plugins even when they are configured.
We need to load tables of service plugins to teach alembic such
tables exist.

Change-Id: I9186d36496069c8253683bd200326b80a6565fa6
Closes-Bug: #1218338"
645,55f6a8ac5d234f004ef06add87d16284e9f048d3,1411378086,,1.0,45,1,2,2,1,0.347816914,False,,,,,True,,,,,,,,,,,,,,,,,1342,1810,1372337,neutron,55f6a8ac5d234f004ef06add87d16284e9f048d3,1,1,“The patch fixes a regression introduced with secgroup rpc refactor by”,"Bug #1372337 in neutron: ""KeyError in neutron server when L2 agent requests info for devices""","if L2 agent uses enhanced-security-group-rpc, in bellow case there will be a KeyError in neutron server:
1. Create security group with IPv6 ingress rule but no IPv4 ingress rule.
  (or delete IPv4 ingress rule from default security group)
2. Launch a VM on an IPv4 subnet, making it member of sec group created earlier
Instance will not get its network info. Neutron server starts reporting following errors and sends them to agent on each request for devices info:
2014-09-24 02:01:51.353 ERROR oslo.messaging.rpc.dispatcher [req-9b631b65-a753-4292-8442-98936a31db74 None None] Exception during message handling: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/securitygroups_rpc.py"", line 75, in security_group_info_for_devices
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self.plugin.security_group_info_for_ports(context, ports)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 201, in security_group_info_for_ports
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     return self._get_security_group_member_ips(context, sg_info)
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/db/securitygroups_rpc_base.py"", line 209, in _get_security_group_member_ips
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher     if ip not in sg_info['sg_member_ips'][sg_id][ethertype]:
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher KeyError: 'IPv4'
2014-09-24 02:01:51.353 TRACE oslo.messaging.rpc.dispatcher
2014-09-24 02:01:51.354 ERROR oslo.messaging._drivers.common [req-9b631b65-a753-4292-8442-98936a31db74 None None] Returning exception 'IPv4' to caller","Fix KeyError when getting secgroup info for ports

The patch fixes a regression introduced with secgroup rpc refactor by
handling the case when security group contains rules for only IPv4 or
IPv6.

Change-Id: I02b174757bfc796a81cdb482c55ba7f9e954131d
Closes-bug: #1372337"
646,56440eb95da79506cc27d92e07f0f5969cc683ce,1379526572,1.0,1.0,60,3,2,2,2,0.764204507,True,6.0,1751247.0,19.0,4.0,False,5.0,6763443.5,6.0,2.0,5.0,423.0,425.0,5.0,384.0,386.0,5.0,405.0,407.0,0.008534851,0.577524893,0.580369844,1446,1201875,1201875,swift,56440eb95da79506cc27d92e07f0f5969cc683ce,1,1, ,"Bug #1201875 in OpenStack Object Storage (swift): ""POST &  X-Copy-From can result in over quota with container quotas""","Bug  #1200271 reported a bug when using X-Copy-From together with account_quotas resulting in an over-quota.
This also affects container_quotas; additionally container_quotas doesn't check POST operations, for example from formpost middleware.","Handle X-Copy-From header in container_quota middleware

Content length of the copied object is checked before
allowing the copy request according to the container quota.

Closes-Bug: #1201875
Change-Id: If44b916791e94ac6c66eee04a5727186ce0e56ae"
647,56440eb95da79506cc27d92e07f0f5969cc683ce,1379526572,1.0,1.0,60,3,2,2,2,0.764204507,True,6.0,1751247.0,19.0,4.0,False,5.0,6763443.5,6.0,2.0,5.0,423.0,425.0,5.0,384.0,386.0,5.0,405.0,407.0,0.008534851,0.577524893,0.580369844,1726,1245694,1245694,Swift,56440eb95da79506cc27d92e07f0f5969cc683ce,0,0,"There is no BUG: “This will probably be added with a currently-in-progress ""feature discoverability"" patch, but it does not exist now.” THE LINK IS WRONG!! It should be:","Bug #1245694 in OpenStack Object Storage (swift): ""Report swift server version""","Currently, there is no easy way to get the Swift server version. If I'm not wrong, the only way to do this is to run the following on the swift server/s.
python -c ""import swift; print swift.__version__""
Instead there should be some way for a client to request for the server version.","Handle X-Copy-From header in container_quota middleware

Content length of the copied object is checked before
allowing the copy request according to the container quota.

Closes-Bug: #1201875
Change-Id: If44b916791e94ac6c66eee04a5727186ce0e56ae"
648,5652e20b0bdc3a707d11d2b9ee521066b8f1150f,1381876787,0.0,1.0,31,2,4,3,1,0.956032798,True,7.0,4504309.0,46.0,19.0,False,21.0,803120.0,30.0,6.0,16.0,439.0,445.0,16.0,381.0,387.0,13.0,143.0,147.0,0.030837004,0.317180617,0.325991189,1620,1233271,1233271,neutron,5652e20b0bdc3a707d11d2b9ee521066b8f1150f,1,1, ,"Bug #1233271 in neutron: ""Floating IPs momentarily removed from router on agent restart""","I noticed that floating IPs would be removed momentarily from the router on agent restart.  This would cause a momentarily outage in network connectivity.  This should be avoided.
I noticed this when testing with this change:  https://review.openstack.org/#/c/30988/ which prevents routers from being destroyed and re-added.","Preserve floating ips when initializing l3 gateway interface

Change-Id: I5a88225d291538cb9db0f8f4afa348192b8b984d
Closes-Bug: #1233271"
649,570a84e92c88b1c7d9fd48d107bc3fad75e32efe,1396846209,,1.0,14,12,1,1,1,0.0,True,2.0,129557.0,21.0,9.0,False,86.0,380990.0,175.0,4.0,440.0,3767.0,4147.0,213.0,3175.0,3330.0,148.0,2828.0,2918.0,0.019235735,0.365220759,0.376839659,729,1155,1301602,nova,570a84e92c88b1c7d9fd48d107bc3fad75e32efe,0,0,Bug in test,"Bug #1301602 in OpenStack Compute (nova): ""nova unit tests fail test_get_domain_info_with_more_return""","Regression caused by https://github.com/openstack/nova/commit/23158ad8b340ed5c53fe6ad0fe582f47467c9127
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 6447, in test_get_domain_info_with_more_return
    mock_domain = libvirt.virDomain('qemu:///system', None)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 213, in __init__
    self._def = self._parse_definition(xml)
  File ""nova/tests/virt/libvirt/fakelibvirt.py"", line 220, in _parse_definition
    tree = etree.fromstring(xml)
  File ""lxml.etree.pyx"", line 3032, in lxml.etree.fromstring (src/lxml/lxml.etree.c:68106)
  File ""parser.pxi"", line 1784, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:102444)
ValueError: can only parse strings
I think the domain should be mocked and shouldn't be created directly.
I'm facing this issue with py26 on OEL 6.3, if it matter.","Correct test boundary for libvirt_driver.get_info

The test_get_domain_info_with_more_return() unit test was causing
failure on some systems where lxml was not capable of handling None for
the xml to parse. However, our unit tests should not be testing libvirt;
the unit tests in libvirt already do that. Our unit tests should simply
test the code boundaries of the unit of code that they wish to stress,
and not anything further.

This patch changes the unit test in question to only test the code
within the libvirt_driver.LibvirtDriver.get_info() method, and nothing
more, and in doing so, we remove any calls at all to libvirt and mock
out where the code passes the boundaries of the get_info() method.

Change-Id: I022cc331a980c2a4d98f16d2d77b24839b528464
Closes-bug: #1301602"
650,571ac18e7c4109fc9728a0ee27f04887f1d8aeea,1401199913,,1.0,1,2,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,925,1361,1323741,cinder,571ac18e7c4109fc9728a0ee27f04887f1d8aeea,1,0,"“The 'num_iscsi_scan_tries' option was renamed to 'num_volume_device_scan_tries', but setting this ISER option still sets the old configuration field which is never read.”","Bug #1323741 in Cinder: ""LVM iSER driver scan_tries not set correctly""","The LVMISERDriver takes an option 'num_iser_scan_tries', but it appears that this is not being processed correctly.
The 'num_iscsi_scan_tries' option was renamed to 'num_volume_device_scan_tries', but setting this ISER option still sets the old configuration field which is never read.
(I have not tested this behavior, just found via code inspection.)","Fix ISER scan retry option

The 'num_iscsi_scan_tries' option was renamed to
'num_volume_device_scan_tries'.

Remove it from the test where the deprecated name is
referenced since it is not used there anyway.

Fix the ISER driver to set the new config field rather than the
old one.  Presumably this made the num_iser_scan_tries option
not work since only the new field is passed to the brick
connector.

Closes-Bug: #1323741

Change-Id: I13ac4cce10894b541054fbda719bbc5fd555e3c1"
651,57e8cdb9dd8e24919bc2243438a6bded53491b3e,1396643802,,1.0,44,3,2,2,1,0.488908591,True,3.0,1804455.0,21.0,9.0,False,5.0,2663881.0,10.0,6.0,19.0,1579.0,1580.0,19.0,1301.0,1302.0,19.0,1416.0,1417.0,0.012795905,0.906589891,0.907229687,692,1118,1298608,cinder,57e8cdb9dd8e24919bc2243438a6bded53491b3e,1,0,"Bug , HP LeftHand driver fails with Paramiko 1.13.0","Bug #1298608 in Cinder: ""HP LeftHand driver fails with Paramiko 1.13.0""","When the HP LeftHand driver is configured in legacy mode it will fail with the following exception, if paramiko 1.13.0 is installed:
2014-03-27 13:33:22.189 DEBUG cinder.openstack.common.lockutils [req-c2080e55-ec3e-40e3-a7a6-329e48d22295 None None] Released file lock ""lefthand"" at /opt/stack
/data/cinder/cinder-lefthand for method ""get_volume_stats""... from (pid=35801) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:239
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/semaphore.py"", line 121, in _do_acquire
    waiter.switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/openstack/common/service.py"", line 483, in run_service
    service.start()
  File ""/opt/stack/cinder/cinder/service.py"", line 103, in start
    self.manager.init_host()
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 308, in init_host
    self.publish_service_capabilities(ctxt)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1105, in publish_service_capabilities
    self._report_driver_status(context)
  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1094, in _report_driver_status
    volume_stats = self.driver.get_volume_stats(refresh=True)
  File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_iscsi.py"", line 121, in get_volume_stats
    data = self.proxy.get_volume_stats(refresh)
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 421, in get_volume_stats
    self._update_backend_status()
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 436, in _update_backend_status
    'clusterName': self.configuration.san_clustername})
  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_lefthand_cliq_proxy.py"", line 109, in _cliq_run_xml
    result_xml = etree.fromstring(out)
  File ""lxml.etree.pyx"", line 3003, in lxml.etree.fromstring (src/lxml/lxml.etree.c:67314)
  File ""parser.pxi"", line 1724, in lxml.etree._parseMemoryDocument (src/lxml/lxml.etree.c:101147)
ValueError: Unicode strings with encoding declaration are not supported. Please use bytes input or XML fragments without declaration.
This bug also exists with the Havana version of the driver. cinder.volume.drivers.san.hp_lefthand.HpSanISCSIDriver
Workaround: install paramiko 1.10.0","Fixes HP LeftHand driver with Paramiko 1.13.0

With Paramiko 1.13.0, the method exec_command now returns Unicode.
This causes a problem when the driver tries to build the XML returned
from the LeftHand array. The XML header returned from the array defines
the encoding as encoding=UTF-8. Therefore, we must now ensure the
encoding passed to the parser is utf-8.

Change-Id: I7b504626e2d9a0ee2b62820b11f56eb136e31987
closes-bug: #1298608"
652,57fd92a3943aa4856de6ceb18a7b704ed7fae7c6,1388114197,,1.0,27,19,4,2,1,0.710514751,True,4.0,12370508.0,21.0,4.0,False,21.0,167973.0,40.0,1.0,54.0,11.0,65.0,54.0,11.0,65.0,49.0,1.0,50.0,0.039032006,0.00156128,0.039812646,232,635,1264424,cinder,57fd92a3943aa4856de6ceb18a7b704ed7fae7c6,0,0,Feature: Some exc.HTTPError subclass needs keyword argument ``explanation`` ,"Bug #1264424 in Cinder: ""Some exc.HTTPError subclass needs keyword argument ``explanation``""","Ex.
  NG: raise webob.exc.HTTPBadRequest('What?')
  OK: raise webob.exc.HTTPBadRequest(explanation='What?')
If there is no ``explanation=``, the message of the argument is
recognized as ``detail``.","Add keyword argument missing at some exc.HTTPError subclass

Some exc.HTTPError subclass needs keyword argument ``explanation``.
If there is no ``explanation=``, the message of the argument is
recognized as ``detail``.

Change-Id: Idfc07b2bd71da31e44212d3296921e18d2024a63
Closes-Bug: #1264424"
653,5856c3e585891103767eda87035dee8ecaee32ab,1403172472,,1.0,23,10,3,3,1,0.968235622,False,,,,,True,,,,,,,,,,,,,,,,,410,825,1280132,nova,5856c3e585891103767eda87035dee8ecaee32ab,1,1,Option format not correctly set,"Bug #1280132 in OpenStack Compute (nova): ""--ephemeral option's format was not correctly set ""","novaclient has following option
--ephemeral size=<size>[,format=<format>]
                        Create and attach a local ephemeral block device of
                        <size> GB and format it to <format>.
so
nova boot --flavor 21 --key_name mykey --image 43ca519b-979b-4803-95ad-b9f160f1a337 --security_group default  --ephemeral size=1 --ephemeral size=2,format=ext4 test12
should work
however, the eph disk created is ext3 ,ignore the option specified by format","Format eph disk with  specified format in libvirt

novaclient has following command parameters:
--ephemeral size=<size>[,format=<format>]
Create and attach a local ephemeral block device of
<size> GB and format it to <format>.

so mkfs should use the specified format instead of using
default format.

Change-Id: I7a8753284d7b1da1a1203e85b430bd0c5012937a
Closes-Bug: #1280132"
654,585f34ff7798ad271121d432e0cae8820bc54389,1384722487,,1.0,24,9,4,2,1,0.890056989,True,2.0,5907558.0,17.0,8.0,False,18.0,2414044.5,48.0,9.0,32.0,1449.0,1450.0,31.0,1283.0,1284.0,32.0,1140.0,1141.0,0.028423773,0.982773471,0.983634798,1791,1253660,1253660,cinder,585f34ff7798ad271121d432e0cae8820bc54389,1,1, ,"Bug #1253660 in Cinder: ""NetApp cinder driver reports 0 free space for the first 60 seconds""","The NetApp driver (in clustered mode) collects free space in an asynchronous background job, which results in the free space being reported as 0 until the job has run at least once (takes 60 seconds). The driver should be changed to report the correct free space immediately upon starting up.","NetApp fix free space as zero during 1st vol stats update

NetApp clustered ontap drivers report space as zero
till first 60 seconds of driver start. This is causing
discomfort for some performance sensitive deployements.
This was due to async nature of the NetApp stats collection job.
Job is changed to sync at driver start to improve customer
experience.

Change-Id: I7d5cbf590897a0d328ece3a60516c92c0ad0ee7f
Closes-bug: #1253660"
655,587a7e35aa7f0c1cccfa366b795424ba55808e1a,1379452925,,1.0,23,24,1,1,1,0.0,True,3.0,90553.0,14.0,6.0,False,7.0,998265.0,16.0,2.0,28.0,382.0,405.0,25.0,371.0,391.0,17.0,363.0,375.0,0.017839445,0.360753221,0.372646184,1571,1226830,1226830,cinder,587a7e35aa7f0c1cccfa366b795424ba55808e1a,1,1,“Wrong arguments orders in”,"Bug #1226830 in Cinder: ""Bug in __init__ of cinder.brick.initiator.connector.RemoteFSConnector""","Wrong arguments orders in super call of cinder.brick.initiator.connector.RemoteFSConnector
class InitiatorConnector(executor.Executor):
    def __init__(self, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        super(InitiatorConnector, self).__init__(root_helper, execute,
                                                 *args, **kwargs)
        if not driver:
            driver = host_driver.HostDriver()
        self.set_driver(driver)
class RemoteFsConnector(InitiatorConnector):
    """"""Connector class to attach/detach NFS and GlusterFS volumes.""""""
    def __init__(self, mount_type, root_helper, driver=None,
                 execute=putils.execute, *args, **kwargs):
        self._remotefsclient = remotefs.RemoteFsClient(mount_type,
                                                       execute, root_helper)
        super(RemoteFsConnector, self).__init__(driver, execute, root_helper,
                                                *args, **kwargs)
driver and execute arguments should be after root_helper","Fix __init__ methods of brick initiator connectors

Wrong order of positional arguments in RemoteFsConnector __init__ call.
This only in RemoteFsConnector.__init__, but I decide also update
__init__ methods of other connectors to avoid errors in future.

Change-Id: I75d2c2d29e0330536e280614db1a13686cfd15e4
Closes-Bug: #1226830"
656,587eb5a6b9acc9f02eee1147dbaafb3782c295d5,1378294162,,1.0,21,1,2,2,1,0.684038436,True,3.0,521370.0,17.0,7.0,False,9.0,80575.0,22.0,5.0,170.0,720.0,720.0,170.0,703.0,703.0,17.0,523.0,523.0,0.018927445,0.550998948,0.550998948,1475,1213964,1213964,cinder,587eb5a6b9acc9f02eee1147dbaafb3782c295d5,1,1, ,"Bug #1213964 in Cinder: ""when using qpid, metadata arguments cause volume cloning to fail""","volume cloning seems to fail when using qpid if some metadata is passed, see the following:
 # cinder create --metadata 'Type=Test' --source-volid d1ce1fc3-10fe-475f-9dc4-51261d04c323 1
 ERROR: The server has either erred or is incapable of performing the requested operation.
interestingly, this does not seem to happen when a new (non clone) volume is created
cinder api.log reports some rather long traceback:
 InternalError: Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 511, in dispatch
    self.engine.dispatch()
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 815, in dispatch
    self.process(ssn)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1050, in process
    self.send(snd, msg)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 1261, in send
    body = enc(msg.content)
  File ""/usr/lib/python2.6/site-packages/qpid/messaging/message.py"", line 28, in encode
    sc.write_primitive(type, x)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 257, in write_map
    sc.write(string.joinfields(map(self._write_map_elem, m.keys(), m.values()), """"))
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 250, in _write_map_elem
    sc.write_primitive(type, v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 73, in write_primitive
    getattr(self, ""write_%s"" % type.NAME)(v)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 300, in write_list
    type = self.encoding(o)
  File ""/usr/lib/python2.6/site-packages/qpid/codec010.py"", line 59, in encoding
    raise CodecException(""no encoding for %r"" % obj)
 CodecException: no encoding for <cinder.db.sqlalchemy.models.VolumeMetadata object at 0x458e610>","Call to_primitive on volumes.rpcapi.create_volume

cinder.volume.rpcapi.create_volume does not convert the request_spec to
primitive before casting the request. This makes requests containing non
primitive types to fail. For example:

cinder create --metadata=Type=test --source-volid $VOLID 1

This will create a new database record and call create_volume on
volume.rpcapi.  This will fail because VolumeMetadata won't be
serialized correctly when calling cast. This, however, is not True when
--source-volid is not passed because in such case,
scheduler.rpcpai.create_volume will be called, which converts
request_spec to primitive correctly.

Closes-Bug: #1213964
Change-Id: I096d815254c9782390fba05ea9cd9af925635402"
657,587fd9021982e3b1fb0c5765df6bad2723701031,1382558435,,1.0,1,1,1,1,1,0.0,True,2.0,86471.0,11.0,1.0,False,9.0,176692.0,21.0,2.0,1005.0,1082.0,1082.0,881.0,958.0,958.0,864.0,936.0,936.0,0.775784753,0.840358744,0.840358744,21,333,1243840,cinder,587fd9021982e3b1fb0c5765df6bad2723701031,1,0,Bug: Error message when it is just could be a warning. Also done because: ‘This is to comply with the desire of the QA team’,"Bug #1243840 in Cinder: ""flaky log ERRORs in c-vol after successful tempest run""","These showed up 9 times out of 156 runs and are of several varieties. Here is a sample of each:
https://review.openstack.org/#/c/51966/7
http://logs.openstack.org/66/51966/7/check/check-tempest-devstack-vm-postgres-full/a884365
2013-10-22 21:06:08.354 | Log File: c-vol
2013-10-22 21:06:08.355 | 2013-10-22 20:57:49.993 24174 ERROR cinder.brick.iscsi.iscsi [req-077902c7-a5e6-414a-b241-2f429766a569 c6dbf14b4d794430bb3b20395dad67d7 44b8cb996d984ea0aba77a30fa9531d0] Failed to create iscsi target for volume id:volume-52749038-fe54-4f93-9be1-50c480df4b6a: Unexpected error while running command.
https://review.openstack.org/#/c/47712/4
http://logs.openstack.org/12/47712/4/check/check-tempest-devstack-vm-neutron/843f8e8
2013-10-22 23:35:40.881 | Log File: c-vol
2013-10-22 23:35:40.882 | 2013-10-22 23:29:57.004 6308 ERROR cinder.brick.local_dev.lvm [req-4e6c087e-c345-4072-9c18-0f39861680d9 75f6a750c42745ca9440629f79dbaa18 21a7f4942ba7490d8198acf0510fe4fe] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-472b303e-7d40-4956-859e-e91506947641, RESPONSE:   /dev/dm-1: stat failed: No such file or directory
https://review.openstack.org/#/c/53255/1
http://logs.openstack.org/55/53255/1/check/check-tempest-devstack-vm-neutron-pg/2319aee
2013-10-23 00:38:37.188 | Log File: c-vol
2013-10-23 00:38:37.188 | 2013-10-23 00:32:36.192 4619 ERROR cinder.brick.local_dev.lvm [req-025aa642-42ac-4a14-8b5c-0165c6c2c268 6105fe9f840049fba93305e06d8cab2c c085cabe4d8b46a19b35708f4ec1c21a] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a, RESPONSE:   Can't remove open logical volume ""volume-f0845e0f-2fbb-4d68-879d-ea21a10d152a""","Downgrade target create failure mesg to warning

Since this will occur more often than we would like and
almost always recovers, change the log level to warning
instead of error.

This is to comply with the desire of the QA team to eradicate
error messages from succesful tempest runs.

Change-Id: If1a841bbeb9a36ff7e103a0e9aff67fda66a7f3b
Closes-Bug: #1243840"
658,5896966d40884a7bdba2878386ef8f2966d08d1a,1385703524,,1.0,2,2,2,2,1,1.0,True,1.0,38295.0,10.0,5.0,False,6.0,4419854.0,12.0,5.0,1.0,741.0,742.0,1.0,732.0,733.0,0.0,674.0,674.0,0.000830565,0.560631229,0.560631229,121,520,1257068,cinder,5896966d40884a7bdba2878386ef8f2966d08d1a,0,0,tests,"Bug #1257068 in Cinder: ""Use assertAlmostEqual instead of failUnlessAlmostEqual in unit tests""","The method failUnlessAlmostEqual has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using failUnlessAlmostEqual therefore we should use assertAlmostEqual instead.","Use assertAlmostEqual instead of failUnlessAlmostEqual in unit tests

The method failUnlessAlmostEqual has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using
failUnlessAlmostEqual therefore we should use assertAlmostEqual instead.

Change-Id: Ic9947028afdb5b1ba8f19103018a6f1bc3fc551f
Closes-Bug: #1257068"
659,58d48ded2bbb90b8639a31b47e37e97c276eac87,1410057464,,1.0,5,44,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1863,1349452,1349452,Nova,58d48ded2bbb90b8639a31b47e37e97c276eac87,1,1,"""Looks like it got changed a while back."", ""It looks like one or both of these changes is causing the issue""","Bug #1349452 in OpenStack Compute (nova): ""apparent deadlock on lock_bridge in n-cpu""","It's not clear if n-cpu is dying trying to acquire the lock ""lock_bridge"" or if it's just hanging.
http://logs.openstack.org/08/109108/1/check/check-tempest-dsvm-full/4417111/logs/screen-n-cpu.txt.gz
The logs for n-cpu stop about 15 minutes before the rest of the test run, and all tests doing things that require the hypervisor executed after that point fail with different errors.","Sync oslo lockutils to nova

The commit to be merged:

942e1aa Use file locks by default again
ac995be Fix E126 pep8 errors
15b8352 Remove oslo.log from lockutils

942e1aa is hopefully the fix for the ""n-cpu stop about 15 minutes 
before the rest of the test run"" problem at the gate. We need to 
make sure this actually fixes the gate problem

Closes-Bug: #1349452

Change-Id: I31265d4e08245879ad72d3e91dcd8be2359ca811"
660,58d7eeede1e2912ab250fd306c85ca1a05de2fbc,1389314774,,1.0,17,17,3,2,1,0.617059114,True,3.0,4286777.0,33.0,12.0,False,11.0,131526.0,20.0,5.0,45.0,2070.0,2107.0,26.0,1697.0,1716.0,39.0,1934.0,1966.0,0.005715102,0.276468067,0.281040149,278,683,1267664,nova,58d7eeede1e2912ab250fd306c85ca1a05de2fbc,0,0,No bug: Private methods should be prefixed with a _ for Clarity,"Bug #1267664 in OpenStack Compute (nova): ""docker driver exposes public methods outside of driver interface - should be private""","The following methods are public and should be prefixed with a _ to indicate that they are private:
     is_daemon_running
     find_container_by_name
     get_available_resource","Prefix private methods with _ in docker driver

Private methods should be prefixed with a _ for
clarity. Although most private methods were
marked this way, a few were not.

Renames and marks private:
 is_daemon_running
 find_container_by_name
 get_available_resource

Closes-Bug: #1267664

Change-Id: Ifc6818b7620af972cc0eec6f9ef43610456895f1"
661,58e6bb5893186517edafe1a4d51710c1362bc9cc,1402372540,,1.0,23,1,2,2,1,0.870864469,False,,,,,True,,,,,,,,,,,,,,,,,959,1397,1328162,neutron,58e6bb5893186517edafe1a4d51710c1362bc9cc,1,1,,"Bug #1328162 in neutron: ""Tempest fails to delete firewall in 300 seconds""","Similar to bug #1314313 but this is another failure.
In some tempest runs a test fails to delete firewall within 300 seconds.
That happens because at the point firewall agent sends deleting confirmation to neutron server, firewall object is already updated to a state unexpected by deleting method.
Example of the issue:
http://logs.openstack.org/18/97218/2/gate/gate-tempest-dsvm-neutron/e03d166/console.html#_2014-06-07_10_33_34_506","Fix race condition with firewall deletion

In some cases when firewall is created and then deleted in short
period of time, there could be a race condition of firewall status
changes. Agent may change firewall status from PENDING_DELETE to ACTIVE
because the agent has just set it up on the backend.
Delete request then is not properly served and firewall remains in ERROR
state and can't be deleted at all.

To fix this changing status from PENDING_DELETE is not allowed.
Deleting firewall in ERROR state is allowed.

Change-Id: Iec3cfcb1e03b33dda8e1f10ca51bd9b61fa8030d
Closes-Bug: #1328162"
662,58f33fd3ef7b66bb570ef757b9f70a023e0c8c59,1396440155,,1.0,0,98,2,2,1,1.0,True,1.0,592161.0,21.0,5.0,False,28.0,92778.5,39.0,5.0,532.0,1714.0,1928.0,429.0,1464.0,1617.0,275.0,875.0,939.0,0.256267409,0.813370474,0.8727948,721,1147,1301337,neutron,58f33fd3ef7b66bb570ef757b9f70a023e0c8c59,0,0,Bug in test,"Bug #1301337 in neutron: ""We have three duplicated tests for check_ovs_vxlan_version""","test_ovs_lib, test_ovs_neutron_agent and test_ofa_neutron_agent have duplicated same unit tests for check_ovs_vxlan_version. The only difference is SystemError (from ovs_lib) and SystemExit (from agents).
The tested logic is 99% same, and unit tests in ovs/ofa agent looks unnecessary.","Remove duplicated tests for check_ovs_vxlan_version

test_ovs_lib, test_ovs_neutron_agent and test_ofa_neutron_agent
have duplicated same unit tests for check_ovs_vxlan_version.
This commits removes tests from ovs/ofa_neutron_agent.

Change-Id: Ia028a4afc1c8154f895187c979d23b626ab4ae3a
Closes-Bug: #1301337"
663,5930e5f9acad1e42b3eb32f62b8cbc1d9b4c6384,1388628873,,1.0,0,1,1,1,1,0.0,True,1.0,529507.0,9.0,3.0,False,34.0,5940640.0,71.0,3.0,9.0,1074.0,1079.0,9.0,832.0,837.0,9.0,901.0,906.0,0.001439263,0.129821531,0.130541163,239,643,1265408,nova,5930e5f9acad1e42b3eb32f62b8cbc1d9b4c6384,0,0,unused code,"Bug #1265408 in OpenStack Compute (nova): ""Remove unused code in nova/api/ec2/__init__.py""","""result = None"" in https://github.com/openstack/nova/blob/master/nova/api/ec2/__init__.py#L546  is unused code, remove it","Remove unused code in nova/api/ec2/__init__.py

""result = None"" in nova/api/ec2/__init__.py#L546 is unused code, remove it

Change-Id: I1b09d8656686bafcc7f42bc77d01f3e14aa51ed9
Closes-Bug: #1265408"
664,59a6cf233b538d6666740de4796fce25ed8265aa,1399625237,1.0,1.0,83,20,5,4,1,0.561189794,True,3.0,1128800.0,27.0,7.0,False,68.0,81270.0,218.0,1.0,77.0,1192.0,1257.0,60.0,1191.0,1239.0,77.0,1184.0,1249.0,0.009798995,0.148869347,0.157035176,877,1311,1317804,nova,59a6cf233b538d6666740de4796fce25ed8265aa,1,0,"""The change to use InstanceActionEvent in compute.utils.EventReporter changed the order of how things are done. “","Bug #1317804 in OpenStack Compute (nova): ""InstanceActionEvent traceback parameter not serializable""","The change to use InstanceActionEvent objects in compute.utils.EventReporter changed the order of how things are done. Before, traceback info were converted to strings before being sent to the conductor. Now, since the object method being used remotes itself, the order becomes the opposite and any captured tracebacks are sent as is, resulting in errors during messaging.
See http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVmFsdWVFcnJvcjogQ2lyY3VsYXIgcmVmZXJlbmNlIGRldGVjdGVkXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTk2MjYzMjYwODZ9","Fix InstanceActionEvent traceback parameter not serializable

The change to use InstanceActionEvent in compute.utils.EventReporter
changed the order of how things are done. Before, traceback info were
converted to a string before being sent to the conductor. Now, since
the object method being used remotes itself, the order becomes the
opposite and any captured tracebacks are sent as is, resulting in
errors during messaging.

This adds a serialize_args decorator for this case, which handles
this specific case for the event_finish_with_failure() classmethod.
A more generic approach is needed, and that will be applied to the
also-affected but currently-unused finish_with_failure() instance
method.

Change-Id: Ibedddd606b9d25ffbbb2b1b5358cf0aa4741083f
Co-Authored-By: Dan Smith <dms@danplanet.com>
Closes-Bug: #1317804"
665,59d4c264bdcf056eb44d9d137336fde7c00f3a1b,1410881335,,1.0,7,1,2,2,1,0.954434003,False,,,,,True,,,,,,,,,,,,,,,,,1325,1793,1370112,neutron,59d4c264bdcf056eb44d9d137336fde7c00f3a1b,1,1,,"Bug #1370112 in neutron: ""NSX plugin should set VNIC_TYPE""","This nova commit: http://git.openstack.org/cgit/openstack/nova/commit/?id=a8a5d44c8aca218f00649232c2b8a46aee59b77e
made VNIC_TYPE a compulsory port bindings attribute.
This broke the NSX plugin which is now not able to boot VMs anymore. Probably other plugins are affected.
Whether VNIC_TYPE is really a required attribute questionable; the fact that port bindings is such a messy interface that can cause this kind of breakages is at least annoying.
Regardless, all plugins must now adapt.
This will also be fixed once a general fix for bug 1370077 is introduced - nevertheless, the NSX plugin can't risk staying broken for more time, and also its 3rd party integration tests are disabled because of this. For this reason we're opening a bug specific for this plugin to fast-track a fix for it.","NSX plugin: set VNIC_TYPE port binding attribute

A recent change has made this attribute required for nova
integration.
This patch adds this attribute to responses generated by the NSX
plugin, and also ensures relevant unit tests are executed for the
vmware NSX plugin.

Change-Id: Ieebab01b406909f66a40cc683763292a9ef6f218
Closes-Bug: #1370112"
666,59da928e945ec58836d34fd561d30a8a446e2728,1405181236,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1856,1341144,1341144,Neutron,59da928e945ec58836d34fd561d30a8a446e2728,1,1,"""Change https://review.openstack.org/105411 introduced the following incorrect server_default value:""","Bug #1341144 in neutron: ""Wrong server_default value for multicast_ip_index in cisco_network_profiles table""","Change https://review.openstack.org/105411 introduced the following incorrect server_default value:
    multicast_ip_index = sa.Column(sa.Integer, default=0,
                                   server_default=sql.false())
in neutron/plugins/cisco/db/n1kv_models_v2.py for table cisco_network_profiles.","Use integer server_default value for multicast_ip_index

Use integer server_default value for multicast_ip_index in
cisco_network_profiles table.

Change-Id: I1948aabb65485aaa1045a101957303c3e6522d21
Closes-bug: #1341144"
667,59fb3c18759bb2529a9c1dea445c2d5caf6746da,1378470427,,1.0,28,4,2,2,1,0.954434003,True,2.0,329085.0,27.0,19.0,False,132.0,154641.5,518.0,5.0,51.0,2411.0,2443.0,51.0,2147.0,2179.0,23.0,2256.0,2261.0,0.004028874,0.378881988,0.379721336,1524,1221620,1221620,nova,59fb3c18759bb2529a9c1dea445c2d5caf6746da,1,1,"“This particular error is caused by this change https://review.openstack.org/#/c/43151/""","Bug #1221620 in OpenStack Compute (nova): ""KeyError: 'service' during schedule of baremetal instance""","Traceback while scheduling both overcloud nodes on tripleo ci
Last succesfull run was 05-Sep-2013 01:54:10 (UTC)
So something changed after this run https://review.openstack.org/#/c/43968/
although scheduling of baremetal node seems to work on seed ....
INFO nova.scheduler.filter_scheduler [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Attempting to build 1 instance(s) uuids: [u'f71e3e47-f2a2-4a13-9
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Failed to scheduler_run_instance: 'service'
WARNING nova.scheduler.utils [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] [instance: f71e3e47-f2a2-4a13-92c0-c3397acaf409] Setting instance to ERR
ERROR nova.openstack.common.rpc.amqp [req-c754d309-92fc-461a-81fb-d5bfe97a0676 99fa1214e35a4cc6b99c9332b8ca66fb d86556c4d57c4dfc87b30f6c66c40a98] Exception during message handling
TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
TRACE nova.openstack.common.rpc.amqp     **args)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 160, in run_instance
TRACE nova.openstack.common.rpc.amqp     context, ex, request_spec)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/manager.py"", line 147, in run_instance
TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 326, in _schedule
TRACE nova.openstack.common.rpc.amqp     hosts = self.host_manager.get_all_host_states(elevated)
TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/host_manager.py"", line 432, in get_all_host_states
TRACE nova.openstack.common.rpc.amqp     service = compute['service']
TRACE nova.openstack.common.rpc.amqp KeyError: 'service'
TRACE nova.openstack.common.rpc.amqp","Fix compute_node_get_all() for Nova Baremetal

Change Ie5ef00c974b810336787e88c78c93c15ca2890d3 introduced
a regression leading to KeyError when a new baremetal node
is scheduled. This is due to the fact, that the mentioned
change assumes, that ComputeNode <--> Service is a 1-1
relationship, which is not true for Nova Baremetal driver.

This patch fixes the tables join in compute_node_get_all()
DB API method to work with 1-M relationship between ComputeNode
and Service models.

Closes-Bug: #1221620

Change-Id: I7c218d06f63cc2bf7d0e358f2f76366601179b0c"
668,5aa29b6b0a34d603e882a80cdc8e936f983b1892,1397774699,,1.0,3,2,2,2,1,0.970950594,True,2.0,815368.0,60.0,16.0,False,212.0,10774.0,1198.0,6.0,1128.0,4698.0,4971.0,835.0,3724.0,3861.0,514.0,3341.0,3561.0,0.065772669,0.426819923,0.454916986,814,1242,1309239,nova,5aa29b6b0a34d603e882a80cdc8e936f983b1892,0,0,feature “we should always load the info_cache so we can avoid the extra query”,"Bug #1309239 in OpenStack Compute (nova): ""_heal_instance_info_cache should info info_cache as an expected_attrs""",The _heal_instance_info cache() is used to heal the cache access instance['info_cache'] so we should always load the info_cache so we can avoid the extra query to get it when accessed.,"Remove unnecessary call to fetch info_cache

The _heal_instance_info cache() always accesses instance['info_cache']
therefore we should always load the info_cache so we can avoid the extra
query to fetch it.

Change-Id: I3c2e005b98abf2a80276562cc05e88ba5ff199bc
Closes-bug: #1309239"
669,5aa4331cc8d9c0e4615f9859395a9ae8c6d40f09,1397033522,,1.0,11,11,12,2,2,0.964984046,True,2.0,857855.0,30.0,9.0,False,28.0,53101.23077,91.0,1.0,1903.0,1001.0,2579.0,1577.0,1000.0,2253.0,996.0,995.0,1667.0,0.128347065,0.128218332,0.214727085,768,1195,1304917,nova,5aa4331cc8d9c0e4615f9859395a9ae8c6d40f09,0,0,Bug in test,"Bug #1304917 in OpenStack Compute (nova): ""VMware: fakes.py should be moved to the tests directory""",The file nova/virt/vmwareapi/fake.py should be moved to the tests directory. This is solely used in the unit tests. There is no need for this to reside in the virt directory.,"VMware: move fake.py to the test directory

The file fake.py (contains the simulated VC/ESX API's) should
reside in the test directory.

TrivialFix

Change-Id: Ifd82c2f4375dd5e60ee4754fad7810d4e6ccf6d4
Closes-bug: #1304917"
670,5abac020f459045ceee75c971e179d9dbce8eac8,1387425140,1.0,1.0,98,15,3,2,1,0.880142781,True,3.0,389584.0,46.0,12.0,False,13.0,2352070.667,25.0,2.0,23.0,1703.0,1704.0,23.0,1481.0,1482.0,15.0,170.0,171.0,0.024922118,0.26635514,0.267912773,209,612,1262488,neutron,5abac020f459045ceee75c971e179d9dbce8eac8,1,1,,"Bug #1262488 in neutron: ""bigswitch multi-tenant floating IP loss""","The bigswitch plugin fails to use the admin context to update the floating IPs on the backend controller.
When one tenant assigns a floating IP, the update to the controller doesn't contain the floating IPs of the other tenants so the controller thinks they have been removed.","BigSwitch: Fixes floating IP backend updates

Changes BigSwitch plugin to correctly use
admin context on floating IP updates to the
backend controller so they correctly contain
floating IPs for all tenants.

Closes-Bug: #1262488
Change-Id: I6f2666c242e6d9b0684943db073a2284d01fa1e0"
671,5b0ec5a3be0d92687c552f197f5474d5066a5a5c,1394793528,,1.0,18,2,2,2,1,0.811278124,True,1.0,5844078.0,11.0,2.0,False,26.0,1867284.5,41.0,1.0,78.0,243.0,321.0,78.0,240.0,318.0,0.0,215.0,215.0,0.000838926,0.181208054,0.181208054,619,1044,1292455,glance,5b0ec5a3be0d92687c552f197f5474d5066a5a5c,1,1,"The info of the first exception is lost, again","Bug #1292455 in Glance: ""In some case lose information of the first exception in store.rbd.Store""","When an exception occurs during exception handling, it lose the
information of the first exception.
In glance.store.rbd.Store.add(), in some cases, before re-transmission
of the exception, the exception is rewritten.","Fix unsaved exception in store.rbd.Store.add()

When an exception occurs during exception handling, it lose the
information of the first exception.

We should use the excutils.save_and_reraise_exception() in this case.

Change-Id: I1154e17d98804270f9578bc34a5fca6629bd9af4
Closes-Bug: #1292455"
672,5b27fe7de22aef53b82402f15b076887bc52670a,1403034700,0.0,1.0,67,10,2,2,1,0.998904744,False,,,,,True,,,,,,,,,,,,,,,,,982,1421,1331170,nova,5b27fe7de22aef53b82402f15b076887bc52670a,1,1,"""Unfortunately, that can result in live migrations failing if your environment is using different versions of the host OS on compute noes as the destination node may not be able to support the machine type used when the VM was originally started.""","Bug #1331170 in OpenStack Compute (nova): ""Live migration fails in heterogeneous host OS environment""","The libvirt driver currently does not set the machine type for a KVM guest by default.  When not specified, libvirt will use the newest one it knows about.  Unfortunately, that can result in live migrations failing if your environment is using different versions of the host OS on compute noes as the destination node may not be able to support the machine type used when the VM was originally started.
A simple solution to this is to provide a new option which allows you to specify the default machine type on a per compute node basis (nova.conf option).  By using this option, you can ensure that VMs are started with a machine type that will allow it to be live migrated to other nodes in the deployment.","libvirt: Allow specification of default machine type

The libvirt driver currently does not set the machine type for a KVM
guest by default. When not specified, libvirt will use the newest one
it knows about. Unfortunately, that can result in live migrations
failing if your environment is using different versions of the host OS
on compute nodes as the destination node may not be able to support the
machine type used when the VM was originally started.

A simple solution to this is to provide a new option which allows you
to specify the default machine type on a per compute node basis
(nova.conf option). By using this option, you can ensure that VMs are
started with a machine type that will allow it to be live migrated to
other nodes in the deployment.

This patch implements that solution by adding the hw_machine_type
option to the [libvirt] group of nova.conf.

DocImpact
Closes-bug: #1331170

Change-Id: I223c70c729315b6ffc01eb293fe70553ef827162"
673,5b34d41828a2a1260b1b92f6bfd91e1264aea7a8,1399125904,,1.0,7,7,2,2,1,0.863120569,True,2.0,957610.0,32.0,7.0,False,14.0,85439.0,21.0,4.0,18.0,319.0,322.0,18.0,313.0,316.0,18.0,304.0,307.0,0.015767635,0.253112033,0.25560166,1022,1465,1335821,neutron,5b34d41828a2a1260b1b92f6bfd91e1264aea7a8,0,0,Bug in test,"Bug #1335821 in neutron: ""Unit tests use inconsistent default tenant id""","Meters and security groups use 'test_tenant' while all other resources use 'test-tenant'.
This means that a test that creates multiple types of resources (While using the the default tenant id) would find that some resources were created under one tenant, and other resources under another tenant.
For example, a test creates a network, subnet, port, security group and meter label + rule. Listing all resources that belong to the 'test-tenant' would return a partial list, leading to confusing results.","Replace occurences of 'test_tenant' with 'test-tenant' in tests

tenant_id will now be consistent among all resources created.
This is important when getting or listing resources by tenant_id.
Most resources are created under the 'test-tenant', while
metering and security groups were created under 'test_tenant'.

Closes-Bug: #1335821
Change-Id: Ifde22814ab5c302e751f2102f6fd7ef52c932126"
674,5b5ba869aa4680bcfa7544c4e718cb5a5dc9c168,1389351250,,1.0,14,4,2,2,1,0.852405179,True,5.0,3657962.0,77.0,30.0,False,7.0,6311452.0,8.0,9.0,65.0,1323.0,1385.0,51.0,1188.0,1237.0,3.0,607.0,607.0,0.005891016,0.895434462,0.895434462,281,686,1267790,neutron,5b5ba869aa4680bcfa7544c4e718cb5a5dc9c168,1,1,,"Bug #1267790 in neutron: ""Fixed metric_index causes ValueError in ip_lib""","ip_lib.IpRouteCommand.get_gateway() raises ValueError to default_route_line like
default via 192.168.99.1 proto static
since it wrongly assumes that the 5th word is a value metric (in the example above, it is ""static"").","Fix ValueError in ip_lib.IpRouteCommand.get_gateway()

As metric is not necessarily the 5th word of the gateway line, the
method should search the string 'metric' in the line and pick the next
word as the metric value.

Change-Id: I2663ddbae82f80b912b364c07f9ab92c5b90b718
Closes-Bug: #1267790"
675,5ba26c8aa5b295a1202fbc6c01eb050ce3f3af3d,1391893198,,1.0,5,1,1,1,1,0.0,True,2.0,230687.0,17.0,4.0,False,1.0,1499539.0,1.0,5.0,566.0,1489.0,1489.0,543.0,1286.0,1286.0,550.0,1325.0,1325.0,0.393009986,0.945791726,0.945791726,337,747,1274123,cinder,5ba26c8aa5b295a1202fbc6c01eb050ce3f3af3d,1,1,Race condition,"Bug #1274123 in Cinder: ""storwize driver is not thread-safe""","when launching multiple VMs at the same time (for example by selecting a higher than one instance count in the dashboard), the action will fail with various errors.
most likely the process goes as follows:
Thread1 + ... + ThreadN at the same time: check if the host is mapped -> it doesn't
Thread1: let's create the host!
...
ThreadN: let's create the host!
Thread1 succeeds
Thread2..N fails
Thread1 doesn't even finish the mapping _most of the time_","Storwize/SVC: Fix races in host-related functions

In the Storwize/SVC driver, initialize_connection checks if a host
object exists on the storage, and if not, creates one. Similarly,
terminate_connection checks if there are any more mappings for this
host, and deletes the host object if there are none. Consequently, there
exist races between two initialize_connection calls, two
terminate_connection calls, or one and one.

The easy solution here is to use locks. Because locks are files on the
local machine, this implies that all cinder-volume processes managing a
given Storwize/SVC contoller run on the same machine.

Change-Id: I62cd992e7150e7d16f1a2b2be6ef61c64b638858
Closes-Bug: #1274123"
676,5be4620ae5bb50c8436de0e11269c85a095ed40b,1386675110,,1.0,151,31,6,5,1,0.739975992,True,9.0,5529831.0,49.0,24.0,False,94.0,295714.6667,311.0,3.0,194.0,1210.0,1360.0,193.0,1047.0,1197.0,27.0,966.0,967.0,0.022617124,0.781098546,0.7819063,3,315,1242942,cinder,5be4620ae5bb50c8436de0e11269c85a095ed40b,1,1, does not handle missing volume group gracefully,"Bug #1242942 in Cinder: ""cinder does not handle missing volume group gracefully (stuck in ""creating"")""","Tested with Havana rc2 from the UCA on Precise.
If the cinder-volumes (in a default configuration) LVM volume group does not exist, cinder will try to create a volume, which is bound to fail. The volume then gets stuck in the ""creating"" state and can't be deleted. The log will contain:
2013-10-17 09:29:58.188 16676 ERROR cinder.brick.local_dev.lvm [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Unable to locate Volume Group cinder-volumes
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Error encountered during initialization of driver: LVMISCSIDriver
2013-10-17 09:29:58.189 16676 ERROR cinder.volume.manager [req-5c03777b-4acc-4784-b463-278dee0d2e08 None None] Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager Traceback (most recent call last):
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 190, in init_host
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager     self.driver.check_for_setup_error()
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager   File ""/usr/lib/python2.7/dist-packages/cinder/volume/drivers/lvm.py"", line 94, in check_for_setup_error
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=message)
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume Group cinder-volumes does not exist
2013-10-17 09:29:58.189 16676 TRACE cinder.volume.manager
2013-10-17 09:30:47.198 16676 WARNING cinder.volume.manager [req-d5f8a463-c097-4518-829b-504bf02763b2 None None] Unable to update stats, driver is uninitialized
Resetting the state with ""cinder reset-state"" will get the volume to the ""available"" state, which it isn't. Deleting or force-deleting will also fail, getting stuck in ""deleting"" state forever. The only solution I found was to directly kill the relevant rows in the volumes table and cinder-manage db sync.
I know in grizzly, cinder-volume would refuse to start if it couldn't find the volume-group. I thought that behavior was better. Failing when attempting to create the volume, instead of getting stuck in the creating state, would also be acceptable.","Move driver initialization check into the method

Volumes and backups managers' methods are decorated with
`require_initialized_driver` which checks whether the driver has been
initialized or not. The decorator fails with a `DriverNotInitialized`
exception if the driver hasn't been initialized.

This early failure leaves volumes and backups in a wrong status which is
not just confusing for the user but it also makes it difficult to do
anything with the resources after they've been left in a 'bogus' status.

For example, when a volume creation is requested, the volume is first
created in the database and its status is set to 'creating'. Then the
scheduler will pick an available volume node and send the task to it. If
the driver has not been initialized, the volume status will be left as
'creating' instead of 'error'.

This patch fixes that issue by moving the driver initialization check
into the various manager's methods. In some cases this check is done at
the very beginning of the method, in some others - either to avoid code
duplication or because the lines above the check made sense to be
executed first - this check is done later in the method.

Change-Id: I2610be6ba1aa7df417f1a1f7bb27af30273e4814
Closes-bug: #1242942"
677,5be55ac161adfec9085121fe59ea3bf59daa92cf,1411579246,,1.0,50,27,2,2,1,0.173781323,False,,,,,True,,,,,,,,,,,,,,,,,1352,1821,1373524,neutron,5be55ac161adfec9085121fe59ea3bf59daa92cf,1,1,,"Bug #1373524 in neutron: ""dvr snat delete binding changed""","When router_gateway_clear happens, the schedule_router calls the unbind_snat_servicenode in the plugin.  This will clear the agent binding from the binding table.  But the l3-agent was expecting the ex_gw_port binding to be present. The agent needs to check its cache of the router['gw_host_port'] value now.  SNAT namespaces will not be deleted in all cases without this fix.","fix dvr snat bindings for external-gw-clear

When router_gateway_clear happens, the
schedule_router calls the unbind_snat_servicenode
in the plugin. This will clear the agent binding
from the binding table. But the l3-agent was
expecting the ex_gw_port binding to be present.
The agent needs to check its cache of the
router['gw_host_port'] value now.

Change-Id: I051fb97d802b0508b30683a33673b85f5ab24000
Closes-bug: #1373524"
678,5becbbec6a686667925f399904cebd97b583c35a,1380822309,,1.0,3,5,1,1,1,0.0,True,4.0,1474512.0,35.0,10.0,False,4.0,2653667.0,4.0,4.0,4.0,1785.0,1785.0,4.0,1632.0,1632.0,4.0,348.0,348.0,0.011933174,0.832935561,0.832935561,1466,1211915,1211915,neutron,5becbbec6a686667925f399904cebd97b583c35a,1,0, ,"Bug #1211915 in neutron: ""Connection to neutron failed: Maximum attempts reached""","http://logs.openstack.org/64/41464/4/check/gate-tempest-devstack-vm-neutron/4288a6b/console.html
Seen testing https://review.openstack.org/#/c/41464/
2013-08-13 17:34:46.774 | Traceback (most recent call last):
2013-08-13 17:34:46.774 |   File ""tempest/scenario/test_network_basic_ops.py"", line 176, in test_003_create_networks
2013-08-13 17:34:46.774 |     router = self._get_router(self.tenant_id)
2013-08-13 17:34:46.775 |   File ""tempest/scenario/test_network_basic_ops.py"", line 141, in _get_router
2013-08-13 17:34:46.775 |     router.add_gateway(network_id)
2013-08-13 17:34:46.775 |   File ""tempest/api/network/common.py"", line 78, in add_gateway
2013-08-13 17:34:46.776 |     self.client.add_gateway_router(self.id, body=body)
2013-08-13 17:34:46.776 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 108, in with_params
2013-08-13 17:34:46.776 |     ret = self.function(instance, *args, **kwargs)
2013-08-13 17:34:46.776 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 396, in add_gateway_router
2013-08-13 17:34:46.777 |     body={'router': {'external_gateway_info': body}})
2013-08-13 17:34:46.777 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 987, in put
2013-08-13 17:34:46.777 |     headers=headers, params=params)
2013-08-13 17:34:46.778 |   File ""/opt/stack/new/python-neutronclient/neutronclient/v2_0/client.py"", line 970, in retry_request
2013-08-13 17:34:46.778 |     raise exceptions.ConnectionFailed(reason=_(""Maximum attempts reached""))
2013-08-13 17:34:46.778 | ConnectionFailed: Connection to neutron failed: Maximum attempts reached","Removing rpc communication from db transaction

In a transaction context of ExtraRoute_db_mixin.update_router() was
called super method that uses subtransaction and communication with l3
agent. In case of agent's heartbeat happens while there is running
transaction, update heartbeat in agents table gets stuck in a deadlock.

Closes-Bug: #1211915
Change-Id: I96e6a9d7172d5a0e3e720a81fcd10f04c40aef07"
679,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,1393770049,,4.0,1,26,8,6,1,0.769484133,False,,,,,True,,,,,,,,,,,,,,,,,504,923,1286712,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,"Bug #1286712 in Cinder: ""Remove useless funciton stub_out_key_pair_funcs""","stub_out_key_pair_funcs was useless, remove it.
-def stub_out_key_pair_funcs(stubs, have_key_pair=True):
-    def key_pair(context, user_id):
-        return [dict(name='key', public_key='public_key')]
-
-    def one_key_pair(context, user_id, name):
-        if name == 'key':
-            return dict(name='key', public_key='public_key')
-        else:
-            raise exc.KeypairNotFound(user_id=user_id, name=name)
-
-    def no_key_pair(context, user_id):
-        return []","General cleanup of unused objects

* Remove unused function fake_get_remote_image_service
* Remove unused local function fake_execute2
* stub_out_key_pair_funcs() does nothing, remove it.
* Removed usage of enumerate()

Closes-Bug: #1286714
Closes-Bug: #1286712
Closes-Bug: #1286742
Closes-Bug: #1286809
Change-Id: I76413c886f6903df2adcf4a4ceb8990486a2ff2f"
680,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,1393770049,,4.0,1,26,8,6,1,0.769484133,False,,,,,True,,,,,,,,,,,,,,,,,505,924,1286714,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,"Bug #1286714 in Cinder: ""Remove unused enumerate""","diff --git a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
index e90c291..7df8ebb 100644
--- a/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
+++ b/cinder/tests/api/contrib/test_extended_snapshot_attributes.py
@@ -91,7 +91,7 @@ class ExtendedSnapshotAttributesTest(test.TestCase):
         res = self._make_request(url)
         self.assertEqual(res.status_int, 200)
-        for i, snapshot in enumerate(self._get_snapshots(res.body)):
+        for snapshot in self._get_snapshots(res.body):
             self.assertSnapshotAttributes(snapshot,
                                           project_id='fake',
                                           progress='0%')","General cleanup of unused objects

* Remove unused function fake_get_remote_image_service
* Remove unused local function fake_execute2
* stub_out_key_pair_funcs() does nothing, remove it.
* Removed usage of enumerate()

Closes-Bug: #1286714
Closes-Bug: #1286712
Closes-Bug: #1286742
Closes-Bug: #1286809
Change-Id: I76413c886f6903df2adcf4a4ceb8990486a2ff2f"
681,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,1393770049,,4.0,1,26,8,6,1,0.769484133,False,,,,,True,,,,,,,,,,,,,,,,,507,926,1286742,cinder,5c1d0a6246cbd3f05f6c41e7cfc7046fa3431726,0,0,cleanup,"Bug #1286742 in Cinder: ""Remove unused function fake_execute2""","diff --git a/cinder/tests/brick/test_brick_linuxscsi.py b/cinder/tests/brick/test_brick_linuxscsi.py
index 47b73dc..e0ec010 100644
--- a/cinder/tests/brick/test_brick_linuxscsi.py
+++ b/cinder/tests/brick/test_brick_linuxscsi.py
@@ -101,15 +101,6 @@ class LinuxSCSITestCase(test.TestCase):
                    )
             return out, None
-        def fake_execute2(*cmd, **kwargs):
-            out = (""350002ac20398383d dm-3 3PARdata,VV\n""
-                   ""size=2.0G features='0' hwhandler='0' wp=rw\n""
-                   ""`-+- policy='round-robin 0' prio=-1 status=active\n""
-                   ""  |- 0:0:0:1  sde 8:64 active undef running\n""
-                   ""  `- 2:0:0:1  sdf 8:80 active undef running\n""
-                   )
-            return out, None
-
         self.stubs.Set(self.linuxscsi, '_execute', fake_execute)
         info = self.linuxscsi.find_multipath_device('/dev/sde')","General cleanup of unused objects

* Remove unused function fake_get_remote_image_service
* Remove unused local function fake_execute2
* stub_out_key_pair_funcs() does nothing, remove it.
* Removed usage of enumerate()

Closes-Bug: #1286714
Closes-Bug: #1286712
Closes-Bug: #1286742
Closes-Bug: #1286809
Change-Id: I76413c886f6903df2adcf4a4ceb8990486a2ff2f"
682,5c321d758c9718d7dde555316ac4fbd2f7acf424,1381164088,,1.0,0,2,1,1,1,0.0,True,2.0,47933.0,15.0,5.0,False,2.0,1099879.0,2.0,3.0,554.0,717.0,1260.0,367.0,684.0,1041.0,25.0,670.0,687.0,0.024276377,0.626517274,0.642390289,1643,1236459,1236459,cinder,5c321d758c9718d7dde555316ac4fbd2f7acf424,1,1,“In ae6b7642e8d32ef5fa75cdcfe55be23c052fd547 we added a key manager with a static key.”,"Bug #1236459 in Cinder: "" key manager is insecure warning messages""","In ae6b7642e8d32ef5fa75cdcfe55be23c052fd547 we added a key manager with a static key.
This key manager (enabled by default) repeated logs the following WARNING message to the Cinder api.log file:
2013-10-07 15:10:17.714 553 WARNING cinder.keymgr.conf_key_mgr [-] This key manager is insecure and is not recommended for production deployments
-----
There are actually two issues here. Logging tons of warning messages by default is not ideal... and should be avoided, especially since at this time there is no ""production ready"" key manager implementation which an end user could configure.","Drop conf_key_mgr warning message!

By default ConfKeyManager logs tons of WARNING message stating
that it isn't production ready...

Given that it is currently the only Cinder key manager option
which can be used/selected I don't think repeatedly logging
warnings is helpful. Lets just drop the warning message
for now and when a good ""production ready"" cinder key manager
implementation is implemented perhaps we can re-add a warning to
this class (hopefully making the production ready impl the default).

Change-Id: Id1fdddc20a963f9fa4749ad57f355cd83d0e14e3
Closes-Bug: #1236459"
683,5c3f212343df997daa48f1f4a1cdd2a29099c288,1406648444,,1.0,12,4,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1083,1533,1346191,nova,5c3f212343df997daa48f1f4a1cdd2a29099c288,1,1,,"Bug #1346191 in OpenStack Compute (nova): ""libvirt _live_snapshot & _swap_volume functions re-define guest with wrong XML document""","In the nova/virt/libvirt/driver.py file, the '_live_snapshot' and '_swap_volume' methods have the following code flow
  xml = dom.XMLDesc(0)
  dom.undefine()
  dom.blockRebase()
  dom.defineXML(xml)
The reason for this is that 'blockRebase' requires the guest to be transient, so we must temporarily delete the persistent config and then re-create it later.
Unfortunately this code is using the wrong XML document when re-creating the persistent config.  'dom.XMLDesc(0)' will return the guest XML document based on the current guest state. Since the guest is running in both these cases, it will get getting the *live* XML instead of the persistent XML.
So these methods are deleting the persistent XML and replacing it with the live XML. These two different XML documents are not guaranteed to contain the same information.
As a second problem, it is not requesting inclusion of security information, so any SPICE/VNC password set in the persistent XML is getting lost
The fix is to replace
  dom.XMLDesc(0)
with
  dom.XMLDesc(libvirt.VIR_DOMAIN_XML_INACTIVE |
                               libvirt.VIR_DOMAIN_XML_SECURE)
in the _live_snapshot and _swap_volume functions.","libvirt re-define guest with wrong XML document

In the nova/virt/libvirt/driver.py file, the '_live_snapshot' and
'_swap_volume' methods have the following code flow
  xml = dom.XMLDesc(0)
  dom.undefine()
  dom.blockRebase()
  dom.defineXML(xml)
The reason for this is that 'blockRebase' requires the guest to be
transient, so we must temporarily delete the persistent config and
then re-create it later.
Unfortunately this code is using the wrong XML document when
re-creating the persistent config. 'dom.XMLDesc(0)' will return the
guest XML document based on the current guest state. Since the guest
is running in both these cases, it will get getting the *live* XML
instead of the persistent XML.So these methods are deleting the
persistent XML and replacing it with the live XML. These two
different XML documents are not guaranteed to contain the same
information.
As a second problem, it is not requesting inclusion of security
information, so any SPICE/VNC password set in the persistent XML
is getting lost.

Change-Id: I4b4e0990ca6c07a9215766f994884a1fb18f3a41
Closes-Bug: #1346191"
684,5c5f3c0684caada0fb7efe33d59bc7f1d66c72d8,1407248013,,1.0,12,12,2,2,1,0.738284866,False,,,,,True,,,,,,,,,,,,,,,,,1249,1708,1363016,cinder,5c5f3c0684caada0fb7efe33d59bc7f1d66c72d8,1,1,,"Bug #1363016 in Cinder: ""Volume initialisation with image might fail due to disk caching""","When using the volume initialisation with an image, not all of the image might be written back on the volume before it gets unmapped due to not flushing/avoiding disk caching on write. The problem seems to occur mostly with volumes that are provided via multipath/Fibre Channel, e.g. where the LUNs disappear without the cinder-volume guest node having full control over it (and the kernel there being able to trigger a flush in time).","Avoid using the disk cache on volume initialisation

When caching is involved, the volume might be
unmapped before the copy actually hit the disk or the
VM starts booting before all of the data has been flushed,
which causes the VM to crash at an arbitrary point in time.

Closes-Bug: #1363016

Change-Id: I7a04f683add8c23b9125fe837c4048ccc3ac224d"
685,5c6ff449bbd7386f0f3e41efc524024434f325df,1395732617,1.0,1.0,25,2,2,2,1,0.950956048,True,6.0,1259225.0,141.0,41.0,False,15.0,687086.0,22.0,9.0,1133.0,1738.0,1738.0,965.0,1410.0,1410.0,674.0,857.0,857.0,0.655339806,0.833009709,0.833009709,679,1105,1296957,neutron,5c6ff449bbd7386f0f3e41efc524024434f325df,1,1,,"Bug #1296957 in neutron: ""Security_Group FirewallDriver default=None cause L2 agent to fail""","Default value for FirewallDriver set to None in security_group_rpc.py.
L2Agent fails when using default value with following error:
/opt/stack/neutron/neutron/agent/securitygroups_rpc.py:129
2014-03-07 08:15:09.120 31995 CRITICAL neutron [req-63f8e61b-9b71-4178-95b9-ab070a4e3b26 None] 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron Traceback (most recent call last):
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/usr/local/bin/neutron-linuxbridge-agent"", line 10, in <module>
2014-03-07 08:15:09.120 31995 TRACE neutron     sys.exit(main())
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 987, in main
2014-03-07 08:15:09.120 31995 TRACE neutron     root_helper)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/linuxbridge/agent/linuxbridge_neutron_agent.py"", line 787, in __init__
2014-03-07 08:15:09.120 31995 TRACE neutron     self.init_firewall()
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/securitygroups_rpc.py"", line 130, in init_firewall
2014-03-07 08:15:09.120 31995 TRACE neutron     self.firewall = importutils.import_object(firewall_driver)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
2014-03-07 08:15:09.120 31995 TRACE neutron     return import_class(import_str)(*args, **kwargs)
2014-03-07 08:15:09.120 31995 TRACE neutron   File ""/opt/stack/neutron/neutron/openstack/common/importutils.py"", line 26, in import_class
2014-03-07 08:15:09.120 31995 TRACE neutron     mod_str, _sep, class_str = import_str.rpartition('.')
2014-03-07 08:15:09.120 31995 TRACE neutron AttributeError: 'NoneType' object has no attribute 'rpartition'
2014-03-07 08:15:09.120 31995 TRACE neutron
This can be fixed by setting default  firewall_driver = neutron.agent.firewall.NoopFirewallDriver or verification on L2 Agent start-up for firewall_driver is not being None.","Add L2 Agent side handling for non consistent security_group settings

Add setting of the firewall_driver to NoopDriver when firewall_driver is None and
add warning if driver combination is not valid.
Modify is_valid_driver_combination to verify default settings: enable_security_group (True) and firewall_driver (None).

Change-Id: I841f9cf96ac6ee2ad17a4e8908d6c8a96f368cca
Closes-Bug: #1296957"
686,5c95824191855b0012268f641e5e738888b8d13f,1377058481,,1.0,7,4,2,2,1,0.684038436,True,2.0,927350.0,15.0,8.0,False,27.0,715817.0,62.0,3.0,112.0,2749.0,2786.0,112.0,2547.0,2584.0,104.0,1914.0,1945.0,0.018210198,0.33211932,0.337495664,1480,1214985,1214985,nova,5c95824191855b0012268f641e5e738888b8d13f,1,1, ,"Bug #1214985 in OpenStack Compute (nova): ""powervm driver is not properly cleaning up compressed images""","This is on the master branch (current havana trunk).  Ran tempest against the powervm driver with a backing VIOS hypervisor, there are no instances left over after the tests are done (everything was successful):
http://paste.openstack.org/show/44772/
But on the backing hypervisor, there are leftover image files:
http://paste.openstack.org/show/44773/
I found that the powervm method that is supposed to remove the files isn't using the --force option with the rm command:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L800
Also, this method isn't even used in the code:
https://github.com/openstack/nova/blob/master/nova/virt/powervm/operator.py#L785","powervm: actually remove files after migration

The powervm driver is attempting to clean up compressed image gz files
in the migration flow but the rm command being issued doesn't use the
--force option, so the files are piling up on the backing hypervisor.

This patch properly removes the files like in other parts of the powervm
code (see blockdev.py for example) and fixes the tests so the
_remove_file method is actually tested.

Closes-Bug: #1214985

Change-Id: I82919aa20447230e317c13f51bdf1c204a2eca39"
687,5cecf9571f93f89ceaa024cae0a7368767643931,1398975945,,1.0,6,3,1,1,1,0.0,True,1.0,694012.0,22.0,7.0,False,13.0,26088.0,20.0,6.0,263.0,1208.0,1379.0,194.0,1046.0,1158.0,105.0,645.0,690.0,0.088480801,0.539232053,0.576794658,852,1282,1315137,neutron,5cecf9571f93f89ceaa024cae0a7368767643931,0,0,feature“plumgrid drivers should be dynamically loaded”,"Bug #1315137 in neutron: ""plumgrid drivers should be dynamically loaded""",In PLUMgrid plugin drivers are not dynamically loaded. They should be loaded by configuration as the rest of the plugins,"Add support to dynamically upload drivers in PLUMgrid plugin

PLUMgrid plugin will be able to dynamycally upload any of its available drivers.
It will simplify any CI testing by changing the configuration file for the plugin
instead of changing the code directly.

Change-Id: I56da881688cfdf8f9a1b655c1080c39ffc0133a5
Closes-bug: #1315137"
688,5cf0659d53eba85f7591565b7f18714e39c1713a,1402415476,,1.0,53,2,3,3,1,0.57481496,False,,,,,True,,,,,,,,,,,,,,,,,951,1387,1326781,glance,5cf0659d53eba85f7591565b7f18714e39c1713a,1,1,,"Bug #1326781 in Glance: ""v2 api returns 200 with blank response (no image data) for download_image policy""","v2 api returns 200 with blank response (no image data) for download_image policy
If you have enabled download_image policy in policy.json to ""role:admin"" then it should return 403 error if user other admin role is calling image-download api.
Presently it is returning 200 with blank response (no image data). If you enable cache filter, then it returns 403 error correctly.
Steps to reproduce:
1. Ensure following flavor is set in glance-api.conf
   [paste-deploy]
   flavor = keystone+cachemanagement
2. Disable cache
   a. Open /etc/glance/glance-api-paste.ini file.
   b. Remove cahce from following sections.
     [pipeline:glance-api-caching]
     [pipeline:glance-api-cachemanagement]
     [pipeline:glance-api-keystone+caching]
     [pipeline:glance-api-keystone+cachemanagement]
     [pipeline:glance-api-trusted-auth+cachemanagement]
   c. Save and exit from file.
   d. Restart the g-api (glance-api) service.
3. Ensure that 'download_image' policy is set in policy.json
   ""download_image"": ""role:admin""
4. Download image using v2 api for role other than admin
   a. source openrc normal_user normal_user
   b. glance --os-image-api-version 2 image-download <image-id>
   Output:
   -------
   ''
   glance-api screen log:
   ----------------------
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] Traceback (most recent call last):
   File ""/usr/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 395, in handle_one_response
  for data in result:
   File ""/mnt/stack/glance/glance/notifier.py"", line 228, in get_data
  for chunk in self.image.get_data():
   File ""/mnt/stack/glance/glance/api/policy.py"", line 233, in get_data
  self.policy.enforce(self.context, 'download_image', {})
   File ""/mnt/stack/glance/glance/api/policy.py"", line 143, in enforce
  exception.Forbidden, action=action)
   File ""/mnt/stack/glance/glance/api/policy.py"", line 131, in _check
  return policy.check(rule, target, credentials, *args, **kwargs)
   File ""/mnt/stack/glance/glance/openstack/common/policy.py"", line 183, in check
  raise exc(*args, **kwargs)
 Forbidden: You are not authorized to complete this action.
 2014-06-05 12:45:00.711 24883 INFO glance.wsgi.server [-] 10.146.146.4 - - [05/Jun/2014 12:45:00] ""GET /v2/images/63826dea-e281-4ffe-821b-f598c747ba54/file HTTP/1.1"" 200 0 0.062499","Fixes v2 return status on unauthorized download

If a user is not allowed to download an image because of a policy,
glance v2 API responds with a  HTTP 200 status and no data instead
of HTTP 403 for no cache only.
The problem is that get_data implementation for notification
proxy is a generator, this situation delays the other proxies
get_data calls (including the policy proxy) for the first time
data is retrieved.
Hence, there is a delay in enforcing policy, so 200 is sent before
the API gets the chance to catch the policy exception.

DocImpact
Closes-Bug: #1326781
Change-Id: I1e50a069a6b7f9eed7160cd5908a5fa30274e227"
689,5d22ec17c7548f3de85ba9e3ad54ce5799dc5fff,1412013262,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1365,1835,1375382,cinder,5d22ec17c7548f3de85ba9e3ad54ce5799dc5fff,1,1,,"Bug #1375382 in Cinder: ""Race condition in iscsi disconnect_volume""","I am seeing an error occur due to a race condition in cinder/brick/connector.py ISCSIConnector's disconnect_volume method. The scenario being that when we do the scsi delete it is expected that the symlink in /dev/disk/by-path is removed if it is no longer in use, and that when we call self.driver.get_all_block_devices() we can check to see if anyone is still using it by assuming if the symlink is there it is in use. Then only if it is no longer in the list we disconnect the iscsi portal.
The issue I am seeing is that there is some delay between calling the scsi delete and the symlink being removed which causes us to see the broken symlink when calling os.listdir(). My understanding is that the rules for udev to clean up the symlinks is done asynchronously, which means we are not guaranteed it will be cleaned up before we call self.driver.get_all_block_devices(). The end result of this causes the iscsi portal to be left open indefinitely when it should have been closed.","Fix race condition in ISCSIConnector disconnect_volume

The list of devices returned by driver.get_all_block_devices() will
sometimes contain broken symlinks as the SCSI device has been deleted
but the udev rule for the symlink has not yet completed.

Adding in a check to os.path.exists() will ensure that we will not
consider the broken symlinks as an “in use” device.

Change-Id: Ibb869e10976f894f9e18e9edec6739c2c3bea68c
Closes-Bug: #1375382"
690,5d39189df6ecb559c3f8b7e2fa3beff25da9f452,1396445614,,1.0,2,2,1,1,1,0.0,True,1.0,501314.0,11.0,4.0,False,87.0,584536.0,199.0,6.0,2209.0,3616.0,5031.0,1712.0,2951.0,3924.0,2110.0,2531.0,3868.0,0.27309185,0.327554981,0.500517464,725,1151,1301384,nova,5d39189df6ecb559c3f8b7e2fa3beff25da9f452,1,1,Bug in logs,"Bug #1301384 in OpenStack Compute (nova): ""Note that XML support *may* be removed, not *will* be""","In Icehouse we marked the v2 API XML support as deprecated.  The log message says it *will* be removed, but should be updated to be more accurate and say *may* be removed, pending finalizing the discussion around it.","Note that XML support *may* be removed.

To be more accurate, note that XML support *may* be removed as early
as the Juno release.  I think there is still more discussion needed
around concrete usage data before the removal date is finalized.

Related thread:
http://lists.openstack.org/pipermail/openstack-dev/2014-April/031608.html

Closes-bug: #1301384
Change-Id: I0415b50ec0b81bb56f5c0fa13bc6d01f8bec7865"
691,5d6cb0c62e0245735a9d511d2f871776055ff224,1395223312,,1.0,9,6,2,2,1,0.836640742,True,3.0,2488899.0,70.0,18.0,False,7.0,3197798.0,7.0,13.0,0.0,814.0,814.0,0.0,743.0,743.0,0.0,538.0,538.0,0.000996016,0.53685259,0.53685259,651,1077,1294568,neutron,5d6cb0c62e0245735a9d511d2f871776055ff224,1,1,Bug. Change the name to avoid conflicts,"Bug #1294568 in neutron: ""Unable to create the Neutron network net_local because of constraints for db2""","CREATE TABLE subnets (tenant_id VARCHAR(255),id VARCHAR(36) NOT NULL, name VARCHAR(255),network_id VARCHAR(36), ip_version INT NOT NULL, cidr VARCHAR(64) NOT NULL,gateway_ip VARCHAR(64),enable_dhcp SMALLINT,shared SMALLINT,ipv6_ra_mode VARCHAR(16), ipv6_address_mode VARCHAR(16),PRIMARY KEY (id), FOREIGN KEY(network_id) REFERENCES networks (id),CHECK (enable_dhcp IN (0, 1)), CHECK (shared IN (0, 1)), CONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')),CONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless')))
for db2, this fails because the name ipv6_modes is used twice for a contraint name.  In db2, A constraint-name must not identify a constraint that was already specified within the same CREATE TABLE statement. (SQLSTATE 42710).
=============
Checked neutron server.log and found
2014-03-18 18:37:45.799 19954 TRACE neutron Traceback (most recent call last):
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/bin/neutron-server"", line 10, in <module>
2014-03-18 18:37:45.799 19954 TRACE neutron     sys.exit(main())
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/server/__init__.py"", line 54, in main
2014-03-18 18:37:45.799 19954 TRACE neutron     neutron_api = service.serve_wsgi(service.NeutronApiService)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 113, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     LOG.exception(_('Unrecoverable error: please check log '
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/excutils.py"", line 68, in __exit_
_
2014-03-18 18:37:45.799 19954 TRACE neutron     six.reraise(self.type_, self.value, self.tb)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 106, in serve_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     service.start()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 75, in start
2014-03-18 18:37:45.799 19954 TRACE neutron     self.wsgi_app = _run_wsgi(self.app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 175, in _run_wsgi
2014-03-18 18:37:45.799 19954 TRACE neutron     app = config.load_paste_app(app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 170, in load_paste_app
2014-03-18 18:37:45.799 19954 TRACE neutron     app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-03-18 18:37:45.799 19954 TRACE neutron     return loadobj(APP, uri, name=name, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-03-18 18:37:45.799 19954 TRACE neutron     return context.create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-03-18 18:37:45.799 19954 TRACE neutron     app = loader.get_app(app_name, global_conf=global_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron     name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-03-18 18:37:45.799 19954 TRACE neutron     app = loader.get_app(pipeline[-1])
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-03-18 18:37:45.799 19954 TRACE neutron     name=name, global_conf=global_conf).create()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 710, in create
2014-03-18 18:37:45.799 19954 TRACE neutron     return self.object_type.invoke(self)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-03-18 18:37:45.799 19954 TRACE neutron     return fix_call(context.object, context.global_conf, **context.local_conf)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/paste/deploy/util.py"", line 56, in fix_call
2014-03-18 18:37:45.799 19954 TRACE neutron     val = callable(*args, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-03-18 18:37:45.799 19954 TRACE neutron     return cls(**local_config)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     plugin = manager.NeutronManager.get_plugin()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-03-18 18:37:45.799 19954 TRACE neutron     return cls.get_instance().plugin
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     cls._create_instance()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-03-18 18:37:45.799 19954 TRACE neutron     return f(*args, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     cls._instance = cls()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     plugin_provider)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-03-18 18:37:45.799 19954 TRACE neutron     return plugin_class()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 105, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     super(Ml2Plugin, self).__init__()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/db_base_plugin_v2.py"", line 225, in __init__
2014-03-18 18:37:45.799 19954 TRACE neutron     db.configure_db()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 34, in configure_db
2014-03-18 18:37:45.799 19954 TRACE neutron     register_models()
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/neutron/db/api.py"", line 53, in register_models
2014-03-18 18:37:45.799 19954 TRACE neutron     base.metadata.create_all(engine)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/schema.py"", line 2571, in create_all
2014-03-18 18:37:45.799 19954 TRACE neutron     tables=tables)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 2302, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron     conn._run_visitor(visitorcallable, element, **kwargs)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1972, in _run_visitor
2014-03-18 18:37:45.799 19954 TRACE neutron     **kwargs).traverse_single(element)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron     return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 67, in visit_metadata
2014-03-18 18:37:45.799 19954 TRACE neutron     self.traverse_single(table, create_ok=True)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/sql/visitors.py"", line 106, in traverse_singl
e
2014-03-18 18:37:45.799 19954 TRACE neutron     return meth(obj, **kw)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/ddl.py"", line 86, in visit_table
2014-03-18 18:37:45.799 19954 TRACE neutron     self.connection.execute(schema.CreateTable(table))
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron     params)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
2014-03-18 18:37:45.799 19954 TRACE neutron     compiled
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron     context)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_conte
xt
2014-03-18 18:37:45.799 19954 TRACE neutron     context)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib/python2.6/site-packages/ibm_db_sa/ibm_db.py"", line 104, in do_execute
2014-03-18 18:37:45.799 19954 TRACE neutron     cursor.execute(statement, parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1335, in execute
2014-03-18 18:37:45.799 19954 TRACE neutron     self._execute_helper(parameters)
2014-03-18 18:37:45.799 19954 TRACE neutron   File ""/usr/lib64/python2.6/site-packages/ibm_db_dbi.py"", line 1247, in _execute_helper
2014-03-18 18:37:45.799 19954 TRACE neutron     raise self.messages[len(self.messages) - 1]
2014-03-18 18:37:45.799 19954 TRACE neutron ProgrammingError: (ProgrammingError) ibm_db_dbi::ProgrammingError: Statement Execute Failed: [IBM][
CLI Driver][DB2/LINUXX8664] SQL0601N  The name of the object to be created is identical to the existing name ""IPV6_MODES"" of type ""CHECK CONST""
.  SQLSTATE=42710 SQLCODE=-601 ""\nCREATE TABLE subnets (\n\ttenant_id VARCHAR(255), \n\tid VARCHAR(36) NOT NULL, \n\tname VARCHAR(255), \n\tnet
work_id VARCHAR(36), \n\tip_version INT NOT NULL, \n\tcidr VARCHAR(64) NOT NULL, \n\tgateway_ip VARCHAR(64), \n\tenable_dhcp SMALLINT, \n\tshar
ed SMALLINT, \n\tipv6_ra_mode VARCHAR(16), \n\tipv6_address_mode VARCHAR(16), \n\tPRIMARY KEY (id), \n\tFOREIGN KEY(network_id) REFERENCES netw
orks (id), \n\tCHECK (enable_dhcp IN (0, 1)), \n\tCHECK (shared IN (0, 1)), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_ra_mode IN ('slaac', 'dhcpv6-
stateful', 'dhcpv6-stateless')), \n\tCONSTRAINT ipv6_modes CHECK (ipv6_address_mode IN ('slaac', 'dhcpv6-stateful', 'dhcpv6-stateless'))\n)\n\n
"" ()
2014-03-18 18:37:45.799 19954 TRACE neutron","Use different name for the same constraint

In DB2, a constraint-name must not identify a constraint
that was already specified within the same CREATE TABLE
statement. The current CREATE TABLE statement uses the same
constraint name 'ipv6_modes' for both ipv6_ra_mode and
ipv6_address_mode column. This change tries to use different
names.

Change-Id: Id4d82fb7e0e570a843e28856e531e25578a4351a
Closes-Bug: #1294568"
692,5d782b33b772987b472ae5aeb5ec13e1bd30335f,1394160618,1.0,1.0,29,3,3,3,1,0.465666329,True,2.0,654994.0,41.0,3.0,False,15.0,395219.0,23.0,2.0,5.0,1153.0,1155.0,5.0,999.0,1001.0,5.0,673.0,675.0,0.006521739,0.732608696,0.734782609,554,975,1289100,neutron,5d782b33b772987b472ae5aeb5ec13e1bd30335f,0,0,"For conformity wit Neutron agents, this needs to be added","Bug #1289100 in neutron: ""SDN-VE plugin agent does not report its state""","The agent for SDN-VE plugin does not show its state periodically and the plugin does not support the ""agent"" extension. For conformity wit Neutron agents, this needs to be added to the plugin and the agent.","Adds state reporting to SDN-VE agent

Adds periodic reporting of state by the agent
and support for agent extension to the plugin.

Change-Id: Ib7c29936f1c10cb9749291ac27b3ed24b20ed2f7
Closes-Bug: #1289100"
693,5da44b9bdb65c54f823ce7867386b9ca0c9a6b25,1399282467,,1.0,12,8,2,2,1,0.934068055,True,2.0,1257021.0,43.0,6.0,False,11.0,190836.0,14.0,4.0,5.0,627.0,627.0,5.0,600.0,600.0,5.0,516.0,516.0,0.004966887,0.427980132,0.427980132,860,1293,1315815,neutron,5da44b9bdb65c54f823ce7867386b9ca0c9a6b25,0,0,"Feature ""Radware LBaaS driver should transfer the tenant-id to the back system when creating an ADC service”","Bug #1315815 in neutron: ""Radware LBaaS driver, transfer tenant id to back system""",Radware LBaaS driver should transfer the tenant-id to the back system when creating an ADC service,"Adding tenant-id while creating Radware ADC service

Adding tenant_id as a parameter when creating Radware ADC service

Changing unit tests to test the udated REST call resource
with tenant as a parameter

Change-Id: I7a3e9bfb492e847195031bb72ef15dae8769e91f
Closes-Bug: #1315815"
694,5dcbddf516750e21359c859f8da0ab829f67d3f9,1380264555,,1.0,2,2,1,1,1,0.0,True,1.0,260296.0,16.0,8.0,False,3.0,13179.0,6.0,5.0,7.0,799.0,803.0,7.0,739.0,743.0,5.0,205.0,208.0,0.015345269,0.52685422,0.534526854,1590,1229548,1229548,neutron,5dcbddf516750e21359c859f8da0ab829f67d3f9,1,1, ,"Bug #1229548 in neutron: ""Advanced service router becomes destination of a floating ip for a short period of time""","If we initiate a connection to a floating ip right after creating/associating it with a VM's port, the connection goes to advanced service router for a short period if time, instead to the VM the floating ip is supposed to be associated with.
The root cause is when creating/associating a floating ip using advanced service router, in addition to create a DNAT rule, we also need to configure the floating ip on advanced service router's vNic so the advanced service router can reply ARP request for that IP. We configure the IP on the vnic before the DNAT rule is configured, therefore, there is a small window, after IP is configured but before DNAT is configured, that the traffic to floating IP will reach advanced service router.
The fix is to configure the DNAT rule before applying the IP on advanced service router's vNic.","Reverse the order of interface update and DNAT rule config

Configure DNAT rule first before adding floating ip address to interface
so advanced service router will not receive packets by accident before
DNAT rule configured.

Verified that traffic goes to the VM the created floating ip associated
with right after config.

Change-Id: I415d1138511f41c209f1f2a9c2f12c2cfd3d16f0
Closes-Bug: #1229548"
695,5dda3a6ab2becb5dd0b58c088f6daad807e12276,1400181739,,1.0,77,7,6,6,1,0.835237725,True,3.0,566757.0,50.0,16.0,False,220.0,256274.1667,914.0,2.0,479.0,1221.0,1500.0,470.0,1220.0,1490.0,456.0,1211.0,1468.0,0.057217979,0.151746588,0.183923876,890,1326,1319943,nova,5dda3a6ab2becb5dd0b58c088f6daad807e12276,1,1,,"Bug #1319943 in OpenStack Compute (nova): ""libvirt driver's to_xml method logs iscsi auth_password if debug""",If you have debug logging enabled the libvirt driver's to_xml method logs the iscsi auth_password in plain text.,"Mask block_device_info auth_password in virt driver debug logs

The block_device_info object can have an auth_password key which is
getting logged at debug level in several virt drivers so we need to
sanitize the message getting logged.

Adds tests to ensure the logged messages are properly sanitized.

Note that bug 1321785 was opened to track the long-term design issues
with storing the password in the block_device_info dict since this can
crop up elsewhere if it's logged.  The immediate fix here is to mask
what's already exposed.

Closes-Bug: #1319943

Change-Id: I0eae07ce3f0f39861eb97ec3dec44895386c7d04"
696,5de69badfc44dd03e5fa21577a66daabc3ade5a0,1387807027,,1.0,38,3,3,2,1,0.937923209,True,13.0,2648295.0,68.0,23.0,False,33.0,217989.3333,102.0,2.0,130.0,678.0,713.0,130.0,517.0,552.0,120.0,523.0,548.0,0.111829945,0.484288355,0.507393715,219,622,1263684,glance,5de69badfc44dd03e5fa21577a66daabc3ade5a0,1,1,Bad eror code,"Bug #1263684 in Glance: ""DB2: GET image with long ID (non-existed) Glance would return 500, but not 404""","GET:
http://9.12.27.148:9292/v1/images/82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r
would return 500 instead of 400.
The root cause is as below:
2013-12-23 07:43:15.387 27806 INFO glance.wsgi.server [687bccf6-ed9e-4efa-988e-a05faafb6c4b 51336fdd98e449ef911f57ad3d03c818 d9f056c22dac4022bf2ebb1ed70fd25e] Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/eventlet/wsgi.py"", line 384, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/keystoneclient/middleware/auth_token.py"", line 571, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 368, in __call__
    response = req.get_response(self.application)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/lib/python2.6/site-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/lib/python2.6/site-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 620, in __call__
    request, **action_args)
  File ""/usr/lib/python2.6/site-packages/glance/common/wsgi.py"", line 646, in dispatch
    return method(*args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/glance/registry/api/v1/images.py"", line 312, in show
    image = self.db_api.image_get(req.context, id)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 315, in image_get
    force_show_deleted=force_show_deleted)
  File ""/usr/lib/python2.6/site-packages/glance/db/sqlalchemy/api.py"", line 343, in _image_get
    raise e
DataError: (DataError) ibm_db_dbi::DataError: Statement Execute Failed: [IBM][CLI Driver] CLI0109E  String data right truncation. SQLSTATE=22001 SQLCODE=-99999 'SELECT images.created_at AS images_created_at, images.updated_at AS images_updated_at, images.deleted_at AS images_deleted_at, images.deleted AS images_deleted, images.id AS images_id, images.name AS images_name, images.disk_format AS images_disk_format, images.container_format AS images_container_format, images.size AS images_size, images.status AS images_status, images.is_public AS images_is_public, images.checksum AS images_checksum, images.min_disk AS images_min_disk, images.min_ram AS images_min_ram, images.owner AS images_owner, images.protected AS images_protected, image_properties_1.created_at AS image_properties_1_created_at, image_properties_1.updated_at AS image_properties_1_updated_at, image_properties_1.deleted_at AS image_properties_1_deleted_at, image_properties_1.deleted AS image_properties_1_deleted, image_properties_1.id AS image_properties_1_id, image_properties_1.image_id AS image_properties_1_image_id, image_properties_1.name AS image_properties_1_name, image_properties_1.""value"" AS image_properties_1_value, image_locations_1.created_at AS image_locations_1_created_at, image_locations_1.updated_at AS image_locations_1_updated_at, image_locations_1.deleted_at AS image_locations_1_deleted_at, image_locations_1.deleted AS image_locations_1_deleted, image_locations_1.id AS image_locations_1_id, image_locations_1.image_id AS image_locations_1_image_id, image_locations_1.""value"" AS image_locations_1_value, image_locations_1.meta_data AS image_locations_1_meta_data \nFROM images LEFT OUTER JOIN image_properties AS image_properties_1 ON images.id = image_properties_1.image_id LEFT OUTER JOIN image_locations AS image_locations_1 ON images.id = image_locations_1.image_id \nWHERE images.id = ?' ('82ff46a0-f49b-4279-bdad-e665c203444477777777777w232323232eer34r3r3r3qer3r3r3r',)","Check image id format before executing operations

DB2 will raise 'CLI0109E String data right truncation' error if
the given query parameter is longer than the column definition.
As a result, user will run into 500 instead of 404 error though
the malformed id is not existed in DB.
This fix will add image id format checking before executing real
operations.

Closes-Bug: #1263684

Change-Id: I1ab3e38724014032d9290e7dde3f8b6d23b6df6c"
697,5defcd0aa61b96933369a545040e57d9665add7a,1406777277,,1.0,32,35,31,25,1,0.988648732,False,,,,,True,,,,,,,,,,,,,,,,,1177,1634,1355565,neutron,5defcd0aa61b96933369a545040e57d9665add7a,0,0,"cleanup. “As a minor cleanup, we should remove the argument in the interests”","Bug #1355565 in neutron: ""config argument to config.setup_logging is unused""","In neutron/common/config.py, the argument to setup_logging is unused:
  def setup_logging(conf):
     product_name = ""neutron""
     logging.setup(product_name)
     LOG.info(_(""Logging enabled!""))
As a minor cleanup, we should remove the argument in the interests of simpler code and avoiding confusion.","Remove unused arg to config.setup_logging()

The cfg.CFG argument is ignored and misleading.  This change removes it
and updates all callers.

Closes-Bug: #1355565
Change-Id: I2fcece85d1abe848e5c01125cfb62c02f2dcbc86"
698,5e3f8db96fd0390c6f66754d0e1105ba127e1aa8,1412817796,,1.0,10,8,3,2,1,0.852792488,False,,,,,True,,,,,,,,,,,,,,,,,1397,1867,1380456,nova,5e3f8db96fd0390c6f66754d0e1105ba127e1aa8,1,1,,"Bug #1380456 in OpenStack Compute (nova): ""Use 400 instead of 422 for security_groups v2 API""","For any invalid request format we should return 400, not 422, we should fix this case for v2 security_groups API. That is also good for sharing the unittest between v2.1 and v2.
To change the API that follow the rule: https://wiki.openstack.org/wiki/APIChangeGuidelines","Use 400 instead of 422 for security_groups v2 API

For any invalid request format we should return 400, not 422,
this patch fixes this case for v2 security_groups API.

This patch follows the rule:
https://wiki.openstack.org/wiki/APIChangeGuidelines

Change-Id: I78f523901918901e55b339d0cbd9cc78dd52044f
Closes-Bug: #1380456"
699,5e4a5f0d8c62ca6e94ae6db16e9fbe0428805158,1406287150,,1.0,7,6,2,2,1,0.779349837,False,,,,,True,,,,,,,,,,,,,,,,,1103,1554,1348584,nova,5e4a5f0d8c62ca6e94ae6db16e9fbe0428805158,1,1,,"Bug #1348584 in OpenStack Compute (nova): ""KeyError in nova.compute.api.API.external_instance_event""","The fix for bug 1333654 ensured events for instance without host are not accepted.
However, the instances without the host are still being passed to the compute API layer.
This is likely to result in keyerrors as the one found here: http://logs.openstack.org/51/109451/2/check/check-tempest-dsvm-neutron-full/ad70f74/logs/screen-n-api.txt.gz#_2014-07-25_01_41_48_068
The fix for this bug should be straightforward.","Do not pass instances without host to compute API

Even if the server external events extension filters out events
whose related instance does not have a host, the corresponding
instance is still sent to the compute API module.
As this might result in KeyError, instance without host should
be filtered out before calling the compute API module.

Change-Id: If5229ec3059076dbc9f4abb6625504e8864c265e
Closes-Bug: #1348584"
700,5e4b0c6fc6670ea036d801ce53444272bc311929,1394175526,0.0,1.0,66,54,7,5,1,0.765804777,True,6.0,832858.0,123.0,19.0,False,40.0,278186.0,105.0,4.0,493.0,601.0,966.0,402.0,508.0,795.0,236.0,340.0,487.0,0.256771398,0.369447454,0.528710726,450,865,1282925,neutron,5e4b0c6fc6670ea036d801ce53444272bc311929,0,0,no bug: ‘can lead to long transaction if ‘,"Bug #1282925 in neutron: ""db_plugin.delete_ports() can lead to long transaction if plugin.deleete_port talks with external system""","db_plugin.delete_ports() can lead to long transaction if plugin.delete_port talks with external system.
it is observed first in nec plugin (bug 1282922), but it affects multiple plugins/drivers.
Note that it is about delete_ports and not about delete_port.
The detail is described in bug 1282922. Quoted from the original bug report.
----
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.
After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout.
----","Avoid long transaction in plugin.delete_ports()

db_plugin.delete_ports() called plugin.delete_port() under
a transaction. It leads to long transaction if plugin.delete_port
talks with external systems. This commit changes each delete_port
outside of a transaction to avoid longer transaction.

plugin.delete_ports is now called by release_dhcp_ports and
dhcp-agent ports can be deleted separately, so this changes
does not break the existing behavior.

delete_ports is renamed to delete_ports_by_device_id
to clarify the usage of this method.

NEC plugin already has this change and it is no longer needed.

_do_side_effect helper method in test_db_plugin is renamed
to more self-descriptive name.

Change-Id: Ied5883a57c7774c3b0778453d84c717b337f88c0
Closes-Bug: #1282925
Related-Bug: #1283522"
701,5e4b600528a2b58cbb38d6b8d55b316602d4d015,1413785796,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1404,1874,1381277,neutron,5e4b600528a2b58cbb38d6b8d55b316602d4d015,1,1,,"Bug #1381277 in neutron: ""Fix docstring on send_delete-port_request in N1kv plugin""",The docstring for _send_delete_port_request wasn't properly updated as part of bug/1373547 to reflect that the last port check and subsequent call to delete_vm_network were removed. Need to update this docstring to reflect that change.,"Modify docstring on send_delete_port_request in N1kv plugin

N1kv plugin: Modify docstring on _send_delete_port_request method

Change-Id: I64d34878ffd8f6db703e4c1d9849032fef9bae96
Closes-Bug: #1381277"
702,5e4e1f7ea71f9b4c7bd15809c58bc7a1838ed567,1412366221,,1.0,8,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1381,1851,1377981,cinder,5e4e1f7ea71f9b4c7bd15809c58bc7a1838ed567,1,1,“cve-2014-7230)”,"Bug #1377981 in Cinder: ""[OSSA 2014-036] Missing fix for ssh_execute (Exceptions thrown may contain passwords) (CVE-2014-7230, CVE-2014-7231)""","Former bugs:
  https://bugs.launchpad.net/ossa/+bug/1343604
  https://bugs.launchpad.net/ossa/+bug/1345233
The ssh_execute method is still affected in Cinder and Nova Icehouse release.
It is prone to password leak if:
- passwords are used on the command line
- execution fail
- calling code catch and log the exception
The missing fix from oslo-incubator to be merged is: 6a60f84258c2be3391541dbe02e30b8e836f6c22","Sync latest processutils from oslo-incubator

An earlier commit (Ia92aab76fa83d01c5fbf6f9d31df2463fc26ba5c) failed
to address ssh_execute(). This change set addresses ssh_execute.

------------------------------------------------

oslo-incubator head:

commit 4990535fb5f3e2dc9b397e1a18c1b5dda94ef1c4
Merge: 9f5c700 2a130bf
Author: Jenkins <jenkins@review.openstack.org>
Date:   Mon Sep 29 23:12:14 2014 +0000

    Merge ""Script to list unreleased changes in all oslo projects""

-----------------------------------------------

The sync pulls in the following changes (newest to oldest):

6a60f842 - Mask passwords in exceptions and error messages (SSH)

-----------------------------------------------

Change-Id: Ie0caf32469126dd9feb44867adf27acb6e383958
Closes-Bug: #1377981"
703,5e8043e8a2675b4485a52d688ea86d1165d81a29,1394141187,,1.0,115,46,2,2,1,0.957248576,True,1.0,1505365.0,30.0,5.0,False,1.0,2403027.0,1.0,4.0,18.0,753.0,756.0,17.0,615.0,617.0,14.0,387.0,388.0,0.016429354,0.424972618,0.426067908,54,382,1247976,neutron,5e8043e8a2675b4485a52d688ea86d1165d81a29,0,0,Add VM (live) migration support to the ML2 cisco nexus mechanism,"Bug #1247976 in neutron: ""ML2 Cisco Nexus MD: Enhance update_port processing""","Port this change in the cisco plugin, https://review.openstack.org/#/c/50389/ (Detect and process live-migration in Cisco plugin) to ML2. Changes under this bug were made to the cisco plugin update_port() method.","ML2 Cisco Nexus MD: VM migration support

Add VM (live) migration support to the ML2 cisco nexus mechanism driver.

Change-Id: I8be2fc1f020ef1fc6c19daba0a9e278629046016
Closes-Bug: #1247976"
704,5e9305a6f934549408a9c18480fc1c000126621e,1412282121,,1.0,69,83,5,5,1,0.68733512,False,,,,,True,,,,,,,,,,,,,,,,,1383,1853,1378398,neutron,5e9305a6f934549408a9c18480fc1c000126621e,0,0,Refactoring “Remove legacy weight from l3 agent”,"Bug #1378398 in neutron: ""Remove legacy weight from l3 agent _process_routers""","Some work in Juno around adding a new router processing queue to the l3_agent.py obsoleted much of the logic in the _process_routers method.  The following can be simplified.
1. No loop is necessary since the list passed always has exactly one router in it.
2. No thread pool is necessary because there is only one thread active and the method waits for it to complete at the end.
3. The set logic is no longer needed.","Refactor _process_routers to handle a single router

The method _process_routers no longer handles multiple routers.  The
only caller of this method would construct a list of exactly one
router in order to make the call.  This made the for loop unnecessary.
The method's logic is too heavy for its current purpose.  This commit
removes much of the weight.

The use of the sets in this method is also no longer necessary.  It
became clear that all of it boiled down to ""if the router is not
compatible with with this agent but it is known in router_info from
before then we need to remove it.""  This is an exceptional condition
that shouldn't be handled in this method so I raise an exception and
handle it in process_router_update where other router removal is
handled.  Logging was added for this exceptional condition.

The eventlet pool was also obsolete.  It was used to spawn two methods
and there was a waitall at the end.  The other refactoring made it
clear that the two spawns were mutually exclusive.  There was only one
thread spawned for any given invocation of the method and the eventlet
pool is overkill.

Change-Id: Ibeac591b08565d10b2a9730e25a54f2cd11fc2bc
Closes-Bug: #1378398"
705,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1404048740,,3.0,496,148,2,2,1,0.997215339,False,,,,,True,,,,,,,,,,,,,,,,,468,883,1284284,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1,1,It ignores a parameter. This implements the fix for the bug...,"Bug #1284284 in Cinder: ""VMware: volume create ignores adapter_type""","I believe this issue is part of the root cause for this nova bug: https://bugs.launchpad.net/nova/+bug/1255317
Basically, when creating a volume from a vmdk image, the adapter_type is ignored and it defaults to LSI logic.
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L368
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/vmware/volumeops.py#L327
So cinder creates a volume with LSI SCSI controller and IDE disk.  And eventually, when I try to boot from a volume in Nova, I get ""No operating system found"".  I don't think you can boot an IDE disk using SCSI adapter.
However, I am able to boot from an image with IDE adapter.  So seems the volume step is causing an issue.
My recreate steps:
1) glance image-create --name cirros-sparse --is-public=True --container-format=bare --disk-format=vmdk --property vmware_disktype=""sparse"" < cirros-0.3.0-i386-disk.vmdk
2) cinder create --name sparse-ide --image-id cd0a8dea-2e2f-43be-97ab-85e21375b757 1
3) Observe in vCenter that the newly created volume has adapter set as LSI logic.
My VMDK image metadata:
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
ericwb@ericwb-virtual-machine:~$ head -n 20disk-vmdk.vmdk
head: 20disk-vmdk.vmdk: invalid number of lines
ericwb@ericwb-virtual-machine:~$ head -n 20 disk-vmdk.vmdk
KDMV?9??
# Disk DescriptorFile
version=1
CID=5269c92d
parentCID=ffffffff
createType=""monolithicSparse""
# Extent description
RW 80325 SPARSE ""disk-vmdk.vmdk""
# The Disk Data Base
#DDB
ddb.virtualHWVersion = ""4""
ddb.geometry.cylinders = ""79""
ddb.geometry.heads = ""16""
ddb.geometry.sectors = ""63""
ddb.adapterType = ""ide""","VMware: Volume from non-streamOptimized image

Volume creation from non-streamOptimized images (preallocated/thin/sparse)
has following problems:

1) Sparse vmdk image is not converted to appropriate virtual disk type
   suitable for attaching to a nova instance.
2) The adapter type in image meta-data is ignored while creating volumes.
3) The vmware:vmdk_type extra_spec property is ignored.
4) Virtual disk extent operation is called with a wrong parameter which
   might result in unwanted disk provisioning type conversion.

This patch fixes the first 3 problems using the following workflow:

a) Create a disk-less backing.
b) Create a virtual disk (single flat extent) from the non-streamOptimized
   image
c) Attach the virtual disk to the backing
d) Clone the backing (if needed) to perform disk provisioning type conversion

Closes-Bug: #1287176
Closes-Bug: #1287185
Closes-Bug: #1284284
Change-Id: Ib7e9fae81d69d2fe490a4b603337f3d5cee1138c"
706,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1404048740,,3.0,496,148,2,2,1,0.997215339,False,,,,,True,,,,,,,,,,,,,,,,,509,929,1287176,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1,1,Bug. Make something optional,"Bug #1287176 in Cinder: ""VMware: vmdk driver not handling vmware_disktype=sparse properly during copy image to volume""","Currently a sparse disk based image is copied as a flat file which is one of the causes of the following bug:
https://bugs.launchpad.net/nova/+bug/1255317
Also, 'vmdk_type' extra spec property of the cinder volume is ignored.
The image should be copied, and then converted to appropriate type based on 'vmdk_type'.","VMware: Volume from non-streamOptimized image

Volume creation from non-streamOptimized images (preallocated/thin/sparse)
has following problems:

1) Sparse vmdk image is not converted to appropriate virtual disk type
   suitable for attaching to a nova instance.
2) The adapter type in image meta-data is ignored while creating volumes.
3) The vmware:vmdk_type extra_spec property is ignored.
4) Virtual disk extent operation is called with a wrong parameter which
   might result in unwanted disk provisioning type conversion.

This patch fixes the first 3 problems using the following workflow:

a) Create a disk-less backing.
b) Create a virtual disk (single flat extent) from the non-streamOptimized
   image
c) Attach the virtual disk to the backing
d) Clone the backing (if needed) to perform disk provisioning type conversion

Closes-Bug: #1287176
Closes-Bug: #1287185
Closes-Bug: #1284284
Change-Id: Ib7e9fae81d69d2fe490a4b603337f3d5cee1138c"
707,5eac161aa81a68e7cc4adb92798f6c3e849da81c,1404048740,,3.0,496,148,2,2,1,0.997215339,False,,,,,True,,,,,,,,,,,,,,,,,510,930,1287185,cinder,5eac161aa81a68e7cc4adb92798f6c3e849da81c,0,0,Add support for an argument,"Bug #1287185 in Cinder: ""VMware: vmdk driver ignoring vmdk_type extra_spec property while copying image with vmware_disktype=thin/preallocated""",The volume's vmdk_type extra_spec property is ignored while copying image with vmware_disktype=thin/preallocated to a volume. The image should be downloaded and converted to appropriate type based on vmdk_type.,"VMware: Volume from non-streamOptimized image

Volume creation from non-streamOptimized images (preallocated/thin/sparse)
has following problems:

1) Sparse vmdk image is not converted to appropriate virtual disk type
   suitable for attaching to a nova instance.
2) The adapter type in image meta-data is ignored while creating volumes.
3) The vmware:vmdk_type extra_spec property is ignored.
4) Virtual disk extent operation is called with a wrong parameter which
   might result in unwanted disk provisioning type conversion.

This patch fixes the first 3 problems using the following workflow:

a) Create a disk-less backing.
b) Create a virtual disk (single flat extent) from the non-streamOptimized
   image
c) Attach the virtual disk to the backing
d) Clone the backing (if needed) to perform disk provisioning type conversion

Closes-Bug: #1287176
Closes-Bug: #1287185
Closes-Bug: #1284284
Change-Id: Ib7e9fae81d69d2fe490a4b603337f3d5cee1138c"
708,5eafe1ccb30462d9b78b13323135fb9f90d1fd54,1393938317,,1.0,25,4,2,1,1,0.849751137,True,2.0,25914.0,19.0,9.0,False,7.0,10632815.5,12.0,5.0,859.0,1311.0,1965.0,613.0,1211.0,1644.0,806.0,1295.0,1898.0,0.107485349,0.172615876,0.252930208,519,940,1287542,nova,5eafe1ccb30462d9b78b13323135fb9f90d1fd54,1,0,Bug because of evolution,"Bug #1287542 in OpenStack Compute (nova): ""Error importing module nova.openstack.common.sslutils: duplicate option: ca_file""","Error importing module nova.openstack.common.sslutils: duplicate option: ca_file
This is seen in the nova gate - for unrelated patches - it might be a bad slave I guess, or it might be happening to all  subsequent patches, or it might be a WTF.
http://logstash.openstack.org/#eyJzZWFyY2giOiJcIkVycm9yIGltcG9ydGluZyBtb2R1bGUgbm92YS5vcGVuc3RhY2suY29tbW9uLnNzbHV0aWxzOiBkdXBsaWNhdGUgb3B0aW9uOiBjYV9maWxlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjkwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTM5MTUyNTE4ODl9 suggest it has only happened once so far.
commit 5188052937219badaa692f67d9f98623c15d1de2
Merge: af626d0 88b7380
Author: Jenkins <email address hidden>
Date:   Tue Mar 4 02:47:02 2014 +0000
    Merge ""Sync latest config file generator from oslo-incubator""
Was the latest merge prior to this, but it may be coincidental.","sync sslutils to not conflict with oslo.messaging

oslo.messaging changed and shipped nova/openstack/common/sslutils.py
in their own tree before it was changed in oslo-incubator. This
created conflicting options with nova, and caused the config
generator to explode (this also would have exploded if we used
any of the sslutils code in the gate, which we don't).

This is a sync of the finally merged oslo-incubator fix (which is
just periods in comments). This now makes the sslutils declaration
the same in both nova and olso.messaging.

Change-Id: I5cf392215825b604f0e3fe9546591a1e754d4478
Closes-Bug: #1287542"
709,5ebc60c48d4a8b6c7bac96d923626df9ae67a2b2,1395058450,,1.0,61,48,2,2,1,0.989714805,True,3.0,235548.0,40.0,12.0,False,146.0,297322.5,607.0,3.0,404.0,2168.0,2366.0,403.0,1934.0,2132.0,386.0,2122.0,2303.0,0.050780737,0.278572366,0.30232253,208,611,1262461,nova,5ebc60c48d4a8b6c7bac96d923626df9ae67a2b2,1,0,Evolution: various resize methods are using the old-style quota class to reserve and commit quota changes directly to the database,"Bug #1262461 in OpenStack Compute (nova): ""resize auto-confirmation failed with nova-conductor because of db access in nova-compute""","I can reproduce this bug in master and stable havana.
this bug is similar to https://bugs.launchpad.net/nova/+bug/1158897
but this bug is cause by _reserve_quota_delta() method, which will need to access DB if quota delta is not empty,
the trace log is:
2013-12-19 01:39:40.879 ERROR nova.compute [-] No db access allowed in nova-compute:
File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/loopingcall.py"", line 125, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/opt/stack/nova/nova/service.py"", line 314, in periodic_tasks
    return self.manager.periodic_tasks(ctxt, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/manager.py"", line 101, in periodic_tasks
    return self.run_periodic_tasks(context, raise_on_error=raise_on_error)
  File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
    task(self, context)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 4535, in _poll_unconfirmed_resizes
    migration=migration)
  File ""/opt/stack/nova/nova/compute/api.py"", line 199, in wrapped
    return func(self, context, target, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 189, in inner
    return function(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 216, in _wrapped
    return fn(self, context, instance, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/api.py"", line 170, in inner
    return f(self, context, instance, *args, **kw)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2158, in confirm_resize
    reservations = self._reserve_quota_delta(context, deltas)
  File ""/opt/stack/nova/nova/compute/api.py"", line 2238, in _reserve_quota_delta
    return QUOTAS.reserve(context, project_id=project_id, **deltas)
  File ""/opt/stack/nova/nova/quota.py"", line 1272, in reserve
    user_id=user_id)
  File ""/opt/stack/nova/nova/quota.py"", line 487, in reserve
    has_sync=True, project_id=project_id)
  File ""/opt/stack/nova/nova/quota.py"", line 354, in _get_quotas
    usages=False)
  File ""/opt/stack/nova/nova/quota.py"", line 264, in get_project_quotas
    project_quotas = db.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/db/api.py"", line 1023, in quota_get_all_by_project
    return IMPL.quota_get_all_by_project(context, project_id)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 48, in __call__
    stacktrace = """".join(traceback.format_stack())
2013-12-19 01:39:40.879 ERROR nova.compute.manager [-] [instance: 4ffaabaf-a480-421f-a7a7-efd148d7c956] Error auto-confirming resize: nova-compute. Will retry later.","Make compute API resize methods use Quotas objects

The compute API's various resize methods are using the old-style quota
class to reserve and commit quota changes directly to the database. This
change uses quotas objects to do the reserve and commit instead so those
operations go through conductor for the database access.

Note that the long-term goal is to pass the quota object over RPC
rather than the reservations list, but that's a larger patch which
is not necessary for the bug fix.

Closes-Bug: #1262461

Change-Id: Id18357f449193d25287f32bb8ffbf26ee2194a22"
710,5ec9baa8857646e0608cd9a3135d172f3b92327c,1386272092,0.0,1.0,25,2,3,3,1,0.852792488,True,5.0,648655.0,33.0,8.0,False,60.0,404473.6667,204.0,2.0,1.0,372.0,372.0,1.0,282.0,282.0,1.0,236.0,236.0,0.001915709,0.227011494,0.227011494,137,538,1257496,glance,5ec9baa8857646e0608cd9a3135d172f3b92327c,1,1,Creating image with bad scheme in location causes 500,"Bug #1257496 in Glance: ""Glance v1: Creating image with bad scheme in location causes 500""","When creating an image in glance v1 and specifyig the location with a bad scheme you receive an HTTP 500. In this case 'http+swift"" is a bad scheme.
glance image-create --name bad-location --disk-format=vhd --container-format=ovf --location=""http+swift://bah""
Request returned failure status.
HTTPInternalServerError (HTTP 500)
2013-12-03 21:24:32.009 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866e
a33fa52945f0 54bc4959075343ff80f460b77e783a49] Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/wsgi.py"", line 389, in handle_one_response
    result = self.application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
    return self.app(env, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 367, in __call__
    response = req.get_response(self.application)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
    application, catch_exc_info=False)
  File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
    app_iter = application(self.environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/paste/urlmap.py"", line 203, in __call__
    return app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
    response = self.app(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
    return resp(environ, start_response)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
    resp = self.call_func(req, *args, **self.kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
    return self.func(req, *args, **kwargs)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 599, in __call__
    request, **action_args)
  File ""/opt/stack/glance/glance/common/wsgi.py"", line 618, in dispatch
    return method(*args, **kwargs)
  File ""/opt/stack/glance/glance/common/utils.py"", line 422, in wrapped
    return func(self, req, *args, **kwargs)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 754, in create
    image_meta = self._reserve(req, image_meta)
  File ""/opt/stack/glance/glance/api/v1/images.py"", line 488, in _reserve
    store = get_store_from_location(location)
  File ""/opt/stack/glance/glance/store/__init__.py"", line 263, in get_store_from_location
    loc = location.get_location_from_uri(uri)
  File ""/opt/stack/glance/glance/store/location.py"", line 73, in get_location_from_uri
    raise exception.UnknownScheme(scheme=pieces.scheme)
UnknownScheme: Unknown scheme 'http+swift' found in URI
2013-12-03 21:24:32.011 6312 INFO glance.wsgi.server [402e831a-935d-4e14-b4c8-64653c14263d 1c3848b015f94b70866ea33fa52945f0 54bc4959075343ff80f460b77e783a49] localhost - - [03/Dec/2013 21:24:32] ""POST /v1/images HTTP/1.1"" 500 139 0.216952","Replace startswith by more precise store matching

Currently a source is considered valid if it only starts with a specific set
of word. For example swift is a valid location scheme, so is swiftpe or
swiftopenstack with the current implementation. This commit compare the scheme
of the location - all of it - to a given set of supported schemes

Change-Id: Ie827c3c3f36fbf958a1270039cab28d4c5fc4fe1
Closes-Bug: #1257496"
711,5ecd5bc3db0ecc8716baa03a6e34f08e1c87fd7e,1387114519,,1.0,25,40,2,2,1,0.998462856,True,8.0,2092841.0,45.0,20.0,False,5.0,4086696.5,10.0,4.0,7.0,666.0,667.0,7.0,506.0,507.0,7.0,512.0,513.0,0.007490637,0.480337079,0.481273408,181,584,1260333,glance,5ecd5bc3db0ecc8716baa03a6e34f08e1c87fd7e,1,1, Malformed property protection rules return error,"Bug #1260333 in Glance: ""Malformed property protection rules return error to end user""","Using a property protections file such as:
[.*]
create = @,!
read = @
update = @
delete = @
The create operation has an invalid rule, duplicate values are not allowed. This should probably result in the service refusing to start, however currently the service will start and operations touching this value will return:
500 Internal Server Error
Malformed property protection rule 'some_property': '@' and '!' are mutually exclusive   (HTTP 500)
to the end user. My feeling is that the end user should not receive any information about the cause of the error, just the 500 status.","Consider  @,! in properties protection rule as a configuration error

In roles based property protection, if '@' and '!' are in the same rule then the
glance api will not start considering this an Invalid Configuration.

DocImpact
Closes-bug: #1260333

Change-Id: I1d304f5c505ae9e2486ff653dda205fc2d851c2b"
712,5eddb2e2245e9c42f458de201ade166b727ee8f4,1394211334,1.0,1.0,135,13,2,2,1,0.998814115,True,35.0,7659167.0,167.0,26.0,False,11.0,2004716.0,28.0,1.0,57.0,447.0,491.0,51.0,445.0,483.0,50.0,427.0,467.0,0.033774834,0.283443709,0.309933775,1611,1232172,1232172,cinder,5eddb2e2245e9c42f458de201ade166b727ee8f4,0,0,feature. “Add the API implementation of extend_volume for the VMware vmdk driver.”,"Bug #1232172 in Cinder: ""vmware: Extend volume not implemented by vSphere cinder driver""","the VMwareVcVmdkDriver currently does not support extending volumes. When executing:
     cinder extend 666a2dae-db61-43bd-9e37-de1c2695c998 5
The following error is seen in the screen-c-vol.log:
     Traceback (most recent call last):
       File ""/opt/stack/cinder/cinder/volume/manager.py"", line 845, in extend_volume
         self.driver.extend_volume(volume, new_size)
       File ""/opt/stack/cinder/cinder/volume/driver.py"", line 430, in extend_volume
         raise NotImplementedError(msg)
     NotImplementedError: Extend volume not implemented","VMware: Implement vmdk extend_volume

Add the API implementation of extend_volume for the VMware
vmdk driver.

Change-Id: Idf09c9e9cf015c78c1c5e91c05b897e3e9b7c006
Closes-Bug: #1232172"
713,5ee167c14c9978409db059054a02635de35a2e93,1406649573,,1.0,4,2,2,2,1,0.650022422,False,,,,,True,,,,,,,,,,,,,,,,,1864,1349805,1349805,Cinder,5ee167c14c9978409db059054a02635de35a2e93,1,0,use the newly updated snapshot-ref object,"Bug #1349805 in Cinder: ""snapshot has wrong status in notification-info when it created done""",", Snapshot's status have update to  'available' in database when snapshot created successfully, but in 'notification-info'  it still being 'creating'.
2014-07-28 10:42:25.711 3580 INFO cinder.volume.manager [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] snapshot f2264c3b-bb46-41fc-b59f-978a222d3ded: created successfully
2014-07-28 10:42:25.712 3580 INFO oslo.messaging._drivers.impl_zmq [req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194 ce5e930bf5a44167b149a2d5fd2302e6 7dfd3b6a98664f7cb78808f57b7984da - - -] 'notifications-info' {'event_type': 'snapshot.create.end',
 'message_id': '69eece40-3d3f-4d65-a239-4ba88d0a8ff5',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-28 02:42:25',
             'deleted': '',
             'display_name': None,
             'snapshot_id': u'f2264c3b-bb46-41fc-b59f-978a222d3ded',
             'status': u'creating',
             'tenant_id': u'7dfd3b6a98664f7cb78808f57b7984da',
             'user_id': u'ce5e930bf5a44167b149a2d5fd2302e6',
             'volume_id': u'd555f2ec-bc78-4f9b-a732-77c67b1fda3a',
             'volume_size': 1},
 'priority': 'INFO',
 'publisher_id': 'snapshot.controller1',
 'request_id': u'req-3b3aadbc-eb00-4695-a24a-8ad8a8fa5194',
 'timestamp': '2014-07-28 02:42:25.712147'}","Update ref used for notifications

When we update a snapshot status in the db, we need to
use the newly updated snapshot-ref object in the following
notification method call.  Otherwise we're sending a
notifciation message with the old/outdated status
information.

NOTE: This also fixes the unit test that was written to
pass with the bug that existed in the notification.

Closes-Bug: #1349805

Change-Id: I32b86d7edd836310c6da7adffa46e915361668f0"
714,5f00cad02da1093d71f636add0810a538cbd444f,1396989934,1.0,1.0,33,40,2,2,1,0.576291461,True,3.0,1351912.0,28.0,12.0,False,22.0,696167.0,55.0,6.0,373.0,703.0,1057.0,211.0,658.0,854.0,35.0,662.0,680.0,0.022973835,0.423101468,0.434588385,708,1134,1300303,cinder,5f00cad02da1093d71f636add0810a538cbd444f,1,1,,"Bug #1300303 in Cinder: ""glusterfs: Rebased snapshot volume is not deleted""","When deleting a volume that was snapshot, the volume is not properly deleted.
Steps to recreate bug:
1.  Create a volume
2.  Attach volume to a running instance.
3.  Take an online snapshot of the volume.
Note that the active volume used by the instance is now switched to volume-<uuid>.<snapshot-uuid>.
4.  Delete the snapshot.
The snapshot volume will be rebased, and the ""active"" volume as seen in volume-<uuid>.info is now set to
volume-<uuid>.<snapshot-uuid>.
Under the glusterfs mount point, there are now only 2 files related to the volume:
  - volume-<uuid>.info
  - volume-<uuid>.<snapshot-uuid>
The original volume base volume-<uuid> is deleted after the rebase.
5.  Detach the volume from the running instance.
6.  Delete the volume.
Under the glusterfs mount point, the  volume-<uuid>.<snapshot-uuid> file is not deleted as expected.","GlusterFS: Delete active snapshot file on volume delete.

If a snapshot is taken of a volume that is attached to an active
instance, the volume file used by the instance will be switched to
the new snapshot file that is created.  When you delete the
snapshot, the base volume file will be merged with the snapshot
file and the base volume is deleted.  Upon a deleting the active
volume, the active snapshot file is not deleted because it does not
have the expected name that cinder is looking for, i.e.
volume-<uuid>.  Instead, the snapshot file has the name
volume-<uuid>.<snapshot-uuid>.  This patch looks at the volume info
file to find any active snapshot file and properly delete it when
the volume is deleted.

Change-Id: Ib0af4401d839ec3bd1eb3a81e1671811e0d4a288
Closes-Bug: #1300303"
715,5f042a64308cb00698b454da09b4a87f2ac3fafd,1396282499,,1.0,13,10,1,1,1,0.0,True,1.0,185244.0,23.0,7.0,False,10.0,2125171.0,8.0,6.0,1154.0,1773.0,1779.0,981.0,1431.0,1436.0,691.0,868.0,871.0,0.657169991,0.825261159,0.828110161,737,1164,1302282,neutron,5f042a64308cb00698b454da09b4a87f2ac3fafd,0,0,Bug in test,"Bug #1302282 in neutron: ""test_iptables_firewall uses invalid MAC addresses""","The unit test test_iptables_firewall.py has invalid MAC addresses in the code - 'ff:ff:ff:ff', an actual MAC has 6 octets and should be 'ff:ff:ff:ff:ff:ff'.  The test still seems to run fine, but it should be cleaned-up.","Fix test MAC addresses to be valid

MAC addresses should have 6 octets - 'ff:ff:ff:ff:ff:ff'.

Change-Id: I9c2457d978ccce15549d3258961093a130375bff
Closes-bug: #1302282"
716,5f37044d9a851fd02d4f1d918b1c276462a76034,1386029960,,1.0,66,2,3,3,1,0.922174531,True,13.0,1234513.0,50.0,20.0,False,28.0,405860.6667,120.0,3.0,14.0,182.0,184.0,14.0,182.0,184.0,12.0,150.0,151.0,0.012524085,0.145472062,0.146435453,1742,1249412,1249412,glance,5f37044d9a851fd02d4f1d918b1c276462a76034,0,0,“The V2 API doesn't not allow empty `container_format` and `disk_format`” feature,"Bug #1249412 in Glance: ""Images v2 api allows you to update container and diskformat after image upload""","Images v2 api allows you to update container and disk format after image upload
But does not allow you to upload an image without it being speficied","V2: disallow image format update for active status

The V2 API doesn't not allow empty `container_format` and `disk_format`
when creating a new image.
Currently, it is possible to update these properties after data have
been saved to the image which is contradictory with the previous
statement.
This change disallows modifications of these properties for images with
a status different than `queued`.

Change-Id: I98c22197ba6d96470976718d48ac53748a19ffc6
Closes-Bug: #1249412"
717,5f56945157f94e71ca02a4b86a2d6528afc99058,1390300196,,1.0,6,4,1,1,1,0.0,True,1.0,233216.0,8.0,3.0,False,6.0,919769.0,11.0,3.0,12.0,261.0,263.0,12.0,260.0,262.0,12.0,224.0,226.0,0.011596789,0.200713649,0.20249777,246,650,1265561,glance,5f56945157f94e71ca02a4b86a2d6528afc99058,0,0,More helpful logs,"Bug #1265561 in Glance: ""Log message printed for unhandled exception is not very helpful.""","Currently, on an unhandled exception, the error message logged through sys.excepthook is not very helpful.
Currently it prints only the exception_value.
https://github.com/openstack/oslo-incubator/blob/master/openstack/common/log.py#L396
Fix:
Make the log message print both the exception_type and exception_value of the unhandled exception.
PS: Currently the traceback is printed only when the VERBOSE is ON.","Sync unhandled exception logging change from Oslo

copy changes from 1 Oslo commit: Ibf00173a07510b2ec0c81a29624004f17810a4e4
This is related to the bug fix for adding error type to unhandled
exception log message

Change-Id: I5645894aad5944f6a91032ae9ac1f2a0904661f7
Closes-Bug: #1265561"
718,5f749768676e6739db1e01a03ddb7f3cb43d48f8,1381931464,,1.0,214,8,7,5,1,0.733174308,True,19.0,15120682.0,387.0,103.0,False,22.0,139497.8571,41.0,7.0,127.0,1518.0,1562.0,127.0,1387.0,1431.0,86.0,197.0,226.0,0.190371991,0.433260394,0.496717724,1629,1234750,1234750,neutron,5f749768676e6739db1e01a03ddb7f3cb43d48f8,1,1, ,"Bug #1234750 in neutron: ""multiple external networks""","I have multiple external networks, and therefore each l3-agent is running with a particular ext network ID.
When I create a lrouter, which is scheduled to be on one l3-agent randomly. If I set the ext-gw for that lrouter later on to be an external network different from the ext-net the l3-agent is on, the entire lrouter somehow goes down, both internal and external interfaces.
I would prefer to see:
1) the operation of putting the lrouter onto the ""wrong"" ext-net is disallowed;
2) or better, we can move the lrouter to the right l3-agent with the correct ext-net.","Reschedule router if new external gateway is on other network

An L3 agent may be associated with just one external network.
If router's new external gateway is on other network then the router
needs to be rescheduled to the proper l3 agent

Change-Id: Ia0ed924403137ac4578ca562b57988292c41c1fe
Closes-Bug: #1234750"
719,5f93c841a80663b68da2fb04df78d5acd0754d68,1410290968,,1.0,50,4,2,2,1,0.445064857,False,,,,,True,,,,,,,,,,,,,,,,,1286,1749,1367363,nova,5f93c841a80663b68da2fb04df78d5acd0754d68,1,1,,"Bug #1367363 in OpenStack Compute (nova): ""Libvirt-lxc will leak nbd devices on instance shutdown""","Shutting down a libvirt-lxc based instance will leak the nbd device. This happens because _teardown_container will only be called when libvirt domain's are running. During a shutdown, the domain is not running at the time of the destroy. Thus, _teardown_container is never called and the nbd device is never disconnected.
Steps to reproduce:
1) Create devstack using local.conf: https://gist.github.com/ramielrowe/6ae233dc2c2cd479498a
2) Create an instance
3) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
4) Shutdown instance
5) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
6) Delete instance
7) Perform ps ax |grep nbd on devstack host. Observe connected nbd device
Nova has now leaked the nbd device.","Libvirt: Always teardown lxc container on destroy

This fixes a bug where shutting down a libvirt-lxc based instance
would leak its underlying nbd device. This was happening because
_teardown_container would only get called if the domain was
present. After this fix, _teardown_container will always get called
on a destroy, which ensures the nbd device gets disconnected.

Change-Id: I8d0044fd038fef80568c6c86e34719ef679aa890
Closes-Bug: #1367363"
720,5fae0fae2164f4fab64e3eff01c9c644d861946a,1389638575,,1.0,35,55,2,2,1,0.2108423,True,3.0,5478434.0,62.0,14.0,False,7.0,9747975.0,11.0,8.0,11.0,1267.0,1267.0,11.0,1122.0,1122.0,11.0,481.0,481.0,0.017492711,0.702623907,0.702623907,284,689,1268460,neutron,5fae0fae2164f4fab64e3eff01c9c644d861946a,0,0,Improve?: Report proper error message ,"Bug #1268460 in neutron: ""PLUMgrid plugin should report proper error messages""",PLUMgrid Director error messages should be reported at plugin level as well.,"Report proper error message in PLUMgrid Plugin

Change-Id: Ifc1bb55f6b025bba77cf9858ed392dbf170075a7
Closes-Bug: #1268460
Signed-off-by: Fawad Khaliq <fawad@plumgrid.com>"
721,5fc030cce44ca0849cc3d2c1830ead6804c70cf9,1385693889,,1.0,11,11,2,2,1,0.945660305,True,2.0,45506.0,13.0,5.0,False,30.0,435151.0,84.0,2.0,0.0,363.0,363.0,0.0,274.0,274.0,0.0,227.0,227.0,0.000968992,0.220930233,0.220930233,111,510,1256734,glance,5fc030cce44ca0849cc3d2c1830ead6804c70cf9,0,0,tests,"Bug #1256734 in Glance: ""Use assertEqual instead of assertEquals in unitttest""","The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead.","Use assertEqual instead of assertEquals in unit tests

The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead.

Change-Id: Ic8f152766d70146300b74f3803d7dab1e4de50e2
Closes-Bug: #1256734"
722,5fc059f13d4f0b2a5fef63095ea3fc710d46b5b3,1380398389,,1.0,1,1,1,1,1,0.0,True,1.0,164470.0,11.0,7.0,False,9.0,3735242.0,11.0,7.0,143.0,2834.0,2879.0,143.0,2467.0,2512.0,132.0,1963.0,1999.0,0.021643613,0.319609439,0.32546786,1462,1209410,1209410,nova,5fc059f13d4f0b2a5fef63095ea3fc710d46b5b3,1,0,“That provokes the next error in Windows:” change in the environment,"Bug #1209410 in OpenStack Compute (nova): ""processutlis.execute not usable in windows""","In Havana time, in the execute method of processutils the condition (os.geteuid() != 0) was added in line 130
    if run_as_root and os.geteuid() != 0:
        if not root_helper:
            raise NoRootWrapSpecified(
                message=('Command requested root, but did not specify a root '
                         'helper.'))
        cmd = shlex.split(root_helper) + list(cmd)
That provokes the next error in Windows:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp     (stdout, stderr) = processutils.execute(*cmd, **kwargs)
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp   File ""C:\Users\Pedro\dev\cinder\cinder\openstack\common\processutils.py"", line 130, in execute
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp     if run_as_root and os.geteuid() != 0:
2013-08-08 00:27:09.937 2148 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'module' object has no attribute 'geteuid'
This should be removed in order to this code be used in nova and cinder windows-based drivers","Fix processutils.execute errors on windows

Added a check for existence of geteuid.

This fix is related to oslo-incubator commit:
d6a963e911b8456c06dceb5ee3cc88a70c08bf82

Closes-Bug: #1209410

Change-Id: Ibaa1f061bf161768890d4708dc945180121726f9"
723,5fd0500e7e262c602dc7bbbff456326598da41bc,1410271115,,1.0,3,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1280,1743,1366859,nova,5fd0500e7e262c602dc7bbbff456326598da41bc,1,0,"“This was recently changed as part of the Ironic -> Nova driver patch series. We still want to canonicalize supported_instances though…""","Bug #1366859 in OpenStack Compute (nova): ""Ironic: extra_spec requirement 'amd64' does not match 'x86_64'""","Using the latest Nova Ironic compute drivers (either from Ironic or Nova) I'm hitting scheduling ERRORS:
Sep 08 15:26:45 localhost nova-scheduler[29761]: 2014-09-08 15:26:45.620 29761 DEBUG nova.scheduler.filters.compute_capabilities_filter [req-9e34510e-268c-40de-8433-d7b41017b54e None] extra_spec requirement 'amd64' does not match 'x86_64' _satisfies_extra_specs /opt/stack/venvs/nova/lib/python2.7/site-packages/nova/scheduler/filters/compute_capabilities_filter.py:70
I've gone ahead and patched in https://review.openstack.org/#/c/117555/.
The issue seems to be that ComputeCapabilitiesFilter does not itself canonicalize instance_types when comparing them which will breaks existing TripleO baremetal clouds using x86_64 (amd64).","Ironic: don't canonicalize extra_specs data

Don't canonicalize the cpu_arch extra_specs field. This is important
because the scheduler filters which use extra specs don't canonicalize
things either and as such you'll have mismatched fields causing
instances not to get scheduled.

This was recently changed as part of the Ironic -> Nova driver patch
series. We still want to canonicalize supported_instances though...

Change-Id: I9342213b5433113816142b1f737119065e9f077f
Closes-bug: #1366859"
724,5fe07c1be34a73c457bd8e6b8a86bfc6e53567d5,1387958607,,1.0,3,8,1,1,1,0.0,True,4.0,475959.0,22.0,7.0,False,10.0,291539.0,11.0,2.0,41.0,1197.0,1216.0,41.0,1032.0,1051.0,30.0,1052.0,1060.0,0.024256651,0.823943662,0.830203443,226,629,1264053,cinder,5fe07c1be34a73c457bd8e6b8a86bfc6e53567d5,0,0,Remove redundant check,"Bug #1264053 in Cinder: ""Redundant size check in volume restore api""","In cinder  volume restore api, we do twice size check when restore volume with a given volume uuid. It's necessary to remove the redundant check.","Redundant size check in volume restore api

In cinder volume restore api, we do twice size check when restore volume
with a given volume uuid. It's necessary to remove the redundant check.

Change-Id: I8f240b2c0d1014d212ff56c2c9b5e193f9d552bd
Closes-Bug: #1264053"
725,601d54c9ef926fa2d8abfe0121019f7ad94eefd5,1395996427,,1.0,6,1,1,1,1,0.0,True,5.0,607245.0,25.0,9.0,False,4.0,8050203.0,4.0,5.0,1221.0,1614.0,1621.0,957.0,1262.0,1269.0,1068.0,1224.0,1224.0,0.688345138,0.788795879,0.788795879,698,1124,1299124,cinder,601d54c9ef926fa2d8abfe0121019f7ad94eefd5,1,1,Windows dependency?,"Bug #1299124 in Cinder: ""Cinder fails to delete volumes on Windows""","This commit https://github.com/openstack/cinder/commit/beecd769af02ba5915be827d28a7b46d970e41b0 has changed the flow of volume creation, so Cinder does not create an iSCSI target anymore when a volume is created. Even so, the Windows Cinder driver tries to remove the according iSCSI target every time a volume is deleted, resulting into an error when the target has not been created. The fix consists in checking if the target exists before attempting to delete it.
Trace: http://paste.openstack.org/show/73912/","Fixes cinder volume delete on Windows

Because of the fact that an iSCSI target is not created anymore
at volume creation time, not all volumes will have a corresponding
target. For this reason, when removing an iSCSI target we must
first check if the target actually exists.

Change-Id: I8e571397df6d9a2eeb05e883b7da8494d4bfa1ad
Closes-Bug: #1299124"
726,6037b9cc20a8ab379855370394e108912ea3a0f2,1386598861,,1.0,12,4,2,2,1,0.988699408,True,3.0,3664026.0,18.0,6.0,False,31.0,2183038.0,42.0,3.0,62.0,1177.0,1221.0,60.0,975.0,1017.0,57.0,1108.0,1147.0,0.008506894,0.162657671,0.168377823,164,566,1259183,nova,6037b9cc20a8ab379855370394e108912ea3a0f2,1,1,Bug…  Solution: Ensure api_paste_conf is an absolute path,"Bug #1259183 in OpenStack Compute (nova): ""wsgi.Loader should ensure the config_path is absolute""","nova-api service will fail to start when the nova-api command is invoked from a directory containing a file with the same name as the one specified in the configuration key 'api_paste_config' if it is not an absolute path (the default is 'api-paste.ini').
[fedora@devstack1 devstack]$ pwd
/home/fedora/devstack
[fedora@devstack1 devstack]$ grep api_paste_conf /etc/nova/nova.conf
api_paste_config = api-paste.ini
[fedora@devstack1 devstack]$ touch api-paste.ini
[fedora@devstack1 devstack]$ nova-api
2013-12-09 09:18:40.082 DEBUG nova.wsgi [-] Loading app ec2 from api-paste.ini from (pid=4817) load_app /opt/stack/nova/nova/wsgi.py:485
2013-12-09 09:18:40.083 CRITICAL nova [-] Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova Traceback (most recent call last):
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/bin/nova-api"", line 10, in <module>
2013-12-09 09:18:40.083 TRACE nova     sys.exit(main())
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/cmd/api.py"", line 49, in main
2013-12-09 09:18:40.083 TRACE nova     max_url_len=16384)
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/service.py"", line 308, in __init__
2013-12-09 09:18:40.083 TRACE nova     self.app = self.loader.load_app(name)
2013-12-09 09:18:40.083 TRACE nova   File ""/opt/stack/nova/nova/wsgi.py"", line 486, in load_app
2013-12-09 09:18:40.083 TRACE nova     return deploy.loadapp(""config:%s"" % self.config_path, name=name)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 247, in loadapp
2013-12-09 09:18:40.083 TRACE nova     return loadobj(APP, uri, name=name, **kw)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 271, in loadobj
2013-12-09 09:18:40.083 TRACE nova     global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 296, in loadcontext
2013-12-09 09:18:40.083 TRACE nova     global_conf=global_conf)
2013-12-09 09:18:40.083 TRACE nova   File ""/usr/lib/python2.7/site-packages/paste/deploy/loadwsgi.py"", line 308, in _loadconfig
2013-12-09 09:18:40.083 TRACE nova     ""argument given"" % uri)
2013-12-09 09:18:40.083 TRACE nova ValueError: Cannot resolve relative uri 'config:api-paste.ini'; no relative_to keyword argument given
2013-12-09 09:18:40.083 TRACE nova","Ensure api_paste_conf is an absolute path

If api_paste_conf is specified as a relative path and the directory from
where the nova-api service is started contains a file with the same
name, the wsgi loader won't try to find the absolute path to that file
which is required when not passing the base directory to the
paste.deploy library.

This change ensures the config_path is an absolute path so it can be
found by the paste.deploy library.

Closes-Bug: #1259183
Change-Id: I0eb6ef1d69d302634b8c842449f997ec10c13933"
727,605e5950751cd4d19ca7fd81c04c52cfa41ce7f6,1378807892,,1.0,2,2,1,1,1,0.0,True,6.0,1164016.0,34.0,15.0,False,12.0,4811743.0,16.0,6.0,63.0,3338.0,3357.0,63.0,3020.0,3039.0,61.0,2407.0,2426.0,0.010359231,0.402339181,0.405513784,1534,1223253,1223253,nova,605e5950751cd4d19ca7fd81c04c52cfa41ce7f6,1,1,"“Subprocess output is bytestrings, not unicode”","Bug #1223253 in OpenStack Compute (nova): ""bad logging from baremetal_deploy_helper.py, line 217""","2013-09-10 07:48:36,767.767 5769 ERROR nova.virt.baremetal.deploy_helper [req-f649058f-8b8f-4392-a221-ca55a96178b0 None None] StdOut  :
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/nova/nova/openstack/common/log.py"", line 517, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
UnicodeDecodeError: 'ascii' codec can't decode byte 0xe2 in position 17: ordinal not in range(128)
Logged from file baremetal_deploy_helper.py, line 217
Program stdout can be arbitrary bytes; we need to convert it to unicode before trying to log it.","Fix logging of failed baremetal commands.

Subprocess output is bytestrings, not unicode, so %r is the right
format to use.

Change-Id: Ia6cd0986d6970093c859775bb321ecea632e0264
Closes-Bug: #1223253"
728,608003408b3aba6a8290427257460287d8a4ce96,1407267337,,1.0,29,6,2,2,1,0.985228136,False,,,,,True,,,,,,,,,,,,,,,,,1151,1608,1353006,neutron,608003408b3aba6a8290427257460287d8a4ce96,1,1,,"Bug #1353006 in neutron: ""AttributeError when setting external gw in DVR case""","This has been found on master:
1) create a DVR router: neutron router-create test --distributed
2) set external gateway: neutron router-gateway-set test public
3) observe stacktrace: http://paste.openstack.org/show/90577/ in L3 Agent's log
This does not seem to upset the L3 Agent and its ability to work correctly, but it would be good to eradicate the trace.","Fix AttributeError when setting external gateway on DVR router

DVR routers will have this manager initialized only after one
or more subnets have been attached to the router. To address
the issue, make sure the manager is defined and handle the snat
rules appropriately.

This patch also makes _update_arp_entry more defensive; this is
because the arp update process can be affected by the same issue:
the router may not have internal ports at the time the request
come in. This is likely when VM's port creation and router
configuration overlap slightly.

Closes-bug: #1353006

Change-Id: Ib46852f5b264e5ef2e2d499d3351f8974e393011
Co-authored-by: Rajeev Grover <rajeev.grover@hp.com>"
729,60816b637d391aa5e5aef9b4b3a432b5ce552cc8,1410404413,,1.0,0,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1296,1762,1368006,neutron,60816b637d391aa5e5aef9b4b3a432b5ce552cc8,1,1,,"Bug #1368006 in neutron: ""ofagent: broken XenAPI support""","ofagent has code for agent-on-DomU support inherited from OVS agent.
However, it's incomplete and broken.  Because ofagent uses a direct
OpenFlow channel instead of ovs-ofctl command to program a switch,
the method to use the special rootwrap can not work.","ofagent: Remove broken XenAPI support

ofagent has code for agent-on-DomU support inherited from OVS agent.
However, it's incomplete and broken.  Because ofagent uses a direct
OpenFlow channel instead of ovs-ofctl command to program a switch,
the method to use the special rootwrap can not work.

Closes-Bug: #1368006
Change-Id: I0bb25a296d1f1e65c233ece6deaef6c095d78707"
730,6086db51b0c3acaf2b88617e3e739e06d894d761,1405394097,,1.0,20,4,3,2,1,0.693697158,False,,,,,True,,,,,,,,,,,,,,,,,1048,1493,1339462,nova,6086db51b0c3acaf2b88617e3e739e06d894d761,1,0,"Platform. “Current Nova vmwareapi cannot configure guest VM's SCSI controller type as “Paravirtual"".""","Bug #1339462 in OpenStack Compute (nova): ""vmware: cannot use adaptertype Paravirtual""","nova icehouse Ubuntu14.04
Version: 1:2014.1-0ubuntu1.2
Current Nova vmwareapi cannot configure guest VM's SCSI controller type as ""Paravirtual"".
(though this requires vmwaretools running on the guest VM)","Vmware:Add support for ParaVirtualSCSIController

This fix allows us to use ParaVirtualSCSIController
adapter type as 'paraVirtual' for 'vmware_adaptertype'.

DocImpact: Add vmware_adaptertype parameter paraVirtual
Change-Id: Id25287310e2aab27808d4be0b6b47303404e4fae
Closes-Bug: #1339462"
731,608a710db5bcf4359e1e445553143cd39ffe194c,1388545346,,1.0,0,1,1,1,1,0.0,True,1.0,162505.0,11.0,5.0,False,2.0,27577930.0,7.0,5.0,9.0,2789.0,2797.0,8.0,2227.0,2234.0,0.0,52.0,52.0,0.003731343,0.197761194,0.197761194,237,641,1265264,nova,608a710db5bcf4359e1e445553143cd39ffe194c,0,0,unused import,"Bug #1265264 in OpenStack Compute (nova): ""Remove unused import""","************* Module destroy_cached_images
W: 31, 0: Unused import logging (unused-import)","Remove unused import

Remove unused import in tools/xenserver/destroy_cached_images.py

Change-Id: I5907c99ff702ebe0fc52e9e95c1b0443e20c5c6f
Closes-Bug: #1265264"
732,609f01a1ddca64ec191cb15a4f6fb93d219c3336,1390527469,3.0,1.0,31,11,2,2,1,0.650022422,True,7.0,2926328.0,131.0,37.0,False,12.0,1911450.0,18.0,5.0,6.0,1179.0,1179.0,6.0,1024.0,1024.0,6.0,572.0,572.0,0.009668508,0.791436464,0.791436464,317,726,1272128,neutron,609f01a1ddca64ec191cb15a4f6fb93d219c3336,0,0,Support Port Binding Extension in Cisco N1kv plugin,"Bug #1272128 in neutron: ""Support Port Binding Extension in Cisco N1kv plugin""",Plugins using libvirt_ovs_bridge config are affected due to changes in nova's VIF plugging code. Fix port crud in the Cisco N1kv Neutron plugin by extending Port Bindings Extension.,"Support Port Binding Extension in Cisco N1kv plugin

Change-Id: I9bcacb2b1f1bb1500b9176a49736aac128938de8
Closes-Bug: #1272128"
733,60c1b9e227fdfcae8e3660ffbd5062eb6a4abb33,1403125697,,1.0,12,1,3,2,1,0.895640306,False,,,,,True,,,,,,,,,,,,,,,,,992,1432,1332334,neutron,60c1b9e227fdfcae8e3660ffbd5062eb6a4abb33,1,1,"“If the user sets the consistency watchdog polling interval to 0, it will poll as fast as possible instead of disabling the watchdog as expected”","Bug #1332334 in neutron: ""Big Switch: Consistency watchdog should stop on polling interval of 0""","If the user sets the consistency watchdog polling interval to 0, it will poll as fast as possible instead of disabling the watchdog as expected.","Big Switch: Stop watchdog on interval of 0

Corrects the behavior of the watchdog process
to exit if the user configures the polling interval
to be 0.

Also adds missing documentation to sample config.

Change-Id: I17b566867c21f42985cc4662f56d32db690f471f
Closes-Bug: #1332334"
734,60c899b9da8d64b4a5979b69c43c77ed9d5bf248,1401821886,0.0,1.0,34,9,4,2,1,0.838036844,False,,,,,True,,,,,,,,,,,,,,,,,751,1178,1303536,nova,60c899b9da8d64b4a5979b69c43c77ed9d5bf248,1,1,,"Bug #1303536 in OpenStack Compute (nova): ""Live migration fails. XML error: CPU feature `wdt' specified more than once""","Description of problem
---------------------------
Live migration fails.
libvirt says ""XML error: CPU feature `wdt' specified more than once""
Version
---------
ii  libvirt-bin                                         1.2.2-0ubuntu2                        amd64        programs for the libvirt library
ii  python-libvirt                                      1.2.2-0ubuntu1                        amd64        libvirt Python bindings
ii  nova-compute                                        1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - compute node base
ii  nova-compute-kvm                                    1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - compute node (KVM)
ii  nova-cert                                           1:2014.1~b3-0ubuntu2                  all          OpenStack Compute - certificate management
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=14.04
DISTRIB_CODENAME=trusty
DISTRIB_DESCRIPTION=""Ubuntu Trusty Tahr (development branch)""
NAME=""Ubuntu""
VERSION=""14.04, Trusty Tahr""
Test env
----------
A two node openstack havana on ubuntu 14.04. Migrating a instance to other node.
Steps to Reproduce
------------------
 - Migrate the instance
And observe /var/log/nova/compute.log and /var/log/libvirt.log
Actual results
--------------
/var/log/nova-conductor.log
2014-04-04 13:42:17.128 3294 ERROR oslo.messaging._drivers.common [-] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner\n    return func(*args, **kwargs)\n', '  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 668, in migrate_server\n    block_migration, disk_over_commit)\n', '  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 769, in _live_migrate\n    raise exception.MigrationError(reason=ex)\n', 'MigrationError: Migration error: Remote error: libvirtError XML error: CPU feature `wdt\' specified more than once\n[u\'Traceback (most recent call last):\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply\\n    incoming.message))\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch\\n    return self._do_dispatch(endpoint, method, ctxt, args)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch\\n    result = getattr(endpoint, method)(ctxt, **new_args)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 88, in wrapped\\n    payload)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 71, in wrapped\\n    return f(self, context, *args, **kw)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 272, in decorated_function\\n    e, sys.exc_info())\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 259, in decorated_function\\n    return function(self, context, *args, **kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 4159, in check_can_live_migrate_destination\\n    block_migration, disk_over_commit)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4094, in check_can_live_migrate_destination\\n    self._compare_cpu(source_cpu_info)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4236, in _compare_cpu\\n    LOG.error(m, {\\\'ret\\\': ret, \\\'u\\\': u})\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\\n    six.reraise(self.type_, self.value, self.tb)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4232, in _compare_cpu\\n    ret = self._conn.compareCPU(cpu.to_xml(), 0)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit\\n    result = proxy_call(self._autowrap, f, *args, **kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call\\n    rv = execute(f,*args,**kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker\\n    rv = meth(*args,**kwargs)\\n\', u\'  File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 3191, in compareCPU\\n    if ret == -1: raise libvirtError (\\\'virConnectCompareCPU() failed\\\', conn=self)\\n\', u""libvirtError: XML error: CPU feature `wdt\' specified more than once\\n""].\n']
2014-04-04 13:52:18.161 3295 ERROR nova.conductor.manager [req-471d2933-354a-4417-af50-c48399e19663 42fab7a8b7434bfc8473767c01e8378d b1cf6337c229491c96ad6e0a96e82979] Migration of instance 47d1fe7d-b812-4588-85eb-aa813267fc82 to host c2 unexpectedly failed.
/var/log/libvirtd.log
2014-03-27 18:23:17.141+0000: 2659: info : libvirt version: 1.2.2
2014-03-27 18:23:17.141+0000: 2659: error : virCPUDefParseXML:413 : XML error: CPU feature `wdt' specified more than once
Expected results
----------------
Successful migration
Additional info
----------------
Related with: https://bugs.launchpad.net/nova/+bug/1267191
On the file /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py, the list info['features'] have the duplicate feature.","libvirt: convert cpu features attribute from list to a set

Currently, the cpu features list which is being sent to libvirt,
when creating a domain or calling compareCPU, must contain only
unique entries. Multiple issues arise when we are updating the
features attribute in LibvirtConfigCPU class (for example during
migration).

This change will change the features attribute from being a list
to a set. This make the LibvirtConfigCPU class keep only unique
features.
Adjusting the LibvirtConfigCPUFeature class to support set
operations by overriding the __eq__, __ne__ and __hash__
methods.

Closes-Bug: #1303536
Change-Id: I6350fe0e827c860aea77cc4fe56f18f5c1483580"
735,60ca9548905b58cea67776a71864486bfd1d69c7,1390931993,,1.0,14,145,4,4,1,0.811292306,True,8.0,7306648.0,66.0,14.0,False,39.0,141705.0,76.0,1.0,25.0,444.0,465.0,21.0,444.0,461.0,25.0,443.0,464.0,0.003618146,0.061786808,0.064709157,336,746,1273852,nova,60ca9548905b58cea67776a71864486bfd1d69c7,0,0,No bug: should be purely DB layer,"Bug #1273852 in OpenStack Compute (nova): "" PCI device object should be purely DB layer""","Currently the PCI device object includes a lot of function like alloc/free/claim etc. However, the NovaObject should not be used this way, and it makes the PCI device object really different with other NovaObject implementation.
We should keep the PCI device object as simple data access, and keep those method to separated functions.","Remove the device handling from pci device object

Remove the device handling code at pci device object to make
it similar to other NovaObject.

Remove some usage in test cases.

Closes-Bug: #1273852

Change-Id: I4487ae0b36de18365d12593f480c05f0fe726769"
736,60da388de842a0e3b11aed8f4f4e2e21ab270660,1391545959,,1.0,2,2,1,1,1,0.0,True,1.0,57060.0,7.0,3.0,False,120.0,600828.0,282.0,3.0,11.0,1103.0,1110.0,10.0,922.0,928.0,10.0,1023.0,1029.0,0.001516196,0.141144039,0.141971054,364,775,1276312,nova,60da388de842a0e3b11aed8f4f4e2e21ab270660,0,0,Mak a more generic msg,"Bug #1276312 in OpenStack Compute (nova): ""VirtualInterfaceMacAddressException message mention 5 attempts when the value is configurable""","The exception ""VirtualInterfaceMacAddressException"" message says
""5 attempts to create virtual interface with unique mac address failed"" when the number of attempts is configurable
by ""create_unique_mac_address_attempts"".
As proposed fix change to a generic message.","Fix VirtualInterfaceMacAddressException message

Previous the message reflected the default value of
create_unique_mac_address_attempts configuration option.

Changed to a generic message.

Change-Id: I55c5f95cfc7ac4183e08a7880cbbb9670a02cef4
Closes-Bug: #1276312"
737,60fc1f64fabd86c38846c76785255c523a98b331,1403701573,,1.0,6,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1008,1450,1334264,neutron,60fc1f64fabd86c38846c76785255c523a98b331,1,1,"“Previously ovs-agent exited with exit code 1 after SIGTERM was received.
    SIGTERM should shutdown agent gracefully and exit code should be 0.” (320b9bd196a87f58229000c6a545a4911340c9ae)","Bug #1334264 in neutron: ""openvswitch agent exits with exit code 1 when handling SIGTERM""","SIGTERM should be handled properly and agent should exit with 0.
https://github.com/openstack/neutron/blob/master/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py#L1444","Exit rpc_loop when SIGTERM is recieved in ovs-agent

Previously ovs-agent exited with exit code 1 after SIGTERM was received.
SIGTERM should shutdown agent gracefully and exit code should be 0.

Closes-Bug: #1334264
Change-Id: I23e81cee5ae9e9fdabbe1377420b0902f47be8a7"
738,61135b6a5f1724888e92471b68f91e4a825bf4f0,1379569053,,1.0,3,12,2,2,1,0.566509507,True,1.0,34382.0,10.0,5.0,False,45.0,467496.5,72.0,4.0,11.0,949.0,952.0,11.0,846.0,849.0,8.0,804.0,805.0,0.008893281,0.795454545,0.796442688,1577,1227477,1227477,cinder,61135b6a5f1724888e92471b68f91e4a825bf4f0,0,0,Refactoring “remove VolumeNotFoundForInstance class”,"Bug #1227477 in Cinder: ""VolumeNotFoundForInstance class is used only in test_xiv_ds8k.py""","If we modify test_xiv_ds8k.py properly,
VolumeNotFoundForInstance class can be removed.","remove VolumeNotFoundForInstance class

VolumeNotFoundForInstance class is used only in test_xiv_ds8k.py.  I
think the exception class that is used only in test code should not be
defined in cinder/exception.py.

Change-Id: I53512d9b06d48e1fb98ef63eac82c033cec37471
Closes-Bug: #1227477"
739,6132f991bdc8515aa665db16fef260ff71a618e6,1405114317,,1.0,34,26,6,4,1,0.785655547,True,4.0,1942497.0,27.0,12.0,False,260.0,4058947.0,1614.0,3.0,15.0,2849.0,2860.0,11.0,1857.0,1866.0,15.0,2750.0,2761.0,0.001998002,0.343531469,0.344905095,1417,1131395,1131395,nova,6132f991bdc8515aa665db16fef260ff71a618e6,1,1,“This settings is ignored and all instances go to a stop state rather than termination.”,"Bug #1131395 in OpenStack Compute (nova): ""EC2 API does not support InstanceInstantiatedShutdownBehavior""","The EC2 API allows the setting of the InstanceInstantiatedShutdownBehavior to either ""stop"" or ""terminate"".
This settings is ignored and all instances go to a stop state rather than termination.
From the comments, it looks like this was removed in I91845a64 -- maybe Vish can comment before I dive in?","Enable terminate for EC2 InstanceInitiatedShutdownBehavior

The EC2 API supports an instance attribute called
InstanceInitiatedShutdownBehavior (IISB) which can be set to
'stop' or 'terminate' (default: 'stop').  When the instance
initiates its own shutdown, this determines whether or not
the instance hangs around in the Shutoff state or is
terminated by the system.

In nova, this is handled by the shutdown_terminate boolean.
IISB = stop      => shutdown_terminate = False
IISB = terminate => shutdown_terminate = True

sync_instance_power_state now invokes compute_api.delete if
shutdown_terminate = True and we detect the instance power state
has gone from Running to Shutdown.

Closes-Bug: #1131395
Change-Id: I284ae7a84384f19131703c4ad44e0e5f5b03f5d4"
740,6167cb55e2f62a645487d66e52b809c9599b3bb8,1400640086,,1.0,3,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,895,1331,1320775,neutron,6167cb55e2f62a645487d66e52b809c9599b3bb8,1,1,,"Bug #1320775 in neutron: ""fwaas:Firewall in ""active"" status is  not working when there is no external g/w to the router""","Steps to Reproduce:
                                   1. create two network connected to the router and each network having a VM
                                   2. create firewall rule of icmp deny
                                   3. attach the firewall rule to the policy
                                   4. Create firewall with that policy and check that firewall is active
                                   5. Try to ping from one vm to another vm.
Actual Results:
                                VM is able to ping even though firewall is active. However the ping fails as expected after creating external gateway to the router.
Expected Results:
                                It should fail since the firewall is active","Do not defer IPTables apply in firewall path

By default, iptables apply is deferred in L3 agent. For
external gateways, iptables is applied immediately (to
enable NAT for floating IP). Similarly, when firewall
is created/updated/deleted, iptable rules are applies
immediately.

Change-Id: I4f652a030ae23a71a2e20af2e8ef0ad5b882b80e
Closes-Bug: #1320775"
741,616d2f9a65f7995298153cdf8a1e94bb62912ebe,1343868030,1.0,,76,10,2,2,1,0.801932502,True,4.0,165490.0,19.0,6.0,False,8.0,1886484.0,19.0,2.0,4.0,875.0,878.0,4.0,873.0,876.0,2.0,303.0,304.0,0.007025761,0.711943794,0.714285714,1809,1257273,1257273,Glance,616d2f9a65f7995298153cdf8a1e94bb62912ebe,1,1,“Prevent creation of http images with invalid URIs”,"Bug #1257273 in Glance: ""Glance download fails when size is 0""","Glance images are not being fetched by glance's API v1 when the size is 0. There are 2 things wrong with this behaviour:
1) Active images should always be ready to be downloaded, regardless they're locally or remotely stored.
2) The size shouldn't be the way to verify whether an image has some data or not.
https://git.openstack.org/cgit/openstack/glance/tree/glance/api/v1/images.py#n455
This is happening in the API v1, but it doesn't seem to be true for v2.","Adds proper response checking to HTTP Store.

The HTTP Store now handles the following cases:

  * Redirects are resolved (infinite redirect chains are prevented).
  * 4xx and 5xx status codes result in a proper exception instead
    of trying to continue with the image.

Fixes bug 1009248.

Change-Id: Ibc44413d630bf35e0c396adc430c39f4295030fd"
742,6186e0cabe5ba854cddd68351e314933f826ee33,1405838242,,1.0,33,28,4,4,2,0.477965632,False,,,,,True,,,,,,,,,,,,,,,,,1072,1521,1343024,neutron,6186e0cabe5ba854cddd68351e314933f826ee33,0,0,"""The OVS core plugin will be removed in Juno 2 but the embrane plugin code is still using it and will stop working.”","Bug #1343024 in neutron: ""remove ovs dependency in embrane plugin""","The OVS core plugin will be removed in Juno 2 but the embrane plugin code is still using it and will stop working.
This bug tracks changing the dependency to ML2","Remove ovs dependency in embrane plugin

This patch changes the dependency of the embrane plugin from ovs to ml2.
Existing users of the old entry point will be upgraded on a case-by-case
base as needed, therefore no generic upgrade procedure is required.

Change-Id: Id192fc32bd6ba7a8926eff1190070b8df5327129
Closes-Bug: #1343024"
743,61a7b584238327d346bd4da642f3f8cb4d1dbf9c,1398949857,,1.0,69,12,7,5,1,0.782949753,True,8.0,1744581.0,97.0,18.0,False,19.0,641181.7143,31.0,3.0,1983.0,285.0,2237.0,1640.0,275.0,1886.0,171.0,266.0,408.0,0.143812709,0.223244147,0.341973244,850,1280,1314994,neutron,61a7b584238327d346bd4da642f3f8cb4d1dbf9c,1,1,,"Bug #1314994 in neutron: ""There are log messages that do not have transtaions""","Following files have log messages that are not translated:
neutron/agent/securitygroups_rpc.py
neutron/plugins/hyperv/agent/security_groups_driver.py
neutron/plugins/hyperv/agent/utilsfactory.py
neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py","Add missing translation support

Update a number of files to add missing translation support.

The patch adds a new hacking check - N320. This ensures that
all log messages, except debug ones, have translations.

A '# noqa' indicates that the validation will not be done on
the specific log message. This should be used in cases where
the translations do not need to be done, for example, the log
message is logging raw data.

Change-Id: I3e1fdd04d87b09dff50950b5e85f5cacfb29afdc
Closes-bug: #1314994"
744,61b2824a1ff9c57066e3277b2f66f6b93a5d5f08,1410343870,,1.0,19,15,3,3,1,0.939270745,False,,,,,True,,,,,,,,,,,,,,,,,1291,1757,1367642,nova,61b2824a1ff9c57066e3277b2f66f6b93a5d5f08,1,1,“backward compatible with V2 APIs”,"Bug #1367642 in OpenStack Compute (nova): """"revertResize/confirmResize"" server actions does not work for v2.1 API""","""revertResize/confirmResize"" server actions does not work for v2.1 API
Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs","Fix ""revertResize/confirmResize"" for V2.1 API

""revertResize/confirmResize"" server actions were missed for V2.1 API.

This patch converts ""revertResize/confirmResize"" server action
for V2.1 API

The differences between v2 and v3 are described on the wiki page
https://wiki.openstack.org/wiki/NovaAPIv2tov3.

Change-Id: I24bdb5b28fafeb60ea7b4ff044b12a519498e592
Closes-Bug: #1367642"
745,61edb744bdaefa72151e99c43d9a6e75e7895f07,1399366591,,1.0,2,2,1,1,1,0.0,True,1.0,48336.0,21.0,7.0,False,3.0,1538759.0,3.0,7.0,57.0,1498.0,1509.0,57.0,1291.0,1302.0,57.0,966.0,977.0,0.047894302,0.798513625,0.807597027,866,1299,1316486,neutron,61edb744bdaefa72151e99c43d9a6e75e7895f07,0,0,Bug in test,"Bug #1316486 in neutron: ""Package is imported instead of module in neutron/tests/unit/services/loadbalancer/drivers/netscaler/test_netscaler_driver.py""",This change was wrong https://review.openstack.org/#/c/89628/5/neutron/tests/unit/services/loadbalancer/drivers/netscaler/test_netscaler_driver.py and needs to be taken back. Previous version imported module correctly and there was no need to change.,"Fix importing module in test_netscaler_driver

During H302 was introduced new bug importing package instead of module.
This patch is reverting back change on this file from
https://review.openstack.org/#/c/89628/5

Closes-Bug: #1316486
Change-Id: I7830fba6687655b133073ace75f32004e72ad650"
746,62040d03b3b7f02ff7f67ed08e0dc867eef183dc,1376652260,,1.0,9,2,4,3,1,0.911533991,True,4.0,511659.0,24.0,10.0,False,10.0,379885.0,15.0,2.0,1.0,473.0,473.0,1.0,426.0,426.0,1.0,93.0,93.0,0.01,0.47,0.47,1470,1212772,1212772,neutron,62040d03b3b7f02ff7f67ed08e0dc867eef183dc,1,0,“external gateway modes doesn't work with ML2”,"Bug #1212772 in neutron: ""external gateway modes doesn't work with ML2""","When ML2 plugin is used, L3 agent won't set SNAT rules between internal networks when used with ML2 plugin.
external gateway modes introduces a new ""enable_snat"" comlumn to Router DB, but as ML2 plugin was merged right after this extension, it isn't included in alembic migration script [1], so this column won't be present for this plugin. As a consequence, L3 agents won't set-up SNAT rules in neutron routers.
[1]https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/128e042a2b68_ext_gw_mode.py","Add ext-gw-mode support to ML2

Closes-Bug: #1212772

ML2 doesn't support ext-gw-mode, resulting in L3 agents not
setting-up NAT rules in neutron routers (currently, l3 agents
will assume enable_snat=false if plugin doesn't provide this
parameter as described in bug 1212868)

This patchset adds the support of ext-gw-mode to ML2 plugin.

In order to support ext-gw-mode alembic migration (as well as
other potential migrations on plugin dbs), folsom_initial
migration script is fixed to include ML2 plugin. Otherwise
databases won't be set-up during migration script (but at
plugin startup by sqlalchemy), resulting in migration failures.

Change-Id: I95ec18e0a4f0e5b661f3a5d679c434f18f100fa6"
747,6206d555c3cd129a9ea83174ee786e1e15a4c48c,1395239929,,1.0,7,5,4,4,1,1.0,True,1.0,149430.0,20.0,6.0,False,28.0,23056.0,81.0,6.0,249.0,1589.0,1612.0,244.0,1351.0,1374.0,206.0,886.0,895.0,0.205765408,0.881709742,0.890656064,526,947,1288188,neutron,6206d555c3cd129a9ea83174ee786e1e15a4c48c,1,1,Fix usage of save_and_reraise_exception,"Bug #1288188 in neutron: ""unwanted lbaas related error logs in q-svc screen""","2014-03-03 09:25:31.621 5910 ERROR root [-] Original exception being dropped: ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 206, in _get_resource\n    r = self._get_by_id(context, model, id)\n', '  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 144, in _get_by_id\n    return query.filter(model.id == id).one()\n', '  File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2323, in one\n    raise orm_exc.NoResultFound(""No row was found for one()"")\n', 'NoResultFound: No row was found for one()\n']
2014-03-03 09:25:31.622 5910 WARNING neutron.services.loadbalancer.drivers.common.agent_driver_base [req-6949f3b4-f991-48b7-8424-b3bfbca7c822 None] Cannot update status: member d439c879-55f7-400f-b6a8-32753f057b05 not found in the DB, it was probably deleted concurrently
There is no need for error log about original exception being dropped as warning log is enough.
This happens due to using save_and_reraise_exception() in loadbalancer_db code:
    def _get_resource(self, context, model, id):
        try:
            r = self._get_by_id(context, model, id)
        except exc.NoResultFound:
            with excutils.save_and_reraise_exception():
                if issubclass(model, Vip):
                    raise loadbalancer.VipNotFound(vip_id=id)
                elif issubclass(model, Pool):
                    raise loadbalancer.PoolNotFound(pool_id=id)
                elif issubclass(model, Member):
                    raise loadbalancer.MemberNotFound(member_id=id)
                elif issubclass(model, HealthMonitor):
                    raise loadbalancer.HealthMonitorNotFound(monitor_id=id)
        return r
where the whole purpose of exception handler is to reraise proper type of exception.
I think save_and_reraise_exception() was designed for cases when new exceptions raised inside exception handler are not expected.
In this particular case I don't see the reason for using save_and_reraise_exception().
As an option I think a parameter can be added to save_and_reraise_exception() constructor to disable logging.","Fix usage of save_and_reraise_exception

Set reraise=False for the cases where the purpose of exception
handler is to reraise proper type of exception

Change-Id: Id6595ba1e160b9033d519ded16e0fbd2f91ccb5d
Closes-Bug: #1288188"
748,621fc02f70fa4fa50b2f05167eefa39b71f72024,1393555492,,1.0,1,1,1,1,1,0.0,True,3.0,4767984.0,57.0,17.0,False,34.0,292081.0,49.0,5.0,683.0,2615.0,3205.0,589.0,2173.0,2675.0,203.0,2552.0,2664.0,0.027312893,0.341812826,0.35680814,494,913,1285886,nova,621fc02f70fa4fa50b2f05167eefa39b71f72024,0,0,"Before was ‘’, now is None","Bug #1285886 in OpenStack Compute (nova): ""update_port passes device_id=None but neutron expects ''""","2014-02-27 14:08:23.013 ERROR nova.network.neutronv2.api [req-598b0d2f-e4e9-40eb-a9d4-027975d08b39 demo demo] Failed to delete neutron port 153f472b-f662-497b-bc7c-3cc362157ab1
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api Traceback (most recent call last):
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 420, in deallocate_for_instance
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     neutron.update_port(port, port_req_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 111, in with_params
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     ret = self.function(instance, *args, **kwargs)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 321, in update_port
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     return self.put(self.port_path % (port), body=body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1245, in put
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1221, in retry_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     headers=headers, params=params)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1164, in do_request
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     self._handle_fault_response(status_code, replybody)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 1134, in _handle_fault_response
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     exception_handler_v20(status_code, des_error_body)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api   File ""/opt/stack/python-neutronclient/neutronclient/v2_0/client.py"", line 84, in exception_handler_v20
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api     message=error_dict)
2014-02-27 14:08:23.013 TRACE nova.network.neutronv2.api NeutronClientException: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 ERROR neutron.api.v2.resource [req-3f133c17-198f-412d-b57e-66bbf0fcfbcb neutron abd2b56aa998417ba5af609a680a138d] update failed
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 466, in update
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 600, in prepare_request_body
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource     raise webob.exc.HTTPBadRequest(msg)
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource HTTPBadRequest: Invalid input for device_id. Reason: 'None' is not a valid string.
2014-02-27 14:08:23.011 TRACE neutron.api.v2.resource","Nova should pass device_id='' instead of None to neutron.update_port()

In the case that neutron raises an error and nova needs to undo the api
calls it's done against neutron it should pass device_id='' instead of
None otherwise neutron raises an exception.

Note: This should have a unit test but it's near impossible
to add one right without duplicating all of _stub_allocate_for_instance
due to the fact that it's using some crazy logic in the tests to make
decisions based on an index that's passed it. X.x

Change-Id: I91334c07401ff379f71fdef1d5bd080cc68a72ab
Closes-bug: #1285886"
749,62317a159ad98ef9ba5208f2fa7708a88692c46f,1406743619,,1.0,43,10,2,2,1,0.987380023,False,,,,,True,,,,,,,,,,,,,,,,,1126,1581,1350469,neutron,62317a159ad98ef9ba5208f2fa7708a88692c46f,1,1,,"Bug #1350469 in neutron: ""Floating ip association and deletion not working in certain scenarios for Nuage plugin""","In certain scenarios explained below, floating ip association
and deletion does not work correctly
1) Create a neutron port and assign a floating ip to it. Now boot
     a vm using the neutron port. The vm boots correctly but
     the floating ip is not assigned to the vm.
2) Boot a vm and assign a floating ip to it. Now delete the
     floating ip. The floating ip is disassociated from the vm
     and deleted from neutron but is not deleted from Nuage
     VSD.","Fix for floating ip association and deletion

Added code to associate fip to a vm in VSD, if
the fip was already associated with the vm port
in Neutron. Also added code in fip deletion,
to get the correct router id based on the check
if the fip is associated to a Neutron port or
not.
Closes-Bug: #1350469
Change-Id: I1aa24f420f656043086a4f501b15722216b843e9"
750,625e48f82b5c7f1d55641b4207afd85b0b798cc2,1398175183,,1.0,20,3,3,3,1,0.987619717,True,2.0,28618.0,20.0,9.0,False,49.0,5680770.333,105.0,4.0,1219.0,3151.0,3714.0,1009.0,2629.0,3069.0,1200.0,3028.0,3574.0,0.15313018,0.386204259,0.455820477,822,1250,1310966,nova,625e48f82b5c7f1d55641b4207afd85b0b798cc2,1,1,,"Bug #1310966 in OpenStack Compute (nova): ""nova network floating IP has direct DB access""","nova-network with floating IPs is currently broken due to a direct DB access
2014-04-22 08:04:32.395 ESC[01;31mERROR oslo.messaging.rpc.dispatcher [ESC[01;36mreq-8fa8da3a-2e61-47e9-a6c6-68f598e979ad ESC[00;36mTestServerAdvancedOps-228418898
TestServerAdvancedOps-892915532ESC[01;31m] ESC[01;35mESC[01;31mException during message handling: nova-computeESC[00m
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 276, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 262, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 329, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 250, in decorated_function
    migration.instance_uuid, exc_info=True)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 237, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 305, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 292, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3472, in resize_instance
    migration_p)
  File ""/opt/stack/nova/nova/network/api.py"", line 45, in wrapped
    return func(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/network/api.py"", line 492, in migrate_instance_start
    self._get_floating_ip_addresses(context, instance)
  File ""/opt/stack/nova/nova/network/api.py"", line 474, in _get_floating_ip_addresses
    instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 700, in instance_floating_address_get_all
    return IMPL.instance_floating_address_get_all(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 52, in __call__
    raise exception.DBNotAllowed('nova-compute')
DBNotAllowed: nova-compute","Fix migrate_instance_*() using DB for floating addresses

This fixes the network api's migrate_instance_start() and
migrate_instance_finish() methods that are still hitting the database
directly for an oddball call to get a list of floating addresses.

Change-Id: Ia310e31d31aaf5c979e41c64af8223202a18e03a
Closes-bug: #1310966"
751,62617e656fe5895de30ce55971b07680f7b0a6b2,1386209656,,1.0,2,2,2,2,1,1.0,True,2.0,60120.0,11.0,3.0,False,4.0,150899.0,5.0,1.0,26.0,1051.0,1075.0,26.0,900.0,924.0,12.0,909.0,919.0,0.010638298,0.744680851,0.752864157,143,545,1258000,cinder,62617e656fe5895de30ce55971b07680f7b0a6b2,0,0,Typo in docstring,"Bug #1258000 in Cinder: ""Fix docstring for snapshot metadata Controller""","""The volume metadata API controller for the OpenStack API."" is inappropriate description for snapshot metadata controller.","Fix docstring for snapshot_metadata controller

Change-Id: I8227c79c35688da03f5b850daec3215d7c79b2a6
Closes-Bug: #1258000"
752,627d5fbc1303b3f9d9b2f89acf21359437f9a928,1395837325,0.0,1.0,144,126,29,15,1,0.932048977,True,9.0,6051064.0,80.0,22.0,False,112.0,525837.7241,521.0,3.0,31.0,267.0,298.0,31.0,264.0,295.0,0.0,233.0,233.0,0.000834028,0.195162636,0.195162636,1826,1284677,1284677,Glance,627d5fbc1303b3f9d9b2f89acf21359437f9a928,1,0,"""Python 3: do not use 'unicode()'"", ""The unicode() function is Python2-specific""","Bug #1284677 in Glance: ""Python 3: do not use 'unicode()'""","The unicode() function is Python2-specific, we should use six.text_type() instead.","Replace unicode() for six.text_type

To support Python3, unicode() calls has been replaced by
six.text_type.
Added utils.exception_to_str(): is the proper way to convert
an exception to string, since it manages logic related to
encoding.

Change-Id: I27101390e4f95e5c7690b1b445b7e75b8bcb9a08
Closes-Bug: #1284677"
753,62c869d2375009190c5358636fd7de2ab9d194a5,1392756477,,1.0,3,1,1,1,1,0.0,True,6.0,3769243.0,90.0,32.0,False,24.0,815027.0,31.0,12.0,2.0,3055.0,3055.0,2.0,2346.0,2346.0,0.0,2919.0,2919.0,0.000135538,0.395771212,0.395771212,182,585,1260538,nova,62c869d2375009190c5358636fd7de2ab9d194a5,0,0, Remove action-args from nova-manage help ,"Bug #1260538 in OpenStack Compute (nova): ""nova-manage useage exposes action-args""","The nova-manage command exposes the action_args options during the usage output for command.
E.g.
$ nova-manage network modify -h
usage: nova-manage network modify [-h] [--fixed_range <x.x.x.x/yy>]
                                  [--project <project name>] [--host <host>]
                                  [--disassociate-project]
                                  [--disassociate-host]
                                  [action_args [action_args ...]]
positional arguments:
  action_args
<snip>
This can cause confusion as users naturally expect there to be more ""actions"" on commands like ""modify"". Even in straightforward cases, this positional argument leaks into usage.
$ nova-manage db version -h
usage: nova-manage db version [-h] [action_args [action_args ...]]
positional arguments:
  action_args
Please consider suppressing documentation on action_args. In addition, expose the __doc__ strings for these functions, which is done in the nova command.","Remove action-args from nova-manage help

During help [action_args [action_args ...]] could be
confusing for users

$ nova-manage network list -h
usage: nova-manage network list [-h]
         [action_args [action_args ...]]

Because of that action_args is set to ""SUPPRESS"" from the help

Change-Id: I0fdc480724d3b59a8eebb2d294f324441c5e46b2
Closes-Bug: #1260538"
754,62cb0dc6257daac5ec9fd1a90ee5721e6543dd76,1394570334,1.0,1.0,63,17,6,2,1,0.831632493,True,13.0,6184524.0,149.0,45.0,False,59.0,340266.3333,167.0,8.0,3.0,4502.0,4502.0,3.0,3470.0,3470.0,3.0,3370.0,3370.0,0.000528262,0.445192816,0.445192816,567,988,1289627,nova,62cb0dc6257daac5ec9fd1a90ee5721e6543dd76,0,0,Add more infor to the error,"Bug #1289627 in OpenStack Compute (nova): ""VMware NoPermission faults do not log what permission was missing""","NoPermission object has a privilegeId that tells us which permission the user did not have. Presently the VMware nova driver does not log this data. This is very useful for debugging user permissions problems on vCenter or ESX.
http://pubs.vmware.com/vsphere-55/index.jsp#com.vmware.wssdk.apiref.doc/vim.fault.NoPermission.html","VMware: Log additional details of suds faults

When a suds requests results in a fault response, the current
code loses details on that fault in the sequence of exceptions
propagated through the driver.  A NoPermission fault will contain
additional metadata on the privilegeId and object type which
needs to be logged.  The fault string will be propagated with this
fix, along with details of a NoPermission fault.

An example of the new exception:
NoPermissionException: Permission to perform this operation was denied.
{u'privilegeId': Resource.AssignVMToPool, u'object': domain-c7}

An example of new exception within retrieveproperties_fault_checker:
Error(s) NoPermission occurred in the call to RetrievePropertiesEx
{'privilegeId': System.Read, 'object': datacenter-16}

Change-Id: Iafbf052750c2835f304b2edf21d7300d1fbd7e5a
Closes-bug: #1289627"
755,630a7f369a76eaf7f942d8989a30f5dc7b09327e,1408546614,,1.0,10,10,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1217,1676,1358881,nova,630a7f369a76eaf7f942d8989a30f5dc7b09327e,0,0,Bug in test,"Bug #1358881 in OpenStack Compute (nova): ""jjsonschema 2.3.0 -> 2.4.0 upgrade breaking nova.tests.test_api_validation tests""","the following two failures appeared after upgrading jsonschema to 2.4.0.  downgrading to 2.3.0 returned the tests to passing.
======================================================================
FAIL: nova.tests.test_api_validation.TcpUdpPortTestCase.test_validate_tcp_udp_port_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  pythonlogging:''
  stderr
  stdout
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 602, in test_validate_tcp_udp_port_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 65536. 65536 is greater than the maximum of 65535'}
actual    = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 65536. 65536.0 is greater than the maximum of 65535'}
======================================================================
FAIL: nova.tests.test_api_validation.IntegerRangeTestCase.test_validate_integer_range_fails
----------------------------------------------------------------------
Traceback (most recent call last):
_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 244 -> 245...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 245 -> 246...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 246 -> 247...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 247 -> 248...
INFO [248_add_expire_reservations_index] Skipped adding reservations_deleted_expire_idx because an equivalent index already exists.
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 248 -> 249...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 249 -> 250...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 361, in test_validate_integer_range_fails
    expected_detail=detail)
  File ""/home/dev/Desktop/nova-test/nova/tests/test_api_validation.py"", line 31, in check_validation_error
    self.assertEqual(ex.kwargs, expected_kwargs)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/dev/Desktop/nova-test/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = {'code': 400,
 'detail': u'Invalid input for field/attribute foo. Value: 0. 0 is less than the minimum of 1'}
actual    = {'code': 400,
 'detail': 'Invalid input for field/attribute foo. Value: 0. 0.0 is less than the minimum of 1'}","Fix expected error details from jsonschema

The jsonschema 2.4.0 release slightly changed some of the error messages
that are provided when validation fails. This updates the unit tests to
expect the proper string and avoid failing as a result.

Change-Id: Ic8e32140e49e394659e7ebbecb28afb704b23b7c
Closes-bug: #1358881"
756,6354e2da57a8d487caf3605d4005134f584cf935,1410778334,,1.0,27,14,2,2,2,0.978869851,False,,,,,True,,,,,,,,,,,,,,,,,1877,1369558,1369558,Swift,6354e2da57a8d487caf3605d4005134f584cf935,1,1,Please ensure that the arg=None that are failing were incorrect when they were inserted.,"Bug #1369558 in OpenStack Object Storage (swift): ""direct_client not passing args between some functions""","The call to _get_direct_account_container in direct_get_account
has several of its args =None instead of set to the value passed
to direct_get_account. Similarly it is not passing the timeout args.
The same applies to _get_direct_account_container in
direct_get_container.
The direct_get_container is only called by the account-reaper
and this bug will have limited impact on it. The marker,
maintained in reap_container, is ignored by direct_get_container.
This is not as bad as it sounds, if the account-reaper successfully
deletes the first 10K objects, assuming the container has > 10K
objects, the next call to direct_get_container will in fact return
the next 10K objects even though it sets marker=None (assuming the
first 10K objects were successfully deleted).","direct_client not passing args between some functions

The call to _get_direct_account_container in direct_get_account
has several of its args =None instead of set to the value passed
to direct_get_account.

The same applies to _get_direct_account_container in
direct_get_container.

The direct_get_container is only called by the account-reaper
and this bug will have limited impact on it. The marker,
maintained in reap_container, is ignored by direct_get_container.
This is not as bad as it sounds, if the account-reaper successfully
deletes the first 10K objects, assuming the container has > 10K
objects, the next call to direct_get_container will in fact return
the next 10K objects even though it sets marker=None (assuming the
first 10K objects were successfully deleted).

This patch also updates test_direct_get_account and
test_direct_get_container to ensure the appropriate
args are included in the connection query_string.

Closes-Bug: #1369558
Change-Id: If1c8aa1240d38354ebc9b1ebca92dc1c8c36cb5f"
757,635c5b611ae765bca3a888b1fcabe6bf0027c948,1393337395,,1.0,25,3,2,2,1,0.99631652,True,5.0,6593202.0,51.0,11.0,False,106.0,641641.0,321.0,1.0,75.0,679.0,701.0,75.0,679.0,701.0,75.0,675.0,697.0,0.010208193,0.090799194,0.093754197,478,895,1284930,nova,635c5b611ae765bca3a888b1fcabe6bf0027c948,0,0,Just add an except. No bug,"Bug #1284930 in OpenStack Compute (nova): ""deallocate_fixed_ip should handle exception to rollback quota""","in nova-network, deallocate_fixed_ip function reserve quota first then do deallocate operations
if any operation failed, the quota reserve operation need to be rollback","Add Quota roll back for deallocate fix ip in nova-network

in nova-network, deallocate_fixed_ip function reserve quota
first then do deallocate operations if any operation failed,
the quota reserve operation need to be rollback.

Change-Id: If57d2b8a24b2f6ed3d90a1357e89782194df013c
Closes-Bug: #1284930"
758,635ceac07969a700756e571ff5777993b59b9829,1381464292,,1.0,32,14,2,2,1,0.965636133,True,4.0,1597064.0,36.0,15.0,False,10.0,1971832.0,18.0,4.0,2.0,205.0,205.0,2.0,205.0,205.0,0.0,100.0,100.0,0.001015228,0.102538071,0.102538071,1647,1236709,1236709,glance,635ceac07969a700756e571ff5777993b59b9829,1,1, ,"Bug #1236709 in Glance: ""argument of glance-cache-manage command is confusing""","step to reproduce:
 *execute ""glance-cache-manage"" command  as fallows
  -""glance-cache-manage queue-image 1fde5cfe-10be-4f38-86e7-9e54710d34f2 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1""
result:
 *messeage is displayed as follows
  -Queue image 45f1c095-5b8c-4537-8dd3-a58f91d7b4e1 for caching? [y/N]
expected result:
   If the number of argument isn't  one,   commad  fails.","checking length of argument list in ""glance-cache-image"" command

Currently, ""image id"" is got by ""pop"" method without checking length
of argument list in ""queue_image"" method(glance/cmd/cache_manage.py)

Consequently, only last element of argument list is used,
other elements are ignored.
 ex)
  glance-cache-image queue-image <image-id1> <image-id2> <image-id3>
  -> <image-id3> is selected and queued.
     <image-id1>/<image-id2> is ignored.

Because ""glance-cache-image"" command is user command(*),
I think that elements which are ignored should not be existed
and checking of length is essential.

In order to check length, I replaced ""try/except"" statement with
""if/else"" statement.

Also, I modified 2 methods at the same point.
 -delete_queued_image
 -delete_cached_image

 * http://docs.openstack.org/developer/glance/cache.html

Change-Id: I5cff1f9b5fe8f718ca74f1c9f58dba469396dc8f
Closes-Bug: #1236709"
759,63d271d4e4a8bab947f90f457ed176e46135d39e,1401770391,,1.0,25,5,3,2,1,0.783580704,False,,,,,True,,,,,,,,,,,,,,,,,945,1381,1326173,neutron,63d271d4e4a8bab947f90f457ed176e46135d39e,1,1,,"Bug #1326173 in neutron: ""BSN: Capabilities check double-json parse""","The capabilities check in the BigSwitch server manager tries to call json.loads on an object that has already been decoded and fails. This
causes the servers to have an empty capability list so none of the newer features are leveraged.","Big Switch: fix capabilities retrieval code

References the raw REST response in the capabilities
parsing code so json.loads doesn't get an object
that may already be decoded and fail.

Closes-Bug: #1326173
Change-Id: Ia3179b7499f35a6ab2e9ce1631ab15ed27d64647"
760,6427b5506de72f0adf224d3643f6df9d4a465bdd,1392619285,,1.0,2,2,1,1,1,0.0,True,1.0,413819.0,9.0,3.0,False,7.0,1828997.0,8.0,3.0,15.0,746.0,760.0,15.0,707.0,721.0,0.0,13.0,13.0,0.003636364,0.050909091,0.050909091,421,836,1280964,cinder,6427b5506de72f0adf224d3643f6df9d4a465bdd,1,0,Incompatible with python3,"Bug #1280964 in Cinder: ""cStringIO.StringIO is incompatible for python 3""","Import cStringIO
cStringIO.StringIO()
should be :
from six.moves import cStringIO
cStringIO.StringIO()=>cStringIO
For Python3 compatible.","Use six.moves cStringIO instead of cStringIO

to keep Python 3.x compatibility, use six.moves.cStringIO to
replace StringIO

Change-Id: Id89c6156d9bba0007e8b16cd2cc28413a7cc5fd3
Closes-Bug: #1280964"
761,642e6acc5bf469dcf419da3bc5dce9d83aa48d6a,1401186232,,1.0,70,7,2,2,1,0.845350937,True,5.0,3750508.0,50.0,15.0,False,159.0,1004493.0,481.0,2.0,142.0,1236.0,1250.0,142.0,1235.0,1249.0,139.0,1226.0,1238.0,0.017508754,0.153451726,0.154952476,696,1122,1298975,nova,642e6acc5bf469dcf419da3bc5dce9d83aa48d6a,0,0,tests and some refactoring ,"Bug #1298975 in OpenStack Compute (nova): ""libvirt.finish_migration is too large and not tested""","This method needs to be split in several small methods then each methods has to be tested.
A possible solution could be:
  * determines the disk size from instance properties
  * methods to convert disk from qcow2 to raw and raw to qcow2
  * method to resize the disk","libvirt: split and test finish_migration disk resize

Adds a '_disk_resize' method to help resizing disk
by checking and preparing the disk to be extend with disk.api.
Update finish_migration to use this helper.

Closes-Bug: #1298975
Signed-off-by: Sahid Orentino Ferdjaoui <sahid.ferdjaoui@cloudwatt.com>

Change-Id: Iefe16915a2e657d848629d7e4a4fcf67298f913e"
762,6438d2587e1b93c73b8461b0e7dbd7bcd5243aea,1389130143,1.0,1.0,26,1,2,2,1,0.503258335,True,3.0,1522005.0,22.0,12.0,False,192.0,6767.0,1069.0,4.0,511.0,3100.0,3100.0,334.0,2487.0,2487.0,454.0,2902.0,2902.0,0.065158241,0.415723901,0.415723901,267,671,1266919,nova,6438d2587e1b93c73b8461b0e7dbd7bcd5243aea,1,1,There is a bug and add an exception for that,"Bug #1266919 in OpenStack Compute (nova): ""Missing InstanceInfoCache entry prevents delete""","If you're trying to delete an instance which is out-of-sync such that it's missing a InstanceInfoCache entry, you'll will receive a traceback: http://paste.openstack.org/show/60685/
Delete in this case should be allowed so that you can cleanup these 'broken' instances.
The solution is to catch the InstanceInfoCacheNotFound exception (like we do with other NotFound exceptions around this code), and continue on.","Allow delete when InstanceInfoCache entry is missing

The existing code wouldn't allow you to delete 'broken' instances that were
(for whatever reason) missing their InstanceInfoCache entry. This patch fixes
it such that those instances can be deleted.

Adds a test case to cover the new exception handling.

Closes-Bug: #1266919

Co-Authored-By: Matt Riedemann <mriedem@us.ibm.com>
Change-Id: I3d566a25e340b5d5de86740141e1eaa5b3742915"
763,645a84f990c90e28548cf35b4b5f242eb0e0c286,1381867068,,1.0,16,8,6,1,1,0.930628224,True,2.0,25081.0,11.0,4.0,False,9.0,4078828.667,9.0,3.0,991.0,1092.0,1092.0,873.0,973.0,973.0,850.0,938.0,938.0,0.779304029,0.85989011,0.85989011,1625,1233861,1233861,cinder,645a84f990c90e28548cf35b4b5f242eb0e0c286,1,1,"“It appears that this is caused by not setting the charset in the migration,”","Bug #1233861 in Cinder: ""Mysql foreign key failure during db migration from 9 to 10""","This happens during the upgrade from grizzly to havana using packages from the Ubuntu Cloud Archive on an Ubuntu 12.04.3 system.
root@openstack01:~# cinder-manage db sync
2013-10-01 23:35:04.977 7923 INFO migrate.versioning.api [-] 9 -> 10...
2013-10-01 23:35:05.000 7923 ERROR 010_add_transfers_table [-] Table |Table('transfers', MetaData(bind=Engine(mysql://cinder:eNgoyam3@127.0.0.1/cinder?charset=utf8)), Col
umn('created_at', DateTime(), table=<transfers>), Column('updated_at', DateTime(), table=<transfers>), Column('deleted_at', DateTime(), table=<transfers>), Column('delete
d', Boolean(), table=<transfers>), Column('id', String(length=36), table=<transfers>, primary_key=True, nullable=False), Column('volume_id', String(length=36), ForeignKey
('volumes.id'), table=<transfers>), Column('display_name', String(length=255), table=<transfers>), Column('salt', String(length=255), table=<transfers>), Column('crypt_ha
sh', String(length=255), table=<transfers>), Column('expires_at', DateTime(), table=<transfers>), schema=None)| not created!
2013-10-01 23:35:05.003 7923 CRITICAL cinder [-] (OperationalError) (1005, ""Can't create table 'cinder.transfers' (errno: 150)"") '\nCREATE TABLE transfers (\n\tcreated_at
 DATETIME, \n\tupdated_at DATETIME, \n\tdeleted_at DATETIME, \n\tdeleted BOOL, \n\tid VARCHAR(36) NOT NULL, \n\tvolume_id VARCHAR(36), \n\tdisplay_name VARCHAR(255), \n\t
salt VARCHAR(255), \n\tcrypt_hash VARCHAR(255), \n\texpires_at DATETIME, \n\tPRIMARY KEY (id), \n\tCHECK (deleted IN (0, 1)), \n\tFOREIGN KEY(volume_id) REFERENCES volume
s (id)\n)ENGINE=InnoDB\n\n' ()
2013-10-01 23:35:05.003 7923 TRACE cinder Traceback (most recent call last):
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 695, in <module>
2013-10-01 23:35:05.003 7923 TRACE cinder     main()
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 692, in main
2013-10-01 23:35:05.003 7923 TRACE cinder     fn(*fn_args)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/bin/cinder-manage"", line 228, in sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return migration.db_sync(version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/migration.py"", line 33, in db_sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return IMPL.db_sync(version=version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migration.py"", line 77, in db_sync
2013-10-01 23:35:05.003 7923 TRACE cinder     return versioning_api.upgrade(get_engine(), repository, version)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/api.py"", line 186, in upgrade
2013-10-01 23:35:05.003 7923 TRACE cinder     return _migrate(url, repository, version, upgrade=True, err=err, **opts)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""<string>"", line 2, in _migrate
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migration.py"", line 43, in patched_with_engine
2013-10-01 23:35:05.003 7923 TRACE cinder     return f(*a, **kw)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/api.py"", line 366, in _migrate
2013-10-01 23:35:05.003 7923 TRACE cinder     schema.runchange(ver, change, changeset.step)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/schema.py"", line 91, in runchange
2013-10-01 23:35:05.003 7923 TRACE cinder     change.run(self.engine, step)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/migrate/versioning/script/py.py"", line 145, in run
2013-10-01 23:35:05.003 7923 TRACE cinder     script_func(engine)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/cinder/db/sqlalchemy/migrate_repo/versions/010_add_transfers_table.py"", line 47, in upg
rade
2013-10-01 23:35:05.003 7923 TRACE cinder     transfers.create()
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/schema.py"", line 614, in create
2013-10-01 23:35:05.003 7923 TRACE cinder     checkfirst=checkfirst)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1479, in _run_visitor
2013-10-01 23:35:05.003 7923 TRACE cinder     conn._run_visitor(visitorcallable, element, **kwargs)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1122, in _run_visitor
2013-10-01 23:35:05.003 7923 TRACE cinder     **kwargs).traverse_single(element)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/sql/visitors.py"", line 111, in traverse_single
2013-10-01 23:35:05.003 7923 TRACE cinder     return meth(obj, **kw)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/ddl.py"", line 89, in visit_table
2013-10-01 23:35:05.003 7923 TRACE cinder     self.connection.execute(schema.CreateTable(table))
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 662, in execute
2013-10-01 23:35:05.003 7923 TRACE cinder     params)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 720, in _execute_ddl
2013-10-01 23:35:05.003 7923 TRACE cinder     compiled
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 874, in _execute_context
2013-10-01 23:35:05.003 7923 TRACE cinder     context)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1024, in _handle_dbapi_exception
2013-10-01 23:35:05.003 7923 TRACE cinder     exc_info
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/util/compat.py"", line 195, in raise_from_cause
2013-10-01 23:35:05.003 7923 TRACE cinder     reraise(type(exception), exception, tb=exc_tb)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 867, in _execute_context
2013-10-01 23:35:05.003 7923 TRACE cinder     context)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/engine/default.py"", line 324, in do_execute
2013-10-01 23:35:05.003 7923 TRACE cinder     cursor.execute(statement, parameters)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/MySQLdb/cursors.py"", line 174, in execute
2013-10-01 23:35:05.003 7923 TRACE cinder     self.errorhandler(self, exc, value)
2013-10-01 23:35:05.003 7923 TRACE cinder   File ""/usr/lib/python2.7/dist-packages/MySQLdb/connections.py"", line 36, in defaulterrorhandler
2013-10-01 23:35:05.003 7923 TRACE cinder     raise errorclass, errorvalue
2013-10-01 23:35:05.003 7923 TRACE cinder OperationalError: (OperationalError) (1005, ""Can't create table 'cinder.transfers' (errno: 150)"") '\nCREATE TABLE transfers (\n\
tcreated_at DATETIME, \n\tupdated_at DATETIME, \n\tdeleted_at DATETIME, \n\tdeleted BOOL, \n\tid VARCHAR(36) NOT NULL, \n\tvolume_id VARCHAR(36), \n\tdisplay_name VARCHAR
(255), \n\tsalt VARCHAR(255), \n\tcrypt_hash VARCHAR(255), \n\texpires_at DATETIME, \n\tPRIMARY KEY (id), \n\tCHECK (deleted IN (0, 1)), \n\tFOREIGN KEY(volume_id) REFERE
NCES volumes (id)\n)ENGINE=InnoDB\n\n' ()
2013-10-01 23:35:05.003 7923 TRACE cinder
mysql> SHOW ENGINE INNODB STATUS;
[...]
LATEST FOREIGN KEY ERROR
------------------------
131001 23:30:06 Error in foreign key constraint of table cinder/transfers:
FOREIGN KEY(volume_id) REFERENCES volumes (id) )ENGINE=InnoDB:
Cannot find an index in the referenced table where the
referenced columns appear as the first columns, or column types
in the table and the referenced table do not match for constraint.
Note that the internal storage type of ENUM and SET changed in
tables created with >= InnoDB-4.1.12, and such columns in old tables
cannot be referenced by such columns in new tables.
See http://dev.mysql.com/doc/refman/5.5/en/innodb-foreign-key-constraints.html
for correct foreign key definition.
[...]","FK lookup failures during migration

There are a couple of cases where migrations have failed
upgrading from Grizzly to Havana, this seems to be isolated
to a couple of migrations so far and the error message is a
failure to lookup/associate the volume-id FK dependency.

It appears that this is caused by not setting the charset
in the migration, so the result is that the initial db setup
uses utf8 and the migrations are using the default latin1.

This patch goes through all of the migrations in Havana that
specify InnoDB and explicitly sets the charset to utf8 to match
the volumes table (and the other original tables).

Change-Id: I43b219ff5e4eea10a7391ad65ef68a80b7460370
Closes-Bug: #1233861"
764,6471776b6b25bb4062238f7c1b732b2d6999ec65,1385597459,0.0,1.0,44,7,4,2,1,0.692828151,True,2.0,770867.0,19.0,10.0,False,21.0,926811.75,47.0,6.0,27.0,2285.0,2287.0,27.0,2041.0,2043.0,27.0,1377.0,1379.0,0.00416295,0.204876598,0.205173952,1781,1252827,1252827,nova,6471776b6b25bb4062238f7c1b732b2d6999ec65,1,1, ,"Bug #1252827 in OpenStack Compute (nova): ""VMWARE: Intermittent problem with stats reporting""","I see that sometimes vmware driver reports 0 stats. Please take a look at the following log file for more information: http://162.209.83.206/logs/51404/6/screen-n-cpu.txt.gz
excerpts from log file:
2013-11-18 15:41:03.994 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for datastore Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for host Reason: None
2013-11-18 15:41:04.029 20162 WARNING nova.virt.vmwareapi.vim_util [-] Unable to retrieve value for resourcePool Reason: None
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free ram (MB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:389
2013-11-18 15:41:04.029 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: free disk (GB): 0 _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:390
2013-11-18 15:41:04.030 20162 DEBUG nova.compute.resource_tracker [-] Hypervisor: VCPU information unavailable _report_hypervisor_resource_view /opt/stack/nova/nova/compute/resource_tracker.py:397
During this time we cannot spawn any server. Look at the http://162.209.83.206/logs/51404/6/screen-n-sch.txt.gz
excerpts from log file:
2013-11-18 15:41:52.475 DEBUG nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter AvailabilityZoneFilter returned 1 host(s) get_filtered_objects /opt/stack/nova/nova/filters.py:88
2013-11-18 15:41:52.476 DEBUG nova.scheduler.filters.ram_filter [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] (Ubuntu1204Server, domain-c26(c1)) ram:-576 disk:0 io_ops:0 instances:1 does not have 64 MB usable ram, it only has -576.0 MB usable ram. host_passes /opt/stack/nova/nova/scheduler/filters/ram_filter.py:60
2013-11-18 15:41:52.476 INFO nova.filters [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] Filter RamFilter returned 0 hosts
2013-11-18 15:41:52.477 WARNING nova.scheduler.driver [req-dc82a954-3cc5-4627-ae01-b3d1ec2155af InstanceActionsTestXML-tempest-716947327-user InstanceActionsTestXML-tempest-716947327-tenant] [instance: 1a648022-1783-4874-8b41-c3f4c89d8500] Setting instance to ERROR state.","VMware: Fix unhandled session failure issues

VMware driver has a re-try mechanism to handle session expiration
failures. Due to a minor bug in the exception handling module, this
failure was unhandled.

The patch fixes this issue and has added tests.

Closes-Bug: #1252827
Change-Id: Ie91adb4b4b57b7cefeed855cdbe4710da86294f0"
765,6478554f531f6ee2fa86226fbc79dd31e556bc06,1395200662,1.0,1.0,140,3,8,7,1,0.816481797,True,8.0,5515682.0,102.0,25.0,False,144.0,602871.125,505.0,3.0,15.0,1271.0,1274.0,15.0,1050.0,1053.0,10.0,965.0,965.0,0.001440545,0.126506024,0.126506024,206,609,1262124,nova,6478554f531f6ee2fa86226fbc79dd31e556bc06,0,0,Evolution: we should allow an admin user to…,"Bug #1262124 in OpenStack Compute (nova): ""Ceilometer cannot poll and publish floatingip samples""","The ceilometer central agent pull and pubulish floatingip samples or other types of samples .but it cannot get valid samples of floatingip.
The reason is ceilometer floatingip poster call nova API  ""list"" metod of nova.api.openstack.compute.contrib.floating_ips.FloatingIPController, this API get floatingips filtered by context.project_id.
The current context.project_id is the id of tenant ""service"".So,the result is {""floatingips"": []}
the logs of nova-api-os-compute is:
http://paste.openstack.org/show/55285/
Here,ceilometer invoke novaclient to list floatingips,and novaclient call nova API,then,the nova API will call nova network API or neutron API with:
    client.list_floatingips(tenant_id=project_id)['floatingips']
Novaclient can not list other tenant's floatingip but only the tenant of current context.
So, I think we should modify the nova API with adding a parameter like ""all_tenant"" which accessed by admin role.
This should be confirmed?","Allow admin user to get all tenant's floating IPs

When getting floatingips by Nova API, the results will be filtered
with the 'tenant_id'.
So, we can only get the floatingips belonging the tenant of current
context.
When ceilometer invokes novaclient to list floatingips, it will get an
empty list because the tenant is 'service'.
we should allow an admin user to index all tenants's floatingip by adding a
parameter 'all_tenants'.

Part1:
    part1 try to implement this and part2 will remove the unused codes.

Change-Id: I7ab1d5ff463fc29928f6811f846c9e204390a412
Closes-bug: #1262124"
766,648063881ebfbb54a362ef32d58263fc0f20981c,1409200654,,1.0,11,6,2,2,1,0.672294817,False,,,,,True,,,,,,,,,,,,,,,,,1244,1703,1362416,neutron,648063881ebfbb54a362ef32d58263fc0f20981c,1,1,,"Bug #1362416 in neutron: ""midonet: Deletes incorrect nested DHCP subnet""","midonet ""delete_dhcp"" function doesn't check subnet length when searching for the DHCP entries to delete.  This means it can delete the wrong subnet entry if two are nested (ie: have the same prefix, but different lengths).
Disclaimer: I don't have midonet equipment and don't know if it is even capable of supporting nested DHCP subnets - I just found this while wondering why 'net_len' variable was unused.","Compare subnet length as well when deleting DHCP entry

When searching for the DHCP subnet entry to delete, the existing code
compares only the subnet prefix and ignores subnet length.  This could
delete the wrong entry/entries if nested subnets are present.

Change-Id: Icf079c42adeca14ef84ec57dc45a5930fde8786d
Closes-Bug: #1362416"
767,64aa8062bc96799993deafbdf184adb72c509f20,1411832978,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1879,1374783,1374783,Swift,64aa8062bc96799993deafbdf184adb72c509f20,0,0,Bug in test files,"Bug #1374783 in OpenStack Object Storage (swift): ""Some statements are evaluated twice in the setUp of the TestObjectReplicator""",It should be referenced by the last saved value. Remove the duplicated statements.,"Some statements are evaluated twice in the setUp of the
TestObjectReplicator

Remove the duplicated statements.

Closes-Bug: #1374783

Change-Id: If2b55e864fea497d7a7b218adf11eb7749c27765"
768,64aa897c8a2408093e277972f1e702e1a4287db2,1380546908,,1.0,12,1,2,2,1,0.779349837,True,6.0,6261470.0,49.0,23.0,False,179.0,260540.0,955.0,6.0,53.0,3725.0,3741.0,53.0,3228.0,3244.0,53.0,2794.0,2810.0,0.008777633,0.454323797,0.456924577,1540,1223843,1223843,nova,64aa897c8a2408093e277972f1e702e1a4287db2,1,1,“run_instance() doesn't properly handle instances being deleting”,"Bug #1223843 in OpenStack Compute (nova): ""run_instance() doesn't properly handle instances being deleting""",The UnexpectedTaskStateError that this currently results in should be caught and handled correctly to avoid unnecessary tracebacks in the compute log.,"Catch exception while building due to instance being deleted

In run_instance() a instance_update() call in _start_building() can
result in a UnexpectedDeletingTaskStateError if the instance gets
deleted while being built.Since this is an expected situation, we
don't want this exception written in the logs or worse, the instance
be set to error state. Instead catch this and raise a proper
BuildAbortException that gets handled above this call.

Change-Id: Ib9d3dd22b33b104f1593d534876a5387df81d11c
Closes-Bug: #1223843"
769,64acc3bd63846a6e7da8d1136f946372c698cb76,1392972166,,1.0,29,1,2,2,1,0.881290899,True,5.0,812717.0,54.0,4.0,False,14.0,191268.5,32.0,2.0,481.0,1163.0,1403.0,403.0,976.0,1167.0,224.0,588.0,664.0,0.265643447,0.695395514,0.785123967,449,864,1282922,neutron,64acc3bd63846a6e7da8d1136f946372c698cb76,1,1,performance bug,"Bug #1282922 in neutron: ""nec pluign: ResourceClosedError from sqlalchemy when release_dhcp_port and API delete-port run in parallel""","(NEC Neutron third party CI blocking issue)
When multiple delete_port are run in parallel, nec plugin detects ResourceClosedError from sqlalchemy. The error itself can occur when a port row is deleted from ports table, but the problem is it occurs even after retrying db_plugin.delete_port by restarting a new transaction in plugin.delete_port().
At now it seems only nec plugin hits this problem, but potentially other plugins can hit this issue.
The case I observed is that delete-port from dhcp-agent (release_dhcp_port RPC call) and delete-port from delete-network API request are run in parallel. plugin.delete-port in nec plugin calls REST API call to an external controller in addition to operates on neutron database.
After my investigation and testing, db_plugin.delete_ports() calls plugin.delete_port() under a transaction.
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1367
This means the transaction continues over API calls to external controller and it leads to a long transaction.
When plugin.delete_ports() and plugin.delete_port() are run at the same time, even if plugin.delete_port() avoid long transaction, db operations in plugin.delete_port() is blocked and they can fail with timeout.
One example is : http://133.242.19.163:8000/neutron-ci-logs/Neutron_Master/917/logs/devstack/q-svc-filtered.txt.gz
2014-02-18 18:38:41.771 16025 ERROR neutron.api.v2.resource [-] delete failed
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/common/log.py"", line 43, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     data['exc'] = unicode(e)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/excutils.py"", line 68, in __exit__
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     six.reraise(self.type_, self.value, self.tb)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/common/log.py"", line 40, in wrapper
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     ret = method(*args, **kwargs)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 419, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     super(NECPluginV2, self).delete_network(context, id)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 968, in delete_network
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     for p in ports)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 967, in <genexpr>
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     only_auto_del = all(p['device_owner'] in AUTO_DELETE_PORT_OWNERS
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/loading.py"", line 65, in instances
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     fetch = cursor.fetchall()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 752, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     self.cursor, self.context)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/base.py"", line 1027, in _handle_dbapi_exception
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     util.reraise(*exc_info)
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 746, in fetchall
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     l = self.process_rows(self._fetchall_impl())
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 715, in _fetchall_impl
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     self._non_result()
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/engine/result.py"", line 720, in _non_result
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource     ""This result object does not return rows. ""
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource ResourceClosedError: This result object does not return rows. It has been closed automatically.
2014-02-18 18:38:41.771 16025 TRACE neutron.api.v2.resource
2014-02-18 18:38:41.772 16025 DEBUG neutron.wsgi [-] Data Request Failed: internal server error while processing your request. type is <type 'unicode'> _to_xml_node /opt/stack/neutron/neutron/wsgi.py:533
2014-02-18 18:38:41.772 16025 INFO neutron.wsgi [-] 10.56.45.201 - - [18/Feb/2014 18:38:41] ""DELETE /v2.0/networks/451384e4-a4f2-4558-8da0-40dc60f1a17f HTTP/1.1"" 500 517 103.278109","nec plugin: Avoid long transaction in delete_ports

db_plugin.delete_ports() can lead to long transaction
if plugin.deleete_port talks with external system.
This commit removes a transaction in delete_ports and
allows NEC plugin to use more granular db transactions
in delete_port. It greatly helps db race conditions and
timeouts in delete_port operations.

To avoid to impact other plugins/drivers by changing
db_plugin.delete_ports directly and to land this patch soon,
this commit overrides delete_ports() in NEC plugin.
Further disssion on transaction in delete_ports will be
discussed under bug 1282925.

Closes-Bug: #1282922
Related-Bug: #1282925

Change-Id: I2c00694ad34eb2058bf7a0ff1c920ceded327d43"
770,6534a89de9cabc274cbdb7d2ecee3d851c456a87,1390633219,,1.0,14,1,2,2,1,0.970950594,True,7.0,1108603.0,48.0,24.0,False,169.0,103310.5,859.0,6.0,101.0,2433.0,2472.0,83.0,1944.0,1976.0,95.0,2318.0,2352.0,0.013422819,0.324244966,0.328998881,328,737,1272623,nova,6534a89de9cabc274cbdb7d2ecee3d851c456a87,1,1,"Bug that allows to restore, when it shouldn’t ","Bug #1272623 in OpenStack Compute (nova): ""nova refuses to start if there are baremetal instances with no associated node""","This can happen if a deployment is interrupted at just the wrong time.
2014-01-25 06:53:38,781.781 14556 DEBUG nova.compute.manager [req-e1958f79-b0c0-4c80-b284-85bb56f1541d None None] [instance: e21e6bca-b528-4922-9f59-7a1a6534ec8d] Current state is 1, state in DB is 1. _init_instance /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py:720
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/service.py"", line 172, in start
    self.manager.init_host()
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 805, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py"", line 684, in _init_instance
    self.driver.plug_vifs(instance, net_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 538, in plug_vifs
    self._plug_vifs(instance, network_info)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 543, in _plug_vifs
    node = _get_baremetal_node_by_instance_uuid(instance['uuid'])
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/driver.py"", line 85, in _get_baremetal_node_by_instance_uuid
    node = db.bm_node_get_by_instance_uuid(ctx, instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/api.py"", line 101, in bm_node_get_by_instance_uuid
    instance_uuid)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/db/sqlalchemy/api.py"", line 112, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/db/sqlalchemy/api.py"", line 152, in bm_node_get_by_instance_uuid
    raise exception.InstanceNotFound(instance_id=instance_uuid)
InstanceNotFound: Instance 84c6090b-bf42-4c6a-b2ff-afb22b5ff156 could not be found.
If there is no allocated node, we can just skip that part of delete.","Don't try to restore VM's in state ERROR.

We don't try to restore VM's that are in a failed BUILDING state, so
attempting to start ERROR VMs is more than a little weird. The one
exception to this rule are VMs that are in RESIZE_MIGRATING, since
recovery is already attempted. It's also a problem, because many ERROR
states aren't recoverable from (at the moment anyhow).

Closes-Bug: #1272623
Change-Id: I0599b83a82ad3ee67a92126d3b57df5b02e20539
Co-Authored-By: Robert Collins <rbtcollins@hp.com>"
771,65726540a19442c2882756b26959fdd1088d442e,1387333642,,1.0,7,2,2,2,1,0.764204507,True,3.0,1267680.0,23.0,10.0,False,140.0,121132.0,428.0,3.0,10.0,1290.0,1293.0,10.0,1073.0,1076.0,10.0,1234.0,1237.0,0.001597676,0.179375454,0.179811184,1696,1240383,1240383,nova,65726540a19442c2882756b26959fdd1088d442e,1,1, ,"Bug #1240383 in OpenStack Compute (nova): ""pci passthrough: unicode object support does not work with the unkown fileds""","2013-10-17 06:00:23.066 ERROR nova.openstack.common.threadgroup [-] (u'A string is required here, not %s', 'list')
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
objects support now requied known the value you saved in the objects, but pci extra_info design for 'unkonw' usage. so can not get the type info.
refer the trace:
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     x.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 65, in run_service
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     service.start()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 164, in start
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.manager.pre_start_hook()
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 801, in pre_start_hook
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.update_available_resource(nova.context.get_admin_context())
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 4868, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     rt.update_available_resource(context)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return f(*args, **kwargs)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 292, in update_available_resource
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     'pci_passthrough_devices')))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 189, in set_hvdevs
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     dev_obj = pci_device.PciDevice.create(dev)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/pci_device.py"", line 174, in create
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     pci_device.update_device(dev_dict)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/pci_device.py"", line 136, in update_device
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     self.extra_info = extra_info
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/base.py"", line 68, in setter
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     field.coerce(self, name, value))
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 308, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     obj, '%s[""%s""]' % (attr, key), element)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 163, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     return self._type.coerce(obj, attr, value)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/objects/fields.py"", line 214, in coerce
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup     value.__class__.__name__)
2013-10-17 06:00:23.066 TRACE nova.openstack.common.threadgroup ValueError: (u'A string is required here, not %s', 'list')","PCI address should be uniform

PCI address should be  ""domain:bus:slot.function"",PF address should
also use this format.

Closes-Bug: #1240383
Change-Id: I303c0905c40d6d07b9c935367d33df73b51fcd56
signed-off-by: Yongli he <yongli.he@intel.com>"
772,657b5106c35e2834d75e77e4702b9fd99bde817f,1394500652,,1.0,5,5,2,2,1,0.970950594,True,2.0,253711.0,21.0,6.0,False,57.0,47733.0,108.0,3.0,74.0,1350.0,1380.0,74.0,1105.0,1135.0,64.0,1200.0,1220.0,0.042763158,0.790131579,0.803289474,585,1006,1290627,cinder,657b5106c35e2834d75e77e4702b9fd99bde817f,1,1,Fix exception msg,"Bug #1290627 in Cinder: ""Exception message of CoraidESMConfigureError is different from the expected""","Expectation:
cinder.exception.CoraidESMConfigureError: ESM configure request failed: Reply is empty.
Actual:
cinder.exception.CoraidESMConfigureError: Reply is empty.","Fix exception message of CoraidESMConfigureError

Since 'message' is the same name as the second argument to the
constructor of CinderException, 'message=""Oorah""' is not a keyword
argument.

Therefore, ""message = _('ESM configure request failed: %(message)s.')""
doesn't work correctly.

Then, remove the redundant period.

Change-Id: Iafc457197fc3993f9f942babcdda0cd79b7bd518
Closes-Bug: #1290627"
773,6594746108afa2db39ca2973b3dd0c37611f955e,1400832680,1.0,1.0,127,1,5,5,1,0.499299148,True,10.0,3774871.0,101.0,29.0,False,206.0,1166794.0,894.0,1.0,0.0,1226.0,1226.0,0.0,1225.0,1225.0,0.0,1218.0,1218.0,0.000125094,0.152489367,0.152489367,1686,1239952,1239952,nova,6594746108afa2db39ca2973b3dd0c37611f955e,1,1, ,"Bug #1239952 in OpenStack Compute (nova): ""boot_from_volume with down of cinder-api return 400 badRequest""","[Nova]2b17aa7f5af2adb418592b7169b50b231989bf37
Now, when I boot VM from volume with down of cinder-api, openstack returns 400.
It seems incompatibility in this case.
When HTTPClient exception ocuured, it should return 500Internal server error.
In fact, Nova turned into normal responses from HTTPClient exception.
$cinder list --all-tenants
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1 | available |     None     |  1   |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
1. die cinder-api service
2. boot_from_volume
$curl -v -H ""X-Auth-Token: $TOKEN"" -H ""Content-type: application/json"" -X POST http://192.168.122.180:8774/v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot -d '{
    ""server"": {
        ""block_device_mapping"": [
            {
                ""delete_on_termination"": 0,
                ""device_name"": ""vda"",
                ""volume_id"": ""2facb85a-5ea1-4a9c-b615-e1dc75f30bb1"",
                ""volume_size"": 1
            }
        ],
        ""flavorRef"": ""1"",
        ""imageRef"": ""ce5505ca-4d22-4a57-bc63-261a1d6dd664"",
        ""name"": ""test-vm"",
        ""networks"": [
            {
                ""port"": ""cb10dd1d-87e5-421c-9382-32fe933317ea""
            }
        ]
    }
}'
* About to connect() to 192.168.122.180 port 8774 (#0)
*   Trying 192.168.122.180... connected
> POST /v2/7020c32ea9384e0291f64f4c1288b397/os-volumes_boot HTTP/1.1
> User-Agent: curl/7.22.0 (x86_64-pc-linux-gnu) libcurl/7.22.0 OpenSSL/1.0.1 zlib/1.2.3.4 libidn/1.23 librtmp/2.3
> Host: 192.168.122.180:8774
> Accept: */*
> X-Auth-Token: --skip--
> Content-type: application/json
> Content-Length: 314
>
* upload completely sent off: 314out of 314 bytes
< HTTP/1.1 400 Bad Request
< Content-Length: 135
< Content-Type: application/json; charset=UTF-8
< X-Compute-Request-Id: req-0a8323af-bd7c-4701-ad5a-afa0774fb213
< Date: Tue, 15 Oct 2013 06:32:31 GMT
<
* Connection #0 to host 192.168.122.180 left intact
* Closing connection #0
{
    ""badRequest"": {
        ""code"": 400,
        ""message"": ""Block Device Mapping is Invalid: failed to get volume 2facb85a-5ea1-4a9c-b615-e1dc75f30bb1.""
    }
}
(For reference, this report is same as openstack-dev mail post.
== [openstack-dev] behaviour about boot-from-volume (possible bug))","Raise HTTPInternalServerError when boot_from_volume with cinder down

This patch changes exception type.
Boot_from_volume api raises BlockDeviceMappingIsInvalid exception
when cinder-api is down.
Also it returns response code 400.
It is not exact. Because request is valid.
Exception was changed to raise HTTPInternalServerError and
return a 500 error.

Closes-Bug: #1239952

Change-Id: Iec8839d46e6186fe544b28b0aafc8aa6faad53b7"
774,65ae11285fc2e3e11d5d07ad1e253f1a4f8ebccf,1385093680,,1.0,3,3,1,1,1,0.0,True,6.0,525862.0,66.0,28.0,False,133.0,14961.0,372.0,2.0,125.0,511.0,608.0,125.0,471.0,568.0,122.0,489.0,583.0,0.018490679,0.073662057,0.087793145,1769,1251822,1251822,nova,65ae11285fc2e3e11d5d07ad1e253f1a4f8ebccf,1,1, ,"Bug #1251822 in OpenStack Compute (nova): ""nova compute log incorrect message when enable/disable host""","nova host-update --status enable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-e4a1d1cc-9799-4748-b065-6bb938126134 None None] Updating compute service status to: disabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628  << This should be ""updating to enabled""
 nova host-update --status disable liugya-ubuntu
DEBUG nova.virt.libvirt.driver [req-90b2cb09-e995-4676-8a1e-7774bb8a0f12 admin admin] Updating compute service status to: enabled from (pid=21833) set_host_enabled /opt/stack/nova/nova/virt/libvirt/driver.py:2628 << This should be ""updating to disabled""","libvirt: Fix log message when disable/enable a host

When disable/enable a host, the log message is not correct. As
nova compute will log message saying the host was disabled when
enable a host and log message saying the host was enabled when
disable a host.

We should do as this:
update to disable -> log message should be the host was disabled
update to enable -> log message should be the host was enabled

Change-Id: I394e92fe6fe5ffa06464f1ba93e9fc2acec942ef
Closes-Bug: #1251822"
775,65b16f50bdd6544244d80c0f07653f4f478d3244,1408525289,,1.0,3,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1219,1678,1359113,neutron,65b16f50bdd6544244d80c0f07653f4f478d3244,0,0,Typos,"Bug #1359113 in neutron: ""Typos in l3_router_plugin's comments""","In the services.l3_router.L3RouterPlugin.create_floatingip(), we have some comments (docstring) to help us to know this function, but there are some nits inside them:
  :param floatingip: data fo the floating IP being created
  :returns: A floating IP object on success
  AS the l3 router plugin aysnchrounously creates floating IPs
  leveraging tehe l3 agent, the initial status fro the floating
  IP object will be DOWN.
data fo the floating IP --> data of the floating IP
aysnchrounously creates --> asynchronously creates
status fro the floating IP --> status for the floating IP","corrects the typos in l3_router_plugin's comments

there were some typos in the docstring of the function
create_floatingip(). This patch corrects them

Change-Id: I869567d5470a27ff037feeabfc1057b8e2da38b7
Closes-Bug: #1359113"
776,65c635d6a5be0b4afaa9289c901f90275a0be102,1387833881,,1.0,66,19,3,2,1,0.587154852,True,6.0,5399524.0,36.0,13.0,False,11.0,1613711.667,28.0,5.0,42.0,882.0,896.0,36.0,839.0,848.0,30.0,836.0,839.0,0.02433281,0.656985871,0.659340659,215,618,1263258,cinder,65c635d6a5be0b4afaa9289c901f90275a0be102,1,0,Specific of Nexenta iSCSI driver,"Bug #1263258 in Cinder: ""nexenta-iscsi-volume-migrate-update-provider-location""","After Nexenta iSCSI volume driver migrate volume provider_location of volume doesn't updated.
And also volume migrate method doesn't delete temporal snapshot on destination host.","Nexenta iSCSI driver: fixed volume_migration

Fixed volume_migration updating volume provider_location.
Fixed temporal snapshot deleting after volume migrated.

Change-Id: Id23749cb3724e1510865425232b38b9fc32690f8
Closes-Bug: #1263258"
777,65dde5624bbe1a1f896fe437fef0ad9bbe168153,1409320535,,1.0,17,18,6,5,1,0.811432621,False,,,,,True,,,,,,,,,,,,,,,,,314,723,1271449,neutron,65dde5624bbe1a1f896fe437fef0ad9bbe168153,1,1,,"Bug #1271449 in neutron: ""neutron-dhcp-agent can not be started when end user only use linuxbridge as l2 agent""","When we only install neutron-linuxbridge-agent package on RHEL6.5, the dhcp agent can not be started successfully. That is because there is hard code to use openvswitch.common plugin in nuetron.agent.linux.ovs_lib.py.
[root@osee15-control01 neutron]# /usr/bin/neutron-dhcp-agent --log-file /var/log/neutron/dhcp-agent.log --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini
Traceback (most recent call last):
  File ""/usr/bin/neutron-dhcp-agent"", line 6, in <module>
    from neutron.agent.dhcp_agent import main
  File ""/usr/lib/python2.6/site-packages/neutron/agent/dhcp_agent.py"", line 27, in <module>
    from neutron.agent.linux import interface
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/interface.py"", line 26, in <module>
    from neutron.agent.linux import ovs_lib
  File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ovs_lib.py"", line 31, in <module>
    from neutron.plugins.openvswitch.common import constants
ImportError: No module named openvswitch.common","remove explicit include of the ovs plugin

On installing only neutron-linuxbridge-agent package, the
dhcp cannot start successfully because of the imports from
ovs plugin. This change removes the explicit include of the
ovs plugin from ovs_lib.py. INVALID_OFPORT has been moved to
ovs_lib.py while VXLAN_UDP_PORT has moved to
plugins/common/constants.py. The imports for these 2 constants
in files which uses it has been corrected to new location.

Closes-Bug: #1271449
Change-Id: I6559cb43d1b10b4f926c453a103b12017b59f259"
778,65de01dcb1c49271ad1e5067d8dfd206788086d4,1407928245,,1.0,7,6,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1186,1643,1356227,neutron,65de01dcb1c49271ad1e5067d8dfd206788086d4,1,1,,"Bug #1356227 in neutron: ""TestLoadBalancerBasic fails and leads to consequent tests failures""","Logstash query:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOnRlbXBlc3Quc2NlbmFyaW8udGVzdF9sb2FkX2JhbGFuY2VyX2Jhc2ljLlRlc3RMb2FkQmFsYW5jZXJCYXNpYy50ZXN0X2xvYWRfYmFsYW5jZXJfYmFzaWMgQU5EIG1lc3NhZ2U6RkFJTEVEIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA3OTE1MTA0MTQwfQ==","Reorder operations in create_vip

Previously VIP's port creation was moved outside the transaction
to avoid deadlocks related to rpc calls.
It led to small chance of VIP being fetched by rpc code with
port being still None.

This patch reorders VIP association with the pool so it's done
only after port has been created.

This patch also leaves small possibility of uncaught exception in
case of pool being concurrently deleted in the process of VIP creation

Change-Id: I584558aecc92db4d19fb72b1d006868b840a4d8c
Closes-Bug: #1356227"
779,65df7336994b03b12aecf9bcdafcfb0d839815f9,1382424332,0.0,1.0,2,2,2,2,1,1.0,True,2.0,1919352.0,17.0,9.0,False,27.0,3777.0,29.0,8.0,13.0,2975.0,2978.0,13.0,2579.0,2582.0,11.0,2739.0,2740.0,0.001874707,0.428058116,0.428214342,6,318,1243062,nova,65df7336994b03b12aecf9bcdafcfb0d839815f9,1,1,Wrong description,"Bug #1243062 in OpenStack Compute (nova): ""wrong description when updating quotas""","when updating quota for a resource, with 'force_update=false', an exception will be raised when the new value is less than the used quota values(used and reserved), but the exception information is not correct, which should be fixed.","Fix wrong description when updating quotas

when updating quota for a resource, with 'force_update=false', an
exception will be raised when the new value is less than the used quota
values(used and reserved). This patch changes the description of
exception.

Change-Id: Iede70444341817afb6953eb604fe4de186622916
Closes-bug: #1243062"
780,65e1031c6aeaf4d88029e33694c0c32ef81e75a6,1393350421,1.0,1.0,32,13,7,5,2,0.835336833,True,6.0,568458.0,37.0,11.0,False,66.0,68047.875,153.0,5.0,212.0,1551.0,1570.0,204.0,1323.0,1342.0,202.0,1386.0,1401.0,0.13800136,0.942895989,0.953093134,473,888,1284362,cinder,65e1031c6aeaf4d88029e33694c0c32ef81e75a6,0,0,No bug. ‘Add versioning output’,"Bug #1284362 in Cinder: ""zone manager doesn't report versioning information""","When the zone manager starts, it doesn't report it's version number or the driver it's using.","Add versioning output for the FC Zone Manager

This patch adds log output of the version
information for the Fibre Channel Zone Manager
and it's drivers during volume manager start up.

Change-Id: I7de5159782315f528a25ffdf69a59caebcc46ee7
Closes-Bug: #1284362"
781,65fa80c361f71158cc492dfc520dc4a63ccfa419,1395283177,,1.0,1,1,1,1,1,0.0,True,3.0,3105405.0,32.0,10.0,False,24.0,1078204.0,32.0,3.0,7.0,787.0,794.0,7.0,717.0,724.0,0.0,730.0,730.0,0.000650195,0.475292588,0.475292588,315,724,1271568,cinder,65fa80c361f71158cc492dfc520dc4a63ccfa419,1,1,Bug: can't handle rpc message. Need move a method,"Bug #1271568 in Cinder: ""can't handle rpc message when cinder-volume start ""","cinder-scheduler send a rpc message to cinder-volume which was killed just now.
Then when cinder-volume restart, it may receive the rpc message and begin to handle it before driver initialized.
Most functions in manager need to judge wether the driver has been initialized, so the rpc request would be reject. But those requests shoud be handle in fact.
The reason is, in service.py, we create consumer before manager call ""init_host()"", and we call ""self.driver.set_initialized()"" in init_host.","init_host should be called before RPC consumer is created

Currently, the init_host method is called after RPC consumer is
created. This behavior will lead to a bug that when a rpc request
is received the manager can not handle it because the driver has not
been initialized!

Change-Id: Ieedba1adeb3dd98eb4ff78427fe347423c016469
Closes-Bug: #1271568"
782,661e3ec795be075eb8f66ff15de1c8ad3603a682,1409041460,,1.0,2,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1163,1620,1354218,neutron,661e3ec795be075eb8f66ff15de1c8ad3603a682,1,1,,"Bug #1354218 in neutron: ""heal script is not idempotent""","tested with mysql
1) upgrade head
2) downgrade havana
3) upgrade head
4) BOOM -> http://paste.openstack.org/show/91769/
5) This is easy [1]
6) Try 1-3 again
7) BOOM -> http://paste.openstack.org/show/91770/
8) This is easy as well [2]
9) Repeat again steps 1-3
10) BOOM -> http://paste.openstack.org/show/91771/
I'm clueless so far about the last failure.
[1]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     #              autoincrement=None, existing_type=None,
     #              existing_server_default=False, existing_nullable=None,
     #              existing_autoincrement=None, schema=None, **kw)
+    bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
-            bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
[2]
--- a/neutron/db/migration/alembic_migrations/heal_script.py
+++ b/neutron/db/migration/alembic_migrations/heal_script.py
@@ -103,12 +103,12 @@ def parse_modify_command(command):
     #              autoincrement=None, existing_type=None,
     #              existing_server_default=False, existing_nullable=None,
     #              existing_autoincrement=None, schema=None, **kw)
+    bind = op.get_bind()
     for modified, schema, table, column, existing, old, new in command:
         if modified.endswith('type'):
             modified = 'type_'
         elif modified.endswith('nullable'):
             modified = 'nullable'
-            bind = op.get_bind()
             insp = sqlalchemy.engine.reflection.Inspector.from_engine(bind)
             if column in insp.get_primary_keys(table) and new:
                 return
@@ -123,7 +123,7 @@ def parse_modify_command(command):
                 existing['existing_server_default'] = default.arg
             else:
                 existing['existing_server_default'] = default.arg.compile(
-                    dialect=bind.engine.name)
+                    dialect=bind.dialect)
         kwargs.update(existing)
         op.alter_column(table, column, **kwargs)","Fix heal_script for MySQL specifics

After running upgrade head->downgrade havana->upgrade head
on MySQL heal_script fails with several mistakes.

This change fix them. More details in related bug description.

Closes-bug: #1354218

Change-Id: I91cb21c4bc744f240bfb8bc6b2ed24b6e01f6095"
783,661fa0c83309f002bbbdd67d81f3e6fbb6bde829,1387708013,,1.0,5,2,1,1,1,0.0,True,5.0,1392505.0,32.0,19.0,False,135.0,219928.0,401.0,5.0,35.0,910.0,917.0,35.0,847.0,854.0,35.0,888.0,895.0,0.005204568,0.128523927,0.129535926,22,334,1243849,nova,661fa0c83309f002bbbdd67d81f3e6fbb6bde829,1,0,New decision,"Bug #1243849 in OpenStack Compute (nova): ""flaky ""Getting disk size"" ERROR in n-cpu log after successful tempest run""","This happened in 14 out of 156 runs. Here is a sample:
https://review.openstack.org/#/c/51751/9
http://logs.openstack.org/51/51751/9/check/check-tempest-devstack-vm-postgres-full/ab64cc3
2013-10-22 20:48:20.036 | 2013-10-22 20:41:24.129 21399 ERROR nova.virt.libvirt.driver [-] Getting disk size of instance-00000069: [Errno 2] No such file or directory: '/opt/stack/data/nova/instances/cd1428aa-fa8a-43d9-8180-888e832c35c2/disk'","Change log from ERROR to WARNING when instance absent

Periodic task will update the host stat so that it will
collect all active instances and get their disk size
during the operation period, the instance might be resized
by nova operations, it will lead to disk file not exist
it's not an error from current design point of view, so
change from ERROR to WARNING to avoid confusion

Change-Id: I3b6f1416c248d9d4d2caaf2cc30a7c944685661c
Closes-Bug: #1243849"
784,665352f169a7d1e7eca590d466241cc313adb7c3,1394047959,,1.0,2,2,1,1,1,0.0,True,12.0,552050.0,197.0,14.0,False,24.0,479169.0,41.0,2.0,697.0,1131.0,1568.0,597.0,946.0,1325.0,225.0,607.0,700.0,0.250832408,0.674805771,0.778024417,577,998,1290478,neutron,665352f169a7d1e7eca590d466241cc313adb7c3,0,0, could cause a timeout ,"Bug #1290478 in neutron: ""Query for port before calling l3plugin.disassociate_floatingips()""","The call to l3plugin.disassociate_floatingips() trigggers several events
that could cause a timeout to occur trying to query the db for the port
therefore this patch changes the code to query first for the port.","Query for port before calling l3plugin.disassociate_floatingips()

The call to l3plugin.disassociate_floatingips() trigggers several events
that could cause a timeout to occur trying to query the db for the port
therefore this patch changes the code to query first for the port.

Closes-bug: #1290478

Change-Id: I69fe9cff4b550240215329e1082a63cd11617faa"
785,6653abe5b11b2fccdc0c02083297335d42d57fe1,1377752392,,1.0,2,1,1,1,1,0.0,True,1.0,1263562.0,11.0,5.0,False,112.0,102422.0,317.0,4.0,6.0,3715.0,3716.0,6.0,2954.0,2955.0,0.0,3197.0,3197.0,0.000170474,0.545175588,0.545175588,1492,1218057,1218057,nova,6653abe5b11b2fccdc0c02083297335d42d57fe1,0,0,Feature “to allow the newly created volume file to be”,"Bug #1218057 in OpenStack Compute (nova): ""Nova libvirt driver not passing REUSE flag when _swap_volume calls  blockRebase""","For filesystem backed Cinder drivers like GPFS, a new volume  file is created before the swap operation is invoked as part of the online volume migration sequence.
For this reason, the call to blockRebase from . _swap_volume (in nova.virt.libvirt.driver) should include the flag  VIR_DOMAIN_BLOCK_REBASE_REUSE_EXT to allow the newly created volume file to be written to.","Add REUSE_EXT in _swap_volume call to blockRebase

_swap_volume is called in cases where new volume file has
already been created.  Add VIR_DOMAIN_BLOCK_REBASE_RESUE_EXT
in the flags passed when calling blockRebase to allow
the existing volume file to be written to.

Closes-Bug: #1218057

Change-Id: I704772ba4bfef474fc788d277c99ba016fb7883f"
786,66721eb2c0f53fc4260b2f0aa9a3811da0f7ddbd,1405030233,,1.0,23,11,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,993,1433,1332382,nova,66721eb2c0f53fc4260b2f0aa9a3811da0f7ddbd,1,1,,"Bug #1332382 in OpenStack Compute (nova): ""block device mapping timeout in compute""","When booting instances passing in block-device and increasing the volume size , instances can go in to error state if the volume takes longer to create than the hard code value set in:
nova/compute/manager.py
  def _await_block_device_map_created(self, context, vol_id, max_tries=180,
                                        wait_between=1):
Here is the command used to repro:
nova boot --flavor ca8d889e-6a4e-48f8-81ce-0fa2d153db16 --image 438b3f1f-1b23-4b8d-84e1-786ffc73a298
--block-device source=image,id=438b3f1f-1b23-4b8d-84e1-786ffc73a298,dest=volume,size=128
--nic net-id=5f847661-edef-4dff-9f4b-904d1b3ac422 --security-groups d9ce9fe3-983f-42a8-899e-609c01977e32
Test_Image_Instance
max_retries should be made configurable.
Looking through the different releases, Grizzly was 30, Havana was 60 , IceHouse is 180.
Here is a traceback:
2014-06-19 06:54:24.303 17578 ERROR nova.compute.manager [req-050fc984-cfa2-4c34-9cde-c8aeea65e6ed
d0b8f2c3cf70445baae994004e602e11 1e83429a8157489fb7ce087bd037f5d9] [instance:
74f612ea-9722-4796-956f-32defd417000] Instance failed block device setup
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
Traceback (most recent call last):
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1394,
in _prep_block_device
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
self._await_block_device_map_created))
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 283,
in attach_block_devices
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
block_device_mapping)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/virt/block_device.py"", line 238,
in attach
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
wait_func(context, vol['id'])
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 909,
in _await_block_device_map_created
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
attempts=attempts)
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]
VolumeNotCreated: Volume 8489549e-d23e-45c2-ae6e-7fdb1a9c30d0 did not finish
being created even after we waited 65 seconds or 60 attempts.
2014-06-19 06:54:24.303 17578 TRACE nova.compute.manager [instance: 74f612ea-9722-4796-956f-32defd417000]","Make the block device mapping retries configurable

When booting instances passing in block-device and increasing the
volume size, instances can go in to error state if the volume takes
longer to create than the hard code value (max_tries(180)/wait_between(1))
set in nova/compute/manager.py

def _await_block_device_map_created(self,
                                    context,
                                    vol_id,
                                    max_tries=180,
                                    wait_between=1):

To fix this, max_retries/wait_between should be made configurable.
Looking through the different releases, Grizzly was 30, Havana was
60 , IceHouse is 180.

This change adds two configuration options:
a)  `block_device_allocate_retries` which can be set in nova.conf
by the user to configure the number of block device mapping retries.
It defaults to 60 and replaces the max_tries argument in the above method.
b) `block_device_allocate_retries_interval` which allows the user
to specify the time interval between consecutive retries. It defaults to 3
and replaces wait_between argument in the above method.

DocImpact
Closes-Bug: #1332382
Change-Id: I16e4cd1a572bc5c2cd91fc94be85e72f576a8c26"
787,66abc6e5392047d4919a95cbafc9282f6a14c684,1404198459,1.0,1.0,4,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1026,1470,1336175,neutron,66abc6e5392047d4919a95cbafc9282f6a14c684,1,0,“commit 9d13ea88 had an argument mismatch which causes the following:”,"Bug #1336175 in neutron: ""ofagent argument mismatch""","commit 9d13ea88 had an argument mismatch which causes the following:
TypeError: _get_ports() takes exactly 2 arguments (1 given)","ofagent: Fix an argument mismatch bug in commit 9d13ea88

recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665
had an argument mismatch bug which causes the following crash.

    TypeError: _get_ports() takes exactly 2 arguments (1 given)

this commit fixes it.

Closes-Bug: #1336175
Change-Id: I46bb1fc1aebf06274bb226014e4a9bc5a83f98ac"
788,66f22e754785638e059c0eba4f48bb5953386838,1395426185,,1.0,16,13,4,3,1,0.684275853,True,1.0,107753.0,26.0,4.0,False,13.0,2185763.5,21.0,4.0,752.0,1885.0,2298.0,641.0,1423.0,1781.0,263.0,816.0,899.0,0.258570029,0.800195886,0.881488737,667,1093,1295790,neutron,66f22e754785638e059c0eba4f48bb5953386838,0,0,"rename a variable, evolution","Bug #1295790 in neutron: ""rename ACTIVE_PENDING to ACTIVE_PENDING_STATUSES""",Looking at the lbaas code it's not very obvious that constants.ACTIVE_PENDING is actually a list of statuses that contain states that are ACTIVE or PENDING. This patch renames ACTIVE_PENDING to ACTIVE_PENDING_STATUSES so it's obvious that this is a list and not just a string called ACTIVE_PENDING.,"rename ACTIVE_PENDING to ACTIVE_PENDING_STATUSES

Looking at the lbaas code it's not very obvious that constants.ACTIVE_PENDING
is a list of statuses that contain statuses that are ACTIVE or
PENDING. This patch renames ACTIVE_PENDING to ACTIVE_PENDING_STATUSES so
it's obvious that this is a list and not just a string called ACTIVE_PENDING.

Closes-bug: #1295790

Change-Id: I7af96bcc6b145c6ab809c0b032ccb18baad9c98e"
789,66f52c0b0d03362b72f65631af3e562cc999f84a,1383360787,,1.0,0,45,1,1,1,0.0,True,3.0,1723307.0,21.0,13.0,False,19.0,1252331.0,46.0,8.0,62.0,815.0,865.0,62.0,746.0,796.0,3.0,315.0,315.0,0.003984064,0.314741036,0.314741036,58,386,1248214,glance,66f52c0b0d03362b72f65631af3e562cc999f84a,0,0,Remove unused method,"Bug #1248214 in Glance: ""Need remove unused method setup_logging  in  glance/common/config.py	""","Method setup_logging in glance/common/config.py wasn't used anywhere.
Similar function is in oslo, and some modules in Glance are using function log.setup from oslo,
like glance/cmd/manage.py#L123.
So we need remove this method .","Remove unused method setup_logging

Method setup_logging in glance/common/config.py wasn't used anywhere.
Similar function is in oslo, and some modules in Glance are using
function log.setup from oslo, like glance/cmd/manage.py#L123

Closes-Bug: #1248214

Change-Id: Icda26943f02df5e495179868aa8b1f4c4e151803"
790,67179bf58f35d54bee12e6e8eaf084e2f70ea6a2,1390838091,,1.0,17,5,1,1,1,0.0,True,7.0,7843208.0,95.0,33.0,False,140.0,47803.0,567.0,10.0,0.0,3945.0,3945.0,0.0,3306.0,3306.0,0.0,2993.0,2993.0,0.000139489,0.417631469,0.417631469,1420,1154303,1154303,nova,67179bf58f35d54bee12e6e8eaf084e2f70ea6a2,0,0,“Improved logs for add/remove security group rules.”,"Bug #1154303 in OpenStack Compute (nova): ""Security group rule AUDIT message could be more useful""","Hi! This is mostly just a wishlist request related to operational usability.
Recently I was investigating a report of changes to security group rules for a tenant and found that the AUDIT log messages captured during security group rule changes was, well, less than useful :)
Example:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Authorize security group ingress default
What would be more useful to know in this particular AUDIT log message would be something like this:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default added TCP ingress (22:22)
or:
2013-03-12 17:25:31 AUDIT nova.compute.api [req-ea8ad999-2154-4631-8d80-e33eeeb5f9b6 a8f944429f2b43758079dfda3a123222 8a25888b704146ab95c1e3e8928253f6] Security group default removed ICMP ingress (-1:-1)
Best,
-jay","Improved logs for add/remove security group rules.

Added more details - protocol and port information to AUDIT log
messages on add/remove rule actions for security groups.

Change-Id: Ib446a63976dade90c51c13f30367a3ee17a739ea
Closes-Bug: #1154303"
791,671aa9f8b7ca5274696f83bde0d4822ee431b837,1408383935,,1.0,29,12,8,7,1,0.901015121,False,,,,,True,,,,,,,,,,,,,,,,,1188,1646,1356552,nova,671aa9f8b7ca5274696f83bde0d4822ee431b837,1,1,,"Bug #1356552 in OpenStack Compute (nova): ""Live migration: ""Disk of instance is too large"" when using a volume stored on NFS""","When live-migrating an instance that has a Cinder volume (stored on NFS) attached, the operation fails if the volume size is bigger than the space left on the destination node. This should not happen, since this volume does not have to be migrated. Here is how to reproduce the bug on a cluster with one control node and two compute nodes, using the NFS backend of Cinder.
$ nova boot --flavor m1.tiny --image 173241e-babb-45c7-a35f-b9b62e8ced78 test_vm
...
$ nova volume-create --display-name test_volume 100
...
| id                  | 6b9e1d03-3f53-4454-add9-a8c32d82c7e6 |
...
$ nova volume-attach test_vm  6b9e1d03-3f53-4454-add9-a8c32d82c7e6 auto
...
$ nova show test_vm | grep OS-EXT-SRV-ATTR:host
| OS-EXT-SRV-ATTR:host                 | t1-cpunode0                                                |
$ nova service-list | grep nova-compute
| nova-compute     | t1-cpunode0 | nova     | enabled | up    | 2014-08-13T19:14:40.000000 | -               |
| nova-compute     | t1-cpunode1 | nova     | enabled | up    | 2014-08-13T19:14:41.000000 | -               |
Now, let's say I want to live-migrate test_vm to t1-cpunode1:
$ nova live-migration --block-migrate test_vm t1-cpunode1
ERROR: Migration pre-check error: Unable to migrate a0d9c991-7931-4710-8684-282b1df4cca6: Disk of instance is too large(available on destination host:46170898432 < need:108447924224) (HTTP 400) (Request-ID: req-b4f00867-df51-44be-8f97-577be385d536)
In nova/virt/libvirt/driver.py, _assert_dest_node_has_enough_disk() calls get_instance_disk_info(), which in turn, calls _get_instance_disk_info(). In this method, we see that volume devices are not taken into account when computing the amount of space needed to migrate an instance:
...
            if disk_type != 'file':
                LOG.debug('skipping %s since it looks like volume', path)
                continue
            if target in volume_devices:
                LOG.debug('skipping disk %(path)s (%(target)s) as it is a '
                          'volume', {'path': path, 'target': target})
                continue
...
But for some reason, we never get into these conditions.
If we ssh the compute where the instance currently lies, we can get more information about it:
$ virsh dumpxml 11
...
    <disk type='file' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source file='/var/lib/nova/mnt/84751739e625d0ea9609a65dd9c0a6f1/volume-6b9e1d03-3f53-4454-add9-a8c32d82c7e6'/>
      <target dev='vdb' bus='virtio'/>
      <serial>6b9e1d03-3f53-4454-add9-a8c32d82c7e6</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07' function='0x0'/>
    </disk>
...
The disk type is ""file"", which might explain why this volume is not skipped in the code snippet shown above. When we use the default Cinder backend, we get something such as:
    <disk type='block' device='disk'>
      <driver name='qemu' type='raw' cache='none'/>
      <source dev='/dev/disk/by-path/ip-192.168.200.250:3260-iscsi-iqn.2010-10.org.openstack:volume-47ecc6a6-8af9-4011-a53f-14a71d14f50b-lun-1'/>
      <target dev='vdb' bus='virtio'/>
      <serial>47ecc6a6-8af9-4011-a53f-14a71d14f50b</serial>
      <alias name='virtio-disk1'/>
      <address type='pci' domain='0x0000' bus='0x00' slot='0x07'
function='0x0'/>
    </disk>
I think that the code in LibvirtNFSVolumeDriver.connect_volume() might be wrong: conf.source_type should be set to something else than ""file"" (and some other changes might be needed), but I must admit I'm not a libvirt expert.
Any thoughts ?","libvirt: Make sure volumes are well detected during block migration

Current implementation of live migration in libvirt incorrectly includes
block devices on shared storage (e.g., NFS) when computing destination
storage requirements. Since these volumes are already on shared storage
they do not need to be migrated. As a result, migration fails if the
amount of free space on the shared drive is less than the size of the
volume to be migrated. The problem is addressed by adding a
block_device_info parameter to check_can_live_migrate_source() to allow
volumes to be filtered correctly when computing migration space
requirements.

This only fixes the issue on libvirt: it is unclear whether other
implementations suffer from the same issue.

Thanks to Florent Flament for spotting and fixing an issue while trying out
this patch.

Co-Authored-By: Florent Flament <florent.flament@cloudwatt.com>
Change-Id: Iac7d2cd2a70800fd89864463ca45c030c47411b0
Closes-Bug: #1356552"
792,671cd61704551a5d1fd753bb890650edd6dcd152,1389534214,,1.0,3,2,1,1,1,0.0,True,1.0,27650.0,5.0,2.0,False,60.0,123.0,136.0,2.0,533.0,1226.0,1416.0,524.0,1053.0,1243.0,517.0,1080.0,1259.0,0.391534392,0.817082389,0.952380952,283,688,1268294,cinder,671cd61704551a5d1fd753bb890650edd6dcd152,0,0,"Imrpove?: If the driver raises an exception, make sure it is printed","Bug #1268294 in Cinder: ""driver exception swallowed during retype""","If a driver raises an exception during retype, only a generic message is printed by the manager.  It would help debugging if the original exception was printed as well.","Print driver exception on retype

If the driver raises an exception, make sure it is printed. Also change
the logging level from info to error because an exception should not be
thrown under normal conditions.

Change-Id: I003959c3f7c7667124ef61911587012fd4e1d667
Closes-Bug: #1268294"
793,674954f731bf4b66356fadaa5baaeb58279c5832,1407464297,1.0,1.0,20,4,4,4,1,0.951553584,False,,,,,True,,,,,,,,,,,,,,,,,1142,1597,1352102,nova,674954f731bf4b66356fadaa5baaeb58279c5832,1,1,"“Commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 restricted
    attaching external networks to admin clients. This patch changes
    it to a policy based check instead with the default setting being
    admin only. “","Bug #1352102 in OpenStack Compute (nova): ""users are unable to create ports on provider networks""","after commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 my users are unable to boot vm attached to provider networks, this is a serious regression for me as we mostly use provider networks.
bug which originated the commit https://bugs.launchpad.net/ubuntu/+source/nova/+bug/1284718","Allow attaching external networks based on configurable policy

Commit da66d50010d5b1ba1d7fc9c3d59d81b6c01bb0b0 restricted
attaching external networks to admin clients. This patch changes
it to a policy based check instead with the default setting being
admin only. This allows operators to more precisely configure who
they wish to allow to attach external networks without having to
give them admin access

Change-Id: I59e71f117f889f2abffddc36c1870ef1e0fe3711
DocImpact: Adds network:attach_external_network policy
Closes-Bug: #1352102"
794,675a39a74faad5f7fe4eb94e1dcf7f4359ed2285,1413212213,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1398,1868,1380552,glance,675a39a74faad5f7fe4eb94e1dcf7f4359ed2285,1,0,API change. “but the glance project code was not updated properly for the required changes.”,"Bug #1380552 in Glance: ""Fix for adopt glance.store library in Glance""","The store module is removed from glance project and new glance_store module is created, but the glance project code was not updated properly for the required changes.
There are few cases in v1 api which needs to be addressed:
1. _get_from_store method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError.
2. _get_size method raises store.NotFound exception from glance_store but it is still trying to catch exception.NotFound exception in glance and the actual exception has not been caught which causes 500 HTTPInternalServerError.","Fix for adopt glance.store library in Glance

The store module is removed from glance project and new glance_store
module is created, but the glance project code was not updated
properly for the required changes.

_get_from_store and _get_size methods raises store.NotFound exception
from glance_store but not caught in the glance api.

Updated exception.NotFound to store.NotFound which returns 404 NotFound
response to the user.

Closes-Bug: #1380552
Change-Id: I068cb90f5db62443115157a027fbdefec9bd4a00"
795,6762acab9ca70bad843cdee542d816ad8e3cb908,1405953341,,1.0,2,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1069,1518,1342756,neutron,6762acab9ca70bad843cdee542d816ad8e3cb908,1,1,,"Bug #1342756 in neutron: ""cisco_ml2_apic_contracts table needs to set nullable=False on tenant_id column""","In DB migrations the following is detected:
INFO  [alembic.autogenerate.compare] Detected NULL on column 'cisco_ml2_apic_contracts.tenant_id'","Set nullable=False on tenant_id in apic_contracts table

The cisco_ml2_apic_contracts table has a tenant_id field that is not nullable
since it is the primary key. The HasTenant mixin does not allow setting
nullable=False, so use sa.Column() explicitly for this.

Change-Id: If8e96e76fad0bfcf2bfd71b0223624f1da4517c9
Closes-bug: #1342756"
796,6791fa41e06beab23bc7832a3bfa9ab28adf1e34,1401451022,,1.0,2,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,888,1324,1319643,cinder,6791fa41e06beab23bc7832a3bfa9ab28adf1e34,1,1,,"Bug #1319643 in Cinder: ""Using random.random() should not be used to generate randomness used for security reasons""","In cinder code : /cinder/transfer/api.py . Below line of code used random.random() to generate a random number, Standard random number generators should not be used to generate randomness used for security reasons. Could we use a crytographic randomness generator to provide sufficient entropy to instead of it?
rndstr = """"
random.seed(datetime.datetime.now().microsecond)
while len(rndstr) < length:
 rndstr += hashlib.sha224(str(random.random())).hexdigest()   ---------------> This line has described issues.
 return rndstr[0:length]","Use os.urandom in volume transfer

This patch replaces a call to random.random() with a call to
os.urandom(), which generates a higher quality random number.

Closes-Bug: #1319643

Change-Id: Ifaa2216d4905f5286884629beac52b25249d621f"
797,68369fe6789d4ab053eb382458562394ce632de8,1385427747,,1.0,13,2,3,3,1,0.883037441,True,2.0,217322.0,21.0,6.0,False,16.0,691401.0,71.0,3.0,10.0,1233.0,1233.0,10.0,1084.0,1084.0,9.0,498.0,498.0,0.01754386,0.875438596,0.875438596,1785,1252921,1252921,neutron,68369fe6789d4ab053eb382458562394ce632de8,1,1, ,"Bug #1252921 in neutron: ""showing nonexistent NetworkGateway throws 500 instead of 404""","I'm implementing nvp network gateway to heat.
  https://blueprints.launchpad.net/heat/+spec/resource-type-nvp-network-gateway
Heat would check resource existence by resource-show after resource deleted.
However, show_network_gateway returns 500 instead of 404 when nvp network gateway isn't exist.
The result of this situation is that Heat is unable to delete nvp network gateway.
I think, neutron should return 404 when resource isn't exist.
Curl:
$ curl -i http://172.23.56.142:9696/v2.0/network-gateways/b5afd4a9-eb71-4af7-a082-8fc625a35b61 -X GET -H ""X-Auth-Token: 56c136ee847f476f9f0ba4c2ca78ae4b"" -H ""Content-Type: application/json"" -H ""Accept: application/json"" -H ""User-Agent: python-neutronclient""
HTTP/1.1 500 Internal Server Error
Content-Type: application/json; charset=UTF-8
Content-Length: 88
Date: Mon, 18 Nov 2013 10:30:44 GMT
{""NeutronError"": ""Request Failed: internal server error while processing your request.""}
Log:
2013-11-18 18:18:03.315 25570 DEBUG keystoneclient.middleware.auth_token [-] Received request from user: 54012987ac014457b9a0a8bcc10928ae with project_id : ed684e101d3243a69db07e744acad6f2 and roles: admin  _build_user_headers /opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py:922
2013-11-18 18:18:03.316 25570 DEBUG routes.middleware [-] Matched GET /network-gateways/3fe90063-9e96-45be-8989-335f582962be.json __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Route path: '/network-gateways/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-11-18 18:18:03.317 25570 DEBUG routes.middleware [-] Match dict: {'action': u'show', 'controller': <wsgify at 65809104 wrapping <function resource at 0x3ebbf50>>, 'id': u'3fe90063-9e96-45be-8989-335f582962be', 'format': u'json'} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-11-18 18:18:03.324 25570 ERROR neutron.api.v2.resource [-] show failed
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 290, in show
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     parent_id=parent_id),
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 258, in _item
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     obj = obj_getter(request.context, id, **kwargs)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1951, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     id, fields)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 248, in get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     gw_db = self._get_network_gateway(context, id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_networkgw_db.py"", line 133, in _get_network_gateway
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     return self._get_by_id(context, NetworkGateway, gw_id)
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 145, in _get_by_id
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     return query.filter(model.id == id).one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/query.py"", line 2190, in one
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource     raise orm_exc.NoResultFound(""No row was found for one()"")
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource NoResultFound: No row was found for one()
2013-11-18 18:18:03.324 25570 TRACE neutron.api.v2.resource
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_msg_id': u'3ce98f89ac844d2a8d38e0ae231ff447', u'_context_read_deleted': u'no', u'_reply_q': u'reply_70a7a3f33d8f4417b5225404d435d264', u'_context_tenant_id': None, u'args': {u'devices': [u'tap3fc478a3-07', u'tapfa59a846-8b', u'tapba4de5a8-85']}, u'namespace': None, u'_unique_id': u'44f59887e1084f9898cdcc57d3bcff78', u'_context_is_admin': True, u'version': u'1.1', u'_context_project_id': None, u'_context_timestamp': u'2013-11-14 10:51:59.225787', u'_context_user_id': None, u'method': u'security_group_rules_for_devices'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-11-18 18:18:05.061 25570 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'is_admin': True, 'timestamp': u'2013-11-14 10:51:59.225787', 'project_id': None, 'read_deleted': u'no'} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276","Fix showing nonexistent NetworkGateway throws 500 instead of 404

Change-Id: I5304bd52f7a5ae22fbc0d48206d7c1d282b34a91
Closes-Bug: #1252921"
798,6847a0ef927929e631e9c6db2e2812528ca93151,1383804756,1.0,1.0,105,33,4,2,1,0.85193266,True,8.0,14340119.0,77.0,48.0,False,10.0,3322221.25,24.0,5.0,0.0,1091.0,1091.0,0.0,932.0,932.0,0.0,1016.0,1016.0,0.000153022,0.155623565,0.155623565,67,395,1248463,nova,6847a0ef927929e631e9c6db2e2812528ca93151,1,1,Issue with the API and add more,"Bug #1248463 in OpenStack Compute (nova): ""Cannot resize VM to a different compute node when using hyperv_utils_v2""","The bug is when you resize a VM to another compute node using Hyper-V VMUtilsV2, there will be an exception in the compute node which the VM  located before resizing.
The exception is ""Cannot find boot VHD file for instance: instance-0000000e"".
After debuged, the issue maybe in funtion get_vm_storage_paths of vmutils.py.
If using Hyper-V v1, the get_vm_storage_paths function in vmutils.py can find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
[u'Microsoft Virtual Keyboard',
u'Microsoft Virtual PS2 Mouse',
u'Microsoft S3 Display Controller',
u'Microsoft Synthetic Diskette Drive',
None,
u'Microsoft Serial Controller',
u'Microsoft Serial Port',
u'Microsoft Serial Port',
u'Microsoft Synthetic Disk Drive',
u'Microsoft Virtual Hard Disk',
u'Microsoft Synthetic DVD Drive',
u'Microsoft Virtual CD/DVD Disk',
u'Microsoft Emulated IDE Controller',
u'Microsoft Emulated IDE Controller',
u'Microsoft Synthetic Mouse',
u'Microsoft Synthetic Display Controller',
u'Microsoft Synthetic SCSI Controller']
If using Hyper-V v2, the get_vm_storage_paths function in vmutils.py can not find the Virtual Hard Disk, all the ResourceSubType of Msvm_ResourceAllocationSettingData are
Microsoft:Hyper-V:Virtual Keyboard
Microsoft:Hyper-V:Virtual PS2 Mouse
Microsoft:Hyper-V:S3 Display Controller
Microsoft:Hyper-V:Synthetic Diskette Drive
None
Microsoft:Hyper-V:Serial Controller
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Serial Port
Microsoft:Hyper-V:Synthetic Disk Drive
Microsoft:Hyper-V:Synthetic DVD Drive
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Emulated IDE Controller
Microsoft:Hyper-V:Synthetic Mouse
Microsoft:Hyper-V:Synthetic Display Controller
Microsoft:Hyper-V:Synthetic SCSI Controller
I also find in Hyper-V v2 I can find Microsoft Virtual Hard Disk from class Msvm_StorageAllocationSettingData.
Maybe the Hyper-V v2 api changed, but the codes in nova didn't change.","Fixes get_vm_storage_paths issue for Hyper-V V2 API

Add get_vm_storage_paths function to vmutilsv2.py

get_vm_storage_paths implementation on Hyper-V V2 API changed
compared to previous V1 API. In previous V1 APIs, using
Msvm_ResourceAllocationSettingData class can get storage
paths, but in V2 API, only using Msvm_StorageAllocationSettingData
class can get the storage paths.

Closes-Bug: #1248463

Change-Id: Ica73221524f162a6ffcd9dc64ee33c85fb5ad31d"
799,684c9b0b10535eb91b2f679de82c460a864f2cc0,1381245572,0.0,1.0,196,11,2,2,1,0.813186725,True,11.0,796688.0,81.0,33.0,False,7.0,2256064.0,9.0,6.0,10.0,596.0,597.0,10.0,575.0,576.0,6.0,230.0,230.0,0.01627907,0.537209302,0.537209302,1587,1229217,1229217,neutron,684c9b0b10535eb91b2f679de82c460a864f2cc0,1,0,Feature or bug3 “The Cisco Nexus plugin currently has no support for migration”,"Bug #1229217 in neutron: ""Cisco plugin migration support""","The Cisco Nexus plugin currently has no support for migration events via update_port when an instance is migrated from Nova. As a result the plugin currently does not do anything when an instance is migrated.
Portbinding support has been added to the Cisco plugin via this bug: https://bugs.launchpad.net/neutron/+bug/1218033
The plugin should be able to detect changes in the binding:host_id and unconfigure/reconfigure ports on the Nexus appropriately.","Detect and process live-migration in Cisco plugin

With Cisco/Nexus plugin, migration is not fully supported. Logic to detect
port binding change needs to be added in update_port(), and provisioning of
nexus switch(es) should be done accordingly

added test code for update_port() in the model layer and the db layer

Closes-Bug: #1229217

Change-Id: I2bd76030711c9d15462e91da9e4c0836a424834f"
800,688be19e8a2170276451926401af8257c4faece5,1411641229,,1.0,11,5,2,2,1,0.988699408,False,,,,,True,,,,,,,,,,,,,,,,,1288,1752,1367540,nova,688be19e8a2170276451926401af8257c4faece5,1,1,,"Bug #1367540 in OpenStack Compute (nova): ""VMware: Failed to launch instance from volume""","If you create a volume from an image and launch an instance from it, the instance fails to be created.
To recreate this bug:
1. Create a volume from an image
2. Launch an instance from the volume
The following error is thrown in n-cpu.log:
2014-09-09 22:33:47.037 ERROR nova.compute.manager [req-e17654a6-a58b-4760-a383-643dd054c691 demo demo] [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Instance failed to spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] Traceback (most recent call last):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2171, in _build_resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     yield resources
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2050, in _build_and_run_instance
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     block_device_info=block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 446, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     admin_password, network_info, block_device_info)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 298, in spawn
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     vi = self._get_vm_config_info(instance, image_info, instance_name)
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 276, in _get_vm_config_info
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     image_info.file_size_in_gb > instance.root_gb):
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]   File ""/opt/stack/nova/nova/virt/vmwareapi/vmware_images.py"", line 92, in file_size_in_gb
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea]     return self.file_size / units.Gi
2014-09-09 22:33:47.037 32639 TRACE nova.compute.manager [instance: 5ed921e8-c4d8-45a1-964a-93d09a43f2ea] TypeError: unsupported operand type(s) for /: 'unicode' and 'int'
It appears that a simple conversion of the file_size to an int in vmware_images.py should fix this.","Fix image metadata returned for volumes

When creating a volume from a glance image, cinder stores the original
image metadata in volume_glance_metadata. This is a key/value store,
and all the values are strings.

When Nova boots an instance from a volume, it passes the image
metadata returned by cinder, which is all strings. If a driver
expects these values to be ints, as they are when booting from an
image, it will get a type error.

This change also pulls size from the volume directly rather than
taking the value from the stored image metadata. This is because the
volume will have been created in 1Gb increments, and is unlikely to be
the same size as the original image. It may also have been
subsequently extended.

Change-Id: I7928f6be1ca99f1502941b9df2b443f2ca63a37b
Closes-Bug: #1367540"
801,68e008b21b463a261461a0f7bbfaa92f4e9a7e92,1401134407,,1.0,3,2,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,922,1358,1323403,nova,68e008b21b463a261461a0f7bbfaa92f4e9a7e92,1,1,,"Bug #1323403 in OpenStack Compute (nova): ""ec2-api crashes on describing volume backed snapshot""","If volume backed snapshot is available for an user euca-describe-images crashes due to 'unknown server error'.
Environment: DevStack
Steps to reproduce:
1 Create a volume backed snapshot
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy:::1 inst
2 List images to ensure the created snapshot is available.
$ glance image-list
3 Describe images by euca2ools
$ euca-describe-images
TypeError: Unknown error occurred.
nova-api.log
2014-05-26 23:16:18.070 ERROR nova.api.ec2 [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] Unexpected TypeError raised: int() argument must be a string or a number,
not 'NoneType'
2014-05-26 23:16:18.071 DEBUG nova.api.ec2.faults [req-105138c5-1b82-42ff-a5fd-4588a4262763 demo demo] EC2 error response: TypeError: Unknown error occurred. ec2_error_response /opt/stack/nova/nova/api/ec2/faults.py:29","Skip none value attributes for ec2 image bdm output.

Fix crash in _format_block_device_mapping when there are None values
for formatting attributes in the mapping.

Change-Id: I0ebdb844d75cdf1580cc88e1ff40fc21aff96f21
Closes-Bug: #1323403"
802,68ed1bcec81e173648a4a86e5f4e21abc6dcf539,1389931090,,1.0,1,1,1,1,1,0.0,True,1.0,387798.0,9.0,4.0,False,13.0,3255980.0,15.0,3.0,50.0,575.0,601.0,50.0,550.0,576.0,38.0,537.0,551.0,0.029082774,0.401193139,0.41163311,300,709,1270034,cinder,68ed1bcec81e173648a4a86e5f4e21abc6dcf539,0,0,Unused variable,"Bug #1270034 in Cinder: ""Remove unused variable in restore_backup method""","Variable err in restore_backup method is not being used, so remove this 'err ' variable.","Remove unused variable in restore_backup method

Variable err in restore_backup method is not being used, so remove this
'err ' variable.

Change-Id: I31f6da5fd55e025d6496aca76a1157ff5a167d9e
Closes-Bug: #1270034"
803,695191fa89387d96e60120ff32965493c844e7f5,1406042700,,1.0,19,6,2,2,1,0.998845536,False,,,,,True,,,,,,,,,,,,,,,,,848,1278,1314677,nova,695191fa89387d96e60120ff32965493c844e7f5,1,1,,"Bug #1314677 in OpenStack Compute (nova): ""nova-cells fails when using JSON file to store cell information""","As recommended in http://docs.openstack.org/havana/config-reference/content/section_compute-cells.html#cell-config-optional-json I'm creating the nova-cells config with the cell information stored in a json file. However, when I do this nova-cells fails to start with this error in the logs:
2014-04-29 11:52:05.240 16759 CRITICAL nova [-] __init__() takes exactly 3 arguments (1 given)
2014-04-29 11:52:05.240 16759 TRACE nova Traceback (most recent call last):
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/bin/nova-cells"", line 10, in <module>
2014-04-29 11:52:05.240 16759 TRACE nova     sys.exit(main())
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/cmd/cells.py"", line 40, in main
2014-04-29 11:52:05.240 16759 TRACE nova     manager=CONF.cells.manager)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 257, in create
2014-04-29 11:52:05.240 16759 TRACE nova     db_allowed=db_allowed)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 139, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova     self.manager = manager_class(host=self.host, *args, **kwargs)
2014-04-29 11:52:05.240 16759 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/cells/manager.py"", line 87, in __init__
2014-04-29 11:52:05.240 16759 TRACE nova     self.state_manager = cell_state_manager()
2014-04-29 11:52:05.240 16759 TRACE nova TypeError: __init__() takes exactly 3 arguments (1 given)
I have had a dig into the code and it appears that CellsManager creates an instance of CellStateManager with no arguments. CellStateManager __new__ runs and creates an instance of CellStateManagerFile which runs __new__ and __init__ with cell_state_cls and cells_config_path set. At this point __new__ returns CellStateManagerFile and the new instance's __init__() method is invoked (CellStateManagerFile.__init__) with the original arguments (there weren't any) which then results in the stack trace.
It seems reasonable for CellStateManagerFile to derive the cells_config_path info for itself so I've patched it locally with
=== modified file 'state.py'
--- state.py	2014-04-30 15:10:16 +0000
+++ state.py	2014-04-30 15:10:26 +0000
@@ -155,7 +155,7 @@
             config_path = CONF.find_file(cells_config)
             if not config_path:
                 raise cfg.ConfigFilesNotFoundError(config_files=[cells_config])
-            return CellStateManagerFile(cell_state_cls, config_path)
+            return CellStateManagerFile(cell_state_cls)
         return CellStateManagerDB(cell_state_cls)
@@ -450,7 +450,9 @@
 class CellStateManagerFile(CellStateManager):
-    def __init__(self, cell_state_cls, cells_config_path):
+    def __init__(self, cell_state_cls=None):
+        cells_config = CONF.cells.cells_config
+        cells_config_path = CONF.find_file(cells_config)
         self.cells_config_path = cells_config_path
         super(CellStateManagerFile, self).__init__(cell_state_cls)
Ubuntu: 14.04
nova-cells: 1:2014.1-0ubuntu1
nova.conf:
[DEFAULT]
dhcpbridge_flagfile=/etc/nova/nova.conf
dhcpbridge=/usr/bin/nova-dhcpbridge
logdir=/var/log/nova
state_path=/var/lib/nova
lock_path=/var/lock/nova
force_dhcp_release=True
iscsi_helper=tgtadm
libvirt_use_virtio_for_bridges=True
connection_type=libvirt
root_helper=sudo nova-rootwrap /etc/nova/rootwrap.conf
verbose=True
ec2_private_dns_show_ip=True
api_paste_config=/etc/nova/api-paste.ini
volumes_path=/var/lib/nova/volumes
enabled_apis=ec2,osapi_compute,metadata
auth_strategy=keystone
compute_driver=libvirt.LibvirtDriver
quota_driver=nova.quota.NoopQuotaDriver
[cells]
enable=True
name=cell
cell_type=compute
cells_config=/etc/nova/cells.json
cells.json:
{
    ""parent"": {
        ""name"": ""parent"",
        ""api_url"": ""http://api.example.com:8774"",
        ""transport_url"": ""rabbit://rabbit.example.com"",
        ""weight_offset"": 0.0,
        ""weight_scale"": 1.0,
        ""is_parent"": true
    }
}","Fix CellStateManagerFile init to failure

Currently, specifying a cells_config file in nova.conf causes
CellStateManager to fail and in turn stops the nova-cells service from
starting. The reason is that CellsManager creates an instance of
CellStateManager with no arguments. CellStateManager __new__ runs and
creates an instance of CellStateManagerFile which runs __new__ and
__init__ with cell_state_cls and cells_config_path set. At this point
__new__ returns CellStateManagerFile and the new instance's __init__
method is invoked (CellStateManagerFile.__init__) with the original
arguments (there weren't any) which then results in:
2014-04-29 11:52:05.240 16759 TRACE nova self.state_manager =
cell_state_manager()
2014-04-29 11:52:05.240 16759 TRACE nova TypeError: __init__() takes
exactly 3 arguments (1 given)

It seems reasonable for CellStateManagerFile to derive the
cells_config_path info for itself so I have updated the code with that
change and added unit tests to catch this bug and to check that the
correct managers are still returned

Change-Id: I9021640515142a3ca95c2d9e7b03e19b529bc175
Closes-Bug: #1314677"
804,6956065f0b336c0d4edeba265fae8c9f7cddb736,1378291336,,1.0,3,3,3,2,1,1.0,True,4.0,500355.0,36.0,15.0,False,23.0,981768.6667,43.0,5.0,35.0,1044.0,1075.0,35.0,968.0,999.0,4.0,900.0,900.0,0.005263158,0.948421053,0.948421053,1508,1220557,1220557,cinder,6956065f0b336c0d4edeba265fae8c9f7cddb736,1,1, ,"Bug #1220557 in Cinder: ""no conversion type in LOG.debug string""","method extend_volume in cinder/volume/drivers/rbd.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required.
method extend_volume in cinder/volume/drivers/sheepdog.py
> LOG.debug(_(""Extend volume from %(old_size) to %(new_size)""),
Convert type 's' is required.","fix log string in conversion type

By using 'grep -rn ""%([a-zA-Z_]*) ""', we could find all cases of
'%(variable)'. And we need add an 's' like '%(variable)s'.

Closes-Bug: #1220557
Change-Id: Ib28b7ddc5e5aabe2b82a499d9c242c1982e7f953"
805,6994ade726f4a33753a11b2b2c24454160c96d31,1406843828,,1.0,80,11,2,2,1,0.968321186,False,,,,,True,,,,,,,,,,,,,,,,,1136,1591,1351066,neutron,6994ade726f4a33753a11b2b2c24454160c96d31,1,1,,"Bug #1351066 in neutron: ""Neutron floatingip delete does not delete the fip_agent_gw_port  for DVR""","Neutron floatingip-delete ""id"" does not delete the associated ""fip_agent_gw_port"".","Fix for floatingip-delete not removing fip_gw port

fip_agent_gateway_port is created dynamically on a
compute node where FIP namespace is created. When
floatingip-disassociate is called this 'port' is
deleted.

But when we call floatingip-delete this port is
not deleted.

This patch fixes the issue mentioned above.

Closes-Bug: #1351066

Change-Id: I1aa51680394547390546fe362abfae7a5d25425b"
806,69ce114232f9102220b45c8242eda3e78872a3e6,1386247704,,1.0,12,6,2,2,1,0.764204507,True,3.0,967985.0,12.0,6.0,False,65.0,165552.0,163.0,6.0,187.0,1131.0,1131.0,186.0,1025.0,1025.0,23.0,708.0,708.0,0.019607843,0.579248366,0.579248366,150,552,1258128,cinder,69ce114232f9102220b45c8242eda3e78872a3e6,1,1,Wrong arg position,"Bug #1258128 in Cinder: ""LVM brick instantiation fails on volume_migration""","The lvm driver uses positional arguments to create a new instance for the LVM brick. Some of those arguments are being passed in the wrong position. Kwargs should be used in this case instead.
http://git.openstack.org/cgit/openstack/cinder/tree/cinder/volume/drivers/lvm.py#n711","LVM migrate: Use keywords for the brick instance

In the `migrate_volume` method a new instance is created when the
dest_vg is not equal to the source vg. This new brick instance is
created using positional arguments instead of keywords. However, some of
those arguments are passed in the wrong positions.

This patch uses keywords for the misplaced arguments.

The patch also changes `test_lvm_migrate_volume_proceed` in order to
fully test the happy path and catch things like this.

Closes-bug: #1258128
Change-Id: I75cf91171709554053fd5b52a4aae4e176e8364e"
807,69dfbd9468282f9ce16cc5b0cb1bcb121632a0d8,1399217203,,1.0,158,156,28,19,1,0.863695078,True,2.0,1522856.0,31.0,7.0,False,60.0,983082.1429,224.0,4.0,19.0,764.0,771.0,19.0,619.0,626.0,19.0,418.0,425.0,0.016583748,0.347429519,0.353233831,1027,1471,1336196,neutron,69dfbd9468282f9ce16cc5b0cb1bcb121632a0d8,0,0,Bug in test,"Bug #1336196 in neutron: ""Inconsistent keyword for automatic deletion of resources during unit tests""","Ports, networks and subnets have a do_delete=True parameter. By default, these resources are deleted at the end of the context manager scope. All other resources use a different semantic: no_delete=False.
This causes confusing situations such as:
with self.subnet(network, do_delete=False) as subnet:
    with self.security_group(no_delete=True) as sg:
        pass
I personally fell to the pitfall of using do_delete for the security group and was surprised when it wasn't deleted at the end of the scope.
Finally, the double negative of no_delete=False is confusing and should be avoided.","Change all occurences of no_delete to do_delete

Previously, ports, networks and subnets had a do_delete=True
parameter. By default, these resources were deleted at the
end of the context manager scope. All other resources used
a different semantic: no_delete=False.

This caused confusing situations such as:
with self.subnet(network, do_delete=False) as subnet:
    with self.security_group(no_delete=True) as sg:
        pass

Now all resources use the same do_delete semantic.

Closes-Bug: #1336196
Change-Id: I4627481813f714819efe85831e2a55975ea71ed4"
808,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1410365122,,2.0,29,10,3,2,1,0.756885862,False,,,,,True,,,,,,,,,,,,,,,,,354,765,1275267,nova,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1,1,,"Bug #1275267 in OpenStack Compute (nova): ""GuestFS fails to mount image for data injection""","A GuestFS error is causing injection to fail. This result in a warning for metadata injection but results in a spawn error for key injection.
This is logged with debug level:
Exception AttributeError: ""GuestFS instance has no attribute '_o'"" in <bound method GuestFS.__del__ of <guestfs.GuestFS instance at 0x3ea9f38>> ignored
febootstrap-supermin-helper: ext2: parent directory not found: /lib: File not found by ext2_lookup
And causes this error: http://paste.openstack.org/show/62293/
Full logs available here:  http://logs.openstack.org/58/63558/8/check/check-tempest-dsvm-neutron-pg/108e4ca/logs
Interestingly, it seems guestfs was not actually used when the relevant patches went throught the gate checks:
https://review.openstack.org/#/c/70237/
https://review.openstack.org/#/c/70354/
This was expected for patch #70354 but sounds strange for patch #70237
Finally, The traceback seeems different from that of bug 1221985","disk/vfs: ensure guestfs capabilities

Ensures that guestfs is available and well configured. Also
when checking for capabilities this commit introduces an in-memory
flag to avoid repeating the process if succeeded.
If we are able to load guestfs but something wrong happens a fatal
error is raised.

DocImpact: We should add a not for ubuntu like systems that use
libguestfs to execute the command 'update-guestfs-appliance' for
configuring it.

Closes-Bug: #1275267
Closes-Bug: #1157922
Change-Id: I97b3a23829ea1f3aadfe08ca6448b35010d2f312"
809,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1410365122,,2.0,29,10,3,2,1,0.756885862,False,,,,,True,,,,,,,,,,,,,,,,,1421,1157922,1157922,nova,6a0e6209cab2ced467b63ce9ce69a41daae669fe,1,0,Wrong logic,"Bug #1157922 in OpenStack Compute (nova): ""Instance state is not set to ERROR when guestfs file-injection fails""","Instances have been observed to remain stuck forever in ""BUILD"" state, with no errors surfaced to `nova show` after nova boot fails with the following guestfs error.
2013-03-20 18:49:08,590.590 ERROR nova.compute.manager [req-f85ccdcd-74f1-4f50-98eb-b68fb8dc8e1a dba071d520c9438ab9fb91077b6f3248 1ba6328ea66c4041bfab7cfcbc2305cf] [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Instance failed to spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] Traceback (most recent call last):
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1055, in _spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     block_device_info)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1517, in spawn
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     admin_pass=admin_password)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1913, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     instance=instance)
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     self.gen.next()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1908, in _create_image
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     mandatory=('files',))
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/api.py"", line 304, in inject_data
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     fs.setup()
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/disk/vfs/guestfs.py"", line 114, in setup
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db]     {'imgfile': self.imgfile, 'e': e})
2013-03-20 18:49:08,590.590 14139 TRACE nova.compute.manager [instance: 5f3fe8ba-a148-48e5-8e19-d2f65968b2db] NovaException: Error mounting /var/lib/nova/instances/5f3fe8ba-a148-48e5-8e19-d2f65968b2db/disk with libguestfs (cannot find any suitable libguestfs supermin, fixed or old-style appliance on LIBGUESTFS_PATH (search path: /usr/lib/guestfs))","disk/vfs: ensure guestfs capabilities

Ensures that guestfs is available and well configured. Also
when checking for capabilities this commit introduces an in-memory
flag to avoid repeating the process if succeeded.
If we are able to load guestfs but something wrong happens a fatal
error is raised.

DocImpact: We should add a not for ubuntu like systems that use
libguestfs to execute the command 'update-guestfs-appliance' for
configuring it.

Closes-Bug: #1275267
Closes-Bug: #1157922
Change-Id: I97b3a23829ea1f3aadfe08ca6448b35010d2f312"
810,6a36d367b54538ef6347c948b3e549f21993771a,1394488847,,1.0,4,8,2,1,1,0.918295834,True,6.0,1226814.0,63.0,23.0,False,36.0,1880630.0,56.0,2.0,108.0,1758.0,1846.0,85.0,1579.0,1646.0,102.0,1734.0,1816.0,0.013629747,0.229588461,0.240439328,581,1002,1290540,nova,6a36d367b54538ef6347c948b3e549f21993771a,1,1,"Revert deprecation warning.. Bug, Ithink","Bug #1290540 in OpenStack Compute (nova): ""neutron_admin_tenant_name deprecation warning is wrong""",ARNING nova.network.neutronv2 [req-eb54925d-c466-4069-be4e-691e155ea85d None None] Using neutron_admin_tenant_name for authentication is deprecated and will be removed in the next release.  Use neutron_admin_tenant_id instead.,"Revert deprecation warning on Neutron auth.

This reverts the deprecation warning added in commit
e80cf75fc0f25f6279200f59a70fd7c6e4766b0f.

V3 supports domain + project uniqueness, which is much more usable
than ids - massively more so for deployers. The initial patch
deprecates name, when that isn't necessary or appropriate - using name
based auth is as valid as id based auth - we just need to namespace
it in V3.

For V3 operators should be able to create:

* A service domain
* A service/service project
* A service/neutron user

to replace the prior

* service tenant
* neutron user

structure without switching to ids.

Closes-Bug: #1290540
Change-Id: I51837b0dc1bc352c0bf315e383951486b3cac034"
811,6a374f21495c12568e4754800574e6703a0e626f,1412073562,,1.0,13,11,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1349,1818,1373159,nova,6a374f21495c12568e4754800574e6703a0e626f,1,1,Requirements were not fulfill,"Bug #1373159 in OpenStack Compute (nova): ""NUMA Topology cell memory sent to xml in MiB, but qemu uses KiB""","Currently when specifying NUMA cell memory via flavor extra_specs or image properties, MiB units are used. According to the libvirt xml domain format documentation (http://libvirt.org/formatdomain.html) , cell memory should be specified in KiB.
In this example, we use the following extra_specs:
""hw:numa_policy"": ""strict"", ""hw:numa_mem.1"": ""2048"", ""hw:numa_mem.0"": ""6144"", ""hw:numa_nodes"": ""2"", ""hw:numa_cpus.0"": ""0,1,2"", ""hw:numa_cpus.1"": ""3""
The flavor has 8192 MB of ram and 4 vcpus.
When using qemu 2.1.0, the following will be seen in the n-cpu logs when booting a machine with NUMA specs.
""libvirtError: internal error: process exited while connecting to monitor: qemu-system-x86_64: total memory for NUMA nodes (8388608) should equal RAM size (200000000)""
Please note that the 200000000 is 8388608 KiB in bytes and hex (simply an issue with the qemu error message). The error shows that 8192 KiB is being requested rather than 8192 MiB. Because the RAM size does not equal the total memory size, the machine fails to boot.
When using versions of qemu lower than 2.1.0 the issue is not obvious, as machines with  NUMA specs boot, but only because of a bug (that has since been resolved) in qemu. This is because the check to ensure that RAM size equals the NUMA node total memory does not happen in versions lower than 2.1.0
In short, we should be using KiB units for NUMA cell memory, or at least be converting from MiB to KiB before creating the xml. Otherwise, NUMA placement will not behave as intended.
To be fair, I haven't had the chance to look at the memory placement in a guest booted using qemu 2.0.0 or lower, though I suspect the memory placement would be incorrect.. If anyone has the chance to look, it would be greatly appreciated.
I am currently investigating the appropriate fix for this alongside Tiago Mello. We made a quick fix in /nova/virt/libvirt/config.py on line 495:
                cell.set(""memory"", str(self.memory * 1024))
Mutiplying by 1024 allowed the machine to properly boot, but it is probably a bit too quick and dirty. Just thought it would be worth mentioning.
Sys-info:
x86_64 machine
Virt-info:
qemu version 2.1.0
libvirt version 1.2.2
Kenerl-info:
3.13.0-35-generic #62-Ubuntu SMP Fri Aug 15 01:58:42 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux
OS-info:
Distributor ID:	Ubuntu
Description:	Ubuntu 14.04.1 LTS
Release:	14.04
Codename:	trusty","libvirt: Make sure NUMA cell memory is in Kb in XML

This patch makes libvirt report memory in Mb (as
hardware.VirtNUMATopologyUsage class expects Mb) and that when
generating XML we use Kb as this is what libvirt expects by default.

Change-Id: Ic4518acf6bcee463009437829646de8c83aff6bf
Closes-bug: #1373159"
812,6a41fe9c5c98a14a355fa81b41aae2c4b0ce0a7b,1408682969,,1.0,4,2,2,1,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1065,1514,1341954,cinder,6a41fe9c5c98a14a355fa81b41aae2c4b0ce0a7b,1,1,,"Bug #1341954 in Cinder: ""suds client subject to cache poisoning by local attacker""","The suds project appears to be largely unmaintained upstream. The default cache implementation stores pickled objects to a predictable path in /tmp. This can be used by a local attacker to redirect SOAP requests via symlinks or run a privilege escalation / code execution attack via a pickle exploit.
cinder/requirements.txt:suds>=0.4
gantt/requirements.txt:suds>=0.4
nova/requirements.txt:suds>=0.4
oslo.vmware/requirements.txt:suds>=0.4
The details are available here -
https://bugzilla.redhat.com/show_bug.cgi?id=978696
(CVE-2013-2217)
Although this is an unlikely attack vector steps should be taken to prevent this behaviour. Potential ways to fix this are by explicitly setting the cache location to a directory created via tempfile.mkdtemp(), disabling cache client.set_options(cache=None), or using a custom cache implementation that doesn't load / store pickled objects from an insecure location.","VMware: Disable suds caching

The default cache implementation in suds store pickled objects in a
predictable path in /tmp which could lead to attacks. This patch
turns off suds caching to address this security issue.

Change-Id: I7daaa25a0677004e03896298e9c3026d5c33c6ac
Closes-Bug: #1341954"
813,6a9fe989e8d20ba43ed1a2bf318bc41b745f318e,1406861750,,1.0,4,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1138,1593,1351127,nova,6a9fe989e8d20ba43ed1a2bf318bc41b745f318e,0,0,Bug in test,"Bug #1351127 in OpenStack Compute (nova): ""Exception in string format operation""","""tox -e docs"" shows the following error
2014-07-31 22:20:04.961 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] Exception in string format operation
2014-07-31 22:20:04.961 23265 TRACE nova.exception Traceback (most recent call last):
2014-07-31 22:20:04.961 23265 TRACE nova.exception   File ""/Users/dims/openstack/nova/nova/exception.py"", line 118, in __init__
2014-07-31 22:20:04.961 23265 TRACE nova.exception     message = self.msg_fmt % kwargs
2014-07-31 22:20:04.961 23265 TRACE nova.exception KeyError: u'flavor_id'
2014-07-31 22:20:04.961 23265 TRACE nova.exception
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] reason:
2014-07-31 22:20:04.962 23265 ERROR nova.exception [req-ad8a37c6-85ae-4218-b8e6-d15eda5a1553 None] code: 404","docs - Fix exception in docs generation

We need to pass the flavor_id when creating the side_effect
using FlavorNotFound exception

Change-Id: Id5d0ac387d2dca2dc70dabba173fc53972751236
Closes-Bug: #1351127"
814,6aa0a44a9d2ed0a4c2d6d515458c94698a1ad3d8,1391537259,,1.0,24,2,3,2,1,0.882895126,True,2.0,968624.0,30.0,5.0,False,12.0,779200.0,20.0,4.0,0.0,667.0,667.0,0.0,551.0,551.0,0.0,266.0,266.0,0.001324503,0.353642384,0.353642384,372,784,1276876,neutron,6aa0a44a9d2ed0a4c2d6d515458c94698a1ad3d8,0,0,Add and update information,"Bug #1276876 in neutron: ""Add update subnet and pass required attributes in Cisco N1kv plugin""","1) Send dns nameserver and dhcp info to the VSM
2) Update subnet properties to VSM","Add and update subnet properties in Cisco N1kv plugin

Add dns nameservers and dhcp info in subnet create.
Update subnet properties.

Change-Id: I70e3f0261f6ba5433bbf08d33e2a34cdbb20fed2
Closes-Bug: #1276876"
815,6abda5b738c5d801ede3328cf0ec1dc9dddc022f,1390481617,,1.0,14,1,2,2,1,0.721928095,True,2.0,4607892.0,53.0,23.0,False,169.0,480870.5,857.0,12.0,51.0,4135.0,4138.0,50.0,3492.0,3495.0,46.0,3007.0,3010.0,0.006578027,0.420993702,0.421413576,244,648,1265494,nova,6abda5b738c5d801ede3328cf0ec1dc9dddc022f,1,1,"It is kept as pause, and shouldn’t","Bug #1265494 in OpenStack Compute (nova): ""Openstack Nova: Unpause after host reboot fails""","Description of problem:
Unpauseing an instance fails if host has rebooted.
Version-Release number of selected component (if applicable):
RHEL: release 6.5 (Santiago)
openstack-nova-api-2013.2.1-1.el6ost.noarch
openstack-nova-compute-2013.2.1-1.el6ost.noarch
openstack-nova-scheduler-2013.2.1-1.el6ost.noarch
openstack-nova-common-2013.2.1-1.el6ost.noarch
openstack-nova-console-2013.2.1-1.el6ost.noarch
openstack-nova-conductor-2013.2.1-1.el6ost.noarch
openstack-nova-novncproxy-2013.2.1-1.el6ost.noarch
openstack-nova-cert-2013.2.1-1.el6ost.noarch
How reproducible:
Every time
Steps to Reproduce:
1. Boot an instance
2. Pause that instance
3. Reboot host
4. Unpause instance
Actual results:
can't unpause instance stuck in status paused, power state - shutdown
Expected results:
Instance should unpause, return to running state
Additional info:
virsh list -all --managed-save
ID is missing from paused instance ""-"" (pausecirros), state -> shut off.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id    Name                           State
----------------------------------------------------
 1     instance-00000003              running
 2     instance-00000002              running
 -     instance-00000001              shut off
[root@orange-vdse ~(keystone_admin)]# nova list  (notice nova status paused)
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID                                   | Name          | Status | Task State | Power State | Networks        |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros        | ACTIVE | None       | Running     | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros   | PAUSED | None       | Shutdown    | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None       | Running     | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
Testing without rebooting host, ID/state (""1""/paused) instance (cirros) are ok and it unpauses ok.
[root@orange-vdse ~(keystone_admin)]# virsh list --all --managed-save
 Id    Name                           State
----------------------------------------------------
 1     instance-00000003              paused
 2     instance-00000002              running
 -     instance-00000001              shut off
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ID                                   | Name          | Status | Task State | Power State | Networks        |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+
| ebe310c2-d715-45e5-83b6-32717af1ac90 | cirros        | PAUSED | None       | Paused      | net=192.168.1.4 |
| 3ef89feb-414f-4524-b806-f14044efdb14 | pausecirros   | PAUSED | None       | Shutdown    | net=192.168.1.5 |
| 8bcae041-2f92-4ae2-a2c2-ee59b067ac76 | suspendcirros | ACTIVE | None       | Running     | net=192.168.1.2 |
+--------------------------------------+---------------+--------+------------+-------------+-----------------+","Correct the state for PAUSED instances on reboot

If an instance is in the PAUSED state before the compute node is
restarted, it will be left in an inconsistent state (impossible to
delete and/or restart it) after the node boots again.

This patch adds a check in the periodic task sync_power_state so when
this situation is detected the instance is stopped and left in a state
from where it can be started again or deleted.

Closes-Bug: #1265494
Change-Id: I7266ace596598522c28a3839091ef50faf4463c2"
816,6ac6225e72bde92f66da8e92c563c140471b949b,1413209076,,1.0,33,18,2,2,1,0.139232999,False,,,,,True,,,,,,,,,,,,,,,,,1400,1870,1380675,cinder,6ac6225e72bde92f66da8e92c563c140471b949b,1,0,"Change requirements. “This results in failures of all storage policy
    related APIs invoked using datastore selector”","Bug #1380675 in Cinder: ""VMware: retype fails with AttributeError 'NoneType' object has no attribute 'service_content'""","Retype fails with
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    host)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vmdk.py"", line 1434, in retype
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    new_profile)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 263, in is_datastore_compliant
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_id = self.get_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/datastore.py"", line 59, in get_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_id = self._vops.retrieve_profile_id(profile_name)
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1361, in retrieve_profile_id
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    for profile in self.get_all_profiles():
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/volumeops.py"", line 1339, in get_all_profiles
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00m    profile_manager = pbm.service_content.profileManager
^[[01;31m2014-10-13 17:28:37.031 TRACE cinder.volume.manager ^[[01;35m^[[00mAttributeError: 'NoneType' object has no attribute 'service_content'","VMware: Fix initialization of datastore selector

The WSDL URL of storage policy service is determined and a session is
created using it in do_setup(). This session is later used to initialize
the datastore selector property (ds_sel), which uses the session for all
storage policy related API calls.

After commit a8fa3ceb1e72bac2ab67f569a2ca009f995f59fd (Integrate
OSprofiler and Cinder), the properties defined in vmdk module are called
before do_setup(). As a result, the ds_sel (datastore selector) property
is initialized with a session instance containing a 'None' PBM (storage
policy service) WSDL URL. This results in failures of all storage policy
related APIs invoked using datastore selector. This patch fixes the
problem by re-initializing the property in do_setup().

Change-Id: Ibdf8b23f9e215000cf9053b81d374066fabd6851
Closes-Bug: #1380675"
817,6ae5899a86c0d863f1c64fdd960b109fb2a5c80f,1378819208,,1.0,13,1,3,2,1,0.690814021,True,3.0,1158016.0,21.0,9.0,False,175.0,192935.0,932.0,5.0,45.0,2116.0,2136.0,45.0,1897.0,1917.0,45.0,1925.0,1945.0,0.007680748,0.321589581,0.324929037,1536,1223345,1223345,nova,6ae5899a86c0d863f1c64fdd960b109fb2a5c80f,1,1, ,"Bug #1223345 in OpenStack Compute (nova): ""Soft-deleted instance files deleted by periodic cleanup task""",The db query used to look for instances in need of cleanup lacks a filter for soft-deleted instances.,"Add filter for soft-deleted instances to periodic cleanup task

The periodic cleanup task added recently does a db query to get
instances in need of cleanup. The filters applied to the query
must have a filter to not include soft-deleted instances.

This fix adds the required filter and adds a test to verify the
correct behavior.

Change-Id: I0c5d7e317d1fdd4e1e618c1c38870e775a715eab
Closes-Bug: #1223345"
818,6b0c899d36d5a98963342e9cffb351fc3a1f4fd2,1376928315,0.0,1.0,29,3,2,2,1,0.625262405,True,4.0,218555.0,37.0,16.0,False,15.0,465139.0,20.0,2.0,6.0,669.0,669.0,6.0,638.0,638.0,6.0,172.0,172.0,0.033492823,0.827751196,0.827751196,1474,1213930,1213930,neutron,6b0c899d36d5a98963342e9cffb351fc3a1f4fd2,1,1, ,"Bug #1213930 in neutron: ""id() builtin used instead of subnet id in error messages""","This occurs twice in _validate_subnet:
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1049
https://github.com/openstack/neutron/blob/master/neutron/db/db_base_plugin_v2.py#L1063","Use subnet id instead of wrong built-in id()

In _validate_subnet(), built-in id() is used as param for exceptions,
this patch fixes it via using proper subnet id.

Closes-Bug: #1213930

Change-Id: I9a88f4dc7b771047f4061fb94ba65f0515afa745"
819,6b640163ab8133085e7bb970d3ac08a86e339ddb,1411876572,,1.0,8,3,2,2,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1363,1833,1374902,nova,6b640163ab8133085e7bb970d3ac08a86e339ddb,1,1,“missing vcpupin”,"Bug #1374902 in OpenStack Compute (nova): ""missing vcpupin elements in cputune for numa case""","Boot instance with flavor as below:
os@os2:~$ nova flavor-show 100
+----------------------------+------------------------+
| Property                   | Value                  |
+----------------------------+------------------------+
| OS-FLV-DISABLED:disabled   | False                  |
| OS-FLV-EXT-DATA:ephemeral  | 0                      |
| disk                       | 0                      |
| extra_specs                | {""hw:numa_nodes"": ""2""} |
| id                         | 100                    |
| name                       | numa.nano              |
| os-flavor-access:is_public | True                   |
| ram                        | 512                    |
| rxtx_factor                | 1.0                    |
| swap                       |                        |
| vcpus                      | 8                      |
+----------------------------+------------------------+
The result is
  <cputune>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>
The cputune should be:
  <cputune>
    <vcpupin vcpu='0' cpuset='0-7,16-23'/>
    <vcpupin vcpu='1' cpuset='0-7,16-23'/>
    <vcpupin vcpu='2' cpuset='0-7,16-23'/>
    <vcpupin vcpu='3' cpuset='0-7,16-23'/>
    <vcpupin vcpu='4' cpuset='8-15,24-31'/>
    <vcpupin vcpu='5' cpuset='8-15,24-31'/>
    <vcpupin vcpu='6' cpuset='8-15,24-31'/>
    <vcpupin vcpu='7' cpuset='8-15,24-31'/>
  </cputune>","Correct missing vcpupin elements for numa case

We should pin each vcpu into specified numa's cpuset. But the current
code only pin the last vcpu in one cell to specified numa's cpuset.

Change-Id: Ia1cda5e117d7e569e8891bd7c3d03bd6148a0c27
Closes-Bug: #1374902"
820,6b7b705be4592c7a5406b1fda62fe83f73d869f6,1406613820,,1.0,25,9,2,2,1,0.787126586,False,,,,,True,,,,,,,,,,,,,,,,,1073,1522,1343142,cinder,6b7b705be4592c7a5406b1fda62fe83f73d869f6,1,1,,"Bug #1343142 in Cinder: ""IBM storwize_svc can not get the right host""","IBM storwize_svc can not get the right host in following case:
Assume I have a SVC host using FC protocol. I configured 3 remote WWPNs for it, for example: rwwpn-1, rwwpn-2, rwwpn-3, and the host doesn't support iscsi so it has no iscsi_name property. When I try to attach a volume and do initialize_connection, I can get the host by only one of the 3 remote WWPNs, for example rwwpn-1, but I can not the host by the other two ones.","Storwize/SVC can not get the right host

When only config FC host, Storwize/SVC driver can not get the host
by remote WWPNs except the first one.

Filter 'iscsi_name' and 'WWPN' from host information, if not configure
iSCSI port, only return the first WWPN.

Change-Id: I27852e49f5dfb3a94eed08bea0e5d81a9d7aef9c
Closes-Bug: #1343142"
821,6b8a5f0e1d26d2721e7ad7fc67099ff8b880d9ec,1407247806,,1.0,19,1,2,2,1,0.468995594,False,,,,,True,,,,,,,,,,,,,,,,,1099,1550,1348056,neutron,6b8a5f0e1d26d2721e7ad7fc67099ff8b880d9ec,1,1,,"Bug #1348056 in neutron: ""Neutron network API throws error code 500 when an Invalid VLAN is provided (should throw 400)""","The neutron network API currently throws a error code 500 for an invalid input against the VLAN field.
The error can be reproduced by having the following JSON request body:
{
            ""network"": {
                ""admin_state_up"": ""false"",
                ""provider:segmentation_id"": ""abc"",
                ""name"": ""Network1"",
                ""provider:physical_network"": ""XYZ"",
                ""provider:network_type"": ""vlan""
            }
        }
An error code 400 should be thrown much like how it is thrown for the other fields - if they correspond to incorrect values.","Fix to throw correct error code for bad attribute

Currently the neutron network API throws up error code 500 for
the extended attribute for segmentation id. This can be reproduced
if the user types in a random string in place of an integer value
for the segmentation id. The proper behavior should throw an error
code 400 with the appropriate failure message. This patch fixes the
same issue and covers it with a test case.

Change-Id: I4735e20f5b8b23c5b2a9d896415c2e84561a279c
Closes-bug: #1348056"
822,6b8a91940b4fd410098e366689f2753010fc1ec1,1394831922,,1.0,1,1,1,1,1,0.0,True,1.0,912623.0,15.0,7.0,False,59.0,862124.0,149.0,5.0,245.0,459.0,561.0,242.0,388.0,487.0,217.0,396.0,472.0,0.182579564,0.332495812,0.396147404,626,1051,1292782,glance,6b8a91940b4fd410098e366689f2753010fc1ec1,1,1,It’s like a typo in msg,"Bug #1292782 in Glance: ""test_create_backup 500 on image get""","Log stash:
message: ""Object GET failed"" AND filename:""logs/screen-g-api.txt""
53 failure in the past 7 days.
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/console.html#_2014-03-14_18_26_47_575
Test runner worker pid: 1541
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/tempest.txt.gz#_2014-03-14_17_55_47_916
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912
http://logs.openstack.org/43/76543/1/gate/gate-tempest-dsvm-full/30a3ee1/logs/screen-g-api.txt.gz#_2014-03-14_17_55_47_912","Pass Message object to webob exception

Provide the Message object within Glance exception to webob exception
instead of try to convert it to a string. Because Message objects could
not be converted by str() since they may contain non-ascii characters.

Closes-Bug: #1292782
Related-Id: I352cda57fe119022c59c6c813b5c8053765b2d3c
Change-Id: I22b9123d8b3fe58d7dbd8fcde5a8dd58000e2da4
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
823,6b8d72df08e4b70b8de4b9797ae1c03be64e0072,1404921588,,1.0,9,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1049,1494,1339775,glance,6b8d72df08e4b70b8de4b9797ae1c03be64e0072,1,1,,"Bug #1339775 in Glance: ""Some v2 exceptions raise unicodeError""","In v2, for some exceptions the exception is not converted to a string properly resulting in:
2014-07-09 15:36:38,714 INFO msg = _(""Image exceeds the storage quota: %s"") % e
2014-07-09 15:36:38,714 INFO File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-07-09 15:36:38,714 INFO raise UnicodeError(msg)
in some cases 'utils.exception_to_str' is used to cast the exception.
We should do this in all cases for consistency/to avoid the stack trace","Some v2 exceptions raise unicodeError

Use utils.exception_to_str to consistently cast the
excpetion to avoid the unicodeError stack trace.

Change-Id: I54fce91c73adca846ec99ed50a57b95d16428ec1
Closes-Bug: #1339775"
824,6b9f9e6e9ae2fcf5c733b261717775970a9a4f62,1384435138,1.0,1.0,5,2,2,2,1,0.591672779,True,16.0,8854546.0,174.0,75.0,False,136.0,95718.0,392.0,4.0,9.0,2102.0,2110.0,7.0,1650.0,1656.0,5.0,1734.0,1738.0,0.000911023,0.263437595,0.264044944,1762,1251235,1251235,nova,6b9f9e6e9ae2fcf5c733b261717775970a9a4f62,1,1, ,"Bug #1251235 in OpenStack Compute (nova): ""IPv6 DAD failure due to hairpinning""","LibvirtGenericVIFDriver, when using the hybrid bridge method of plugging instances, needs to disable hairpinning to prevent IPv6 ICMPv6 packets from being sent back to the instance, which will cause IPv6 configuration to fail, because the instance will believe that the address it has configured has already been used.","LibVirt: Disable hairpin when using Neutron

When hairpinning is enabled, ICMPv6 messages that handle
duplicate address detections return to the instance, causing IPv6
SLAAC configuration to fail

http://tools.ietf.org/html/rfc4862#section-5.4.3

Closes-Bug: #1251235

Change-Id: I65e1d40d33d6291bfd5558c7c346fc5fbf92cc56"
825,6bd147df43b1d352230d94f52e0fb4c56e7885d6,1406263487,,1.0,45,2,2,2,1,0.419921097,False,,,,,True,,,,,,,,,,,,,,,,,1102,1553,1348479,neutron,6bd147df43b1d352230d94f52e0fb4c56e7885d6,1,1,,"Bug #1348479 in neutron: ""_extend_extra_router_dict does not handle boolean correctly""","Method:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L50
is used to add extension attributes to the router object during the handling of the API response. When attributes are unspecified, the router is extended with default values.
In the case of boolean attributes things don't work as they should, because a default value as True takes over on the right side of the boolean expression on:
https://github.com/openstack/neutron/blob/master/neutron/db/l3_attrs_db.py#L56
The end user so is led to believe that the server did not honor the request, when effectively it did.","Handle bool correctly during _extend_extra_router_dict

Ensure that extension attributes are always used to
override the chosen defaults. This was not working
in the case of default boolean True, as the testing
condition was wrong.

Closes-bug: #1348479

Change-Id: I22bce82c6078a96c0eb4a67e6decb6e9205721a8"
826,6bd4dae9680aae924ef24750e15f00711c14b1a3,1385649307,,1.0,3,3,1,1,1,0.0,True,1.0,139858.0,12.0,9.0,False,15.0,1969060.0,24.0,9.0,0.0,641.0,641.0,0.0,593.0,593.0,0.0,582.0,582.0,0.000831947,0.485024958,0.485024958,1778,1252692,1252692,cinder,6bd4dae9680aae924ef24750e15f00711c14b1a3,1,1, ,"Bug #1252692 in Cinder: ""Unrecognized Content-Type application/json""","Unrecognized Content-Type provided in request get_body /opt/stack/new/cinder/cinder/api/openstack/wsgi.py:796
Cinder api service logs the above entry, on every request made by the python-cinderclient, for example ""cinder list"".
 'application/json' should be an accepted content type, without any additional log.
You can see the above issue as the result of all tempest-devstack gate job as well.","Fixing check order for empty body in get_body()

Cinder API logged the following debug message for each request with
empty body:
""Unrecognized Content-Type provided in request get_body""
Check order was fixed in get_body function to follow the correct flow
and display the right message (empty body).

Change-Id: Ie561983a4da791a412fea3fc390dab718dfce191
Closes-Bug: #1252692"
827,6c085e1d6652df013ec176d46ffb234065373193,1407944014,,1.0,3,13,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1184,1641,1356051,nova,6c085e1d6652df013ec176d46ffb234065373193,1,1,,"Bug #1356051 in OpenStack Compute (nova): ""Cannot load 'instance' in the base class - problem in floating-ip-list""","I tried the following on VMware using the VMwareVCDriver with nova-network:
1. Create an instance
2. Create a floating IP: $ nova floating-ip-create
3. Associate a floating IP with the instance: $ nova floating-ip-associate test1 10.131.254.249
4. Attempt a list of the floating IPs:
$ nova floating-ip-list
ERROR (ClientException): The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-dcb17077-c670-4e2a-8a34-715a8afc5f33)
It failed and printed out the following messages in n-api logs:
2014-08-12 13:54:29.578 ERROR nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Caught error: Cannot load 'instance' in the base class
2014-08-12 13:54:29.578 TRACE nova.api.openstack Traceback (most recent call last):
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 124, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return req.get_response(self.application)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-08-12 13:54:29.578 TRACE nova.api.openstack     application, catch_exc_info=False)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-08-12 13:54:29.578 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/keystonemiddleware/auth_token.py"", line 565, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return self._app(env, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     response = self.app(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return resp(environ, start_response)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 908, in __call__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     content_type, body, accept)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 974, in _process_stack
2014-08-12 13:54:29.578 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1058, in dispatch
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return method(req=request, **action_args)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 146, in index
2014-08-12 13:54:29.578 TRACE nova.api.openstack     self._normalize_ip(floating_ip)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/floating_ips.py"", line 117, in _normalize_ip
2014-08-12 13:54:29.578 TRACE nova.api.openstack     floating_ip['instance'] = fixed_ip['instance']
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 447, in __getitem__
2014-08-12 13:54:29.578 TRACE nova.api.openstack     return getattr(self, name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 67, in getter
2014-08-12 13:54:29.578 TRACE nova.api.openstack     self.obj_load_attr(name)
2014-08-12 13:54:29.578 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/objects/base.py"", line 375, in obj_load_attr
2014-08-12 13:54:29.578 TRACE nova.api.openstack     _(""Cannot load '%s' in the base class"") % attrname)
2014-08-12 13:54:29.578 TRACE nova.api.openstack NotImplementedError: Cannot load 'instance' in the base class
2014-08-12 13:54:29.579 INFO nova.api.openstack [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] http://10.131.179.211:8774/v2/875c3f62ef75400487e4a68679f8e239/os-floating-ips returned with HTTP 500
2014-08-12 13:54:29.580 DEBUG nova.api.openstack.wsgi [req-86d8f466-cfae-42ac-8340-9eac36d6fc71 demo demo] Returning 500 to user: The server has either erred or is incapable of performing the requested operation. from (pid=12246) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1200","Fix NotImplementedError in floating-ip-list

A list of existing floating IP (nova floating-ip-list) fails
when using VMwareVCDriver/LibvirtDriver and nova-network.
The following error is thrown: ""NotImplementedError: Cannot load
'instance' in the base class"". The problem is caused by
_normalize_ip, which tries to set an 'instance' field in
FloatingIP that is needed by _translate_floating_ip_view. The
instance is derived from the FixedIP that is assigned to the
'fixed_ip' field in FloatingIP. However, FixedIP may not have
an 'instance' field set, which can cause the NotImplementedError.

The following patch removes _normalize_ip and assigns the
'instance_id' required by _translate_floating_ip_view to
the 'instance_uuid' in FixedIP, since only the instance ID
is required and not the entire instance object.

Change-Id: I2d7937e969bd87e53557842a22f3861f0d307b5e
Closes-Bug: #1356051"
828,6c1eec67d2e7def7561bde906df9a50006a86b10,1407802225,,1.0,12,2,2,2,1,0.371232327,False,,,,,True,,,,,,,,,,,,,,,,,1165,1622,1354285,neutron,6c1eec67d2e7def7561bde906df9a50006a86b10,1,1,,"Bug #1354285 in neutron: ""L3-agent using MetaInterfaceDriver failed""","MetaInterfaceDriver communicates with neutron-server using REST API.
If a user intend to use internalurl for neutron-server endpoint, MataInterfaceDriver fails.
This is because MetaInterfaceDriver does not specify endpoint_type. Thus it assumes using publicurl.
---
class MetaInterfaceDriver(LinuxInterfaceDriver):
    def __init__(self, conf):
        super(MetaInterfaceDriver, self).__init__(conf)
        from neutronclient.v2_0 import client
        self.neutron = client.Client(
            username=self.conf.admin_user,
            password=self.conf.admin_password,
            tenant_name=self.conf.admin_tenant_name,
            auth_url=self.conf.auth_url,
            auth_strategy=self.conf.auth_strategy,
            region_name=self.conf.auth_region
        )
---
Note that MetaInterfaceDriver is used with Metaplugin only.","Add endpoint_type parameter to MetaInterfaceDriver

Previously MetaInterfaceDriver communicates with neutron-server
via 'publicURL' because it uses python-neutronclient without
endpoint_type and 'publicURL' is the default.
It causes error when a user intends to use internal or admin URL
for communication between MetaInterfaceDriver and neutron-server.

Change-Id: Id321d5a16a08a4bda618f98bb1eb7cb1b0b9eb08
Closes-bug: #1354285"
829,6c2bec5d9e40c68c75cc56e077e2af97d1359d20,1394837854,,1.0,13,4,3,3,1,0.912733241,True,2.0,413599.0,34.0,6.0,False,10.0,987713.6667,14.0,2.0,500.0,2009.0,2227.0,405.0,1701.0,1896.0,243.0,308.0,456.0,0.252066116,0.319214876,0.472107438,159,561,1258438,neutron,6c2bec5d9e40c68c75cc56e077e2af97d1359d20,1,1,,"Bug #1258438 in neutron: ""Can't create a firewall for admin tenant when at least one other tenant has a firewall""","Only one firewall is allowed per tenant. This works as expected for non-admin tenants.
When a new firewall is added in the context of admin, this fails if some other tenant already has a firewall. This is because 'get_firewall_count' returns sum of all firewalls in the system. Addition of a new firewall for admin fails with the following error message.
500-{u'NeutronError': {u'message': u'Exceeded allowed count of firewalls for tenant tenant-2. Only one firewall is supported per tenant.', u'type': u'FirewallCountExceeded', u'detail': u''}}
fwaas_plugin.py
----------------
def create_firewall(self, context, firewall):
        LOG.debug(_(""create_firewall() called""))
        tenant_id = self._get_tenant_id_for_create(context,
                                                   firewall['firewall'])
        fw_count = self.get_firewalls_count(context)
        if fw_count:
            raise FirewallCountExceeded(tenant_id=tenant_id)
----------------
=> fw_count = self.get_firewalls_count(context)
In the context of admin, the function counts other tenant's firewall.","Ensure to count firewalls in target tenant

Previously admin tenant cannot create a firewall if other tenant
already created a firewall. We need to count firewalls only in
a target tenant.

Change-Id: I3e6d151d00d4a487bdd858e94929fab8960511a2
Closes-Bug: #1258438"
830,6c91bdfe24e275ede65df369d714ca119d5ce5f2,1393888508,,1.0,5,1,1,1,1,0.0,True,2.0,219377.0,26.0,2.0,False,6.0,3386.0,7.0,2.0,690.0,777.0,1248.0,592.0,619.0,1016.0,220.0,375.0,468.0,0.247757848,0.421524664,0.525784753,514,934,1287419,neutron,6c91bdfe24e275ede65df369d714ca119d5ce5f2,1,1,wrong parameter passed,"Bug #1287419 in neutron: ""NSX: update_port passes neutron_sg ids and not nsx sg_ids""","2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.api_client.client [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Server Error Message: Entity '50d064c4-0591-43ee-8dfd-ac2409c13a5e' not registered.
2014-03-03 15:06:33.913 ERROR neutron.plugins.vmware.nsxlib.switch [req-ffc2204c-e957-48b3-9bd8-cbbd66fa9645 demo 3cb63bd6fa2d4ea397df6de7770c2e8a] Port or Network not found, Error: An unknown exception occurred.
2014-03-03 15:06:33.914 ERROR NeutronPlugin [-] Unable to update port id: 6b7c9502-8e58-4930-b440-7022f47fff89.
2014-03-03 15:06:33.914 TRACE NeutronPlugin Traceback (most recent call last):
2014-03-03 15:06:33.914 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1307, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin     ret_port.get(addr_pair.ADDRESS_PAIRS))
2014-03-03 15:06:33.914 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/vmware/nsxlib/switch.py"", line 324, in update_port
2014-03-03 15:06:33.914 TRACE NeutronPlugin     port_id=lport_uuid, net_id=lswitch_uuid)
2014-03-03 15:06:33.914 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 6b7c9502-8e58-4930-b440-7022f47fff89 could not be found on network b460b092-9482-48ee-9c5f-04ec384fbc8c","NSX: passing wrong security_group id mapping to nsx backend

When the async secgroup work was added the update_port() code was not changed
to pass in the nsx security_group uuids. This patch fixes that so that the
nsx uuids are passed in instead of neutron.

Closes-bug: #1287419

Change-Id: I1a075564f2548764de15b85b2970b5f360412eb2"
831,6c9f81b97ededf2b6401861f7d876851ff07b34b,1389897905,,1.0,514,450,18,4,1,0.781617358,True,3.0,1828546.0,15.0,4.0,False,36.0,2591358.944,64.0,3.0,209.0,1259.0,1300.0,201.0,1087.0,1128.0,188.0,1107.0,1138.0,0.141573034,0.829962547,0.853183521,302,711,1270178,cinder,6c9f81b97ededf2b6401861f7d876851ff07b34b,0,0,No bug: code needs update from Oslo,"Bug #1270178 in Cinder: ""common/rpc code needs update from Oslo""","The rpc code from Oslo is quite out of date, and missing at least one bugfix of interest (bug 1189711).","Sync RPC module from Oslo

22e971a safe_log Sanitize Passwords in List of Dicts
8b2b0b7 Use hacking import_exceptions for gettextutils._
23f6029 Use six.seraise() instead of `raise exc, val, tb`
6d0a6c3 Correct invalid docstrings
7cac1ac Fix mis-spellings
ef406a2* Create a shared queue for QPID topic consumers
e6494c2 Use six.iteritems to make dict work on Python2/3
e227c0e* Properly reconnect subscribing clients when QPID broker restarts
16fb43b Replace data structures' attribute with six module
27b21bc Unify different names between Python2/3 with six.moves
12bcdb7 Remove vim header
3970d46 Fix typos in oslo
1771a77 Adjust import order according to PEP8 imports rule
0717d1d matchmaker_redis compatibility with redis-py 2.4
f88d59a Drop RPC securemessage.py
39f6589 Use localisation
8a3996a Fix missing space in help text
4bfb7a2 Apply six for metaclass
d7d74a7 Add `versionutils` for version compatibility checks
3cdd157 Add third element to RPC versions for backports
76972e2* Support a new qpid topology
284b13a Raise timeout in fake RPC if no consumers found
9721129 exception: remove
7b0cb37 Don't eat callback exceptions
69abf38 requeue instead of reject
28395d9 Fixes files with wrong bitmode
bec54ac Fix case error in qpid exchange type ""direct""
61c4cde Ensure context type is handled when using to_dict
223f9e1 Clarify precedence of secret_key_file
a035f95 Don't shadow cfg import in securemessage
0f88575 Remove redundant global keyword in securemessage
848c4d5 Some nitpicky securemessage cleanups
5c71c25 Allow non-use of cfg.CONF in securemessage
9157286 RPC: Add MessageSecurity implementation
2031e60 Refactors boolean returns
a047a35 Make ZeroMQ based RPC consumer threads more robust
34a6842 On reconnecting a FanoutConsumer, don't grow the topic name
f52446c Add serializer param to RPC service
5ff534d Add config for amqp durable/auto_delete queues
f9f1b4f Revert ""Add support for heartbeating in the kombu RPC driver""
1a2df89 Enable H302 hacking check
7bfd443 Avoid shadowing Exception 'message' attribute
99b7c35 Convert kombu SSL version string into integer
c37f6aa Add support for heartbeating in the kombu RPC driver
3006787 Sanitize passwords in _safe_log
dea334a Replace sys.exit by a RPCException
3969355 Fix exception arg typo
22ec8ff Make AMQP based RPC consumer threads more robust
13650b1 rpc: remove some unused serialization code
e204885 Optionally reject messages on exception.
688832f Remove unused zmq relay functionality
719eba4 Don't reconnect to exclusive fanout consumers.
980fe5f Allow exceptions to hop up cells
80476f0 Add can_send_version() to RpcProxy.
7119e29 Enable hacking H404 test.
32e2a25 zmq: remove someone unused code from ZmqClient
6d42ced Remove the amqp_rpc_single_reply_queue option from Havana
7ce5441 Fix problem with long messages in Qpid
e3545f8 Enable hacking H402 test
15d8d69 Silence exceptions from qpid connection.close()
484a1df Enable hacking H403 test
35660da Enable hacking H401 test
1a9a744 MatchMakerStub - make it work & add test
35aad91 Use zero-copy in ZeroMQ proxy (zmq-rpc-receiver)
b677b13 Remove rootwrap from IPC directory creation
93ee6e3 Add generic serialization support
fe2f108 Enable hacking H702 localization test
0a14e1d Enable hacking H703: Multiple positional placeholders
df7ea83 Allow RPC_API_NAMESPACE on RpcProxy objects
aa89d8b Support capping message versions in the client.
1d7920a Extract matchmaker_ring to own module
3e33692 Merge ""Removes len() on empty sequence evaluation""
44c79fc Merge ""python3: use 'as' syntax for exception assignment""
0c54b72 python3: use 'as' syntax for exception assignment
a514693 Removes len() on empty sequence evaluation
fde1e15 Convert unicode for python3 portability
0c9047c* Ensure that qpid connection is closed.
120ddef Improve Python 3.x compatibility

(* after hash indicates this was already ported to Cinder)

Oslo version:
7a51572 Merge ""Implement cache abstraction layer""
Date:   Wed Jan 15 19:31:16 2014 +0000

Related-Bug: #1189711
Closes-Bug: #1270178

Change-Id: I19572c5e98c1c1037a6a622d63fbdea8d6001532"
832,6ccc654a0509c4e308713f9e6fd95266cd112c9d,1386949149,,1.0,5,2,1,1,1,0.0,True,2.0,9731840.0,32.0,18.0,False,10.0,143382.0,23.0,5.0,191.0,1271.0,1302.0,186.0,1110.0,1141.0,171.0,1113.0,1135.0,0.137161085,0.888357257,0.905901116,186,589,1260773,cinder,6ccc654a0509c4e308713f9e6fd95266cd112c9d,1,1,,"Bug #1260773 in Cinder: ""LVM Thin pool free space calculation fails if pool not activated""","We should always activate the pool at initialization time since we are going to use it anyway.
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 205, in _child_process
2013-12-13 10:05:14.095 TRACE cinder.service     launcher.run_server(server)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 96, in run_server
2013-12-13 10:05:14.095 TRACE cinder.service     server.start()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/service.py"", line 388, in start
2013-12-13 10:05:14.095 TRACE cinder.service     self.manager.init_host()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 286, in init_host
2013-12-13 10:05:14.095 TRACE cinder.service     self.publish_service_capabilities(ctxt)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 912, in publish_service_capabilities
2013-12-13 10:05:14.095 TRACE cinder.service     self._report_driver_status(context)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 904, in _report_driver_status
2013-12-13 10:05:14.095 TRACE cinder.service     volume_stats = self.driver.get_volume_stats(refresh=True)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 345, in get_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service     self._update_volume_stats()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/volume/drivers/lvm.py"", line 358, in _update_volume_stats
2013-12-13 10:05:14.095 TRACE cinder.service     self.vg.update_volume_group_info()
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 396, in update_volume_group_info
2013-12-13 10:05:14.095 TRACE cinder.service     self.vg_thin_pool)
2013-12-13 10:05:14.095 TRACE cinder.service   File ""/opt/stack/cinder/cinder/brick/local_dev/lvm.py"", line 149, in _get_thin_pool_free_space
2013-12-13 10:05:14.095 TRACE cinder.service     consumed_space = float(data[0]) / 100 * (float(data[1]))
2013-12-13 10:05:14.095 TRACE cinder.service ValueError: empty string for float()
2013-12-13 10:05:14.095 TRACE cinder.service
2013-12-13 10:05:14.109 INFO cinder.service [-] Child 5052 exited with status 2
2013-12-13 10:05:14.110 INFO cinder.service [-] _wait_child 1
2013-12-13 10:05:14.110 INFO cinder.service [-] wait wrap.failed True
$ sudo lvchange -a n stack-volumes/stack-volumes-pool
$ sudo lvs -o size,data_percent --separator : stack-volumes/stack-volumes-pool
  LSize:Data%
  9.00g:","LVM: Activate Thin Pool LV upon initialization

If the LVM thin pool is not active, space calculation fails,
as data_percent is not known.  Activate the pool upon initialization,
since we intend to use it anyway.

Slightly refactors _get_thin_pool_free_space so that it is clear
which missing/unexpected value is breaking things if a similar error
occurs for some other reason.

Closes-Bug: #1260773

Change-Id: I7cb187746c1ac297b82254c6efa37ba1c5fbb3e1"
833,6cd183a4bac7ae79bca380c960686ea33892f066,1397611666,,1.0,8,8,2,2,1,1.0,True,2.0,657155.0,30.0,9.0,False,26.0,4387570.0,40.0,4.0,3.0,3259.0,3259.0,3.0,2604.0,2604.0,3.0,3142.0,3142.0,0.000511902,0.402226772,0.402226772,789,1216,1307338,nova,6cd183a4bac7ae79bca380c960686ea33892f066,1,1,,"Bug #1307338 in OpenStack Compute (nova): ""Return incorrect message in keypair-show and keypair-delete""","Reproduce:
1.nova keypair-list
+----------+-------------------------------------------------+
| Name     | Fingerprint                                     |
+----------+-------------------------------------------------+
| root_key | 41:f3:fc:23:07:1d:99:cc:fd:e4:7a:a3:20:ba:78:25 |
+----------+-------------------------------------------------+
2.nova keypair-show root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-542fa1da-0ab0-4624-b662-7d7c908508e2)
3.nova keypair-delete root
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-2f8587a3-ee5e-4134-ba5d-a2b3f0968cbc)
expected:
1.nova keypair-show root
ERROR: No keypair with a name or ID of 'root' exists.
2.nova keypair-delete root
ERROR: No keypair with a name or ID of 'root' exists.","Fix the explanations of HTTPNotFound for keypair's API

Gets the ""explanation"" from HTTPNotFound exception and returns
correct message. the commit Ia784e09cda986a421e83c2820dfd0a2267867cdd
has fixed this kind of bug already, but this bug is remained.

Closes-Bug: #1307338
Change-Id: I6811eecba86938e4253f437b7677cb7e4468fd67"
834,6cef3c9b75cedb0fe6ff901128452415128d60b0,1406228749,,1.0,11,10,2,1,1,0.453716339,False,,,,,True,,,,,,,,,,,,,,,,,1135,1590,1351020,nova,6cef3c9b75cedb0fe6ff901128452415128d60b0,1,1,,"Bug #1351020 in OpenStack Compute (nova): ""FloatingIP fails to load from database when not associated""","A FloatingIP can be not associated with an FixedIP, which will cause its fixed_ip field in the database model to be None. Currently, FloatingIP's _from_db_object() method always assumes it's non-None and thus tries to load a FixedIP from None, which fails.","Correct some IPAddress DB interaction in objects

We know that DB API methods want string addresses as parameters,
but often an object method will be called with data from that or
another object, which will be a netaddr.IPAddress in the case of
that sort of data. So, we should coerce those to strings to make
sure SA is happy.

This also fixes a small bit of DB-loading logic where a FloatingIP
would try to load its associated FixedIP explicitly. Obviously,
if it's not associated, it shouldn't try to do that.

Related to blueprint compute-manager-objects-juno
Closes-bug: #1351020

Change-Id: I97b4a86846020b58950f7c051cb003b7b09e938a"
835,6d1037335bbe969cfbc6d9657c24865ca226c7e9,1378302530,,1.0,5,3,2,2,1,0.811278124,True,2.0,116502.0,16.0,10.0,False,3.0,1280526.0,4.0,5.0,94.0,907.0,923.0,94.0,873.0,889.0,53.0,244.0,247.0,0.195652174,0.887681159,0.898550725,1509,1220692,1220692,neutron,6d1037335bbe969cfbc6d9657c24865ca226c7e9,1,1, ,"Bug #1220692 in neutron: ""LBaaS HAProxy agent outputs traceback in get_stats""","I found the following error in q-lbaas log after creating a vip on a pool.
2013-09-04 21:41:59.830 10678 DEBUG neutron.openstack.common.periodic_task [-] Running periodic task LbaasAgentManager.collect_stats run_periodic_tasks /opt/stack/neutron/neutron/openstack/common/periodic_task.py:176
2013-09-04 21:41:59.831 10678 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Error upating stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 137, in collect_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     stats = driver.get_stats(pool_id)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 168, in get_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     pool_stats['members'] = self._get_servers_stats(parsed_stats)
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/opt/stack/neutron/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 188, in _get_servers_stats
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     if stats['type'] == TYPE_SERVER_RESPONSE:
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager KeyError: 'type'
2013-09-04 21:41:59.831 10678 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager","LBaaS: make haproxy stats parsing more safe

Change-Id: Ic21b310608bb98be29ea50ab7c56ca859a9ed5c0
Closes-Bug: #1220692"
836,6d49834976bc08d2b8f7939bc2a217f5cc81db47,1395938204,,1.0,42,4,3,2,1,0.955023209,True,7.0,3027422.0,98.0,10.0,False,11.0,848569.0,17.0,4.0,9.0,1411.0,1415.0,9.0,1164.0,1168.0,6.0,623.0,624.0,0.006724304,0.599423631,0.600384246,702,1128,1299156,neutron,6d49834976bc08d2b8f7939bc2a217f5cc81db47,1,0,Bug because of evolution,"Bug #1299156 in neutron: ""Hyper-V agent does not disable security group rules""","A new config option was introduced recently, enable_security_group. This config option specifies if the agent should have the security group enabled or not.
The Hyper-V agent does not take this config option into account, which means the security groups rules are always applied.","Fixes Hyper-V agent security groups disabling

Adds a check if the security groups have been disabled and removes
the switch port's ACLs accordingly.

Change-Id: I20d716a3f182b8ea62da6b436b150aa9dafdb1c5
Closes-Bug: #1299156"
837,6d59790b9cd42309417ae1866e40a06dcb557abe,1410567416,,1.0,87,80,3,2,1,0.627659991,False,,,,,True,,,,,,,,,,,,,,,,,1253,1713,1364232,cinder,6d59790b9cd42309417ae1866e40a06dcb557abe,1,1,,"Bug #1364232 in Cinder: ""EMC VMAX driver does not handle newlines and whitespace consistently in xml""","This is my first openstack bug report. I believe this should go to Xing Yang.
The EMC VMAX driver parses an XML config file (default is /etc/cinder/cinder_emc_config.xml) to retrieve different pieces of config information. The parsing helper methods are not consistent in their expectation of the format of this file.
For example, I provide a file with newlines and white-space like the following:
<?xml version=""1.0"" encoding=""UTF-8""?>
<EMC>
  <EcomServerPort>5988
  </EcomServerPort>
  <Array>000198700498</Array>
  <EcomServerIp>5.55.55.5
  </EcomServerIp>
  <Pool>PowerVC_thin</Pool>
<EcomUserName>admin</EcomUserName><PortGroups><PortGroup>PORTGROUP1</PortGroup><PortGroup>PORTGROUP2</PortGroup></PortGroups></EMC>
This shows newlines and indenting for some properties, and no newlines or indenting for others.
Then, EMCVMAXUtils.get_ecom_server() will return:
    (u'5.55.55.5\n  ', u'5988\n  ')
Where the newlines and whitespace are included in the address and port strings. This causes the driver to fail.
If I take out the whitespace and newlines, then the IP address and port are parsed correctly.  However, the portgroups are not parsed correctly if I remove whitespace and newlines as is shown in the last line of the file.  In this case EMCVMAXUtils.parse_file_to_get_port_group_name() returns:
    u'PORTGROUP1PORTGROUP2'
Where it should return one of the port groups at random. Instead it munges the two together. Can the EMCVMAXUtils class be fixed to handle hand tolerate normal XML?","Handle config file with newlines and whitespaces

EMC VMAX driver does not handle newlines and whitespaces
consistently in the XML config file. This patch addressed
this issue.

Change-Id: I45f61f167feee9174ba2bc3229695b680f8e946d
Closes-Bug: #1364232"
838,6d7b2e007cf477a7f212312bad4ce0cbe8032d1a,1406784438,,1.0,7,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1189,1647,1356665,neutron,6d7b2e007cf477a7f212312bad4ce0cbe8032d1a,0,0,Bug in test,"Bug #1356665 in neutron: ""WSGI unittest fails if HTTP_PROXY if set""","urllib2.urlopen uses $http_proxy/$HTTP_PROXY environment variables by
default.  If set (and pointing to a remote host), then the WSGI tests
that spin up a local server and connect to it using
http://127.0.0.1:$port/ instead connect to $port *on the proxy*,
and (hopefully) fail.
We shouldn't follow HTTP_PROXY when trying to connect to test servers.","Ignore http_proxy while connecting to test WSGI server

urllib2.urlopen uses $http_proxy/$HTTP_PROXY environment variables by
default.  If set (and pointing to a remote host), then the WSGI tests
that spin up a local server and connect to it using
http://127.0.0.1:$port/ instead connect to $port *on the proxy*,
and (hopefully) fail.

This change uses urllib2 in a way that ignores proxy settings.

Change-Id: I4d94fcc06925d0c947345a07ae20352e1898e46d
Closes-Bug: #1356665"
839,6d9bb4bdc915b1ed6ac9d49012eb185a369cc624,1394472463,,1.0,7,2,3,1,1,0.579380164,True,2.0,283090.0,20.0,8.0,False,2.0,1199811.0,2.0,4.0,219.0,1346.0,1428.0,209.0,1101.0,1182.0,209.0,1196.0,1272.0,0.138613861,0.79009901,0.840264026,481,898,1285029,cinder,6d9bb4bdc915b1ed6ac9d49012eb185a369cc624,0,0,tests,"Bug #1285029 in Cinder: ""Fibre Channel Zone Manager hacking violations""","Some hacking violations have come up after the FCZM landed and need to be addressed.  John commented on several of them in the post merge review for this patch.
https://review.openstack.org/#/c/76011","Fixed some FCZM unit tests hacking issues

This patch is a follow up to a review that merged
included and had some issues raised after it was merged.
This patch includes some hacking cleanup and some
unit test cleanup for the Fibre Channel Zone Manager.

Change-Id: Ibb6974b2279f08720c2469893245bc888d35899c
Closes-Bug: #1285029"
840,6dc290a88a3240b9f454d5140ab14d499d9ab1b2,1379626478,0.0,1.0,33,14,4,2,1,0.841917509,True,4.0,924422.0,40.0,22.0,False,189.0,269901.0,1098.0,5.0,1646.0,1567.0,2834.0,1430.0,1343.0,2418.0,1555.0,1445.0,2626.0,0.255794838,0.237711655,0.43185928,1573,1226965,1226965,nova,6dc290a88a3240b9f454d5140ab14d499d9ab1b2,1,1, ,"Bug #1226965 in OpenStack Compute (nova): ""nova sync_power_states broken when db is stopped, but VM is running""","the Nova has the logic to synchronize the state of virtual machine with the record in database, when the state is stopped but
the virtual machine is in running state, it will try to stop the virtual machine with compute api call. But the compute api
call has the check that only allow to execute the call when VM in ACTIVE,RESCUED, ERROR state.  So the sync logic is broken here.
option 1:
   allow to run stop API when VM in stopped state
option 2:
   add another method in API  such as force_stop , and sync_power_states will use this api to stop the VM.
option 3:
    sync_power_states to call the rpcapi to stop the VM","Fix stopping instance in sync_power_states

When an instance is marked as stopped in the DB but is actually running,
the sync_power_states periodic task will try to stop it.  It does this
using the compute API.  The compute API checks the vm state before
proceeding.  The STOPPED state is treated as invalid.  This patch allows
the caller to bypass the state check and makes use of that in this
periodic task.

Change-Id: I447b6dbd1da72ed33067659708e1755bf8c933d0
Closes-bug: #1226965"
841,6dd5cc503cc05c00c5f9d831480539c67f6e2a48,1400608731,1.0,1.0,6,6,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,899,1335,1321298,nova,6dd5cc503cc05c00c5f9d831480539c67f6e2a48,1,1,,"Bug #1321298 in OpenStack Compute (nova): ""Periodic task cause errors in _finish_resize""","In the event that an end user sets resize_confirm_window to something small (say 1 in this example) there is a possibility that the periodic task can run in nova/compute/manager.py:ComputeManager._finish_resize() after the migration has been updated but before the instances has been updated.
http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n3570
One possible solution to this would be to reverse the order, and update the instance before updating the migration, in which case the migration will get updated in _confirm_resize: http://git.openstack.org/cgit/openstack/nova/tree/nova/compute/manager.py#n5018","Fix migration and instance resize update order

This commit switches the order in which migrations and instances are
updated when resizing an instance. Previously, if you set
resize_confirm_window to some small value, your resize instance could go
into error state because the periodic task was run after the migration
was updated but before the instance object was updated. This change makes it
so the instance is always updated before the migration.

The _test_finish_resize test case has also been refactored to enforce
saving the instance object before the migration object. In the event
that the instance object is saved after the migration, like the previous
implementation, the test cases will fail since the states of the
migrations will be out of sync in the _mig_save assertions.

Closes-Bug: #1321298
Change-Id: I0490fc4b4e03b36eb01a06a95ea761f4cf8df469"
842,6ddd9f93f82427ce909c7773f7a806361035a0b2,1405104982,,1.0,28,50,2,2,1,0.73206669,False,,,,,True,,,,,,,,,,,,,,,,,1025,1468,1336127,nova,6ddd9f93f82427ce909c7773f7a806361035a0b2,1,1,,"Bug #1336127 in OpenStack Compute (nova): ""The volumes will be deleted when  creating a virtual machine fails with the parameter delete_on_termination being set true, which causes that the rescheduling fails""","when specifying a volume or an image with a user volume to create a virtual machine, if the virtual machine fails to be created for the first time with the parameter delete_on_termination being set “true”, the specified volume or the user volume will be deleted, which causes that the rescheduling fails.
for example:
1. upload a image
| 62aa6627-0a07-4ab4-a99f-2d99110db03e | cirros-0.3.2-x86_64-uec | ACTIVE
2.create a boot volume by the above image
cinder create --image-id 62aa6627-0a07-4ab4-a99f-2d99110db03e --availability-zone nova 1
| b821313a-9edb-474f-abb0-585a211589a6 | available | None | 1 | None | true | |
3. create a virtual machine
nova boot --flavor m1.tiny --nic net-id=28216e1d-f1c2-463b-8ae2-330a87e800d2 tralon_disk1 --block-device-mapping vda=b821313a-9edb-474f-abb0-585a211589a6::1:1
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume b821313a-9edb-474f-abb0-585a211589a6. (HTTP 400) (Request-ID: req-486f7ab5-dc08-404e-8d4c-ac570d4f4aa1)
4. use the ""cinder list"" to find that the volume b821313a-9edb-474f-abb0-585a211589a6 has been deleted
+----+--------+------+------+-------------+----------+-------------+
| ID | Status | Name | Size | Volume Type | Bootable | Attached to |
+----+--------+------+------+-------------+----------+-------------+
+----+--------+------+------+-------------+----------+-------------+","Don't remove delete_on_terminate volumes on a reschedule

When cleaning up volumes before a reschedule if delete_on_terminate is
True the volume would be deleted.  That's not the desired behavior so
the volume cleanup has been moved to take place when a build is aborted.

Change-Id: I142370c0555495b4d51736f4f6b8070a3c112a59
Closes-bug: #1336127"
843,6def27b434a53aa8c1aca62640a6c072cc21209e,1394708338,,1.0,29,51,2,2,1,0.168660931,True,6.0,4253923.0,71.0,27.0,False,43.0,77422.0,128.0,6.0,415.0,5037.0,5037.0,395.0,3807.0,3807.0,407.0,4024.0,4024.0,0.053769109,0.530442804,0.530442804,396,809,1279317,nova,6def27b434a53aa8c1aca62640a6c072cc21209e,0,0,imrpove tests,"Bug #1279317 in OpenStack Compute (nova): ""xen: agent tests take longer than 15 secs""","gkotton@ubuntu:~/nova$ tox nova.tests.virt.xenapi.test_xenapi
py26 create: /home/gkotton/nova/.tox/py26
ERROR: InterpreterNotFound: python2.6
py27 develop-inst-nodeps: /home/gkotton/nova
py27 runtests: commands[0] | python -m nova.openstack.common.lockutils python setup.py test --slowest --testr-args=nova.tests.virt.xenapi.test_xenapi
[pbr] Excluding argparse: Python 2.6 only dependency
running test
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests --list
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmp349WwU
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpccaidD
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpMvMcYb
running=OS_STDOUT_CAPTURE=${OS_STDOUT_CAPTURE:-1} \
OS_STDERR_CAPTURE=${OS_STDERR_CAPTURE:-1} \
OS_TEST_TIMEOUT=${OS_TEST_TIMEOUT:-160} \
${PYTHON:-python} -m subunit.run discover -t ./ ./nova/tests  --load-list /tmp/tmpzJ2QHG
Ran 201 (+199) tests in 16.446s (+0.396s)
PASSED (id=350)
Slowest Tests
Test id                                                                                                                  Runtime (s)
-----------------------------------------------------------------------------------------------------------------------  -----------
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_agent_upgrade_fails_silently                              16.223
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_agent_not_implemented                               16.222
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_fails_with_agent_bad_return                               16.034
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_add_aggregate_host_raise_err                              1.116
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_maintenance_mode                                                 0.297
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off                                  0.277
nova.tests.virt.xenapi.test_xenapi.XenAPIVMTestCase.test_spawn_vlanmanager                                                0.195
nova.tests.virt.xenapi.test_xenapi.XenAPIAggregateTestCase.test_remote_master_non_empty_pool                              0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIMigrateInstance.test_migrate_disk_and_power_off_with_zero_gb_old_and_new_works   0.179
nova.tests.virt.xenapi.test_xenapi.XenAPIDom0IptablesFirewallTestCase.test_static_filters                                 0.171
py33 create: /home/gkotton/nova/.tox/py33
ERROR: InterpreterNotFound: python3.3","xenapi: fixup agent tests

The root cause of the slow agent tests was not mocking this method:
compute_utils.add_instance_fault_from_exc

This cleans up the tests to share a bit more code, and ensures they
actually check that the appropriate exception is raised.

This change adds a log message to report errors that could occur in
running systems that also helped identify this issue in the unit tests.

test_spawn_fails_agent_not_implemented was a duplicate test and has been
removed.

Change-Id: I6fd31520fffd5e888ce0c2b58af2715f0d929ce1
Closes-Bug: #1279317"
844,6dfb3ede190cb013f747617e75218953330c54ca,1385106011,,1.0,1,1,1,1,1,0.0,True,2.0,251286.0,11.0,7.0,False,3.0,1084.0,7.0,4.0,11.0,694.0,698.0,11.0,663.0,667.0,6.0,656.0,656.0,0.005942275,0.557724958,0.557724958,1794,1253910,1253910,cinder,6dfb3ede190cb013f747617e75218953330c54ca,0,0,“fix docstring”,"Bug #1253910 in Cinder: ""Fixes inappropriate description about migrate_volume_completion""","""Migrate a volume to the specified host."" as the description of migrate_volume_completion func is inappropriate.","Fix docstring for _migrate_volume_completion

""Migrate a volume to the specified host."" as the description of
_migrate_volume_completion func is incorrect.

Closes-Bug: #1253910
Change-Id: Ica944d9a7fa962ff56a9f0107ac1519623d2e551"
845,6e1fd7b9b51498af5f688a41d6636ff90b9a56f5,1411661243,,1.0,109,14,2,2,1,0.872161788,False,,,,,True,,,,,,,,,,,,,,,,,816,1244,1310135,nova,6e1fd7b9b51498af5f688a41d6636ff90b9a56f5,1,1,,"Bug #1310135 in OpenStack Compute (nova): ""Stopping an instance via the Nova API when using the Nova Ironic driver incorrectly reports powerstate""","When using the Ironic Nova driver, a stopped server is still presented as Running even when the server is stopped. Checking via the Ironic API correctly shows the instance as powered down:
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks          |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | ACTIVE | -          | Running     | private=10.1.0.10 |
+--------------------------------------+---------+--------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ nova stop 5b43d631-91e1-4384-9b87-93283b3ae958
stack@ironic:~/logs/screen$ nova list
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| ID                                   | Name    | Status  | Task State | Power State | Networks          |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
| 5b43d631-91e1-4384-9b87-93283b3ae958 | testing | SHUTOFF | -          | Running     | private=10.1.0.10 |
+--------------------------------------+---------+---------+------------+-------------+-------------------+
stack@ironic:~/logs/screen$ ping 10.1.0.10
PING 10.1.0.10 (10.1.0.10) 56(84) bytes of data.
From 172.24.4.2 icmp_seq=1 Destination Host Unreachable
From 172.24.4.2 icmp_seq=5 Destination Host Unreachable
From 172.24.4.2 icmp_seq=6 Destination Host Unreachable
From 172.24.4.2 icmp_seq=7 Destination Host Unreachable
From 172.24.4.2 icmp_seq=8 Destination Host Unreachable
--- 10.1.0.10 ping statistics ---
9 packets transmitted, 0 received, +5 errors, 100% packet loss, time 8000ms
stack@ironic:~/logs/screen$ ironic node-list
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| UUID                                 | Instance UUID                        | Power State | Provisioning State | Maintenance |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+
| 91e81c38-4dce-412b-8a1b-a914d28943e4 | 5b43d631-91e1-4384-9b87-93283b3ae958 | power off   | active             | False       |
+--------------------------------------+--------------------------------------+-------------+--------------------+-------------+","Ironic driver must wait for power state changes

The Ironic virt driver was previously returning from power_on,
power_off, and reboot calls immediately after making the request to
Ironic -- but before Ironic had necessarily acted upon it. This could
lead to Nova acting as though the state change was complete before it
actually was.

This patch introduces a new private method which polls Ironic until it
signals that the power state change is complete. With this, the
power_on, power_off, and reboot methods wait for the action to complete
before returning to ComputeManager.

Closes-bug: #1310135

Change-Id: I0bbdd8bc8807a0df43a420825f497842b387c4d5"
846,6e287c0f2bb7d4994d50f1763f412277e4dac6f7,1382467073,,1.0,147,157,15,4,1,0.357036224,True,7.0,6042563.0,41.0,17.0,False,90.0,115281.0,308.0,8.0,38.0,1014.0,1046.0,38.0,985.0,1017.0,2.0,730.0,730.0,0.002692998,0.656193896,0.656193896,1638,1235358,1235358,cinder,6e287c0f2bb7d4994d50f1763f412277e4dac6f7,1,0,“The problem appears to be in cinder when using the gluster or nfs drivers.”,"Bug #1235358 in Cinder: ""invalid volume when source image virtual size is bigger than the requested size""","I created a volume from an image and booted an instance from it
when instance boots I get this: 'selected cylinder exceeds maximum supported by bios'
If I boot an instance from the same image I can boot with no issues so its just booting from the volume.","Fail when image is bigger than the volume

When creating a new volume from a qcow2 image stored in glance, we will
be given the physical size of the image instead of the virtual size.
Most drivers will convert that image to raw after downloading for
resizing it to the requested volume size afterwards.

If the virtual size of the image is bigger than the requested one, the
resulting volume might end up being unusable after the resize even
though the creation went good.

This patch will make the volume creation fail if the virtual size of
the image exceeds the one of the requested volume.

Closes-Bug: #1235358
Change-Id: I254cd9e3acf5d9eb7eb913b37d27a14e97568dec"
847,6e30bbc126a1ad4eb87e735c812832f4fcc0054e,1380847693,,1.0,9,5,2,2,1,0.591672779,True,3.0,591041.0,19.0,10.0,False,13.0,1270636.0,19.0,5.0,2284.0,4066.0,4459.0,1975.0,3472.0,3807.0,2143.0,3010.0,3367.0,0.345583495,0.485332044,0.542875564,1628,1234479,1234479,nova,6e30bbc126a1ad4eb87e735c812832f4fcc0054e,1,1, ,"Bug #1234479 in OpenStack Compute (nova): ""ipmi power probing is too fast""","baremetal nodes are failing to boot for me:
2013-10-03 01:55:15,392.392 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpMxP1Xr power on execute /opt/stack/venvs/nova/local/lib/pyt
hon2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,573.573 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power Control: Up/On
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
2013-10-03 01:55:15,707.707 7354 DEBUG nova.openstack.common.processutils [-] Running cmd (subprocess): ipmitool -I lanplus -H 10.10.16.27 -U Administrator -f /tmp/tmpRYI8dn power status execute /opt/stack/venvs/nova/local/lib
/python2.7/site-packages/nova/openstack/common/processutils.py:147
2013-10-03 01:55:15,890.890 7354 DEBUG nova.virt.baremetal.ipmi [-] ipmitool stdout: 'Chassis Power is off
', stderr: '' _exec_ipmitool /opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/virt/baremetal/ipmi.py:136
Note the gap between up and status checking is only 200ms (from IPMI return to status check) but in that interval this hardware doesn't toggle to on.
After the deploy fails, a manual check with
ipmitool -I lanplus -H 10.10.16.40 -U Administrator -P <password> chassis power status
returns
Chassis Power is on","Baremetal: Be more patient with IPMI and BMC

Before we called 'power status; power on' in a loop which made the
IPMI/BMCs not behave well.  Also the total time we would wait (2.5
seconds wasn't always enough).  So make sure power on/off is only called
once and wait up to 10 seconds for the power state change to go into
effect.

This patch has been tested on real baremetal using
https://wiki.openstack.org/wiki/TripleO/TripleOCloud

This bug is also linked to ironic so the change will be made there as
well.

Change-Id: I5a4d7c84ebdf9c1f7d8d0570dbc31764c31f1fc6
Closes-Bug: #1234479"
848,6e5f6041a38ddb0b2643dff66d264f3594f1a875,1394651115,1.0,1.0,45,24,2,2,1,0.886540893,True,11.0,4413171.0,133.0,41.0,False,39.0,71562.0,64.0,3.0,720.0,1936.0,2567.0,614.0,1695.0,2222.0,212.0,1851.0,1978.0,0.028089147,0.244230516,0.260978505,600,1024,1291565,nova,6e5f6041a38ddb0b2643dff66d264f3594f1a875,0,0,remove unnecessary querying,"Bug #1291565 in OpenStack Compute (nova): ""validate_networks does unnecessary querying to neutron in some cases""","This patch optimizes validate_networks so that it only queries neutron
when needed. Previously, this method would perform an additional net_list,
list_ports, and show_quota regardless if a request contains only
port_ids. If a request only contains port ids we do not need to check neutron
for quota as these ports are already allocated.","Optimize validate_networks to query neutron only when needed

This patch optimizes validate_networks so that it only queries neutron
when needed. Previously, this method would perform an additional net_list,
list_ports, and show_quota regardless if a request contains only
port_ids. If a request only contains port ids we do not need to check neutron
for quota as these ports are already allocated.

Change-Id: Ia7abc7d95b663f165fc95385dda3bab13b7a43e6
Closes-bug: #1291565"
849,6e8a094c48dc77fdfe68c72c7a09b7819710d4b6,1383548532,1.0,1.0,73,16,6,6,1,0.852043477,True,11.0,2781143.0,75.0,44.0,False,112.0,297395.0,356.0,3.0,68.0,1907.0,1959.0,68.0,1426.0,1478.0,51.0,1693.0,1728.0,0.007986484,0.260175088,0.265550607,45,373,1247106,nova,6e8a094c48dc77fdfe68c72c7a09b7819710d4b6,1,1,Inconsistent states,"Bug #1247106 in OpenStack Compute (nova): ""SHUTOFF instance resize status does not match ACTIVE instance resize status""","OpenStack allows an instance to be resized in either a SHUTOFF or ACTIVE state.  However, the resize behavior between the states is not consistent.  In particular, resize of an ACTIVE instance will result in the instance's status to be mapped to RESIZE while a resize task is in progress.  The same is not true of SHUTOFF
Before resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | None       | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | ACTIVE  | None       | Running     | VLAN164=10.164.0.16  |
During resize request:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | resize_finish | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | RESIZE  | resize_prep   | Running     | VLAN164=10.164.0.16  |
After resize and confirmation:
| f13dd4a2-eb8f-4a8b-91a1-dafed08049ad | rtheis-migrate-test-1    | SHUTOFF | None             | Shutdown    | VLAN164=10.164.0.14  |
| 813e5a44-41ba-4ee7-8b7b-442d3fc017a7 | rtheis-sce-test-1        | ACTIVE  | None       | Running     | VLAN164=10.164.0.16  |","Add status mapping for shutoff instance when resize

Current code always map instance's status into Shutoff when
getting status of resizing instance with power_state shutoff.
This isn't consistent with active instance, we should add
status mapping for power-off resizing instance.

Closes-Bug: #1247106
Co-Authored-By: chenxiao <chenxiao@cn.ibm.com>
Change-Id: Id4bff71b2c46ec9dccea6d8ac36eba273e48ea5d"
850,6ed57972093835f449ad645b3783bbb8b3c4245e,1412368863,,1.0,24,4,8,8,1,0.995076045,False,,,,,True,,,,,,,,,,,,,,,,,1388,1858,1378786,nova,6ed57972093835f449ad645b3783bbb8b3c4245e,0,0,“Update rpc version aliases for juno”,"Bug #1378786 in OpenStack Compute (nova): ""Update rpc version aliases for juno""","Update all of the rpc client API classes to include a version alias
for the latest version implemented in Juno.  This alias is needed when
doing rolling upgrades from Juno to Kilo.  With this in place, you can
ensure all services only send messages that both Juno and Kilo will
understand.","Update rpc version aliases for juno

Update all of the rpc client API classes to include a version alias
for the latest version implemented in Juno.  This alias is needed when
doing rolling upgrades from Juno to Kilo.  With this in place, you can
ensure all services only send messages that both Juno and Kilo will
understand.

Closes-bug: #1378786
Change-Id: Ia81538130bf8530b70b5f55c7a3d565903ff54b4
(cherry picked from commit f98d725103c53e767a1cddb0b7e2c3822309db17)"
851,6ee68137c5ea9af99c1db4e2a2ce555f43a9e3e1,1381473321,,1.0,6,2,1,1,1,0.0,True,12.0,16110506.0,63.0,18.0,False,22.0,216439.0,47.0,6.0,66.0,2772.0,2791.0,66.0,2538.0,2557.0,63.0,1667.0,1683.0,0.010191083,0.265605096,0.268152866,1674,1238430,1238430,nova,6ee68137c5ea9af99c1db4e2a2ce555f43a9e3e1,1,1,“The error message is not sufficient enough for debug of hyper-v”,"Bug #1238430 in OpenStack Compute (nova): ""When resize failed in hyperv, the error message isn't sufficient for debugging.""","2013-10-09 18:43:35.867 5236 ERROR nova.compute.manager [req-a8a83c04-03b4-4579-897f-a9825d17231e 657da0d8cf4440eca4a4ebad6fa1248a 0f7a0b0633c64e8888e6324bfe88dc16] [instance: a278d507-2112-40c2-912b-84b104300ed7] Error: Cannot resize a VHD to a smaller size
It would be helpful to debug the original and newer size in this case.","Hyper-v: Change the hyper-v error log for debug when resize failed

The error message is not sufficient enough for debug of hyper-v
when resize error occured, this patch adding the necessary info
in error log.

Change-Id: Ic8ecb8f08139face2c7bdca258de5e719d81833c
Closes-bug: #1238430"
852,6f24ca2496e37f3f75496c0e4191e71e55fd7b5d,1403891853,1.0,1.0,6,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1002,1444,1333370,cinder,6f24ca2496e37f3f75496c0e4191e71e55fd7b5d,0,0,"“ This moves and renames nova.utils.cpu_count() utility method from nova
    to oslo. Added in commit 75c96a48fc7e5dfb59d8258142b01422f81b0253, this”","Bug #1333370 in Cinder: ""Set osapi_volume_workers equal to number of CPUs""","I started a change in devstack here but we decided to move this into Cinder itself:
https://review.openstack.org/#/c/98492/
Nova made the same change for it's API and conductor workers in Icehouse:
https://review.openstack.org/#/c/69266/","Use (# of CPUs) osapi_volume_workers by default

This changes the default number of cinder API workers to be equal to the
number of CPUs available on the host, rather than defaulting to 1 as it
did before.

Commit 75c96a48fc7e5dfb59d8258142b01422f81b0253 did the same thing in
Nova in Icehouse. Similar changes are being made to Glance and Trove as
well.

DocImpact: osapi_volume_workers will now be equal to the number of
           CPUs available by default if not explicitly specified
           in cinder.conf.

UpgradeImpact: Anyone upgrading to this change that does not have
           osapi_volume_workers specified in cinder.conf will now be
           running multiple API workers by default when they restart
           the cinder-api service.

Closes-Bug: #1333370

Change-Id: I8dec104800c18618e0c8422bbb93d5dc19c12876"
853,6fcb34b825b6fe4ae750f6d38d9cf62fa0254a30,1407221462,,1.0,40,30,5,5,1,0.878967624,False,,,,,True,,,,,,,,,,,,,,,,,622,1047,1292644,nova,6fcb34b825b6fe4ae750f6d38d9cf62fa0254a30,0,0, nova.compute.api should return Objects ,"Bug #1292644 in OpenStack Compute (nova): ""nova.compute.api should return Objects""","nova.compute.api should return Aggregate Objects, and they should be converted into the REST API expected results in the aggregates API extensions.","Making nova.compute.api to return Aggregate Objects

nova.compute.api should return Objects and they should be
converted into the REST API expected result in the API extensions

Change-Id: I4bfd83ab92c719b1776da6445e6a4ea78d2ddfd6
Closes-Bug: #1292644"
854,7026d96ce44522c95c785376178347d2b0a9d750,1395089635,,1.0,19,6,2,2,1,0.721928095,True,1.0,226963.0,29.0,4.0,False,2.0,944681.0,6.0,4.0,8.0,1337.0,1338.0,8.0,1144.0,1145.0,5.0,750.0,750.0,0.006085193,0.761663286,0.761663286,646,1072,1294368,neutron,7026d96ce44522c95c785376178347d2b0a9d750,1,1,,"Bug #1294368 in neutron: ""Hyper-V agent security groups rules are not applied correctly""","The Hyper-V agent in Windows Server 2012 R2 is currently applying the security group rules wrong due to two reasons:
First, when assigning a weight for a rule, the agent takes into account the weight of the default reject rules, which causes certain reject rules to have a higher priority than allow rules.
Secondly, if a rule with no port_range_min and no port_range_max defined is applied first, it is replaced by the rules with the port range defined. This can cause faulty connectivity for the virtual machine, including for the default security group.","Fixes Hyper-V agent security groups enable issue

Fixes the weight of the applied allow rules by ignoring
the weight of the reject rules.
Fixes the override allow rules issue by fixing the
ACL filtering condition.

Change-Id: I38ddd7142d0fa45f308460153d29580f276ce07e
Closes-Bug: #1294368"
855,7028f074202a59111f461b9fa935f44b6d118f41,1403847489,,1.0,2,4,3,2,1,0.920619836,False,,,,,True,,,,,,,,,,,,,,,,,1016,1459,1334938,nova,7028f074202a59111f461b9fa935f44b6d118f41,1,1,,"Bug #1334938 in OpenStack Compute (nova): ""established floating ip connection won't get disconnected after disassociating floating ip""","Established connections via floating ip won't get disconnected after we disassociating that floating ip.
As mentioned in maillist by Vish, we should move clean_conntrack() from migration code to  remove_floating_ip.
https://github.com/openstack/nova/blob/master/nova/network/floating_ips.py#L575","Clean conntrack records when removing floating ip

Established connections via floating ip won't get disconnected after
disassociating.
This patch move clean_conntrack from migration code to remove_floating_ip to
make sure that established connections on floating ip will get interrupted
immediately after disassociating.

Change-Id: I88d258ea39ea7c76e4cc60a636eb1e3e6aaa6f22
Closes-Bug: #1334938"
856,702e1fbf7ad5dd961dfd35cd6a0e54d4d6da5e34,1389811577,0.0,1.0,35,2,2,2,1,0.84185219,True,7.0,4229356.0,103.0,33.0,False,24.0,79111.0,39.0,5.0,70.0,1429.0,1439.0,70.0,1296.0,1306.0,65.0,608.0,617.0,0.094827586,0.875,0.887931034,294,702,1269567,neutron,702e1fbf7ad5dd961dfd35cd6a0e54d4d6da5e34,1,1,performance issue,"Bug #1269567 in neutron: ""L3 agent making RPC calls to get external network id""","In _process_routers, the L3 agent makes an RPC call each time that _process_routers is called to get the external network id as long as it was not configured using the gateway_external_network_id configuration option.
This adds some time process a router.  Since the external id will not be changing, we should be able to get away with fetching and saving this value once.","L3 agent fetches the external network id once

Rather than fetching the id of the external network each time that
_process_routers is called, get it once and remember it.  If the agent
is ever requested to connect to a different ext-net then it will fetch
the current ext-net to double check for the unlikely event that the
ext-net has changed.  If it has then it will remember the new ext-net.

This is only applicable in the case where there is only one ext-net
that has not been configured explicitly in the config file.  That was
the only case that would cause an RPC message in the first place.

Change-Id: I40bbbf6233131ea5d40122ef9495fd3cb7dc823a
Closes-Bug: #1269567"
857,7056ec6a16fd8707564ec4b0a05cab461ee2a80d,1401895668,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,948,1384,1326429,swift,7056ec6a16fd8707564ec4b0a05cab461ee2a80d,1,1,,"Bug #1326429 in OpenStack Object Storage (swift): ""formpost middleware crashes for uploads larger 2 GiB""","If a user tries to upload a file that is larger than 2GiB the upload fails due to an error OverflowError.
Actually this happens in python/socket.py, but it can be fixed either in swift/common/utils.py line 2406 or swift/common/middleware/formpost.py line 245. In both cases it's possible to use a long(-1) as the default instead of None (which would be converted to an int(-1) later on).
Note that this happens on 64bit systems.","Fix file uploads > 2 GiB in formpost middleware

Formpost middleware fails to upload files larger then 2 GiB due to
an Overflow error. The reason is that the readline() will use a
readline(int(content_length)) later on and fail if it is larger than
2GiB.  Since it is not required to read the whole content into memory
to detect the boundary only read the amount of required bytes.

The underlying error is located in Python 2.7 and is related to
cStringIO: http://bugs.python.org/issue7358

Closes-Bug: #1326429
Change-Id: I196edda647921c2691d278cebd1cca80ebd360f2"
858,706a8b1ff30c011169846ed5e38f06aa9f15faf2,1393273129,,1.0,39,4,2,2,1,0.693127415,True,4.0,183615.0,63.0,13.0,False,6.0,529206.5,17.0,2.0,10.0,1063.0,1071.0,9.0,919.0,927.0,2.0,607.0,608.0,0.003496503,0.708624709,0.70979021,470,885,1284314,neutron,706a8b1ff30c011169846ed5e38f06aa9f15faf2,0,0,No bug. ‘stats table needs columns to be bigint’,"Bug #1284314 in neutron: ""poolstatisticss table should have columns of type bigint""","2014-02-24 17:44:45.585 5282 TRACE neutron.openstack.common.db.sqlalchemy.session
2014-02-24 17:44:45.589 5282 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/amqp.py"", line 438, in _process_data
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/common/rpc.py"", lin
e 44, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/rp
c/dispatcher.py"", line 172, in dispatch
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/services/loadbalanc
er/drivers/stingray/plugin_driver.py"", line 155, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.plugin.update_pool_stats(context, pool_id, data=stats)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/db/loadbalancer/loa
dbalancer_db.py"", line 500, in update_pool_stats
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.update_status(context, Member, member, stats_status)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 447, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.rollback()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/util/langhelpers
.py"", line 58, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     compat.reraise(exc_type, exc_value, exc_tb)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 444, in __exit__
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 354, in commit
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"",
 line 334, in _prepare_impl
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/db
/sqlalchemy/session.py"", line 545, in _wrap
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp DBError: (DataError) integer out of range
2014-02-24 17:44:45.589 5282 TRACE neutron.openstack.common.rpc.amqp  'UPDATE poolstatisticss SET bytes_in=%(bytes_in)s, bytes_out=%(byte
s_out)s, active_connections=%(active_connections)s, total_connections=%(total_connections)s WHERE poolstatisticss.pool_id = %(poolstatist
icss_pool_id)s' {'bytes_in': 290286902, 'poolstatisticss_pool_id': u'0df91b58-e04b-4a36-bed8-8fcb3f73ed6e', 'total_connections': 2362054,
 'active_connections': 1, 'bytes_out': 3152985859}","stats table needs columns to be bigint

Bandwidth measurements trivially overrun 32bit counters.  Change storage
type to BigInteger from Integer.

Closes-Bug: #1284314
Change-Id: I20db25f374de66b8443ff50bac979bff634d8a14
Signed-off-by: Stephen Gran <stephen.gran@guardian.co.uk>"
859,7070e4a66f576b2e6e4a4f1121445200e7854942,1398732798,1.0,1.0,275,15,10,8,1,0.728296646,True,8.0,6210598.0,85.0,26.0,False,145.0,347243.5,459.0,2.0,11.0,1399.0,1407.0,11.0,1395.0,1403.0,3.0,1372.0,1372.0,0.000506907,0.173995691,0.173995691,52,380,1247844,nova,7070e4a66f576b2e6e4a4f1121445200e7854942,0,0,The client needs to provide a more granular exception. Feature,"Bug #1247844 in OpenStack Compute (nova): ""PortLimit thrown when specifying used fixed ip""","PortLimit thrown when specifying used fixed ip
The nova/network/neutronv2/api.py _create_port will catch errors thrown from the neutron client.  The code asserts that a 409 error is an over-quota error.  However, this will hide other errors that may be occurring within the system.
The code is currently taking a calculated risk by assuming that all 409 errors that come through this code path will be over quota's. However, another high traffic code path is specifying a fixed_ip.  If a user specifies a fixed ip address, this code will now incorrectly throw a PortLimitExceeded error.  This leads the users to believe that they have run out of their quota limit.
Example exception (then wrapped exception):
NV-6FC38FD Neutron error creating port on network 7d360984-e12c-4bb3-819d-4b93c4ca4269
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 182, in _create_port
    port_id = port_client.create_port(port_req_body)['port']['id']
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 108, in with_params
    ret = self.function(instance, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 308, in create_port
    return self.post(self.ports_path, body=body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1188, in post
    headers=headers, params=params)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1111, in do_request
    self._handle_fault_response(status_code, replybody)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 1081, in _handle_fault_response
    exception_handler_v20(status_code, des_error_body)
  File ""/usr/lib/python2.6/site-packages/neutronclient/v2_0/client.py"", line 93, in exception_handler_v20
    message=msg)
NeutronClientException: 409-{u'NeutronError': {u'message': u'Unable to complete operation for network 7d360984-e12c-4bb3-819d-4b93c4ca4269. The IP address 10.0.0.2 is in use.', u'type': u'IpAddressInUse', u'detail': u''}}
Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1236, in _allocate_network_async
    dhcp_options=dhcp_options)
  File ""/usr/lib/python2.6/site-packages/nova/network/api.py"", line 49, in wrapper
    res = f(self, context, *args, **kwargs)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 358, in allocate_for_instance
    LOG.exception(msg, port_id)
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 335, in allocate_for_instance
    security_group_ids, available_macs, dhcp_opts))
  File ""/usr/lib/python2.6/site-packages/nova/network/neutronv2/api.py"", line 191, in _create_port
    raise exception.PortLimitExceeded()
PortLimitExceeded: Maximum number of ports exceeded","Support detection of fixed ip already in use

Update the validate_networks method to detect when a fixed ip is
already in use and raise the FixedIpAlreadyInUse exception

Change-Id: I07f83ce654c909b1158315dfc21a40654bc34a47
Closes-Bug: #1247844"
860,7090595658c2f931008fda62d612ea702509e930,1408837003,,1.0,4,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1228,1687,1360719,nova,7090595658c2f931008fda62d612ea702509e930,0,0,Bug in test,"Bug #1360719 in OpenStack Compute (nova): ""unit test test_killed_worker_recover taking 160 seconds""","nova patch https://review.openstack.org/#/c/104099/ caused the following unit tests to take 160 seconds:
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITest.test_killed_worker_recover
nova.tests.integrated.test_multiprocess_api.MultiprocessWSGITestV3.test_killed_worker_recover
This is because Server.wait() now waits for all workers to finish, but test_killed_worker_recover doesn't attempt to kill the workers like some of the other tests in MultiprocessWSGITest","Make test_killed_worker_recover faster

I3f7bd0ba5ef1bb716a97635252abe251053a669d changed how server.wait()
works so that it wouldn't return until all children are done. But
test_killed_worker_recover doesn't try to kill the children, unlike other
tests in test_multiprocess_api. This didn't cause the unit test to fail
because tearDown catches the unit test timeout, fixtures.TimeoutException,
which triggers after 160 seconds.

Now instead of taking 160 seconds test_killed_worker_recover, which is
run twice, takes under 3 seconds.

Change-Id: I8cbd08cb873cd81a65005764759ac9ca23fa6430
Closes-Bug: #1360719"
861,709410d243a97d35c3da314b41bab039eac75736,1387097597,,2.0,52,8,3,2,1,0.923339384,True,9.0,4704688.0,57.0,21.0,False,10.0,54695.0,16.0,3.0,5.0,3434.0,3435.0,5.0,2290.0,2291.0,5.0,3132.0,3133.0,0.000874508,0.456638974,0.456784725,165,567,1259267,nova,709410d243a97d35c3da314b41bab039eac75736,1,1,,"Bug #1259267 in OpenStack Compute (nova): ""Nova Docker: Metadata service doesn't work""","I was playing around with cloud-init. I wanted to use cloud init as substitute for the missing environment variables feature.
The basic idea is to define variables in the user data and inject the them before starting the service.
Following:
docker run -e ""MY_Variable=MyValue"" -d centos
would look like in openstack:
nova boot --image centos:latest --user-data ""MY_Variable=MyValue"" myinstance
Unfortunately does the metadata service not work inside of a docker container. After some testing I figured out that the reason for that is that a container uses as default gateway the docker network (docker ip address). The metadata service simple rejects the call since the IP address of docker container is not associated with the nova instance.
Note: The metadata service itself can be accessed (http://169.254.169.254) but it is not possible to access the actual data (http://169.254.169.254/2009-04-04 - Status 404)
I was able to work around the issue by simply changing the route inside the container:
# Hack: In order to receive data from the metadata service we must make sure we resolve the data via our nova network.
#
# A docker container in openstack has two NICs.
# - eth0 has a IP address on the docker0 bridge which is usually an e.g. 172.0.0.0 IP address.
# - pvnetXXXX is a IP address assigned by nova.
#
# Extract the NIC name of the nova network.
#
NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
while [ ""$NOVA_NIC"" == """" ] ; do
   echo ""Find nova NIC...""
   sleep 1
   NOVA_NIC=$(ip a | grep pvnet | head -n 1 | cut -d: -f2)
done
echo ""Device $NOVA_NIC found. Wait until ready.""
sleep 3
# Setup a network route to insure we use the nova network.
#
echo ""[INFO] Create default route for $NOVA_NIC. Gateway 10.0.0.1""
ip r r default via 10.0.0.1 dev $NOVA_NIC
# Shutdown eth0 since icps will fetch enabled enterface for streaming.
ip l set down dev eth0
This approach is obviously a poor solution since it has certain expectation of the network.
Another solution might be extend the docker driver to add a firewall rule which will masquerade requests on 169.254.169.254 with the actual nova network IP address
I third solution would need improvements in docker. If docker would have a network mode which allows to assign the IP from outside this issue would be solved. That of course is a just which must be accepted by the docker community.
Network Threads:
- https://groups.google.com/forum/#!topic/docker-dev/YfCeX8TBweA
Simple script to test metadata inside the container:
#!/bin/bash
status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
while [ $status != '200' ]; do
   echo ""Cannot access metadata, status: '$status', try again...""
   date # easier to see in docker logs that loop is still running.
   status=$(curl -I -s -o /dev/null -w ""%{http_code}"" http://169.254.169.254/2009-04-04)
   sleep 1
done
echo ""Yes we got some user data:""
curl http://169.254.169.254/2009-04-04","Nova Docker: Metadata service doesn't work

The metadata services rejects the request because the remote IP address
belongs to the docker network. The remote IP address is correct by using
the nova network as default route.

Change-Id: I410a9bfea560f669515b31db7f007515b4d5c4e7
Closes-Bug: #1259267
Closes-Bug: #1261021"
862,709410d243a97d35c3da314b41bab039eac75736,1387097597,,2.0,52,8,3,2,1,0.923339384,True,9.0,4704688.0,57.0,21.0,False,10.0,54695.0,16.0,3.0,5.0,3434.0,3435.0,5.0,2290.0,2291.0,5.0,3132.0,3133.0,0.000874508,0.456638974,0.456784725,188,591,1261021,nova,709410d243a97d35c3da314b41bab039eac75736,1,1,,"Bug #1261021 in OpenStack Compute (nova): ""Nova Docker network in container has wrong network mask""","Network in container has wrong network mask.
In devstack you have a default network defined like following:
    nova network-list
    +--------------------------------------+---------+-------------+
    | ID                                   | Label   | Cidr        |
    +--------------------------------------+---------+-------------+
    | f7c6e98d-d900-4df2-8523-0c8dd3a4ad7f | private | 10.0.0.0/24 |
    +--------------------------------------+---------+-------------+
If I start up a new container via nova and look at the created network device:
    sudo ip netns exec <container-id> ip a | grep pvnetr
    66: pvnetr70121: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP qlen 1000
        inet 10.0.0.2/8 brd 10.255.255.255 scope global pvnetr70121
Then you can see that it has a 10.0.0.2/8 network.
I would expect to find a 10.0.0.2/24","Nova Docker: Metadata service doesn't work

The metadata services rejects the request because the remote IP address
belongs to the docker network. The remote IP address is correct by using
the nova network as default route.

Change-Id: I410a9bfea560f669515b31db7f007515b4d5c4e7
Closes-Bug: #1259267
Closes-Bug: #1261021"
863,70f7761e6f053297266fd5cc63212e74e73ec1cc,1391511574,,1.0,3,31,2,2,1,0.997502546,True,14.0,3865555.0,134.0,44.0,False,197.0,324359.0,1091.0,5.0,21.0,3782.0,3785.0,21.0,2907.0,2910.0,21.0,3611.0,3614.0,0.003036158,0.498481921,0.498895943,1820,1274317,1274317,Nova,70f7761e6f053297266fd5cc63212e74e73ec1cc,1,1,"""the case where we do want the method to execute very often was missed""","Bug #1274317 in OpenStack Compute (nova): ""heal_instance_info_cache_interval config is not effective""","There is configuration item in /etc/nova/nova.conf that controls how often the instance info should be updated. By default the value is 60 seconds. However, the current implementation only uses that value to prevent over clocked.  Configure it to a different value in nova.conf does not has impact how often the task is executed.
If I change the code in  /usr/lib/python2.6/site-packages/nova/compute/manager.py with the spacing parameter, the configured value will be in action. Please fix this bug.
@periodic_task.periodic_task(spacing=CONF.heal_instance_info_cache_interval)
    def _heal_instance_info_cache(self, context):","Specify spacing on periodic_tasks in manager.py

Some methods in compute/manager.py were using the @periodic_task
decorator to ensure they were called regularly, but were not passing
in the `spacing` kwarg.  In this case the method can only be called at
the default rate, calculated as min(60 seconds, <spacings from all
periodic_tasks>), regardless of the poll interval set in config.

Although there was throttling code which prevented the methods from
executing too often, the case where we do want the method to execute
very often was missed.  This patch ensures that setting the config
values low will cause the method to be called as frequently.

Change-Id: I1530c8b3b6ac71f4d9e2e41188c0eca4a6556e01
Closes-Bug: #1274317"
864,70feb102ae86d3421e46a07710e4cbab35408d02,1405371694,,1.0,64,14,3,2,1,0.778608228,False,,,,,True,,,,,,,,,,,,,,,,,1064,1512,1341738,nova,70feb102ae86d3421e46a07710e4cbab35408d02,1,1,,"Bug #1341738 in OpenStack Compute (nova): ""Multiple delete_on_terminate volumes may not get deleted""","There's no catching of exceptions in _cleanup_volumes in compute/manager.py so a raised exception will short circuit cleaning up later volumes.
    def _cleanup_volumes(self, context, instance_uuid, bdms):
        for bdm in bdms:
            LOG.debug(""terminating bdm %s"", bdm,
                      instance_uuid=instance_uuid)
            if bdm.volume_id and bdm.delete_on_termination:
                self.volume_api.delete(context, bdm.volume_id)","Defer raising an exception when deleting volumes

When looping through block device mappings to delete volumes with
delete_on_terminate set the loop should not exit on an exception.
Instead the loop will continue and the last exception can optionally be
raised.

Change-Id: If1c6ec30c6f49009ef42f9a3b3df1594783b0ae5
Closes-Bug: #1341738"
865,710cf4e3a7ee3b89e462ec5ff2291e1177556819,1384238203,1.0,1.0,12,5,2,2,1,0.873981048,True,5.0,6775105.0,61.0,32.0,False,11.0,433447.0,24.0,6.0,1.0,1639.0,1639.0,1.0,1397.0,1397.0,1.0,1563.0,1563.0,0.000304368,0.238015523,0.238015523,1748,1250324,1250324,nova,710cf4e3a7ee3b89e462ec5ff2291e1177556819,1,1, ,"Bug #1250324 in OpenStack Compute (nova): ""Resize on hyperV does not preserve the config drive image""","When we configure this item config_drive_cdrom=True on hyperV, then boot an instance using config drive. A config drive image named 'configdrive.iso' will be created. But when we resize this instance, The config drive image will not preserved in the instance.","Hyper-V:Preserve config drive image after the instance is resized

If we configure config_drive_cdrom=True in nova.conf,
the config drive image will be lost after the instance
is resized on hyperV. This is because nova compute will
not copy the disk file if the file's ResourceSubType
is not 'Virtual Hard Disk'.

This commit adds 'Virtual CD/DVD Disk' into disk_resources
to preserve the config drive image after the instance is
resized.

Closes-Bug: #1250324

Change-Id: Ia125a8b132d194ddc2230754fd1b813698b410a8"
866,717a0a0ab4436e2904360fe4e34226af7d0e412d,1398859498,,1.0,2,2,2,2,1,1.0,True,1.0,87405.0,14.0,4.0,False,30.0,1031709.0,55.0,4.0,282.0,2198.0,2368.0,273.0,1856.0,2024.0,221.0,377.0,524.0,0.185929648,0.316582915,0.439698492,847,1277,1314472,neutron,717a0a0ab4436e2904360fe4e34226af7d0e412d,1,1,,"Bug #1314472 in neutron: ""ofagent: AttributeError: 'OFANeutronAgent' object has no attribute 'updated_ports'""","OFANeutronAgent.__init__() calls setup_rpc(), which spawns a thread to consume RPCs.
self.updated_ports, which is accessed by an RPC handler, port_update(), has not been initialized at this point.","OVS and OF Agents: Create updated_ports attribute before setup_rpc

setup_rpc spawns rpc-consuming thread that can access
self.updated_ports before it is created in the constructor

Change-Id: Icc447c3bb7099125aae30e2430a6ad98ba00f6f9
Closes-Bug: #1314472"
867,71853a32111ec9a36f8a2082b41b52519b66be85,1395033878,,1.0,4,19,1,1,1,0.0,True,3.0,216047.0,19.0,8.0,False,18.0,2138314.0,25.0,2.0,419.0,1363.0,1488.0,353.0,1099.0,1221.0,405.0,1206.0,1327.0,0.265186153,0.788373612,0.867406924,591,1013,1291108,cinder,71853a32111ec9a36f8a2082b41b52519b66be85,0,0,tests,"Bug #1291108 in Cinder: ""test_force_delete_snapshot fails with no volume device file path""","2014-03-11 18:50:26.524 | FAIL: cinder.tests.api.contrib.test_admin_actions.AdminActionsTest.test_force_delete_snapshot
2014-03-11 18:50:26.524 | tags: worker-0
2014-03-11 18:50:26.524 | ----------------------------------------------------------------------
2014-03-11 18:50:26.524 | Empty attachments:
2014-03-11 18:50:26.524 |   pythonlogging:''-1
2014-03-11 18:50:26.524 |   stderr
2014-03-11 18:50:26.524 |   stdout
2014-03-11 18:50:26.524 |
2014-03-11 18:50:26.525 | pythonlogging:'': {{{
2014-03-11 18:50:26.525 | Starting cinder-volume node (version 2014.1)
2014-03-11 18:50:26.525 | Starting volume driver FakeISCSIDriver (2.0.0)
2014-03-11 18:50:26.525 | volume cedd21ea-cd89-45ca-82d8-5a7cf1e0e6b5: skipping export
2014-03-11 18:50:26.525 | Updating volume status
2014-03-11 18:50:26.525 | Initializing extension manager.
2014-03-11 18:50:26.525 | Loaded extension: os-vol-tenant-attr
2014-03-11 18:50:26.525 | Loaded extension: os-types-extra-specs
2014-03-11 18:50:26.525 | Loaded extension: os-vol-host-attr
2014-03-11 18:50:26.525 | Loaded extension: os-volume-encryption-metadata
2014-03-11 18:50:26.525 | Loaded extension: OS-SCH-HNT
2014-03-11 18:50:26.525 | Loaded extension: os-availability-zone
2014-03-11 18:50:26.526 | Loaded extension: os-vol-image-meta
2014-03-11 18:50:26.526 | Loaded extension: os-snapshot-actions
2014-03-11 18:50:26.526 | Loaded extension: os-quota-sets
2014-03-11 18:50:26.526 | Loaded extension: os-volume-actions
2014-03-11 18:50:26.526 | Loaded extension: os-volume-manage
2014-03-11 18:50:26.526 | Loaded extension: os-image-create
2014-03-11 18:50:26.526 | Loaded extension: qos-specs
2014-03-11 18:50:26.526 | Loaded extension: backups
2014-03-11 18:50:26.526 | Loaded extension: encryption
2014-03-11 18:50:26.526 | Loaded extension: os-used-limits
2014-03-11 18:50:26.526 | Loaded extension: os-types-manage
2014-03-11 18:50:26.526 | Loaded extension: os-vol-mig-status-attr
2014-03-11 18:50:26.527 | Loaded extension: os-extended-services
2014-03-11 18:50:26.527 | Loaded extension: os-quota-class-sets
2014-03-11 18:50:26.527 | Loaded extension: os-volume-transfer
2014-03-11 18:50:26.527 | Loaded extension: os-volume-unmanage
2014-03-11 18:50:26.527 | Loaded extension: os-hosts
2014-03-11 18:50:26.527 | Loaded extension: os-extended-snapshot-attributes
2014-03-11 18:50:26.527 | Loaded extension: os-services
2014-03-11 18:50:26.527 | Loaded extension: os-admin-actions
2014-03-11 18:50:26.527 | POST http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action
2014-03-11 18:50:26.527 | http://localhost/v2/fake/snapshots/5ccb04ea-6a62-4e57-9299-e8ae34ecc34b/action returned with HTTP 202
2014-03-11 18:50:26.527 | Arguments dropped when creating context: {'user': 'admin', 'tenant': 'fake'}
2014-03-11 18:50:26.527 | snapshot 5ccb04ea-6a62-4e57-9299-e8ae34ecc34b: deleting
2014-03-11 18:50:26.528 | Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Exception during message handling: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.528 | Traceback (most recent call last):
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-11 18:50:26.528 |     incoming.message))
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-11 18:50:26.528 |     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-11 18:50:26.528 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-11 18:50:26.528 |     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-11 18:50:26.528 |   File ""cinder/volume/manager.py"", line 166, in lso_inner1
2014-03-11 18:50:26.528 |     return lso_inner2(inst, context, snapshot_id, **kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/openstack/common/lockutils.py"", line 247, in inner
2014-03-11 18:50:26.529 |     retval = f(*args, **kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 165, in lso_inner2
2014-03-11 18:50:26.529 |     return f(*_args, **_kwargs)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 542, in delete_snapshot
2014-03-11 18:50:26.529 |     {'status': 'error_deleting'})
2014-03-11 18:50:26.529 |   File ""cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-03-11 18:50:26.529 |     six.reraise(self.type_, self.value, self.tb)
2014-03-11 18:50:26.529 |   File ""cinder/volume/manager.py"", line 530, in delete_snapshot
2014-03-11 18:50:26.529 |     self.driver.delete_snapshot(snapshot_ref)
2014-03-11 18:50:26.529 |   File ""cinder/volume/drivers/lvm.py"", line 252, in delete_snapshot
2014-03-11 18:50:26.529 |     self._delete_volume(snapshot, is_snapshot=True)
2014-03-11 18:50:26.530 |   File ""cinder/volume/drivers/lvm.py"", line 128, in _delete_volume
2014-03-11 18:50:26.530 |     self._clear_volume(volume, is_snapshot)
2014-03-11 18:50:26.530 |   File ""cinder/volume/drivers/lvm.py"", line 155, in _clear_volume
2014-03-11 18:50:26.530 |     raise exception.VolumeBackendAPIException(data=msg)
2014-03-11 18:50:26.530 | VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Volume device file path /tmp/tmp.udMSkMDFJg/tmpKYdE0X/tmpSeiWSd/tmpzCdDA-cow does not exist.
2014-03-11 18:50:26.530 | }}}
2014-03-11 18:50:26.530 |
2014-03-11 18:53:19.295 | Traceback (most recent call last):
2014-03-11 18:53:19.295 |   File ""cinder/tests/api/contrib/test_admin_actions.py"", line 312, in test_force_delete_snapshot
2014-03-11 18:53:19.296 |     snapshot['id'])
2014-03-11 18:53:19.296 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 393, in assertRaises
2014-03-11 18:53:19.296 |     self.assertThat(our_callable, matcher)
2014-03-11 18:53:19.296 |   File ""/home/jenkins/workspace/gate-cinder-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-03-11 18:53:19.296 |     raise mismatch_error
2014-03-11 18:53:19.296 | MismatchError: <function snapshot_get at 0x2348848> returned <cinder.db.sqlalchemy.models.Snapshot object at 0x6d02050>
http://logs.openstack.org/68/79568/1/gate/gate-cinder-python27/b823a23/console.html","Simplify test force delete snapshot unit test

Make this test simple in just verifying if we have a snapshot and we do
a force delete that we get back an ok response from the controller.
Removing out RPC call and verifying snapshot was deleted since that's
out of the scope here.

Closes-Bug: #1291108
Change-Id: If10b1057cf170e77527a0933a1ddf1a6a59baa1d"
868,71d900543217b8fe1c9cdc93f1a6503b3ea9c7c5,1386042233,,1.0,32,2,2,2,1,0.787126586,True,5.0,9079115.0,41.0,14.0,False,35.0,9020538.5,71.0,6.0,21.0,3895.0,3911.0,21.0,3341.0,3357.0,8.0,2897.0,2900.0,0.00133077,0.428508059,0.428951649,123,522,1257135,nova,71d900543217b8fe1c9cdc93f1a6503b3ea9c7c5,0,0,Not an error. Just in case.,"Bug #1257135 in OpenStack Compute (nova): ""Make sure ""volumeId"" in request body while attaching volume to server""","We should check whether the para ""volumeId""  is in request body.","Make sure ""volumeId"" in req body on volume actions

We should check whether the parameter ""volumeId"" is in request body while
attaching a volume to an instance and while swapping volume.

Change-Id: I0be2ae4c80615c0304cd1a0d8ca0a4282a0867c7
Closes-Bug: #1257135"
869,71deb11f6fd5186cabcc42431316deb5dbfa3dfe,1393568790,,1.0,1,1,1,1,1,0.0,True,3.0,344264.0,45.0,10.0,False,24.0,717456.0,40.0,3.0,684.0,1210.0,1631.0,589.0,1012.0,1382.0,214.0,632.0,719.0,0.24543379,0.72260274,0.821917808,496,915,1285993,neutron,71deb11f6fd5186cabcc42431316deb5dbfa3dfe,1,1,Crashes,"Bug #1285993 in neutron: ""neutron-server crashes when running on an empty database""","operation:
$ mysql
> create database neutron_ml2 character set utf8;
> exit
$ neutron-server --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini
error:
2014-02-28 15:02:46.550 TRACE neutron ProgrammingError: (ProgrammingError) (1146, ""Table 'neutron_ml2.ml2_vlan_allocations' doesn't exist"") 'SELECT ml2_vlan_allocations.physical_network AS ml2_vlan_allocations_physical_network, ml2_vlan_allocations.vlan_id AS ml2_vlan_allocations_vlan_id, ml2_vlan_allocations.allocated AS ml2_vlan_allocations_allocated \nFROM ml2_vlan_allocations FOR UPDATE' ()
investigation:
This problem introduced by https://review.openstack.org/#/c/74896/ .
Not that this problem does not occur if nuetron-db-manage is run before running neutron-server since ml2_vlan_allocations table is created by neutron-db-manage.
I did skip running neutron-db-manage usually and it was no problem. Is it prohibited now ?","ML2: database needs to be initalized after drivers loaded

Previously, if you started neutron-server with an empty database some
of the tables that drivers use are not automatically created. That said,
one should probably run neutron-db-manage manually to create the tables
and not rely on neutron to do this. This regression was cause in 326b85.

Change-Id: I2c578733de0213945b31fba86a3b0ea45c02295a
Closes-bug: #1285993
Co-Authored-By: Itsuro Oda <oda@valinux.co.jp>"
870,71ecc6ba46433999cf6b6c346670337b875832a0,1376652260,1.0,1.0,6,6,2,2,1,0.979868757,True,2.0,267644.0,10.0,4.0,False,11.0,299571.0,11.0,3.0,2.0,881.0,881.0,1.0,820.0,820.0,2.0,175.0,175.0,0.014925373,0.875621891,0.875621891,1471,1212868,1212868,neutron,71ecc6ba46433999cf6b6c346670337b875832a0,1,1,“ This patch changes L3 agent to enable SNAT by default if plugin doesn't support ext-gw-mode extension.”,"Bug #1212868 in neutron: ""L3 agent won't set up SNAT rules to external networks if plugin doesn't support ext-gw-mode extension""","If neutron plugin doesn't support ext-gw-mode, L3 agent won't set-up SNAT rules on router (By default, it'll assume enable_snat==false). In order to behave like prior to ext-gw-mode extension introduction, it should enbale SNAT rules in that case.","Enable SNAT by default in L3 agents

If ext-gw-mode extension isn't supported by plugin, it won't
return enable_snat param in router info. Agent will currently
default to enable_snat = false, which changes from expected
default behaviour prior to ext-gw-mode introduction.

This patch changes L3 agent to enable SNAT by default if plugin
doesn't support ext-gw-mode extension.

Change-Id: I35e8f8c20392bff9ac2f875f2c9a1038ab06ad7b
Closes-Bug: #1212868"
871,720d5abfc56e4cfdbc135400f4799c2d1d8cd8c3,1386181013,,1.0,7,2,1,1,1,0.0,True,6.0,439750.0,38.0,13.0,False,8.0,7350434.0,14.0,4.0,0.0,242.0,242.0,0.0,220.0,220.0,0.0,193.0,193.0,0.000959693,0.186180422,0.186180422,130,529,1257234,glance,720d5abfc56e4cfdbc135400f4799c2d1d8cd8c3,0,0,Add logs. Add a feature,"Bug #1257234 in Glance: ""HTTP Store is hiding errors from the remote location""","If the remote location fails to return the image and raises an error, the http store doesn't show it in the logs, which makes debugging very difficult:
https://github.com/openstack/glance/blob/master/glance/store/http.py
        # Check for bad status codes
        if resp.status >= 400:
            reason = _(""HTTP URL returned a %s status code."") % resp.status
            raise exception.BadStoreUri(loc.path, reason)","Added error logging for http store

http store logs an error message when the remote location
fails to return the image, returns invalid status or
if it exceeds maximum redirects

Change-Id: I43c74395bc4632a97761ae4c11474bfeb823c85a
Closes-Bug: #1257234"
872,721e7f939859fbfe6b0c79ef3a6d5e43c916da65,1396290929,,1.0,34,19,2,2,1,0.563102824,True,5.0,732224.0,50.0,14.0,False,44.0,428974.0,73.0,6.0,2890.0,4740.0,5843.0,2280.0,3721.0,4483.0,2255.0,3308.0,4335.0,0.292455276,0.428960332,0.562094892,709,1135,1300325,nova,721e7f939859fbfe6b0c79ef3a6d5e43c916da65,1,1,broken by version 46922068ac167f492dd303efb359d0c649d69118. “I was able to reproduce this. Good find! My fault :)”,"Bug #1300325 in OpenStack Compute (nova): ""nic port ordering is not honored""","This bug was fixed by https://bugs.launchpad.net/nova/+bug/1064524 previously but broken by version 46922068ac167f492dd303efb359d0c649d69118.
Instead of iterating the already ordered port list, the new code iterates the list from neutron and the result is random ordering.","Ensure network interfaces are in requested order

_build_network_info_model was iterating current_neutron_ports
instead of port_ids which contains ports in their correctly requested
order. Because of this the requested nic order was no longer being
perserved. This patch fixes this and also changes the order of ports
in test_build_network_info_model() so this case is tested.

Change-Id: Ia9e71364bca6cbc24ebc1c234e6a5af14f51cd62
Closes-bug: #1300325"
873,724493d21fdfcbb4c095b54975c0c1d612f0a856,1380563278,1.0,1.0,67,37,2,2,1,0.795679297,True,8.0,10971405.0,91.0,31.0,False,152.0,318286.5,749.0,6.0,2.0,1845.0,1847.0,2.0,1589.0,1591.0,2.0,1751.0,1753.0,0.00048725,0.284554166,0.284879,1529,1222656,1222656,nova,724493d21fdfcbb4c095b54975c0c1d612f0a856,1,1, ,"Bug #1222656 in OpenStack Compute (nova): ""Forget to change volume's status when an error occurs in swap volume""","If the blockrebase(swap volume) fails, the status of the attached volume remains of detaching.
Tried Commit ID:b037993984229bb698050f20e8719b8c06ff2be3
1.Before Swap Volume
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d |   in-use  |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
2.Swap volume running
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | attaching |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
3.An error occurs　in swap volume
For example, cancel a blockrebase job.
libvirtError: virDomainGetBlockJobInfo() failed
4.After an error occurs
$ cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+
| 66230802-aed6-4a90-9dec-42fec910d13d | detaching |    vol01     |  1   |     None    |  False   | 86d8d79c-be27-43c4-b148-b5637c435899 |
| c3d51a52-764a-4007-976a-136b544c561b | available |    vol02     |  1   |     None    |  False   |                                      |
+--------------------------------------+-----------+--------------+------+-------------+----------+--------------------------------------+","Clean swap_volume rollback, on libvirt exception.

Handling swap_volume command's rollback on the manager side
instead of the API, to allow clean rollback.

Change-Id: I7d5c1e66bf01fd12fcaa783c1c3c90d92b009aea
Closes-Bug: #1222656"
874,72ade27ea322056b993da8cfd389476461a845af,1385003674,,1.0,11,5,1,1,1,0.0,True,3.0,2846793.0,19.0,8.0,False,19.0,194206.0,26.0,2.0,48.0,583.0,608.0,48.0,476.0,501.0,22.0,388.0,392.0,0.042124542,0.712454212,0.71978022,1787,1253207,1253207,Swift,72ade27ea322056b993da8cfd389476461a845af,0,0,Bug in test files … Should we have to take into account3,"Bug #1253207 in OpenStack Object Storage (swift): ""functional tests use wrong date format""","In test/functional/tests.py, the TestFileComparison suite uses ""time.asctime("" to format dates in if-modified-since type headers.  They should be in HTTP-date or ISO-86601 or whatever format is correct there.","test 3 date format in functional tests

According to HTTP/1.1, servers MUST accept all three formats:

Sun, 06 Nov 1994 08:49:37 GMT     # RFC 822, updated by RFC 1123
Sunday, 06-Nov-94 08:49:37 GMT    # RFC 850, obsoleted by RFC 1036
Sun Nov 6 08:49:37 1994           # ANSI C's asctime() format

In functional tests, a date value header has 3 kinds of format will be
tested.

Change-Id: I679ed44576208f2a79bffce787cb55bda4b39705
Closes-Bug: #1253207"
875,72cd430e971589f7872a9d4e8f173c68b4955375,1396291932,,1.0,12,13,3,2,1,0.730962929,True,4.0,1903867.0,70.0,15.0,False,212.0,9086.0,1185.0,4.0,9.0,2992.0,2995.0,9.0,2498.0,2501.0,6.0,2873.0,2873.0,0.000907323,0.372521063,0.372521063,710,1136,1300380,nova,72cd430e971589f7872a9d4e8f173c68b4955375,1,1,,"Bug #1300380 in OpenStack Compute (nova): ""errors_out_migration decorator does not work with RPC calls""","The errors_out_migration decorator in nova/compute/manager.py is attempting to use positional arguments to get the migration parameter.  However, at run time, the decorated methods are called via RPC calls which specify all of their arguments as keyword arguments.  The decorator needs to be fixed so that it works with RPC calls.","Change errors_out_migration decorator to work with RPC

This decorator in nova/compute/manager.py was written to work with
positional arguments, but at run time the decorated methods are
called via RPC with keyword arguments.  This change fixes the
decorator and the associated test cases.

Closes-Bug: #1300380

Change-Id: I9f87151a71e5217e7eff97cdc5cad2c85b620fa5"
876,72dd81343e73baf838bc58d413413f0d57018f15,1404330288,,1.0,26,22,4,4,1,0.593139062,False,,,,,True,,,,,,,,,,,,,,,,,1429,1182131,1182131,nova,72dd81343e73baf838bc58d413413f0d57018f15,1,1,“Expected result: no stackstrace to be thrown”,"Bug #1182131 in OpenStack Compute (nova): ""nova-compute: instance created in self-referencing secgroup produces KeyError""","Hi,
Steps to reproduce:
1) create a security group that is referencing itself, for example
euca-create-group test2
euca-authorize test2 -P tcp -p 22 -s 0.0.0.0/0
euca-authorize test2 -P tcp -p 6666 -o test2
2) create any instance in this security group
euca-run-instance .. -g test2 ..
Expected result:
no stackstrace to be thrown
Actual result:
stacktrace with KeyError appears in the log. The iptable rules are created correctly and instance ends up in running state.
File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 390, in refresh_instance_security_rules
  return self.driver.refresh_instance_security_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 2269, in refresh_instance_security_rules
  self.firewall_driver.refresh_instance_security_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/firewall.py"", line 440, in refresh_instance_security_rules
  self.do_refresh_instance_rules(instance)
File ""/usr/lib/python2.6/site-packages/nova/virt/firewall.py"", line 457, in do_refresh_instance_rules
  network_info = self.network_infos[instance['id']]
KeyError: 4168
It seems that self.network_infos is accessed in wrong order for the security group that is referencing itself. The stacktrace is from 'do_refresh_instance_rules' which expects network info to be already present for the instance that is being created. Reported KeyError is the id of newly created instance. The dictionary entry is added few seconds later processing the same request.
Fortunately, this issue does not appear to have any negative impact aside the stacktrace in the log.
Openstack version: Folsom 2012.2.4
Attaching verbose log from nova-compute.
Regards,
Brano Zarnovican","Avoid referencing stale instance/network_info dicts in firewall

This makes the virt.firewall code cleaner in terms of referencing
the cached instance and network_info code it stores. Before this
patch, concurrent instance operations could modify these two dicts
so that while we're iterating instances, the network_info dict
is suddenly missing information we need.

The right fix for this is to use instance objects and their
associated info_cache objects, but that's a larger fix and one
not as well-suited to backporting to previous releases which
suffer from this as well.

The approach taken here is that we store the instance and
network_info cache together in the same dict that we can pop()
from atomically (this is not really necessary, but helps to
prevent introducing more of these cases). When we iterate over
the contents, we iterate over a copy of the keys, being careful
not to let a suddenly-missing key break us, and passing the
details all the way down the stack instead of having deeper calls
hit the cache dicts again.

Change-Id: I33366f50024a82451842d045b830ab19b59879c3
Closes-bug: #1182131"
877,72f66917fe79996b9715c47eae8e9d479b210c2d,1405336634,,1.0,46,46,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1098,1549,1347963,neutron,72f66917fe79996b9715c47eae8e9d479b210c2d,0,0,Bug in test,"Bug #1347963 in neutron: ""Network profile tests have incorrect create request""","Some unit test cases in the unit test module for cisco n1kv plugin has incorrect create resource calls causing breakage in build.
This was exposed via a fix for bug/1330095","Cisco: Fix test cases which make incorrect create requests

Some test cases in the test_n1kv_plugin module are sending more than
one dictionary while creating the network profile resources. This
patch corrects these erroneous test cases while preserving the test
cases intended behavior.

Change-Id: I99de0b708218cf5217ecf47c74a30f75d0733e25
Closes-bug: #1347963"
878,7320422e5a496350906dcd2a1cd5487088cf63e5,1395515142,,1.0,32,13,3,2,1,0.872424996,True,6.0,193032.0,45.0,15.0,False,17.0,1738086.0,29.0,2.0,40.0,1542.0,1542.0,29.0,1278.0,1278.0,38.0,1385.0,1385.0,0.025242718,0.897087379,0.897087379,670,1096,1295906,cinder,7320422e5a496350906dcd2a1cd5487088cf63e5,1,1,There's a problem in the attach volume logic ,"Bug #1295906 in Cinder: ""EMC SMI-S driver needs to check if a volume is attached to a specific host""","There's a problem in the attach volume logic of the EMC SMI-S driver.  The existing logic checks if a volume is already attached to a host, but it doesn’t check whether it is attached to the host inidicated by the connector info.
So we need to add a check to see if a volume is already attached to a specific host.  If not, it will do the attach.","Fixes a problem in attach volume in EMC driver.

This patch fixes a problem in attach volume in EMC SMI-S driver.
The existing logic checks if a volume is already attached to any host,
but it doesn't check whether a volume is already attached to the specific
host that nova wants cinder to attach.  As a result, initialize_connection
could return success (thinking it is already attached), but nova will
fail to discover the LUN later and fail the attach.

This patch adds a check to see if a volume is already attached to a
specific host.  If not, it will do the attach.  The reason that the
volume being attached already could be due to a nova live-migration
use case.  Cinder doesn't support multiple attaches currently, but
allows a volume to be attached multiple times from nova during
live-migration.

Change-Id: I05a2f57cd8708d7fcbe902ec13665a9cfb44db07
Closes-Bug: #1295906"
879,7360e67d32dbd8c98ef59c46bf0c7fff16f48d0c,1394584410,1.0,1.0,3,10,1,1,1,0.0,True,4.0,236879.0,61.0,11.0,False,11.0,1733076.0,15.0,2.0,19.0,916.0,926.0,19.0,754.0,764.0,19.0,300.0,310.0,0.021164021,0.318518519,0.329100529,590,1012,1291103,neutron,7360e67d32dbd8c98ef59c46bf0c7fff16f48d0c,1,1,broken,"Bug #1291103 in neutron: ""admin_state_up check broken for update network in PLUMgrid Neutron Plugin""","in update_network API call, the network dictionary has the contents: {u'network': {u'admin_state_up': True}}
_network_admin_state functions expects network name in the dictionary and it raises error. This has to be fixed. Network name should not be expected.
2014-03-11 16:20:41.920 29794 ERROR neutron.api.v2.resource [-] update failed
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 486, in update
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     obj = obj_updater(request.context, id, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 233, in inner
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     retval = f(*args, **kwargs)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 133, in update_network
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     self._network_admin_state(network)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/plumgrid/plumgrid_plugin/plumgrid_plugin.py"", line 597, in _network_admin_state
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource     raise plum_excep.PLUMgridException(err_msg=err_message)
2014-03-11 16:20:41.920 29794 TRACE neutron.api.v2.resource PLUMgridException: An unexpected error occurred in the PLUMgrid Plugin: Network Admin State Validation Falied:","Fix in admin_state_up check function

Change-Id: If221cf5546e6432b31dd11ec322ddd4b69d161b2
Closes-Bug: #1291103
Signed-off-by: Fawad Khaliq <fawad@plumgrid.com>"
880,73705174c9cdd847501e36385ffb942c17c8ef61,1396448066,,1.0,43,24,4,2,1,0.9895278,True,7.0,2358168.0,116.0,13.0,False,8.0,11233432.25,7.0,4.0,40.0,989.0,1018.0,40.0,824.0,853.0,22.0,594.0,605.0,0.021335807,0.551948052,0.562152134,726,1152,1301396,neutron,73705174c9cdd847501e36385ffb942c17c8ef61,1,1,,"Bug #1301396 in neutron: ""Enum type is changed incorrectly in migration 1341ed32cc1e_nvp_netbinding_update for""","In migration 1341ed32cc1e_nvp_netbinding_update Enum type had been changed incorrectly from ('flat', 'vlan', 'stt', 'gre') to ('flat', 'vlan', 'stt', 'gre', 'l3_ext')  for PostgeSQL so in database this type was not changed.
This could be checked as it shown there http://paste.openstack.org/show/74835/.
The same problem is taken place for vlan_type in migrations 38fc1f6789f8_cisco_n1kv_overlay from ('vlan', 'vxlan', 'trunk',
'multi-segment') to ('vlan', 'overlay', 'trunk', 'multi-segment') and in 46a0efbd8f0_cisco_n1kv_multisegm from ('vlan', 'vxlan') to
('vlan', 'vxlan', 'trunk', 'multi-segment').
This could be checked as it shown there http://paste.openstack.org/show/75387/","Fix incorrect change of Enum type

In migration 1341ed32cc1e_nvp_netbinding_update Enum type had been
changed incorrectly from ('flat', 'vlan', 'stt', 'gre')
to ('flat', 'vlan', 'stt', 'gre', 'l3_ext') for PostgeSQL.

The same problem is taken place for vlan_type in migrations
38fc1f6789f8_cisco_n1kv_overlay from ('vlan', 'vxlan', 'trunk',
'multi-segment') to ('vlan', 'overlay', 'trunk', 'multi-segment')
and in 46a0efbd8f0_cisco_n1kv_multisegm from ('vlan', 'vxlan') to
('vlan', 'vxlan', 'trunk', 'multi-segment').

In this change request was added separate method for changing Enum
type for PostgreSQL.

Closes-bug: #1301396

Change-Id: I27197fb7405630a55178be8516a4b62bd135e05c"
881,739b3cac3210e6a1ce942238ca3efa3ddf49cb4d,1411466314,,1.0,74,74,5,3,1,0.810515525,False,,,,,True,,,,,,,,,,,,,,,,,1878,1372862,1372862,Neutron,739b3cac3210e6a1ce942238ca3efa3ddf49cb4d,1,0,"""in neutron code, there still are some codes to enable translation tag for debug level log""","Bug #1372862 in neutron: ""There is no need to enable globalizaion for debug level log""","Currently, in neutron code, there still are some codes to enable translation tag for debug level log.","Remove the translation tag for debug level logs in vmware plugin

There is no need to translate the debug level logs in vmware plugin for neutron.

Closes-bug: #1372862

Change-Id: If07c06cc3a1eb3dd147191bb8ab9198d4198cd69"
882,73b3bf91df00059c69dc1dd81e4554ec24c647b1,1381176129,,1.0,23,2,3,3,1,0.875893931,True,6.0,294484.0,49.0,16.0,False,37.0,270103.0,165.0,3.0,831.0,2206.0,2729.0,815.0,1640.0,2151.0,812.0,1867.0,2374.0,0.13060241,0.300080321,0.381526104,1654,1237102,1237102,nova,73b3bf91df00059c69dc1dd81e4554ec24c647b1,1,1, ,"Bug #1237102 in OpenStack Compute (nova): ""Conductor does not properly copy objects during change tracking""",The conductor object_action() method does a shallow copy of the instance in order to do change tracking after the method is called. This is not sufficient as complex types like dicts and lists will not be copied and then the change detection logic will think those fields have not changed.,"Fix conductor's object change detection

Conductor was doing a copy.copy() on the inbound object to later
detect changes that should be sent back to the caller. This does not
copy things like Instance.system_metadata and thus is incapable of
properly detecting changes that should be tracked.

This patch makes conductor use obj_clone(), and imports Chris
Behrens' __deepcopy__ fix for objects so that deepcopy works.

Closes-bug: #1237102
Change-Id: I46ae8b0694dc31a90c1a5cdf76757d877877f072"
883,73b4239fbef5e52494cf3ffc3293dbb79b52d91c,1405185564,,1.0,126,21,4,3,1,0.571399065,False,,,,,True,,,,,,,,,,,,,,,,,1033,1477,1336596,neutron,73b4239fbef5e52494cf3ffc3293dbb79b52d91c,0,0,Feature. “This change addresses the proper clean up of tables. i.e.”,"Bug #1336596 in neutron: ""Cisco N1k: Clear entries in n1kv specific tables on rollbacks""","During rollback operations, the resource is cleaned up from the neutron database but leaves a few stale entries in the n1kv specific tables.
Vlan/VXLAN allocation tables are inconsistent during network rollbacks.
VM-Network table is left inconsistent during port rollbacks.
Explicitly clearing ProfileBinding table entry (during network profile rollbacks) is not required as delete_network_profile internally takes care of it.","Clear entries in Cisco N1KV specific tables on rollback

During rollback operations, resources are cleaned up from neutron
database but leaves a few stale entries in the n1kv specific tables.
This change addresses the proper clean up of tables. i.e. VLAN/VXLAN
allocation tables, profile binding and vm network table.

Change-Id: I7a09d34f3a9dee0a43b76c5d781ee9eb9938953a
Closes-bug: #1336596"
884,73c87a280e77e03d228d34ab4781ca2e3b02e40e,1391075050,,1.0,1,1,1,1,1,0.0,True,3.0,851628.0,55.0,21.0,False,47.0,284762.0,102.0,9.0,1691.0,2163.0,3255.0,1442.0,1906.0,2782.0,817.0,1646.0,2027.0,0.11340635,0.228337724,0.281159018,342,753,1274439,nova,73c87a280e77e03d228d34ab4781ca2e3b02e40e,1,0,performance issue. Change in requirements,"Bug #1274439 in OpenStack Compute (nova): ""VMware: operations against VC/ESX are taking at least 5 seconds""","The default values of the task_poll_interval is 5 seconds. This means that any operation against the VC will wait at least 5 seconds.
An example of this - a spawn operation would take on average take 25 seconds. When this parameter what changed to 0.2 - 1.0 seconds the operation would take on average 9 seconds.","VMware: update the default 'task_poll_interval' time

The original means that each operation against the backend takes at
least 5 seconds. The default is updated to 0.5 seconds.

DocImpact
    Updated default value for task_poll_interval from 5 seconds to
    0.5 seconds

Change-Id: I867b913f52b67fa9d655f58a2e316b8fd1624426
Closes-bug: #1274439"
885,73da55e4ef626283ae58a97c7ad89854ec77daa3,1394636012,,1.0,164,9,8,2,1,0.795666683,True,5.0,4749954.0,45.0,11.0,False,57.0,918015.0,124.0,1.0,88.0,813.0,890.0,69.0,813.0,871.0,71.0,808.0,868.0,0.009498681,0.106728232,0.114643799,596,1020,1291364,nova,73da55e4ef626283ae58a97c7ad89854ec77daa3,1,1,,"Bug #1291364 in OpenStack Compute (nova): ""_destroy_evacuated_instances fails randomly with high number of instances""","In our production environment (2013.2.1), we're facing a random error thrown while starting nova-compute in Hyper-V nodes.
The following exception is thrown while calling '_destroy_evacuated_instances':
16:30:58.802 7248 ERROR nova.openstack.common.threadgroup [-] 'NoneType' object is not iterable
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
(...)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup   File ""C:\Python27\lib\site-packages\nova\compute\manager.py"", line 532, in _get_instances_on_driver
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup     name_map = dict((instance['name'], instance) for instance in instances)
2014-03-05 16:30:58.802 7248 TRACE nova.openstack.common.threadgroup TypeError: 'NoneType' object is not iterable
Full trace: http://paste.openstack.org/show/73243/
Our first guess is that this problem is related with number of instances in our deployment (~3000), they're all fetched in order to check evacuated instances (as Hyper-V is not implementing ""list_instance_uuids"").
In the case of KVM, this error is not happening as it's using a smarter method to get this list based on the UUID of the instances.
Although this is being reported using Hyper-V, it's a problem that could occur in other drivers not implementing ""list_instance_uuids""","Adds list_instance_uuids to the Hyper-V driver

In case of large number of servers, the _destroy_evacuated_instances
fails randomly. Implementing list_instance_uuids solves the issue.

Co-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>
Co-Authored-By: Ionut Balutoiu <ibalutoiu@cloudbasesolutions.com>
Change-Id: I9c58168c012b342bb5dfa0c62a7c39a327f442b3
Closes-bug: #1291364"
886,73ee7346675ffb38276583ac7b941970f7037181,1382619617,,1.0,29,5,3,3,1,0.685387093,True,5.0,12719981.0,68.0,18.0,False,10.0,1510339.667,14.0,5.0,89.0,2069.0,2112.0,89.0,1825.0,1868.0,88.0,1967.0,2010.0,0.013839216,0.306017727,0.31270409,26,338,1244203,nova,73ee7346675ffb38276583ac7b941970f7037181,0,0,The code shows that add a feature,"Bug #1244203 in OpenStack Compute (nova): ""Evacuate operation should honor the enable_instance_password conf value""","Currently the Evacuate operation (enabled by the os-evacuate) extension always returns a generated (or user supplied) password, although not all hypervisors support password injection.
For server create and rebuild and rescue operations the configuration option ""enable_instance_password=False"" can be used to suppress returning a meaningless and confusing password, and the evacuate operation should also honor this setting","Fixes evacuate doesn't honor enable password conf for v3

The evacuate api should honor enable_instance_password conf
setting just like server creation, rebuild and rescue api does,
enables this flag will returning the instance password from
server API call evacuate, If the hypervisor does not support
password injection then the password returned will not be
correct, this patch just fixes v3 evacuate api, since change
return value of v2 could potentially break apps which depend
on this being returned.

This patch also changes the evacuate response code to 202 to
keep consistency with api.

DocImpact
Change-Id: I2b9e5a067323ca84e2040e175a0425de3c0fafba
Closes-Bug: #1244203"
887,73fcf4628089dd784889062e916b80d3fc9988a2,1413514797,,1.0,8,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1408,1878,1382318,nova,73fcf4628089dd784889062e916b80d3fc9988a2,1,0,"Bug in ExtL. “The libvirt config code shouldn't be casting values to str(), it should be using six.text_type.” The bug was fix in 2014 and the pc was done in 2012config code shouldn't be casting values to str(), it should be using six.text_type.”","Bug #1382318 in OpenStack Compute (nova): ""NoValidHost failure when trying to spawn instance with unicode name""","Using the libvirt driver on Juno RC2 code, trying to create an instance with unicode name:
""\uff21\uff22\uff23\u4e00\u4e01\u4e03\u00c7\u00e0\u00e2\uff71\uff72\uff73\u0414\u0444\u044d\u0628\u062a\u062b\u0905\u0907\u0909\u20ac\u00a5\u5642\u30bd\u5341\u8c79\u7af9\u6577""
Blows up:
http://paste.openstack.org/show/121560/
The libvirt config code shouldn't be casting values to str(), it should be using six.text_type.","libvirt: use six.text_type when setting text node value in guest xml

Trying to spawn an instance with a unicode name using the libvirt driver
fails with a UnicodeDecodeError because the value is cast to str().

The fix is to use six.text_type for the cast.

Closes-Bug: #1382318

Change-Id: I4628b94459a3c1e757d388916f1268884cb02038"
888,74145b625a2544eb0d72c811a20c64c9987bedf0,1412439601,,1.0,58,1,5,4,1,0.827897171,False,,,,,True,,,,,,,,,,,,,,,,,1380,1850,1377447,nova,74145b625a2544eb0d72c811a20c64c9987bedf0,1,0,"API Change - Change requirements, “a8a5d44c8aca218f00649232c2b8a46aee59b77e that make each tuple have four items Old code only accept three items for each tuple:”","Bug #1377447 in OpenStack Compute (nova): ""pci_request_id break the upgrade from icehouse to Juno""","The rpc api build_and_run_instance should be back-compatible with icehouse, as the code https://github.com/openstack/nova/blob/master/nova/compute/rpcapi.py#L887
It turn the request_network object into tuple that can be understand by icehouse code.
But the commit a8a5d44c8aca218f00649232c2b8a46aee59b77e change the request_network parameter. It add pci_request_id for each item. When request_network object turn it to tuple, it will add pci_request_id into
tuple also, that make each tuple have four items. https://github.com/openstack/nova/blob/master/nova/objects/network_request.py#L37
Old code only accept three items for each tuple: https://github.com/openstack/nova/blob/2014.1/nova/network/neutronv2/api.py#L237
Then the rpc api back-compatiblity is broken.
Then juno controller boot instance to icehouse compute node, will get error as below:
2014-10-04 21:08:17.455 ERROR nova.compute.manager [req-58b84295-ce36-479f-b50e-dfe4f86cc1d9 admin demo] [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] Insta
nce failed to spawn
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] Traceback (most recent call last):
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 2014
, in _build_resources
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     yield resources
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1917
, in _build_and_run_instance
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     block_device_info=block_device_info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line
2246, in spawn
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     admin_pass=admin_password)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line
2677, in _create_image
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     content=files, extra_md=extra_md, network_info=network_
info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 16
5, in __init__
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     ec2utils.get_ip_info_for_instance_from_nw_info(network_info)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 147, in get_ip_info_for_instance_from_nw_info
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     fixed_ips = nw_info.fixed_ips()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/model.py"", line 407, in _sync_wrapper
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     self.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/model.py"", line 439, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     self[:] = self._gt.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     return self._exit_event.wait()
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 124, in wait
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     current.throw(*self._exc)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 207, in main
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     result = function(*args, **kwargs)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1510, in _allocate_network_async
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     dhcp_options=dhcp_options)
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 238, in allocate_for_instance
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4]     for network_id, fixed_ip, port_id in requested_networks:
2014-10-04 21:08:17.455 TRACE nova.compute.manager [instance: c643eff5-ffd0-4eea-98a0-bce55108b0a4] ValueError: too many values to unpack","Fix pci_request_id break the upgrade from icehouse to juno

commit a8a5d44c8aca218f00649232c2b8a46aee59b77e add pci_request_id
as one item for the request_network tuple. But the icehouse code
assume only three items in the tuple.

This patch filters pci_request_id out from the tuple.

Change-Id: I991e1c68324fe92fac647583f3ec8f6aec637913
Closes-Bug: #1377447"
889,74289aacb5363bd72b449580325a651117e5342d,1381386912,1.0,1.0,67,40,2,2,1,0.272335908,True,5.0,2078777.0,35.0,12.0,False,40.0,29179.0,97.0,3.0,1296.0,3716.0,3716.0,1173.0,3215.0,3215.0,479.0,2777.0,2777.0,0.076628352,0.44348659,0.44348659,1663,1237795,1237795,nova,74289aacb5363bd72b449580325a651117e5342d,1,1, ,"Bug #1237795 in OpenStack Compute (nova): ""VMware: restarting nova compute reports invalid instances""",When nova compute restarts the running instances on the hypervisor are queried. None of the instances would be matched - this would prevent the instance states being in sync with the state in the database. See _destroy_evacuated_instances (https://github.com/openstack/nova/blob/master/nova/compute/manager.py#L531),"VMware: fix bug for reporting instance UUID's

Ensure that the correct UUID's are reported to the compute
manager. The fix will return only valid UUID's and not all
of the names of the instances running.

Closes-Bug: #1237795

Change-Id: I32bb6f4783dcc80697abebd5a7f45ef9950a0896"
890,743b74d59d3279478c512a0a559642b1d05284bc,1389112364,1.0,1.0,20,3,3,2,1,0.836477065,True,44.0,4173055.0,325.0,110.0,False,51.0,215066.3333,128.0,3.0,1607.0,744.0,2096.0,1387.0,732.0,1865.0,744.0,724.0,1216.0,0.106748818,0.103883078,0.174380284,258,662,1266579,nova,743b74d59d3279478c512a0a559642b1d05284bc,1,1,Race condition,"Bug #1266579 in OpenStack Compute (nova): ""VMware: Unable to launch instance because vmware_base already exists""","When running the Tempest tests in parallel, 2 tests are failing due to instances spawning with ERROR:
  setUpClass (tempest.api.compute.admin.test_fixed_ips.FixedIPsTestXml)
  setUpClass (tempest.api.compute.admin.test_fixed_ips_negative.FixedIPsNegativeTestJson)
The VimFaultException seen in the nova scheduler log is:
  Cannot complete the operation because the file or folder [datastore1] vmware_base already exists
Full Traceback here (non-wrapped version here: http://paste.openstack.org/show/60502/):
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1053, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1456, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1453, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 587, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 437, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1549, in _check_if_folder_file_exists
    ds_ref)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1534, in _mkdir
    createParentDirectories=False)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 795, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 200, in vim_request_handler
    raise error_util.VimFaultException(fault_list, excep)
VimFaultException: Server raised fault: 'Cannot complete the operation because the file or folder [datastore1] vmware_base already exists'","VMware: fix race for datastore directory existence

Treat the exception that is raised when creating a directory
and it already exists. This may happen due to two processes trying
to create the same directory at the same time.

Change-Id: Ia0ebd674345734e7cfa31ccd400fdba93646c554
Closes-bug: #1266579"
891,744c1bd2f1ae1caf3cd6d0c07d61b034a21204bc,1402396909,,1.0,3,5,2,2,1,0.954434003,False,,,,,True,,,,,,,,,,,,,,,,,920,1356,1323259,neutron,744c1bd2f1ae1caf3cd6d0c07d61b034a21204bc,0,0,"Feature ""To enable reuse of the ovs agent with the Intel® DPDK Accelerated Open vSwitch the proposal is to change the generation of the agent_id to use the hostname instead of the mac address of the br-int.”","Bug #1323259 in neutron: ""Open vSwitch Agent ID initalisation prevents reuse of agent with userspace only vSwitches""","Problem description
===================
In the Open vSwitch agent, the Agent id is currently based off the mac address of the br-int.
Userspace Open vSwitch derivatives such as the Intel® DPDK Accelerated Open vSwitch do not currently create a tap device in the kernel to back the ovs bridges local port.
This limitation prevents reuse of the OpenVSwitch agent between both switches.
By allowing integration of high throughput vSwitch implementations via existing agents, NFV workloads can be enabled in OpenStack without significant extension of the current codebase.
Proposed change
===============
To enable reuse of the ovs agent with the Intel® DPDK Accelerated Open vSwitch the proposal is to change the  generation of the  agent_id to use the hostname instead of the mac address of the br-int.
-        mac = self.int_br.get_local_port_mac()
-        self.agent_id = '%s%s' % ('ovs', (mac.replace("":"", """")))
+        self.agent_id = 'ovs-agent-%s' % socket.gethostname()
For several plugins such as the nec,mlnx,hyperv and onconvergence agents the hostname is used to create the agent id.
Using the hostname will normalise the agent_id between these 5 neutron agents.
at present the Agent_id is only used in log messages. by using the hostname instead of mac of br-int log readability will also be improved as it will be easier to identify which node the log is from if log aggregation is preformed across a cluster.
Patch
===============
initial patch submitted for review
https://review.openstack.org/#/c/95138/1","changes ovs agent_id init to use hostname instead of mac

In the Open vSwitch agent,
the Agent id is currently based off the mac address of the br-int.
Userspace only Open vSwitch derivatives such as Intel's DPDK
Accelerated Open vSwitch do not currently create a tap device in the kernel
to back the ovs bridges local port.
This limitation prevents reuse of the OpenVSwitch agent between both switches.

To enable reuse of the ovs agent with Intel's DPDK Accelerated Open vSwitch,
the proposal is to change the  generation of the  agent_id to use the hostname,
instead of the mac address of the br-int.

For several plugins such as the nec,mlnx,hyperv and onconvergence agents
the hostname is used to create the agent id.
Using the hostname will normalise the agent_id between these 5 neutron agents,
additionally log readability will also be improved,
if log aggregation is preformed across a cluster
as it will be easier to identify which node the log is from.

the hostname is retrived from cfg.CONF.host

Closes-Bug: #1323259
Change-Id: I9abfac17a74d298f1a17a0931fc98ac00234ac0b"
892,748d4fdaf62667f39ce65e5053792b54dd684698,1406711224,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1121,1576,1350252,neutron,748d4fdaf62667f39ce65e5053792b54dd684698,0,0,Bug in test,"Bug #1350252 in neutron: ""test_l3_plugin fails when run as single test""","tox  -e py27 neutron.tests.unit.test_l3_plugin
fails as follows. This is bacause necessary oslo config isn't initialized properly by L3AgentDbIntTestCase and L3AgentDbSepTestCase
The error log follows.
 $ tox -e py27 neutron.tests.unit.test_l3_plugin
py27 develop-inst-nodeps: /home/yamahata/openstack/tacker/neutron-l3-plugin/upstream/neutron-l3-db-refacotr-0
py27 runtests: commands[0] | python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin
running testr
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit} --list
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpNLNI_l
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpBl_vq6
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpO3NkaL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpc3FrWL
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmppz6MIP
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmprcHfnW
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpVW99Uk
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpMC3U19
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpiqP4jI
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpK6j9JS
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmp_cFhYl
running=OS_STDOUT_CAPTURE=1 OS_STDERR_CAPTURE=1 OS_LOG_CAPTURE=1 ${PYTHON:-python} -m subunit.run discover -t ./ ${OS_TEST_PATH:-./neutron/tests/unit}  --load-list /tmp/tmpNXI0Mi
======================================================================
FAIL:
neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_floatingips
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_router_gateway_op_agent
tags: worker-10
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbIntTestCase.test_l3_agent_routers_query_floatingips
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2060, in setUp
    self.core_plugin = TestL3NatIntPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: neutron.tests.unit.test_l3_plugin.L3AgentDbSepTestCase.test_l3_agent_routers_query_ignore_interfaces_with_moreThanOneIp
tags: worker-1
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""neutron/tests/unit/test_l3_plugin.py"", line 2073, in setUp
    self.core_plugin = TestNoL3NatPlugin()
  File ""neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/neutron/.tox/py27/local/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
======================================================================
FAIL: process-returncode
tags: worker-10
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
======================================================================
FAIL: process-returncode
tags: worker-1
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 304 (-13543) tests in 27.382s (-600.855s)
FAILED (id=17, failures=6 (-2))
error: testr failed (1)
ERROR: InvocationError: '/neutron/.tox/py27/bin/python -m neutron.openstack.common.lockutils python setup.py testr --slowest --testr-args=neutron.tests.unit.test_l3_plugin'
__________________________________________________________________ summary __________________________________________________________________
ERROR:   py27: commands failed","test_l3_plugin: L3AgentDbInteTestCase L3AgentDbSepTestCase fails

L3AgentDbInteTestCase and L3AgentDbSepTestCase fails when they are run
independently without other test case. something like
tox -e py27 neutron.tests.unit.test_l3_plugin
It's because necessary oslo.config options aren't properly initialized
when instantiating service plugin.
Initialize config before instantiating plugin.

Change-Id: Ic6dd28e3caf8b9e3322bf2df99e67adb138cb234
Closes-Bug: #1350252"
893,74a292ceba0be6aaa7ea1911cac2f102eb3db90e,1409919035,,1.0,25,58,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1270,1732,1365829,neutron,74a292ceba0be6aaa7ea1911cac2f102eb3db90e,0,0,Bug in test,"Bug #1365829 in neutron: ""ipv6 tests leave ipv6 flag disabled""","The test_disabled test for the IPv6 utilities leaves a module level flag set to disabled, which will break any following tests that depend on IPv6  being enabled (e.g. the iptables tests).
http://logs.openstack.org/49/113749/8/check/gate-neutron-python27/8787946/console.html
To reproduce, you can run these two tests serially.
neutron.tests.unit.test_ipv6.TestIsEnabled.test_disabled
neutron.tests.unit.test_security_groups_rpc.TestSecurityGroupAgentEnhancedRpcWithIptables.test_prepare_remove_port","Avoid testing code duplication which introduced testing bugs

SecurityGroupAgentEnhancedRpcTestCase duplicated code in
SecurityGroupAgentRpcTestCase setUp, also
TestSecurityGroupAgentEnhancedRpcWithIptables duplicated
code from TestSecurityGroupAgentWithIptables setUp()
introducing bugs by improper initialization, like a missing
    self.iptables.use_ipv6 = True
which in combination with tests.unit.test_ipv6.TestIsEnabled
produced inconsistent testing results.

Change-Id: Ic0b596b6fe1d8273df197e9426abad818832c00a
Closes-bug: #1365829"
894,74ade48634e2b72ec0b09792e97ac77dfb5fd999,1390504702,,1.0,8,23,1,1,1,0.0,True,1.0,101462.0,13.0,7.0,False,4.0,10835022.0,4.0,4.0,2015.0,2098.0,3531.0,1639.0,1772.0,2890.0,1917.0,1980.0,3334.0,0.268364349,0.277179236,0.466629355,313,722,1271331,nova,74ade48634e2b72ec0b09792e97ac77dfb5fd999,0,0,tests,"Bug #1271331 in OpenStack Compute (nova): ""unit test failure in gate nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping""","We are occasionally seeing the test nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping fail in the gate due to
Traceback (most recent call last):
  File ""nova/tests/db/test_sqlite.py"", line 53, in test_big_int_mapping
    output, _ = utils.execute(get_schema_cmd, shell=True)
  File ""nova/utils.py"", line 166, in execute
    return processutils.execute(*cmd, **kwargs)
  File ""nova/openstack/common/processutils.py"", line 168, in execute
    result = obj.communicate()
  File ""/usr/lib/python2.7/subprocess.py"", line 754, in communicate
    return self._communicate(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1314, in _communicate
    stdout, stderr = self._communicate_with_select(input)
  File ""/usr/lib/python2.7/subprocess.py"", line 1438, in _communicate_with_select
    data = os.read(self.stdout.fileno(), 1024)
OSError: [Errno 11] Resource temporarily unavailable
logstash query: message:""FAIL: nova.tests.db.test_sqlite.TestSqlite.test_big_int_mapping""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRkFJTDogbm92YS50ZXN0cy5kYi50ZXN0X3NxbGl0ZS5UZXN0U3FsaXRlLnRlc3RfYmlnX2ludF9tYXBwaW5nXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImFsbCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzOTAzMzk1MTU1NDcsIm1vZGUiOiIiLCJhbmFseXplX2ZpZWxkIjoiIn0=","Re-write sqlite BigInteger mapping test

This test makes sure that our hook that compiles BigInteger() into
INTEGER for sqlite instead of BIGINT is working as we expect.
Unfortunately, we've seen some problems running the sqlite command to
check the schema.

This patch rewrites the test to only use sqlalchemy APIs and avoid
calling out to the sqlite command for verification.

Change-Id: Id3964d0dd5b0ddf08ab745d6878840ff06b2f0dd
Closes-bug: #1271331"
895,74d10939903984d5f06c1749a8707fa3257e44ff,1408449276,,1.0,44,12,5,4,1,0.736087792,False,,,,,True,,,,,,,,,,,,,,,,,1204,1662,1357379,neutron,74d10939903984d5f06c1749a8707fa3257e44ff,1,1,“CVE-2014-6414“,"Bug #1357379 in neutron: ""[OSSA 2014-031] policy admin_only rules not enforced when changing value to default (CVE-2014-6414)""","If a non-admin user tries to update an attribute, which should be updated only by admin, from a non-default value to default,  the update is successfully performed and PolicyNotAuthorized exception is not raised.
The reason is that when a rule to match for a given action is built there is a verification that each attribute in a body of the resource is present and has a non-default value. Thus, if we try to change some attribute's value to default, it is not considered to be explicitly set and a corresponding rule is not enforced.","Forbid regular users to reset admin-only attrs to default values

A regular user can reset an admin-only attribute to its default
value due to the fact that a corresponding policy rule is
enforced only in the case when an attribute is present in the
target AND has a non-default value.

Added a new attribute ""attributes_to_update"" which contains a list
of all to-be updated attributes to the body of the target that is
passed to policy.enforce.

Changed a check for whether an attribute is explicitly set.
Now, in the case of update, the function should not pay attention
to a default value of an attribute, but check whether it was
explicitly marked as being updated.

Added unit-tests.

Closes-Bug: #1357379
Related-Bug: #1338880
Change-Id: I6537bb1da5ef0d6899bc71e4e949f2c760c103c2"
896,751b1a04c9b14bdaf20d555e0aaa41bc276955c4,1386236093,1.0,1.0,37,1,2,2,1,0.998000884,True,16.0,3169448.0,109.0,52.0,False,10.0,433847.0,14.0,9.0,2.0,3761.0,3761.0,2.0,3151.0,3151.0,2.0,2769.0,2769.0,0.000442022,0.408133196,0.408133196,107,506,1256264,nova,751b1a04c9b14bdaf20d555e0aaa41bc276955c4,1,1,Doesnt respect cpu limit…,"Bug #1256264 in OpenStack Compute (nova): ""[Docker] Driver doesn't respect CPU limit""","The Nova Docker driver doesn't seem to respect the CPU limit defined by nova.
Please note I just reviewed the latest source code, haven't tested the behavior.
My assumption is that a docker container has access to all CPU resources by default.
On the docker command line this can be handled:
    docker run -c=<relative-weight> centos","Docker Driver doesn't respect CPU limit

The docker driver doesn't respect the CPU settings of the configured
flavor.

Docker/LXC supports only relative cpu allocation. The relative
cpu allocation doesn't prevent that a container can use all CPUs but it
can at least ensure that a container with more CPUs gets more CPU
cycles.

DocImpact - Docker containers are now given CPU shares according to
the vcpu setting of the flavor used when creating the container.
Previously, the vcpu field was ignored and all containers got equal
CPU shares, regardless of the vcpu setting.

Change-Id: I7184eddfd8bbd2cbaa07a48ebb40bc9aa3fbd4a9
Closes-Bug: #1256264"
897,7521e6dce39a9ec99d35892780a39bf8049e38db,1400765076,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,574,995,1290317,cinder,7521e6dce39a9ec99d35892780a39bf8049e38db,1,1,message format error,"Bug #1290317 in Cinder: ""UnboundLocalError: local variable 'copy_error' referenced before assignment""","Hi , i was trying migrate a volume from one backend to another
while when operation is in copy_volume_data (cinder/volume/driver.py)
error occured while copy data from src_volume to dst_volume
root@DVT-Ubuntu-srv2:~# cinder --version
1.0.6
Below is trace back ==================================================
2014-03-06 17:38:36.768 12090 ERROR cinder.openstack.common.rpc.amqp [req-beec71a7-1eec-43af-bb83-a66c58376b42 bfe8d3624c044e5db34bc8abd12cd752 42d88e9851cc4af4afa6a0896834b915] Exception during message handling
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/utils.py"", line 808, in wrapper
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 798, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.db.volume_update(ctxt, volume_ref['id'], updates)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 791, in migrate_volume
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self._migrate_volume_generic(ctxt, volume_ref, host)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 729, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     new_volume['migration_status'] = None
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/manager.py"", line 709, in _migrate_volume_generic
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     remote='dest')
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/cinder/volume/driver.py"", line 326, in copy_volume_data
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp     force=copy_error)
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp UnboundLocalError: local variable 'copy_error' referenced before assignment
2014-03-06 17:38:36.768 12090 TRACE cinder.openstack.common.rpc.amqp
2014-03-06 17:39:17.404 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._publish_service_capabilities run_periodic_tasks /usr/lib/python2.7/dist-packages/cinder/openstack/common/periodic_task.py:176
2014-03-06 17:39:17.404 12096 DEBUG cinder.manager [-] Notifying Schedulers of capabilities ... _publish_service_capabilities /usr/lib/python2.7/dist-packages/cinder/manager.py:135
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] Making asynchronous fanout cast... fanout_cast /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:640
2014-03-06 17:39:17.405 12096 DEBUG cinder.openstack.common.rpc.amqp [-] UNIQUE_ID is b0e5e286d0094ff9a5e908fb49b5106d. _add_unique_id /usr/lib/python2.7/dist-packages/cinder/openstack/common/rpc/amqp.py:345
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] Closed channel #1 _do_close /usr/lib/python2.7/dist-packages/amqp/channel.py:88
2014-03-06 17:39:17.407 12096 DEBUG amqp [-] using channel_id: 1 __init__ /usr/lib/python2.7/dist-packages/amqp/channel.py:70
2014-03-06 17:39:17.408 12096 DEBUG amqp [-] Channel open _open_ok /usr/lib/python2.7/dist-packages/amqp/channel.py:420
2014-03-06 17:39:17.408 12096 DEBUG cinder.openstack.common.periodic_task [-] Running periodic task VolumeManager._report_driver_status run_p
from the source code of Havana, following exception handling may somehow contain defect:
 312         try:
 313             size_in_mb = int(src_vol['size']) * 1024    # vol size is in GB
 314             volume_utils.copy_volume(src_attach_info['device']['path'],
 315                                      dest_attach_info['device']['path'],
 316                                      size_in_mb)
 317             copy_error = False
 318         except Exception:
 319             with excutils.save_and_reraise_exception():
 320                 msg = _(""Failed to copy volume %(src)s to %(dest)d"")
 321                 LOG.error(msg % {'src': src_vol['id'], 'dest': dest_vol['id']})
 322                 copy_error = True
 323         finally:
 324             self._copy_volume_data_cleanup(context, dest_vol, properties,
 325                                            dest_attach_info, dest_remote,
 326                                            force=copy_error)
 327             self._copy_volume_data_cleanup(context, src_vol, properties,
 328                                            src_attach_info, src_remote,
 329                                            force=copy_error)","Fix a message format error in migration cleanup

In some situations, the volume migration failed. After the cleanup
begin, code was trying to format a string value as an int,
this caused ""copy_error = True"" not being invoked, resulted
in UnboundLocalError exception.

Change-Id: I445394cc06fe221804901e8014d60e870f0fe2f7
Closes-Bug: #1290317"
898,752b7ab29ddc75d8ae54c52d824ecc94a9551b56,1379992781,,1.0,26,1,2,1,1,0.380946586,True,1.0,204078.0,6.0,2.0,False,5.0,1670260.5,6.0,2.0,331.0,1016.0,1169.0,313.0,912.0,1057.0,79.0,318.0,335.0,0.212765957,0.848404255,0.893617021,1589,1229508,1229508,neutron,752b7ab29ddc75d8ae54c52d824ecc94a9551b56,1,0,"“From the initial commit of NEC plugin, network_id of packetfilters table is nullable=False [1], but in folsom_initial db migration script nullable is set to True.”","Bug #1229508 in neutron: ""nec plugin: packetfilter table network_id nullable should be False in folsom_initial db migration ""","From the initial commit of NEC plugin, network_id of packetfilters table is nullable=False [1],
but in folsom_initial db migration script nullable is set to True.
nullable=False is just a more strict constraint than nullable=True,
so NEC plugin works but it is better to be fixed in the migration.
I will add the migration to network_id to nullable=False both upgrade and downgrade
to make sure nullable=False in any revision.
[1] https://github.com/openstack/neutron/blob/stable/folsom/quantum/plugins/nec/db/models.py#L53","Ensure nullable=False for netid in packetfilters table

From the initial commit of NEC plugin, network_id of packetfilters
table is nullable=False, but in folsom_initial db migration script
nullable is set to True. This commit ensure nullable=False for
network_id in packetfilters table in any migration revision.

Change-Id: I2f1ebc16b57d8d6548255079c66d326d97fda5b6
Closes-Bug: #1229508"
899,75564b4af164786f43f00a12bd16f26f4b0b9197,1409884667,,1.0,2,630,10,3,1,0.701868763,False,,,,,True,,,,,,,,,,,,,,,,,1268,1728,1365516,cinder,75564b4af164786f43f00a12bd16f26f4b0b9197,0,0,"this is basicly ""dead code"" so it should be removed","Bug #1365516 in Cinder: ""Revert parent commit for iSCSI refactor that didn't make it in Juno""","During Juno we had plans to clean up the iSCSI and target code.  Sadly the work rotted in gerrit without reviews and didn't make it into this release.  In the process there was a commit that merged that layed some foundation work:
Change-Id: Iaa55e31e3dadc7dcb58112302c3807a8f92bcada
Without the follow up work however this is basicly ""dead code"" so it should be removed from the Juno release and added back in with Kilo when we're ready to go again.","Revert iSCSI Target objects as independent objects

Removing from gate queue since failing unit tests

The second half of this code didn't make it in, so this is essentially
dead code. We'll readd this code back in Kilo.

This reverts commit d10fb796f491152486e394481e920479bab9e3d1.

Closes-Bug: #1365516
Change-Id: Ia4b3b4ef4a7b6f3829fe576ec370b6d74a7bc922"
900,7599d9d0526e22977caf6de13c751884a5d8f2f5,1385598885,,1.0,20,12,4,4,1,0.943480147,True,17.0,9243690.0,163.0,50.0,False,21.0,723310.5,45.0,5.0,1.0,653.0,653.0,1.0,589.0,589.0,1.0,283.0,283.0,0.003412969,0.484641638,0.484641638,88,486,1255150,neutron,7599d9d0526e22977caf6de13c751884a5d8f2f5,1,1,Add unit tests BUT find some errors,"Bug #1255150 in neutron: ""Missing plugin unit test coverage for allowed_address_pairs""","Some plugins, at least openvswitch and ml2, are not inheriting the test case for allowed address pairs.
This means that the bits related to address pairs use cases in these plugins are currently not covered by unit tests.","Fix ml2 & nec plugins for allowedaddresspairs tests

Enabling the allowedaddresspairs tests uncovered that update_port()
was not returning the expected updated port info (still contained
original info).

Change-Id: I88f252e1348d272edd114fbee69e2309d3740213
Closes-bug: #1255150"
901,759b3dcefc708aade88fd9c5906f86de9c7c67a8,1401481144,,1.0,6,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,935,1371,1325116,cinder,759b3dcefc708aade88fd9c5906f86de9c7c67a8,1,1,,"Bug #1325116 in Cinder: ""hp_lefthand_rest_proxy no handler for logger during tests""","No handlers could be found for logger when running lefthand tests.
Here's an example from https://jenkins03.openstack.org/job/gate-cinder-python26/477/console:
2014-05-30 17:27:15.848 | No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy""","hp_lefthand_rest_proxy no handler for logger during tests

Create the proxy during do_setup() instead of __init__()
so that the logger is ready for an import error during tests.

Closes-Bug: #1325116
Change-Id: I88145d97df5d746e92bc8302bf516ca9eaa05d00"
902,75a0ec684c4a972ad7703399b64ddf9300a7acf3,1401084623,,1.0,11,1,3,2,1,0.808014151,False,,,,,True,,,,,,,,,,,,,,,,,919,1355,1323173,nova,75a0ec684c4a972ad7703399b64ddf9300a7acf3,1,1,,"Bug #1323173 in OpenStack Compute (nova): ""Record wrong action name when migrate instance""","Actions are recorded in database while users take atcions in specific
instance. These actions can be listed by comand 'nova instance-action-list'.
Resize and migrate use same code path, but always record the action as
'resize' even user migrate instance.","Record right action name while migrate

Actions are recorded in database while users take atcions in specific
instance. These actions can be listed by comand 'nova instance-action-list'.
Resize and migrate use same code path, but always record the action as
'resize' even user migrate instance. This commit check the action
firstly then record right action.

Change-Id: I3462275fa3022173d55dfa0c87328d4bb518239b
Closes-Bug: #1323173"
903,75e39fc7b232ac54422ba6a3b5e58308e54297b1,1378399723,,1.0,6,2,2,1,1,1.0,True,1.0,236138.0,7.0,3.0,False,9.0,668577.0,27.0,3.0,5.0,425.0,425.0,5.0,422.0,422.0,5.0,403.0,403.0,0.00625,0.420833333,0.420833333,1513,1220947,1220947,cinder,75e39fc7b232ac54422ba6a3b5e58308e54297b1,1,1,"“should have been synchronized with the synchronize annotation, but were not.”","Bug #1220947 in Cinder: ""3par extend_volume methods not synchronized""","The HP3PAR FC and iSCSI drivers are not synchronizing entry into methods ""extend_volume"".","Synchronize extend_volume methods in 3PAR drivers

The extend_volume methods in the HP3PAR FC and
iSCSI drivers should have been synchronized with
the synchronize annotation, but were not.

Change-Id: Ic7f18b7314e10dfb499b16f43b72367fe3ed1176
Closes-Bug: #1220947"
904,764c5ec749821d36bb0215dc6002d3caea95d3b1,1409953680,,1.0,130,13,4,4,1,0.796266533,False,,,,,True,,,,,,,,,,,,,,,,,1872,1366205,1366205,Cinder,764c5ec749821d36bb0215dc6002d3caea95d3b1,1,1, ,"Bug #1366205 in Cinder: ""Delete consistency group failed""","In recent testing, delete CG failed because it compares host@backend with host@backend#pool (in delete_consistencygroup in volume/manager.py).  So extract_host needs to be called to fix this problem.
Another issue is about deleting a CG with no host.  This will throw exception when extract_host(group['host']) is called in delete_consistencygroup in volume/rpcapi.py.  The solution is to check the host field in consistencygroup/api.py and delete it from db there.","Delete consistency group failed

Delete CG failed because it compares host@backend with host@backend#pool
(in delete_consistencygroup in volume/manager.py). The fix is to call
extract_host before doing the comparison.

Another issue is about deleting a CG with no host. This will throw exception
when extract_host(group['host']) is called in delete_consistencygroup in
volume/rpcapi.py. The solution is to check the host field in consistencygroup/
api.py and delete it from db there.

Change-Id: Ife28a4f91bd8a4e123c74dac9748ee31cc7b7306
Closes-Bug: #1366205"
905,765b6c98e08577038d33c9a5d745bb75c6b9f06d,1392995896,,1.0,21,1,2,2,1,0.945660305,True,2.0,68494.0,14.0,7.0,False,11.0,3280903.0,20.0,3.0,0.0,369.0,369.0,0.0,368.0,368.0,0.0,265.0,265.0,0.000858369,0.22832618,0.22832618,453,868,1283080,glance,765b6c98e08577038d33c9a5d745bb75c6b9f06d,1,0,Evolution bug. ‘It seems this change is missing from Glance’,"Bug #1283080 in Glance: ""KeyError: 'user_identity'""","Using all default setting for setting up devstack. Once devstack is done, g-reg and g-api console will show same error like this.
KeyError: 'user_identity'
18c2599c92e1db002 with project_id : a068a8a689554202999265989f44f448 and roles: admin  _build_user_headers /opt/stack/py
thon-keystoneclient/keystoneclient/middleware/auth_token.py:951
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 851, in emit
    msg = self.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 684, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 724, in format
    return fmt.format(record)
  File ""/opt/stack/glance/glance/openstack/common/log.py"", line 648, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'","Fix logging context to include user_identity

The user_identity generated created from user, tenant, domain
user_domain and project_domain.
The new domain related values are default to None

Closes-Bug: #1283080

Change-Id: I5e43142afba3492ecf05b65ba24ee70f158f88de"
906,767fb98b885327d1ad1cd380682ae4745aa78387,1376308013,,1.0,7,5,1,1,1,0.0,True,1.0,974815.0,7.0,3.0,False,123.0,250332.0,455.0,3.0,41.0,3107.0,3132.0,41.0,2592.0,2617.0,14.0,2118.0,2125.0,0.002672844,0.377583749,0.378831076,1465,1211307,1211307,nova,767fb98b885327d1ad1cd380682ae4745aa78387,1,1,"“is executed successfully on MySQL and SQLite (though WHERE clause never evaluates to True, so related entities aren't actually deleted), but fails on PostgreSQL,”","Bug #1211307 in OpenStack Compute (nova): ""instance_group_delete() filters related entities by group UUID rather than by ID""","Table 'instance_groups' has two columns to unambiguously identify the specific row:
1) id (Integer, primary_key=True, autoincrement=True)
2) uuid  (String)
The former is used internally to bind related entities by FKs (InstanceGroupMember, InstanceGroupPolicy and InstanceGroupMetadata), and the latter is accepted by public DB API methods (this must be a miss in the DB schema design, because uuid  could be easily used for both use cases).
Having two 'id' columns is both misleading and error-prone. E. g. instance_group_delete() deletes the instance group (and all related entities) given its UUID value:
def instance_group_delete(context, group_uuid):
    """"""Delete an group.""""""
    session = get_session()
    with session.begin():
        count = _instance_group_get_query(context,
                                          models.InstanceGroup,
                                          models.InstanceGroup.uuid,
                                          group_uuid,
                                          session=session).soft_delete()
        if count == 0:
            raise exception.InstanceGroupNotFound(group_uuid=group_uuid)
        # Delete policies, metadata and members
        instance_models = [models.InstanceGroupPolicy,
                           models.InstanceGroupMetadata,
                           models.InstanceGroupMember]
        for model in instance_models:
            model_query(context, model, session=session).\
                    filter_by(group_id=group_uuid).\
                    soft_delete()
Related entities are filtered by 'group_id' column, but 'group_uuid' value is passed. Despite that these two columns are of different types, this statement is executed successfully on MySQL and SQLite (though WHERE clause never evaluates to True, so related entities aren't actually deleted), but fails on PostgreSQL, which is more strict when checking data types of values.","Fix instance_group_delete() DB API method

Table 'instance_groups' has two columns that can be used to
unambigously identify the specific row:
  - 'id' (Integer, primary_key=True, autoincrement=True)
  - 'uuid' (String)

The former is used internally to bind related instances by FKs
(InstanceGroupMember, InstanceGroupPolicy and InstanceGroupMetadata),
and the latter is accepted by public methods of DB API. This must
be a miss in DB schema design, because 'uuid' could be used for both
use cases.

instance_group_delete() deletes the instance group (and all related
instances) given its UUID value. When related entities are deleted
they are filtered by the FK value - the 'group_id' column, however,
the 'group_uuid' value is passed.

Such DELETE statement is executed successfully (though WHERE clause never
evaluates to True, so the query is logically incorrect) on MySQL and SQLite,
which are often too 'soft' when checking data types of passed values.

This patch fixes the instance_group_delete() method and renames the 'group_uuid'
argument of _instance_group_model_get_query() to 'group_id', because this one
is actually always called with id value rather than with uuid.

Closes-Bug: #1211307

Change-Id: Ic8bcb4b218f570f26420904aeb74eb14567235fb"
907,771327adbe9e563506f98ca561de9ded4d987698,1408574914,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1159,1616,1353697,neutron,771327adbe9e563506f98ca561de9ded4d987698,1,0,“ML2 RPC base version is wrongly bumped to 1.1 and it breaks hyper-v agent with ML2 plugin.”,"Bug #1353697 in neutron: ""Hyper-V agent raises UnsupportedRpcVersion: Specified RPC version, 1.1, not supported by this endpoint.""","The Hyper-V agent raises:
2014-08-06 10:42:37.096 2052 ERROR neutron.openstack.common.rpc.amqp [req-46340a1a-9143-45c9-b645-2612d41f20a6 None] Exception during message handling
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\amqp.py"", line 462, in _process_data
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\site-packages\neutron\openstack\common\rpc\dispatcher.py"", line 178, in dispatch
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp     raise rpc_common.UnsupportedRpcVersion(version=version)
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp UnsupportedRpcVersion: Specified RPC version, 1.1, not supported by this endpoint.
2014-08-06 10:42:37.096 2052 TRACE neutron.openstack.common.rpc.amqp
The issue does not affect functionality, but it creates a lot of noise in the logs since the error is logged at each iteration.","Fixes Hyper-V issue due to ML2 RPC versioning

The ML2 RPC version 1.1 breaks the hyper-v agent and
consequentially the RPC version in the Hyper-V agent needs
to be set to 1.1 to match the ML2 RPC base API version.

Change-Id: Idc4ae8d7dcff4331aca148a8f2a7a2b01679b3a8
Closes-Bug: #1353697"
908,773352e39f371cda9c2f1be4c55accf4b7dc017b,1394841072,,1.0,9,4,1,1,1,0.0,True,1.0,564095.0,21.0,5.0,False,7.0,9249997.0,8.0,5.0,501.0,1256.0,1510.0,406.0,1109.0,1298.0,244.0,607.0,700.0,0.252837977,0.62745098,0.723426213,624,1049,1292742,neutron,773352e39f371cda9c2f1be4c55accf4b7dc017b,0,0,It’s a bug related to the tests,"Bug #1292742 in neutron: ""supported_extension_aliases in cisco network_plugin grows in unit tests""","In Cisco network_plugin unit tests, supported_extension_aliases grows while running unit tests.
It is because cisco network plugin extends supported_extension_aliases.
supported_exntension_aliases is a class attribute, so it will not reset to the original even after each unit test finished.
Message like this is annoying when debugging unit tests.
Note that in a real environment the plugin is initialized only once and there is no negative impact.
2014-03-15 08:35:39,867     INFO [neutron.manager] Loading core plugin: neutron.plugins.cisco.network_plugin.PluginV2
2014-03-15 08:35:39,928     INFO [neutron.plugins.openvswitch.ovs_neutron_plugin] Network VLAN ranges: {'physnet1': [(1000, 1100)]}
2014-03-15 08:35:39,957     INFO [neutron.manager] supported_extension_aliases=['credential', 'Cisco qos', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'ag
ent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', '
agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute',
 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute
', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_schedule
r', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sched
uler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'binding', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_sch
eduler', 'extra_dhcp_opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_
opt', 'allowed-address-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-addres
s-pairs', 'provider', 'external-net', 'router', 'ext-gw-mode', 'binding', 'quotas', 'agent', 'extraroute', 'l3_agent_scheduler', 'dhcp_agent_scheduler', 'extra_dhcp_opt', 'allowed-address-pairs']
2014-03-15 08:35:39,957     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,957     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.manager] Service L3_ROUTER_NAT is supported by the core plugin
2014-03-15 08:35:39,958     INFO [neutron.api.extensions] Initializing extension manager.
2014-03-15 08:35:39,959    ERROR [neutron.api.extensions] Extension path 'unit/extensions' doesn't exist!
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: __init__.py
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: __init__.pyc
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: _credential_view.py
2014-03-15 08:35:39,959     INFO [neutron.api.extensions] Loading extension file: _qos_view.py","cisco: Do not change supported_extension_aliases directly

cisco network plugin extends supported_extension_aliases in __init__
but supported_exntension_aliases is a class attribute and it is not
reset to the original even after each unit test finished.
To avoid this this patch copies supported_extension_aliases to
an instance attribute and extends it to ensure the class variable
is not changed. This reduces unnecessary logs in unit tests.

Change-Id: I3a7313f5ca2d10b1ae6ea961d9d05611aee055c0
Closes-Bug: #1292742"
909,7753ce5fc3e489857e785dac08c951d32050f8d5,1377755164,,1.0,3,3,1,1,1,0.0,True,1.0,15990.0,10.0,5.0,False,5.0,1940255.0,5.0,4.0,8.0,1282.0,1286.0,8.0,1185.0,1189.0,8.0,114.0,118.0,0.035714286,0.456349206,0.472222222,1493,1218185,1218185,neutron,7753ce5fc3e489857e785dac08c951d32050f8d5,0,0,Test files,"Bug #1218185 in neutron: ""Use assertEqual instead of assertEquals""","https://review.openstack.org/#/c/27818/  introduces thee spot of assertEquals() which is deprecated
see https://review.openstack.org/#/c/27818/8/neutron/tests/unit/openvswitch/test_ovs_db.py","Use assertEqual instead of assertEquals

Closes-Bug: #1218185

For unittest, assertEquals is deprecated in py3, use assertEqual
instead.

Change-Id: Ia71b51a0f2625aec5fb4644853a79c2ea80a799c"
910,77a58ecd0a65b688d9645a3c11a6dfb87e180449,1396935009,,1.0,6,7,6,2,1,0.992450904,True,1.0,50163.0,26.0,2.0,False,11.0,931021.5,42.0,2.0,69.0,1676.0,1698.0,69.0,1421.0,1443.0,60.0,843.0,860.0,0.054905491,0.759675968,0.774977498,762,1189,1304558,neutron,77a58ecd0a65b688d9645a3c11a6dfb87e180449,0,0,"Refactoring “This should be removed once the root cause of the patching issue is found.
“","Bug #1304558 in neutron: ""BigSwitch: Remove duplication of HTTPConnection""","httplib.HTTPConnection is being assigned to a module-level variable in the Big Switch server manager module. This was a work-around to mock not correctly stopping patches targeting the normal library path.
This should be removed once the root cause of the patching issue is found.
https://github.com/openstack/neutron/blob/f64eacfd27220c180f6afc979087b35aa1385550/neutron/plugins/bigswitch/servermanager.py#L74","BSN: Remove module-level ref to httplib method

Removes a module-level variable that was just a
pointer to the httplib.HTTPConnection class. It's
purpose was to give mock.patch a target that wouldn't
affect other code using the httplib.HTTPConnection
library. However, it is unnecessary now that the
root cause of the original patches not being stopped
by mock.patch.stopall was fixed in bug 1303605 (change
Ia5b374c5f8d3a7905d915de4f1f8d4f3a6f0e58d).

Closes-Bug: #1304558
Change-Id: Idf814b075a80245bb9751466c2b58d719a1b81f3"
911,77a7d14542600f2badcdf048fe6b586a0ff27e30,1405766839,1.0,1.0,8,3,2,2,1,0.684038436,False,,,,,True,,,,,,,,,,,,,,,,,1093,1544,1347156,nova,77a7d14542600f2badcdf048fe6b586a0ff27e30,1,1,,"Bug #1347156 in OpenStack Compute (nova): ""deleting floating-ip in nova-network does not free quota""","It seems that when you allocate a floating-ip in a tenant with nova-network, its quota is never returned after calling 'nova floating-ip-delete' ecen though 'nova floating-ip-list' shows it gone. This behavior applies to each tenant individually. The gate tests are passing because they all run with tenant isolation. But this problem shows in the nightly run without tenant isolation:
http://logs.openstack.org/periodic-qa/periodic-tempest-dsvm-full-non-isolated-master/2bc5ead/console.html","Commit quota when deallocate floating ip

It seems that when you allocate a floating-ip in a tenant with nova-network,
its quota is never returned after calling 'nova floating-ip-delete'
even though 'nova floating-ip-list' shows it gone.
This behavior applies to each tenant individually. The gate tests are passing
because they all run with tenant isolation.

The root cause of the problem is cooperation between
commit 23a27e47 and cbbb9de5. db layer code return floatingip_ref
but object layer didn't. This patch fixed the problem by adding return
value from object layer.

Change-Id: Ide1a338b6c33676311028e8738150e146324a8ee
Closes-Bug: #1347156"
912,77aee9e71523c543286d05a2f553d4a30778d17c,1378829609,1.0,1.0,71,27,4,4,1,0.520616466,True,14.0,6274534.0,91.0,27.0,False,9.0,660967.0,9.0,2.0,4.0,575.0,579.0,4.0,488.0,492.0,1.0,144.0,145.0,0.006514658,0.472312704,0.475570033,1580,1228008,1228008,neutron,77aee9e71523c543286d05a2f553d4a30778d17c,0,0,Bug or feature3 “should support update with value None”,"Bug #1228008 in neutron: ""extra_dhcp_opt extension should support update with value None""","we should support update extra_dhcp_opt with value None, if opt_value is None, then delete extra_dhcp_opt on that port","Adds delete of a extra_dhcp_opt on a port

Add support for delete of extra_dhcp_opt(s) on a port. Where
[{'opt_name': 'opt_to_delete', 'opt_value': None}].

Closes-Bug: #1228008

Change-Id: I75baeff91575cac6546fe2cc6fcf7a0d8e92853f"
913,77b6a9f129977799bc5c6ccec6caaf707fa3047c,1397149070,,1.0,30,1,2,2,1,0.770629069,True,4.0,470297.0,22.0,4.0,False,93.0,34641.0,250.0,1.0,240.0,45.0,284.0,215.0,45.0,259.0,218.0,9.0,226.0,0.139312977,0.006361323,0.144402036,707,1133,1300148,cinder,77b6a9f129977799bc5c6ccec6caaf707fa3047c,1,1,,"Bug #1300148 in Cinder: ""cannot attach volume as regular user with lioadm iscsi helper""","In usual devstack setup but with the following config changes:
/etc/cinder/cinder.conf:
[DEFAULT]
iscsi_helper = lioadm
As admin user I was able to attach the volume to vm, but as demo user it fails with require_admin_context.
 ../screen-logs/screen-c-vol.log:
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 797, in initialize_connection
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     self.driver.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 540, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     self.target_helper.remove_export(context, volume)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/volume/iscsi.py"", line 232, in remove_export
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     volume['id'])
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/api.py"", line 234, in volume_get_iscsi_target_num
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     return IMPL.volume_get_iscsi_target_num(context, volume_id)
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/cinder/cinder/db/sqlalchemy/api.py"", line 118, in wrapper
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher     raise exception.AdminRequired()
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher AdminRequired: User does not have admin privileges
2014-03-31 10:49:20.906 787 TRACE oslo.messaging.rpc.dispatcher
 ../screen-logs/screen-n-cpu.log:
2014-03-31 10:04:25.586 ERROR nova.compute.manager [req-99dc297a-db00-4409-a634-8a17a8313a84 demo demo] [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Failed to attach a6fd9536-151d-48ac-b084-7e5419efad78 at /dev/vdc
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Traceback (most recent call last):
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 4140, in _attach_volume
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     do_check_attach=False, do_driver_attach=True)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 44, in wrapped
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     ret_val = method(obj, context, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/virt/block_device.py"", line 225, in attach
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 173, in wrapper
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     res = method(self, ctx, volume_id, *args, **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/nova/nova/volume/cinder.py"", line 271, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     connector)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 321, in initialize_connection
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     {'connector': connector})[1]['connection_info']
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/v1/volumes.py"", line 250, in _action
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     return self.api.client.post(url, body=body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 210, in post
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     return self._cs_request(url, 'POST', **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 174, in _cs_request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     **kwargs)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]   File ""/opt/stack/new/python-cinderclient/cinderclient/client.py"", line 157, in request
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]     raise exceptions.from_response(resp, body)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d] Forbidden: User does not have admin privileges (HTTP 403) (Request-ID: req-7a872133-72f4-4765-b055-4791629d73f8)
2014-03-31 10:04:25.586 32469 TRACE nova.compute.manager [instance: e74e3b7c-314a-4249-8ead-06288e07f49d]","Fix dropped exception for create_export in vol manager

If self.driver.create_export() fails, model_update is
False and this exception is therefore not logged,
causing things to break later in the execution path in
unexpected ways.

Closes-Bug: #1300148

Change-Id: I77600f3311efd71fa644adb6d65e4b19ab306203"
914,77dd352933ca4272bfc376bd2f9ea3ff781a9bf8,1401974145,,1.0,6,6,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,950,1386,1326778,nova,77dd352933ca4272bfc376bd2f9ea3ff781a9bf8,1,1,"“This reverts commit 6dd5cc5.""","Bug #1326778 in OpenStack Compute (nova): ""resize test fails with ""Returning 400 to user: Instance has not been resized"" after VERIFY_RESIZE state""","Fails here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/console.html
The error in the n-api log is here:
http://logs.openstack.org/93/96293/2/gate/gate-tempest-dsvm-postgres-full/a644174/logs/screen-n-api.txt.gz#_2014-06-05_06_50_26_679
From the console log, it looks like it goes into an error state after going to verify_resize state:
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,573 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 200 GET http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55 0.103s
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,573 State transition ""RESIZE/resize_finish"" ==> ""VERIFY_RESIZE/None"" after 16 second wait
2014-06-05 06:50:26.714 |     2014-06-05 06:50:26,681 Request (MigrationsAdminTest:test_list_migrations_in_flavor_resize_situation): 400 POST http://127.0.0.1:8774/v2/7d6640f8f8e34866b5bd00e109fe90b7/servers/88591e95-69a2-4e34-a294-90b79d8f0d55/action 0.106s
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTWlncmF0aW9uc0FkbWluVGVzdFwiIEFORCBtZXNzYWdlOlwiSFRUUCBleGNlcHRpb24gdGhyb3duXFw6IEluc3RhbmNlIGhhcyBub3QgYmVlbiByZXNpemVkXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDE5NzIzNDQyMjh9
8 hits in 7 days, looks like this started on 6/5.  Fails in check and gate queues.","Revert ""Fix migration and instance resize update order""

This reverts commit 6dd5cc503cc05c00c5f9d831480539c67f6e2a48.

Change-Id: Id438276c3d71155ad6a9dfd31280a49e39905ae5
Closes-Bug: #1326778"
915,77ddd463bd2e96041c0b6ecea1c46c4ac4d55851,1395119089,,1.0,33,18,2,2,1,0.977417818,True,10.0,4245609.0,198.0,40.0,False,20.0,628781.0,32.0,4.0,2.0,807.0,808.0,2.0,656.0,657.0,2.0,432.0,433.0,0.003027245,0.436932392,0.437941473,648,1074,1294445,neutron,77ddd463bd2e96041c0b6ecea1c46c4ac4d55851,0,0,I think is implementation of keeping a record of the problems,"Bug #1294445 in neutron: ""Record and log reason for dhcp agent resync""","A dhcp resync can be triggered at a number of points, but the actual resync is done asynchronously by a helper thread. This means by the time the resync happens, it's hard to establish what actually caused it.
I've seen a number of problems in production systems that cause excessive resyncs. One is a ipv6/dnsmasq issue (rhbz#1077487) and another is db corruption with duplicate entries [1]. The resync triggers a whole lot of logs itself, so it becomes very unclear how to establish any causality.
What I propose is to keep track of what triggered the resync with some helpful information.
[1] The logs will contain output like ""DBDuplicateEntry
(IntegrityError) (1062, ""Duplicate entry
'6d799c6a-7a09-4c1e-bb63-7d30fd052c8a-d3e3ac5b-9962-428a-a9f8-6b2' for
key 'PRIMARY'"") ..."" in this case","Record and log reason for dhcp agent resync

A dhcp resync can be triggered at a number of points, but the actual
resync is done asynchronously by a helper thread.  This means by the
time the resync happens, it's hard to establish what actually caused
it.

There have been a number of problems in production systems that cause
excessive resyncs.  One is a ipv6/dnsmasq issue (rhbz#1077487) and
another is db corruption with duplicate entries [1].  The resync
triggers a whole lot of logs itself, so it becomes very unclear how to
establish any causality.

This change keeps track of what triggered the resync with some helpful
information.  We add a schedule_resync() function to to replace a
explicit set of needs_resync which tracks why it was called.

[1] The logs will contain output like ""DBDuplicateEntry
(IntegrityError) (1062, ""Duplicate entry
'6d799c6a-7a09-4c1e-bb63-7d30fd052c8a-d3e3ac5b-9962-428a-a9f8-6b2' for
key 'PRIMARY'"") ..."" in this case

Closes-Bug: #1294445
Change-Id: I9b1c6202f5a6bbad8589a8b64b2a38c9d9edb43b"
916,78059968e212d537c813f398da7f76714380bf4e,1409136521,,1.0,33,3,2,2,1,0.88797632,False,,,,,True,,,,,,,,,,,,,,,,,1212,1671,1358554,neutron,78059968e212d537c813f398da7f76714380bf4e,0,0,Bug in test,"Bug #1358554 in neutron: ""test_volume_boot_pattern fails with DVR routers""","This happens on master and appears to be a regression of https://review.openstack.org/#/c/112465/
Steps to reproduce:
- Deploy devstack with DVR *ON*
- Run tempest test test_volume_boot_pattern (more details available in [1])
- Watch the test fail
Once change 112465 is reverted, the test will be successful.
This does not imply that the change is at fault, instead, that another failure mode has been unveiled.
In fact, prior to change 112465, call [2] was made with the wrong agent id, which caused the delete_port operation to abort altogether; this is wrong and addressed in 112465. However, the boot from volume test, and its sequence of operations revealed that the clean-up logic is not working as it should.
[1] - https://github.com/openstack/tempest/blob/master/tempest/scenario/test_volume_boot_pattern.py#L31
[2] - https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L1029","Update DVR Binding when router_id changes

In this patch I modified update_dvr_port_binding to update the
binding if router_id changes.
When a new router namespace is created on a host, sync_router
will call _ensure_host_set_on_port that will update the DVR binding.
With the current code update_dvr_port_binding won't update the
binding if there's an existing binding unless the binding
vif_type is VIF_TYPE_BINDING_FAILED . There's a race when
the router namespace is destroyed and then recreated on the same
host.
The DVR binding is deleted only when the message
update_device_down is processed. If this message is processed
after the update_dvr_port_binding triggered by the namespace creation,
update_dvr_port_binding won't update the binding. When the router
interface will be detected as UP, no DVR binding will be found.

Closes-bug: #1358554
Change-Id: I37fd4ed67dc2019b57e36d082b584c517d8f67a8"
917,7811c8a8ffce78dc14815203858854c450f1c8f6,1398410697,,1.0,3,3,2,2,1,0.918295834,True,2.0,452568.0,34.0,5.0,False,25.0,306396.0,46.0,4.0,4.0,1050.0,1052.0,4.0,884.0,886.0,3.0,663.0,664.0,0.003384095,0.561759729,0.562605753,817,1245,1310460,neutron,7811c8a8ffce78dc14815203858854c450f1c8f6,1,1,,"Bug #1310460 in neutron: ""wrong key ""router.interface"" reported by ceilometer log""","In my testbed, ceilometer throws an exception which is caused by the wrong key format of neutron:
2014-04-01 10:52:39.135 6104 ERROR notification [-] Insert msg to Mongodb occured error:Traceback (most recent call last):
  File ""/usr/lib/python2.6/site-packages/ceilometer/billing/backend.py"", line 74, in insert_data
    self.db[collection].insert(data, safe=True)
  File ""/usr/lib64/python2.6/site-packages/pymongo/collection.py"", line 357, in insert
    continue_on_error, self.__uuid_subtype), safe)
InvalidDocument: key 'router.interface' must not contain '.'
It seems that mongodb doesn't support ""."" in key.
The workaround is to change ""."" with ""_"" in l3_db.py.","Wrong key router.interface reported by ceilometer

Ceilometer with MongoDB doesn't accept the key with dot.

The dot in 'router.interface' should be replaced with '_',
which is also the standard implementation of other keys in neutron.

Change-Id: If7fcf7658db9ee2d7d1248bb0b016b4389c32c77
Closes-Bug: #1310460"
918,783e3243f98a90550a8b7ec8ce279709ab8db372,1386854569,,2.0,204,154,3,3,2,0.659616394,True,10.0,2760708.0,28.0,6.0,False,19.0,3206686.333,35.0,2.0,82.0,1193.0,1254.0,82.0,1028.0,1089.0,29.0,1067.0,1075.0,0.020026702,0.712950601,0.718291055,180,583,1260322,cinder,783e3243f98a90550a8b7ec8ce279709ab8db372,0,0,Implementation/Evolution,"Bug #1260322 in Cinder: ""Cinder sample config does not contain groups other than [DEFAULT]""","The sample configuration file of cinder shipped with Havana and master lacks config groups other than [DEFAULT], which makes the configuration not actually work.
One example is the key ""connection="", which is in the sample configuration in the group [DEFAULT], but actually needs to be in the group [database] in order to be found by the code. But there are dozens of examples like that.","Switch to Oslo's config generator

This fixes the sample configuration to contain proper
config groups alongway.

Regenerate sample config with oslo's config generator.

DocImpact: Removes the deprecated memcached_server
option from DEFAULT (now in keystone_authtoken group)

Closes-Bug: #1206650
Closes-Bug: #1260322

Change-Id: I170715bfe6ec7dad9f7317e7bb56cbb68034c4ec"
919,783e3243f98a90550a8b7ec8ce279709ab8db372,1386854569,,2.0,204,154,3,3,2,0.659616394,True,10.0,2760708.0,28.0,6.0,False,19.0,3206686.333,35.0,2.0,82.0,1193.0,1254.0,82.0,1028.0,1089.0,29.0,1067.0,1075.0,0.020026702,0.712950601,0.718291055,1455,1206650,1206650,cinder,783e3243f98a90550a8b7ec8ce279709ab8db372,0,0,“This fixes the sample configuration to contain proper config groups alongway.”,"Bug #1206650 in Cinder: ""Cinder does not use Oslo for config generator ""","We should update Cinder for using Oslo when we generate configuration files.
Here are the steps :
- Add generator.py from Oslo
- Add generator bash script used by other projects to generate configuration
files
- Update openstack-common.conf file
- Generate configuration file","Switch to Oslo's config generator

This fixes the sample configuration to contain proper
config groups alongway.

Regenerate sample config with oslo's config generator.

DocImpact: Removes the deprecated memcached_server
option from DEFAULT (now in keystone_authtoken group)

Closes-Bug: #1206650
Closes-Bug: #1260322

Change-Id: I170715bfe6ec7dad9f7317e7bb56cbb68034c4ec"
920,78aa12567d226fe73df483d75e0d1980e6d3625e,1379720452,3.0,1.0,29,9,2,2,1,0.926819064,True,6.0,357645.0,30.0,9.0,False,5.0,3955386.5,5.0,3.0,200.0,522.0,646.0,164.0,472.0,573.0,33.0,125.0,149.0,0.092140921,0.341463415,0.406504065,1582,1228442,1228442,neutron,78aa12567d226fe73df483d75e0d1980e6d3625e,1,1, ,"Bug #1228442 in neutron: ""FWaaS plugin should allow creating only one firewall per tenant""","The reference implementation of the FWaaS iptables agent/driver supports only one firewall per tenant in Havana release. However, the FWaaS plugin will let you create more than one firewall. This should not be allowed since it creates an inconsistent state with only the most recent policy being applied.","Fix FWaaS plugin to allow one firewall per tenant

The reference implementation of the FWaaS iptables
agent/driver supports only one firewall per tenant
in Havana release. However, the FWaaS plugin will
let you create more than one firewall. This is
being fixed in this patch to not allow creating
the second firewall if a firewall already exists
for the tenant.

Change-Id: I8f1cad9791723ba919b5774a63982c204686ddfe
Closes-Bug: #1228442"
921,78d62186e5b0388f740d42cb8da5798cd67d7880,1387281165,,1.0,0,3,2,2,1,0.918295834,True,2.0,2981159.0,23.0,7.0,False,37.0,1107535.0,91.0,4.0,90.0,1135.0,1192.0,88.0,961.0,1017.0,53.0,1061.0,1086.0,0.00785112,0.15440535,0.158040128,197,600,1261728,nova,78d62186e5b0388f740d42cb8da5798cd67d7880,0,0,tests,"Bug #1261728 in OpenStack Compute (nova): ""Interprocess file locks aren't usable in unit tests""","Base test case class has a fixture, that overrides CONF.lock_path value, which means that every test case will have CONF.lock_path set to its own temporary dir path. This makes interprocess locks unusable in unit tests, which is likely to break tests when they are run concurrently using testr (e.g. tests using MySQL/PostgreSQL might want to be run exclusively).","Fix interprocess locks when running unit-tests

When running unit-tests concurrently using testr, we might
need to use interprocess file locks (e.g. for running various
tests using the same DB in MySQL/PostgreSQL).

At the same time file locks are only usable when they are placed
to one dir, and we have a test fixture that overrides the lock_path
config value for each test case with a new temporary directory.

The solution is to create the one temprorary directory for locks
to be used by all tests before running tests and then remove it
after running tests. The corresponding helper has been already
put to openstack.common.lockutils module, we just need to reuse
it properly, i.e. change the way unit tests are run in tox.ini.

Closes-Bug: #1261728

Change-Id: I76f95a9f7fdd31c15e6cf4fd6316c7569284f780"
922,7910385825ccfa705785af360fcd5717656e3557,1380565541,,1.0,51,45,3,2,1,0.698746661,True,18.0,5719706.0,103.0,50.0,False,44.0,711303.0,106.0,8.0,115.0,3439.0,3492.0,106.0,2871.0,2918.0,111.0,2382.0,2435.0,0.018187723,0.386976291,0.395582981,1597,1229994,1229994,nova,7910385825ccfa705785af360fcd5717656e3557,1,1, ,"Bug #1229994 in OpenStack Compute (nova): ""VMwareVCDriver: snapshot failure when host in maintenance mode""","Image snapshot through the VC cluster driver may fail if, within the datacenter containing the cluster managed by the driver, there are one or more hosts in maintenance mode with access to the datastore containing the disk image snapshot.
A sign that this situation has occurred is the appearance in the nova compute log of an error similar to the following:
2013-08-02 07:10:30.036 WARNING nova.virt.vmwareapi.driver [-] Task [DeleteVirtualDisk_Task] (returnval){
value = ""task-228""
_type = ""Task""
} status: error The operation is not allowed in the current state.
What this means is that even if all hosts in cluster are running fine in normal mode, a host outside of the cluster going into maintenance mode may
lead to snapshot failure.
The root cause of the problem is due to an issue in VC's handler of the VirtualDiskManager.DeleteVirtualDisk_Task API, which may incorrectly pick a host in maintenance mode to service the disk deletion even though such an operation will be rejected by the host under maintenance.","VMware: fix snapshot failure when host in maintenance mode

The root cause is due to a bug in the VC's handling of the
VirtualDiskManager.DeleteVirtualDisk_Task API, which allows the picking
of any host in a datacenter with access to the datastore participating
in the disk deletion picked be to perform the operation, even when the
host is in maintenance mode and hence will always reject the call when
sent.

The fix uses an alternative API (FileManager.DeleteDatastoreFile_Task)
to delete the vmdk and -flat vmdk files separately. This API does not
suffer from the above-mentioned failure mode.

Closes-Bug: #1229994

Change-Id: I786365847673e5192a21b654cba951b2e7a6f291"
923,793224b2668f6522c8197bd3c5e3a7612dc4c9f0,1394486846,,1.0,76,1,4,2,1,0.691504595,True,2.0,1765616.0,48.0,11.0,False,15.0,2255.0,32.0,4.0,2.0,621.0,621.0,2.0,511.0,511.0,2.0,308.0,308.0,0.003191489,0.328723404,0.328723404,584,1005,1290561,neutron,793224b2668f6522c8197bd3c5e3a7612dc4c9f0,1,1,,"Bug #1290561 in neutron: ""Failure in Cisco N1KV controller due to lack of L3 port info""","A recent API change in Cisco N1KV controller requires L3 params for port creation.
N1KV Neutron plugin does not pass these params in the REST call to the controller.
The fix is to add the missing params in the REST body.","Add missing parameters for port creation

Recent API changes in N1KV controller require parameters such as
subnet id and IP address to handle port creation successfully.
Without these mandatory paramters, the REST call fails.
This patch addresses that by passing in the missing parameters
in the REST body.

Change-Id: I4cec6868051f492b9d1245ab201ff6c1a0837848
Closes-Bug: #1290561"
924,793604653f68e2d7d184182d0a976a7d43c07607,1394549098,,1.0,55,25,3,2,1,0.9891986,True,4.0,3734933.0,67.0,16.0,False,85.0,641793.6667,357.0,5.0,413.0,1378.0,1378.0,393.0,1249.0,1249.0,405.0,1342.0,1342.0,0.0536895,0.177598519,0.177598519,243,647,1265465,nova,793604653f68e2d7d184182d0a976a7d43c07607,1,1,,"Bug #1265465 in OpenStack Compute (nova): ""xenapi: auto disk config drops boot partition flag""","When the XenAPI driver resizes a boot partition, it does not take care to add back the boot partition flag.
With PV images, this is not really needed, because Xen doesn't worry about the partition being bootable, but for HVM images, it is stops the image from booting any more.","xenapi: make auto_config_disk persist boot flag

Currently auto_config_disk creates a new partition that ignores many
settings on the original partition.

The key flag that should be respected is the ""boot"" flag. When present
on the old partition, it should be present on the new partition.

Change-Id: I0408112cd60046422d6863651323fc9a7313d488
Closes-bug: #1265465"
925,793c76344715790531d39436508b188b536e2ff5,1380452984,0.0,1.0,155,2,6,3,1,0.610517711,True,9.0,16389631.0,141.0,36.0,False,5.0,1794123.5,5.0,5.0,2.0,1871.0,1871.0,2.0,1733.0,1733.0,1.0,356.0,356.0,0.004987531,0.890274314,0.890274314,1584,1228827,1228827,neutron,793c76344715790531d39436508b188b536e2ff5,1,1, ,"Bug #1228827 in neutron: ""add request timeout handling for Mellanox Neutron Agent""","Mellanox Neutron Agent communicates with eswitchd to execute vports discovery and configuration.
In case eswitchd is not reachable, request timeout is received. The current handling of the timeout was to mark out of sync flag, flush the agent cache and try again. This behavior leads to miss the removed vports and their configuration is not removed. The request timeout was received in case group of VMs was removed at once. The solution is to add number of retries with increasing waiting time in case  request timeout is received. If request timeout persists, exit the agent process.","Add request timeout handling for Mellanox Neutron Agent

Add request timeout handling for messages sent to eswitch Daemon.
Using configurable number of retries and increasing waiting interval
between retries resend the message.
If request timeout persists, eswitch daemon is not reachable.
In such case, exit the agent process.

Closes-Bug: #1228827

Change-Id: If1290bedafe7a0dd8a61cbe328510075edeb1e5b"
926,794a1325dfc8a6937412a597f19acecf08d4b34a,1386109735,,1.0,85,29,4,3,1,0.682202181,True,11.0,1519107.0,77.0,25.0,False,12.0,4519670.0,14.0,5.0,21.0,1158.0,1168.0,20.0,1027.0,1036.0,16.0,474.0,479.0,0.02828619,0.790349418,0.798668885,136,537,1257467,neutron,794a1325dfc8a6937412a597f19acecf08d4b34a,0,0,feature,"Bug #1257467 in neutron: ""extra_dhcp_opts allows empty strings""","the extra_dhcp_opts allows empty string '      ', as an option_value, which dnsmasq has been detected ans segment faulting when encountering a  tag:ece4c8aa-15c9-4f6b-8c42-7d4e285734bf,option:server-ip-address, option in the opts file.
Checks are need in the create and update protions of the extra_dhcp_opts extension to prevent this.","extra_dhcp_opt add checks for empty strings

When passing extra-dhcp-opt into the port-create where an empty string
is provided as opt_value='   ', the create and update routine will load the
empty string into the DB. The result when written to the opts file is:
""tag:ece4c8aa-15c9-4f6b-8c42-7d4e285734bf,option:server-ip-address"", which
when read by dnsmasq has been tested to cause dnsmasq to segment fault.

Change-Id: I31de4a3d27092bb219d20221c5ef5a6b22e050dc
Closes-Bug: #1257467"
927,79ab96e34ba5b8dd3e4e542dd3a7f65624b13367,1397147168,1.0,1.0,17,19,2,2,1,0.918295834,True,1.0,95234.0,28.0,5.0,False,38.0,3419998.5,83.0,3.0,1189.0,4348.0,4875.0,986.0,3051.0,3494.0,1170.0,4145.0,4656.0,0.150610932,0.533247588,0.598971061,777,1204,1306009,nova,79ab96e34ba5b8dd3e4e542dd3a7f65624b13367,1,1,,"Bug #1306009 in OpenStack Compute (nova): ""nova-api-metadata requires db access when configured with nova-network""","When nova-network is being used, nova-api-metadata requires db access which in some configurations means the compute nodes still connect to the database.
It connects to the db when trying to map the fixed ip address to an instance uuid:
fixed_ip = network.API().get_fixed_ip_by_address(ctxt, address)
This should be changed to do an rpc call to conductor.","Fix network-api direct database hits in metadata server

The metadata server was using a couple of methods from the network
api that caused direct-to-database accesses. Those are no longer
necessary with the object modeling of network resources. This patch
converts those to use objects and avoid the direct database
traffic.

Change-Id: Iea48f74387afa35af3889eebdfa1af169a52ad1d
Closes-bug: #1306009"
928,79bfb1bf343484e98aa36dcc663a5370baf4cab7,1410535914,,1.0,17,5,2,2,1,0.994030211,False,,,,,True,,,,,,,,,,,,,,,,,1285,1748,1367344,nova,79bfb1bf343484e98aa36dcc663a5370baf4cab7,1,1,“Commit f0ff4d51057080e769407e873e5ed212f15b773d caused the problem.”,"Bug #1367344 in OpenStack Compute (nova): ""Libvirt Watchdog support is broken when ComputeCapabilitiesFilter is used""","The doc (http://docs.openstack.org/admin-guide-cloud/content/customize-flavors.html , section ""Watchdog behavior"") suggests to use the flavor extra specs property called ""hw_watchdog_action"" to configure a watchdog device for libvirt guests. Unfortunately, this is broken due to ComputeCapabilitiesFilter trying to use this property to filter compute hosts, so that scheduling of a new instance always fails with NoValidHostFound error.","Fix libvirt watchdog support

Using the flavor extra_specs property ""hw_watchdog_action"" was broken.
Scheduling of a new instance always failed with NoValidHostFound error
because of ComputeCapabilitiesFilter, which treated this property as a
host capability to be checked.

Commit f0ff4d51057080e769407e873e5ed212f15b773d caused the problem.

To fix this watchdog_action property is put into 'hw:' scope, so
that it will be ignored by ComputeCapabilitiesFilter in scheduler and
handled in libvirt driver. The doc must be fixed accordingly.

Now driver accepts both 'hw_watchdog_action' and
'hw:watchdog_action', tests were edited for these cases.
Were added TODO items to delete the compat code in the next release.

DocImpact
Closes-Bug: #1367344

Change-Id: Ic5344ec34a130ee5a0ed2c7348af0b9d79e3508e"
929,79f1e8a9c1f308586077483d849e66dcdc83144f,1412044386,,1.0,52,2,2,2,1,0.852405179,False,,,,,True,,,,,,,,,,,,,,,,,1367,1837,1375597,neutron,79f1e8a9c1f308586077483d849e66dcdc83144f,1,1,,"Bug #1375597 in neutron: ""exception will kill router rescheduling call""",An encountered exception in the router rescheduling loop will cause the loop to terminate and not be tried again. Retries should continue at the normal interval.,"Catch exceptions in router rescheduler

Catch and log exceptions in router rescheduling loop
rather than just dying which would stop all future
router rescheduling attempts.

This prevents transient DB connectivity issues from
permanently breaking the rescheduler until the process
restarts.

Closes-Bug: #1375597
Change-Id: I2ab37847074fa6bbdd2b13fd03b8742996dcfc78"
930,79f6ccd1b5b67c23d438332e9d8641d07426a9a1,1403062376,,1.0,3,3,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,988,1427,1331839,neutron,79f6ccd1b5b67c23d438332e9d8641d07426a9a1,1,1,,"Bug #1331839 in neutron: ""vpn agent fails to remove iptables rule on vpn-site-connection deletion""","The following warning appears when deleting VPNaaS' vpn-site-connection object:
2013-12-15 13:57:04.274 6899 WARNING neutron.agent.linux.iptables_manager [-] Tried to remove rule that was not there: 'POSTROUTING' u'-s 10.35.214.0/24 -d 10.35.7.0/24 -m policy --dir out --pol ipsec -j ACCEPT ' True False","Pass 'top' to remove_rule so that rule matching succeeds

When deleting a vpn-site-connection, deleting the nat rule would
fail because it was created with top=True, but top defaults to
'false' in remove_rule and was not being passed. This caused the
rule matching to fail and the rule to not be deleted.

Change-Id: I51012a783314c97e85b31fc8a73be4cbb8ee7dc5
Closes-Bug: #1331839"
931,7a2053c7a27bdec02e97b46736c5fcce5f2d053a,1395582522,,1.0,62,1,2,2,1,0.549108745,True,3.0,2885548.0,63.0,8.0,False,22.0,481316.0,40.0,5.0,0.0,927.0,927.0,0.0,741.0,741.0,0.0,497.0,497.0,0.000974659,0.485380117,0.485380117,630,1056,1293184,neutron,7a2053c7a27bdec02e97b46736c5fcce5f2d053a,1,1,Bug,"Bug #1293184 in neutron: ""Can't clear shared flag of unused network""","A network marked as external can be used as a gateway for tenant routers, even though it's not necessarily marked as shared.
If the 'shared' attribute is changed from True to False for such a network you get an error:
Unable to reconfigure sharing settings for network sharetest. Multiple tenants are using it
This is clearly not the intention of the 'shared' field, so if there are only service ports on the network there is no reason to block changing it from shared to not shared.","Allow unsharing a network used as gateway/floatingip

If network has ports that connect it as gateway to another network, such
as an external network, or ""floating ip"" ports it should still be
possible to ""unshare"" it since these ports represent usage of the
network as an external network, not as a VM data network.

Change-Id: Ic33ae7ac193e4fc9fe06a4cfd579a4aacf0e6354
Closes-bug: #1293184"
932,7a34be0ec0cd0cb9555fe64ff6c486faae1ae91d,1380550506,,1.0,5,1,2,2,1,0.918295834,True,3.0,14592758.0,26.0,10.0,False,26.0,299272.0,54.0,8.0,37.0,2148.0,2155.0,37.0,1901.0,1908.0,34.0,1211.0,1217.0,0.005687358,0.196945076,0.197920052,1619,1233188,1233188,nova,7a34be0ec0cd0cb9555fe64ff6c486faae1ae91d,1,1, ,"Bug #1233188 in OpenStack Compute (nova): ""Cant create VM with rbd backend enabled""","nova-compute.log:
2013-09-30 15:52:18.897 12884 ERROR nova.compute.manager [req-d112a8fd-89c4-4b5b-b6c2-1896dcd0e4ab f70773b792354571a10d44260397fde1 b9e4ccd38a794fee82dfb06a52ec3cfd] [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Error: libvirt_info() takes exactly 6 arguments (7 given)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] Traceback (most recent call last):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1037, in _build_instance
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     set_access_ip=set_access_ip)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1410, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     LOG.exception(_('Instance failed to spawn'), instance=instance)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1407, in _spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2069, in spawn
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     write_to_disk=True)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3042, in to_xml
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     disk_info, rescue, block_device_info)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2922, in get_guest_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     inst_type):
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2699, in get_guest_storage_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     inst_type)
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 2662, in get_guest_disk_config
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86]     self.get_hypervisor_version())
2013-09-30 15:52:18.897 12884 TRACE nova.compute.manager [instance: f133ebdb-2f6f-49ba-baf3-296163a98c86] TypeError: libvirt_info() takes exactly 6 arguments (7 given)","Make rbd.libvirt_info parent class compatible

Rbd.libvirt_info function definition misses hypervisor_version argument added in change:
https://review.openstack.org/32379

Closes-Bug: #1233188
Change-Id: Ib68d743e783af0f6d82d2ba180869ee642e86050"
933,7a67a5676fb82ddb717e6b606cc702c31523b9fa,1387417879,,1.0,43,2,2,2,1,0.764204507,True,2.0,3016541.0,32.0,5.0,False,7.0,174868.0,8.0,3.0,1.0,998.0,998.0,1.0,896.0,896.0,1.0,472.0,472.0,0.003120125,0.737909516,0.737909516,1418,1136252,1136252,neutron,7a67a5676fb82ddb717e6b606cc702c31523b9fa,1,1,I’m not sure at all. I would say it has a BIC since the beginning the meta plugin had wrong logic,"Bug #1136252 in neutron: ""[Metaplugin] fails with VIF extension""","Current metaplugin didn't implements get_ports or get_port to proxy request for actual plugins.
so extension such as VIF extension will not be added the result of port.","Fix lack of extended port's attributes in Metaplugin

Previously, there are some extended port's attributes did not
appear in the result of GET port API when using Metaplugin.
This become a critial issue currently since lack of port binding
information disturbs normal port binding operation in a compute
node, for example.

Metaplugin did not delegate get_port/get_ports to target plugins.
This is cause of the problem because right plugin instance is not
passed to a hook which handles extended attributes.
Now, get_port/get_ports of target plugins are called so that
extended port's attributes are handled properly.

Change-Id: Ifed2505a97ceca56c4cedd22efa49adcdf3317b8
Closes-Bug: #1136252"
934,7a72303aff86ba2b06c41e73b35989cddd884652,1408145175,,1.0,34,7,2,2,1,0.947435136,False,,,,,True,,,,,,,,,,,,,,,,,1202,1660,1357261,cinder,7a72303aff86ba2b06c41e73b35989cddd884652,1,1,,"Bug #1357261 in Cinder: ""SolidFire: Volume not found isn't properly handled in accept_transfer()""","For some reason, when Solidfire driver tried to accept_transfer(), the backend failed to find the volume (the volume is there, but that is another issue to dig into), then the driver puked:
2014-08-14 14:54:48.350 36969 ERROR cinder.volume.drivers.solidfire [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Volume cbb95e17-f66e-4081-b3aa-521889ed436d, not found on SF Cluster.
2014-08-14 14:54:48.351 36969 ERROR cinder.openstack.common.rpc.amqp [req-cc45c0a8-4760-4890-bd2a-93498fadad7b 6a08f3e43965436fb028eda1005ec77b b14c5ae7dd504e6c8ab1f8f2e568fb46] Exception during message handling
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/manager.py"", line 731, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     new_project)
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/openstack/cinder/2013.2.3.r2.1/lib/python2.7/site-packages/cinder/volume/drivers/solidfire.py"", line 750, in accept_transfer
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp     'volumeID': sf_vol['volumeID'],
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp TypeError: 'NoneType' object has no attribute '__getitem__'
2014-08-14 14:54:48.351 36969 TRACE cinder.openstack.common.rpc.amqp
accept_transfer() should handle Volume Not Found like attach/detach_volume() do.","Catch vol not found in SolidFire transfer

The accept_transfer call in SolidFire wasn't catching
the volume_not found exception like it should be (and
like other methods do).

This patch just adds a check for volume is None on the
get_volume call during accept_transfer and raises an
appropriate exception if we run into that case.

While we're at it, add some more detail to the ""NotFound""
error messages to include the driver operation as per
request of reviewers.

Change-Id: Ie6b56953028865eb06ebdaae3416339b8a371351
Closes-Bug: #1357261"
935,7a7291794eb718fc0a6105efacf88b2ee5594043,1406681154,,1.0,15,3,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1120,1575,1350119,neutron,7a7291794eb718fc0a6105efacf88b2ee5594043,1,1,"“This code is incorrect: https://github.com/openstack/neutron/blob/master/neutron/db/l3_dvr_db.py#L297""","Bug #1350119 in neutron: ""get_agent_gw_ports_exist_for_network in l3_dvr_db.py should use arrays for filter values""","This code is incorrect:  https://github.com/openstack/neutron/blob/master/neutron/db/l3_dvr_db.py#L297
The result is that no port is found by the query.  Because of this, a new port gets created each time the L3 agent restarts and tries to find the external gw port for the DVR router.","Pass filters in arrays in get_agent_gw_ports_exist_for_network

Change-Id: I8925d7072b7cae7645e772b8a769a7e6f03ec64f
Closes-Bug: #1350119"
936,7aa0c1d9a4a02add6b00aeb764e9ba86a3c460ff,1381727282,,1.0,6,5,1,1,1,0.0,True,4.0,637793.0,26.0,11.0,False,2.0,1218.0,5.0,2.0,28.0,591.0,597.0,28.0,568.0,574.0,28.0,554.0,560.0,0.026629936,0.509641873,0.515151515,1655,1237185,1237185,cinder,7aa0c1d9a4a02add6b00aeb764e9ba86a3c460ff,1,0, ,"Bug #1237185 in Cinder: ""AttributeError occurs when Huawei HVS driver attaching volume with incorrect target IP""","If the iscsi target port IP is set incorrectly, the driver will get a None value. Here the dirver should raise an exception.
The log:
2013-09-26 00:39:41.832 ERROR cinder.openstack.common.rpc.amqp [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Exception during message handling
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 561, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/huawei_hvs.py"", line 78, in initialize_connection
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     return self.common.initialize_connection_iscsi(volume, connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 518, in initialize_connection_iscsi
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     (iscsi_iqn, target_ip) = self._get_iscsi_params(connector)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1287, in _get_iscsi_params
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     target_iqn = self._get_tgt_iqn(target_ip)
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/huawei/rest_common.py"", line 1087, in _get_tgt_iqn
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp     split_list = ip_info.split(""."")
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'split'
2013-09-26 00:39:41.832 TRACE cinder.openstack.common.rpc.amqp
2013-09-26 00:39:41.833 ERROR cinder.openstack.common.rpc.common [req-c8478604-b0fd-47f4-a86e-4d6ac5e92bb8 46d5d9f00d9d4d86845ec9f57b5399d5 f5aa9910f241481e9518f331be373d32] Returning exception 'NoneType' object has no attribute 'split' to caller","Fix Huawei HVS driver AttributeError

This patch checks the iSCSI port info and raises an exception
if it can not get the port info from the configuration.

Closes-bug: #1237185
Change-Id: I341ccbe77abd25421e5ee0b2d07ace3759fbccef"
937,7aa4f65a8c17aa037deff0f5b534ed694c17e62a,1380837789,,1.0,33,7,3,3,1,0.855004587,True,4.0,599829.0,28.0,14.0,False,60.0,772864.6667,159.0,6.0,232.0,1374.0,1374.0,232.0,1203.0,1203.0,218.0,1018.0,1018.0,0.20524836,0.955014058,0.955014058,1449,1203152,1203152,cinder,7aa4f65a8c17aa037deff0f5b534ed694c17e62a,1,0,,"Bug #1203152 in Cinder: ""long flashcopy operations  in the storwize_scv driver will block in _delete_vdisk()""","There is a loop inside cinder/volume/drivers/storwize_svc.py _delete_vdisk() function that will
wait on flashcopy to finish before the vdisk can be deleted. If trying to delete a cinder volume
that is created from snapshot or another volume before the flashcopy finishes, the volume
service process will loop and wait for the flashcopy to be done. Since the code is blocked
in the _delete_vdisk code, volume service is blocked and won't respond to REST API
or update status. The service will be marked offline.
I am waiting for the person who found this bug to test a change that puts the while loop into an
inline function that I then run with FixedIntervalLoopingCall.
I hope to have a patch to post here later today once we have been able to test the code I wrote.","long flashcopy operation may block volume service

Storwize family uses flashcopy for snapshot or volume clone. The
volume delete has to wait until flashcopy finishes or errors out.
The _delete_vdisk() will poll volume FlashCopy status in a loop.
This may block volume serivce heartheat since it is in the same
. The solution is to use openstack FixedIntervalLoopingCall
to run the FlashCopy status poll in a timer thread.

The cinder volume mananger will resume delete operation for those
volumes that are in the deleting state during volume service startup.
Since Storwize volume delete may wait for a long time, this can cause
volume service to have long delay before it becomes available.
A greenpool is used to offload those volume delete operations.

Change-Id: Ie01a441a327e1e318fa8da0040ae130731b7a686
Closes-Bug: #1203152"
938,7aabee6f76fb390faac68f8195dab4018917bae4,1391517672,,1.0,2,2,1,1,1,0.0,True,1.0,176814.0,13.0,4.0,False,8.0,630107.0,16.0,4.0,3.0,704.0,705.0,3.0,673.0,674.0,3.0,665.0,666.0,0.002890173,0.481213873,0.481936416,148,550,1258100,cinder,7aabee6f76fb390faac68f8195dab4018917bae4,1,1,Typo in help,"Bug #1258100 in Cinder: ""Time unit is missing in the description of option vmware_task_poll_interval in cinder.conf.sample""","Time unit is missing in the description of option ""vmware_task_poll_interval"" in cinder.conf.sample. This description is used in the auto-generated config reference at http://docs.openstack.org/trunk/config-reference/content/vmware-vmdk-driver.html","vmdk: To add missing time unit in driver option

Time unit is missing in the description of vmdk driver option
""vmware_task_poll_interval"". Due to this, the vmdk driver config
reference document doesn't have the timeunit in its description
column. This fix adds the missing time unit (seconds) in the
driver option.

Closes-Bug: #1258100
Change-Id: I2c49c65ca9a48a2dcc6898a32b4bbc0433e8916b"
939,7ab7e70559733a97eb0c77442bfaae7e7e10b790,1392956121,,1.0,0,14,2,2,1,0.863120569,True,1.0,179712.0,18.0,3.0,False,16.0,808602.0,18.0,3.0,672.0,1232.0,1594.0,586.0,1070.0,1383.0,208.0,694.0,730.0,0.247044917,0.821513002,0.864066194,448,863,1282873,neutron,7ab7e70559733a97eb0c77442bfaae7e7e10b790,0,0,remove code,"Bug #1282873 in neutron: ""wsgi.run_server no longer used""",wsgi.run_server no longer used,"wsgi.run_server no longer used

Removes run_server method and test that is not used by anything.

Change-Id: I280c3e249b8e3f46de5a3d364a4a34ffc5acd064
Closes-bug: #1282873"
940,7ac4fc76f2d777fd4e0ec4245eaf989f99917be2,1408028780,,1.0,83,9,4,2,1,0.630211438,False,,,,,True,,,,,,,,,,,,,,,,,1201,1659,1357239,cinder,7ac4fc76f2d777fd4e0ec4245eaf989f99917be2,1,1,,"Bug #1357239 in Cinder: ""VMware: Volume stuck in uploading state even after upload to image failure""","Uploading a 2Gi volume to image service resulted in volume being stuck in uploading state for a long time.
As per the logs, the image service raised an internal server error. In this case, the volume should return back to available state.
Cinder volume logs:
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-]
HTTP/1.1 500 Internal Server Error
date: Thu, 14 Aug 2014 12:26:27 GMT
content-length: 91
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-f741d884-6cdd-4304-b809-63469b12aaa8
500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
 from (pid=35027) log_http_response /opt/stack/python-glanceclient/glanceclient/common/http.py:167
2014-08-14 17:56:27.846 DEBUG glanceclient.common.http [-] Request returned failure status: 500 from (pid=35027) _http_request /opt/stack/python-glanceclient/glanceclien
t/common/http.py:263
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 112, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 320, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 318, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 167, in call
    return getattr(client.images, method)(*args, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/v1/images.py"", line 332, in update
    'PUT', url, headers=hdrs, body=image_data)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 328, in raw_request
    return self._http_request(url, method, **kwargs)
  File ""/opt/stack/python-glanceclient/glanceclient/common/http.py"", line 264, in _http_request
    raise exc.from_response(resp, body_str)
HTTPInternalServerError: 500 Internal Server Error
Failed to upload image 3cc9f1f9-8749-4556-81f5-8a0c2bfa4f1f
    (HTTP 500)
Removing descriptor: 10","VMware: Handle exceptions raised by image update

Exceptions raised by glance image update are not handled while writing
to image. This results in volume being stuck in uploading state for
a long time even after image transfer failure. This patch fixes the
exception handling.

Change-Id: I7651c96968caa49c5421d68ee4dbe9e55f7f53b1
Closes-Bug: #1357239
Partial-Bug: #1269345"
941,7ad82b95f611678f71fd500b6541e04ff8359d15,1392974728,0.0,1.0,6,4,1,1,1,0.0,True,1.0,246311.0,15.0,2.0,False,26.0,32837.0,34.0,2.0,482.0,1100.0,1333.0,403.0,959.0,1137.0,225.0,620.0,675.0,0.266195524,0.731448763,0.79623086,451,866,1282946,neutron,7ad82b95f611678f71fd500b6541e04ff8359d15,0,0,tests,"Bug #1282946 in neutron: ""db_plugin.test_delete_port does not test port-delete""","test_delete_port in test_db_plugin does not test delete operation:
    def test_delete_port(self):
        with self.port() as port:
            req = self.new_show_request('port', self.fmt, port['port']['id'])
            res = req.get_response(self.api)
            self.assertEqual(res.status_int, webob.exc.HTTPNotFound.code)","Fix test_db_plugin.test_delete_port

Also add PortNotFound check to test_delete_port_public_network.

Change-Id: Iec461364345b200e2d17ba26a6c5362eec6d93ef
Closes-Bug: #1282946"
942,7ae506a4b1829fbd8cbecc0a6b267f76230face7,1396902467,,1.0,79,8,5,2,1,0.930299482,True,14.0,3728061.0,141.0,32.0,False,62.0,459833.0,181.0,5.0,4.0,3795.0,3795.0,4.0,3032.0,3032.0,4.0,2838.0,2838.0,0.000644912,0.366180833,0.366180833,604,1028,1291741,nova,7ae506a4b1829fbd8cbecc0a6b267f76230face7,1,1,Bug. Didnt resize,"Bug #1291741 in OpenStack Compute (nova): ""VMWare: Resize action does not change disk""","In ""nova/virt/vmwareapi/vmops.py""
def finish_migration(self, context, migration, instance, disk_info,
                         network_info, image_meta, resize_instance=False,
                         block_device_info=None, power_on=True):
        """"""Completes a resize, turning on the migrated instance.""""""
        if resize_instance:
            client_factory = self._session._get_vim().client.factory
            vm_ref = vm_util.get_vm_ref(self._session, instance)
            vm_resize_spec = vm_util.get_vm_resize_spec(client_factory,
                                                        instance)
            reconfig_task = self._session._call_method(
                                            self._session._get_vim(),
                                            ""ReconfigVM_Task"", vm_ref,
                                            spec=vm_resize_spec)
.....................
finish_migration uses vm_util.get_vm_resize_spec() to get resize parameters.
But in ""nova/virt/vmwareapi/vm_util.py""
def get_vm_resize_spec(client_factory, instance):
    """"""Provides updates for a VM spec.""""""
    resize_spec = client_factory.create('ns0:VirtualMachineConfigSpec')
    resize_spec.numCPUs = int(instance['vcpus'])
    resize_spec.memoryMB = int(instance['memory_mb'])
    return resize_spec
the get_vm_resize_spec action does not set up disk size to resize.","VMware: Resize operation fails to change disk size

The finish_migration step will resize the cpu and memory to match
the new flavor chosen, but not the disk size. Additional code was
added to extend the disk size using ExtendVirtualDisk_Task.

DocImpact

Change-Id: Iff10443f603d329d01a74a620079f80518b75a50
Closes-bug: #1291741"
943,7b8402ed3ba734836119441bdf1a6d6c661c8df2,1396899611,,1.0,3,1,1,1,1,0.0,True,2.0,123988.0,16.0,8.0,False,36.0,1401002.0,74.0,5.0,2214.0,3877.0,5029.0,1711.0,2726.0,3607.0,2115.0,3583.0,4664.0,0.272961816,0.462332301,0.601780186,755,1182,1303983,nova,7b8402ed3ba734836119441bdf1a6d6c661c8df2,0,0, Feature “Enable ServerGroup scheduler filters by default”,"Bug #1303983 in OpenStack Compute (nova): ""Enable ServerGroup scheduler filters by default""","The Icehouse release includes a server group REST API.  For these groups to actually function properly, the server group scheduler filters must be enabled.  So, these filters should be enabled by default since the API is also enabled by default.  If the API is not used, the scheduler filters will be a no-op.
http://lists.openstack.org/pipermail/openstack-dev/2014-April/032068.html","Enable ServerGroup scheduler filters by default

The Icehouse release includes a server group REST API. For these
groups to actually function properly, the server group scheduler
filters must be enabled. So, these filters should be enabled by
default since the API is also enabled by default. If the API is not
used, the scheduler filters will be a no-op.

DocImpact
Change-Id: Ia52d7fcfff52377d8beae2acd9b7eba9c6c74e09
Closes-bug: #1303983"
944,7b9daa829fe137cb3da377c89b293f01e5fa0871,1389275589,,1.0,25,2,1,1,1,0.0,True,2.0,748518.0,24.0,4.0,False,1.0,579058.0,2.0,2.0,231.0,1992.0,2108.0,214.0,1744.0,1844.0,164.0,251.0,331.0,0.244807122,0.37388724,0.492581602,276,681,1267468,neutron,7b9daa829fe137cb3da377c89b293f01e5fa0871,0,0,add sanity checks ,"Bug #1267468 in neutron: ""Add more sanity checks to VMware NSX config/backend""",neutron-check-nsx-config test the configuration for Neutron and VMware NSX. Extend this tool to check the backend even further.,"VMware NSX: add sanity checks for NSX cluster backend

Ensure that all transport nodes registered are up
and running. A failure here, may prevent a lot of
failures down the lines.

Change-Id: I3473928e296ec6792f34bc27a4ae797fed337e7c
Closes-bug: #1267468"
945,7b9ed4edeef7d599c164c8682fbbb36066d98abb,1401687782,1.0,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,942,1378,1325771,neutron,7b9ed4edeef7d599c164c8682fbbb36066d98abb,0,0,"“it doesn't have a consistency hash yet, so it is currently setting the HTTP header to ‘False'.""","Bug #1325771 in neutron: ""BSN: Consistency hash should be empty, not 'False'""","When the Big Switch plugin first starts, it doesn't have a consistency hash yet, so it is currently setting the HTTP header to 'False'. This requires special-casing on the backend to handle. It should just be blank.","BSN: Set hash header to empty instead of False

Sets the consistency hash header to empty instead
of False since 'False' is handled like a string on
the backend and requires special-casing to detect.

Closes-Bug: #1325771
Change-Id: Iee1651574c01a32e78167a9bbed4e0433abbdec2"
946,7c1c8c0b4d36ec1795c11950652fa42564352264,1412720759,,1.0,16,5,2,2,2,0.958711883,False,,,,,True,,,,,,,,,,,,,,,,,1848,1329093,1329093,Swift,7c1c8c0b4d36ec1795c11950652fa42564352264,1,1, ,"Bug #1329093 in OpenStack Object Storage (swift): ""ssync doesn't sync X-Static-Large-Object""","if you upload a static large object:
https://gist.github.com/clayg/ae43968b46022ef79235
Then delete some of the .data files and run ssync (object-replicator)
Some of the newly replicated .data files don't have the X-Static-Large-Object metadata set (maybe some other metadata as well?)","Ssync does not replicate custom object headers

Closes-Bug: #1329093

Change-Id: Ie9d80089a38d7a9b3464c66237d4d2d23331ebd5"
947,7ca4bea7e3d8688c847bdbcfe3e60900c61c297c,1395231176,1.0,1.0,210,6,3,2,1,0.469617636,True,2.0,682152.0,37.0,14.0,False,10.0,1238441.0,27.0,6.0,8.0,3432.0,3433.0,8.0,2860.0,2861.0,8.0,1440.0,1441.0,0.005863192,0.938762215,0.939413681,479,896,1284979,cinder,7ca4bea7e3d8688c847bdbcfe3e60900c61c297c,1,1,Non authenticated error.,"Bug #1284979 in Cinder: ""vmware:  ""Not authenticated error occurred "" on creation volume from existing volume""","1)  cinder create   --name vol1 1
2)  cinder create --source-volid 749c9cfd-e969-4199-a090-87daff1a9d54 --display-name vo2 1 [here volume created but status : error]
+-------------------+--------------------------------------+
|      Property     |                Value                 |
+-------------------+--------------------------------------+
|    attachments    |                  []                  |
| availability_zone |                 nova                 |
|      bootable     |                false                 |
|     created_at    |      2014-02-25T12:35:43.000000      |
|    description    |                 None                 |
|         id        | d55009de-0454-48a8-b73d-01f65c071b6c |
|      metadata     |                  {}                  |
|        name       |            fastvol_clone1            |
|        size       |                  1                   |
|    snapshot_id    |                 None                 |
|    source_volid   | 749c9cfd-e969-4199-a090-87daff1a9d54 |
|       status      |                error                 |
|      user_id      |   bb6690d82af84ae4b7152498088b517f   |
|    volume_type    |              fast_clone              |
+-------------------+--------------------------------------+
3) if you try to delete above created volume  - volume not getting deleted and volume status continuously say ""Error in deleting""
c-vol log :  at step2
2014-02-26 10:34:09.395 ERROR cinder.volume.drivers.vmware.api [-] Not authenticated error occurred. Will create session and try API call again: Error(s): NotAuthenticated occurred in the cal
l to RetrievePropertiesEx..
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api Traceback (most recent call last):
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 194, in _invoke_api
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     return api_method(*args, **kwargs)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim_util.py"", line 212, in get_objects
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     options=options)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 174, in vim_request_handler
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     retrieve_properties_ex_fault_checker(response)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/vim.py"", line 153, in retrieve_properties_ex_fault_checker
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api     exc_msg_list)
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api VimFaultException: Error(s): NotAuthenticated occurred in the call to RetrievePropertiesEx.
2014-02-26 10:34:09.395 TRACE cinder.volume.drivers.vmware.api
2014-02-26 10:34:09.611 ERROR suds.client [-] <?xml version=""1.0"" encoding=""UTF-8""?>
<SOAP-ENV:Envelope xmlns:ns0=""urn:vim25"" xmlns:ns1=""http://schemas.xmlsoap.org/soap/envelope/"" xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:SOAP-ENV=""http://schemas.xmlsoap.org
/soap/envelope/"">
   <ns1:Body>
      <ns0:Login>
         <ns0:_this type=""SessionManager"">SessionManager</ns0:_this>
         <ns0:userName>root</ns0:userName>
         <ns0:password>vmware</ns0:password>
      </ns0:Login>
   </ns1:Body>
</SOAP-ENV:Envelope>
2014-02-26 10:34:09.613 ERROR cinder.openstack.common.loopingcall [-] in dynamic looping call
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall Traceback (most recent call last):
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall   File ""/opt/stack/cinder/cinder/openstack/common/loopingcall.py"", line 123, in _inner
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall     idle = self.f(*self.args, **self.kw)
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall   File ""/opt/stack/cinder/cinder/volume/drivers/vmware/api.py"", line 83, in _func
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall     raise excep
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall VimFaultException: Server raised fault: 'Cannot complete login due to an incorrect user name or password.'
2014-02-26 10:34:09.613 TRACE cinder.openstack.common.loopingcall","vmware: Use SessionIsActive to find stale session

An API invocation with a stale session returns an empty response. In order
to distinguish it from an API returning valid empty response, the session
is recreated and the API is retried. If an empty response is received even
after the retry, it can be assumed that the API response is actually empty.
But this behavior results in authentication error from the VMware server
when there is an active session and the API response is actually empty.
This change fix this behavior by using the SessionIsActive check to
identify stale session.

Change-Id: I6ddc4028bc5319cd22006de0590c13d6868c3494
Closes-Bug: #1284979"
948,7ca696c7355ba93dd89e69a76162c6d9a3429a2b,1411153871,,1.0,15,1,2,2,1,0.954434003,False,,,,,True,,,,,,,,,,,,,,,,,1338,1806,1371696,neutron,7ca696c7355ba93dd89e69a76162c6d9a3429a2b,1,1,,"Bug #1371696 in neutron: ""Cannot add or update a child row: a foreign key constraint fails ml2_dvr_port_bindings""","We've hit this foreign key constraint error.  This is due to a sync message coming in from the L3 agent over RPC.  The message contains n update for  a port that has just been deleted.  This is just log noise because it all gets worked out quickly following the error.  The full trace is here [1].
2014-09-18 21:22:39.735 29984 TRACE oslo.messaging.rpc.dispatcher DBReferenceError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`neutron`.`ml2_dvr_port_bindings`, CONSTRAINT `ml2_dvr_port_bindings_ibfk_1` FOREIGN KEY (`port_id`) REFERENCES `ports` (`id`) ON DELETE CASCADE)') 'INSERT INTO ml2_dvr_port_bindings (port_id, host, router_id, vif_type, vif_details, vnic_type, profile, cap_port_filter, driver, segment, status) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)' ('0fe7b532-343e-4ba0-83d9-c51b1c55f533', 'devstack-trusty-hpcloud-b4-2246632', '45248fa2-4372-4ad9-8e60-afabe39c6f6a', 'unbound', '', 'normal', '', 0, None, None, 'DOWN')
[1] http://paste.openstack.org/show/113360/","Fix foreign key constraint error on ml2_dvr_port_bindings

Get the port before creating the DVR binding. This should guarantee that, if
the port is found, no foreign key violation is triggered.

Closes-bug: #1371696

Change-Id: I90c3d6460c14ef27823be95bc26aae38eba4879a"
949,7d028b77a4bde48ed2729f370773c9d510156686,1390859331,,1.0,25,4,2,2,1,0.849751137,True,2.0,395695.0,13.0,5.0,False,21.0,1938346.5,35.0,3.0,0.0,808.0,808.0,0.0,749.0,749.0,0.0,763.0,763.0,0.000732601,0.55970696,0.55970696,285,691,1268513,cinder,7d028b77a4bde48ed2729f370773c9d510156686,0,0,Add support for special char,"Bug #1268513 in Cinder: ""Can not translate special char ""&"" in XML function create_volume_metadata volumes_client.py  ""","1. I add special char like """"!@#$%^&*"" in volume ""metadata = {""key1"": ""value1"",
                    : ""!@#$%^&*""} .Then try to insert it into volume.
2. I test it successfully in  volumes_client.create_volume_metadata Json function.
3. When I test it in XML function, it report error like below
Traceback (most recent call last):
  File ""/tempest/tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
    metadata)
  File ""/tempest/tempest/services/volume/xml/volumes_client.py"", line 381, in create_volume_metadata
    self.headers)
  File ""/tempest/tempest/common/rest_client.py"", line 302, in post
    return self.request('POST', url, headers, body)
  File ""/tempest/tempest/common/rest_client.py"", line 436, in request
    resp, resp_body)
  File ""/tempest/tempest/common/rest_client.py"", line 530, in _error_checker
    raise exceptions.ServerFault(message)
ServerFault: Got server fault
Details: The server has either erred or is incapable of performing the requested operation.
4. After our investigation, we find when we remove special char ""&"" , it will be ok. So I think in XML function, it can not translate these special char to correct format.
I also run it in tempest, it also run same error
FAIL: tempest.api.volume.test_volume_metadata.VolumeMetadataTestXML.test_create_volume_metadata_specialchart_blank[gate
2014-01-13 11:13:15.079 | Traceback (most recent call last):
2014-01-13 11:13:15.079 | File ""tempest/api/volume/test_volume_metadata.py"", line 130, in test_create_volume_metadata_specialchart_blank
2014-01-13 11:13:15.079 | metadata)
2014-01-13 11:13:15.079 | File ""tempest/services/volume/xml/volumes_client.py"", line 380, in create_volume_metadata
2014-01-13 11:13:15.079 | self.headers)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 302, in post
2014-01-13 11:13:15.080 | return self.request('POST', url, headers, body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 436, in request
2014-01-13 11:13:15.080 | resp, resp_body)
2014-01-13 11:13:15.080 | File ""tempest/common/rest_client.py"", line 530, in _error_checker
2014-01-13 11:13:15.080 | raise exceptions.ServerFault(message)
2014-01-13 11:13:15.080 | ServerFault: Got server fault
2014-01-13 11:13:15.080 | Details: The server has either erred or is incapable of performing the requested operation","Add support for special char in volume metadata

Using special characters such as ""&"" in volume metadata caused SAX
parser to split text into multiple child nodes, as stated in the
specs for SAX parsers. In this case extract_text would return
'None' because text was return if there was exactly
one child node. Concatenating the values of child nodes accounts
for SAX parsers splitting contigous characters into
multiple chunks.

Change-Id: I3709789d33ff7b07eaeaf725f5183105443d0abc
Closes-Bug: #1268513"
950,7d44629c2e8003f5bbb87e9b697eec54b4cce793,1382358396,,1.0,3,2,2,2,1,0.970950594,True,3.0,2113481.0,40.0,22.0,False,13.0,1821098.5,16.0,7.0,305.0,1869.0,1925.0,256.0,1730.0,1776.0,30.0,407.0,417.0,0.065677966,0.86440678,0.88559322,0,312,1242662,neutron,7d44629c2e8003f5bbb87e9b697eec54b4cce793,1,1,Exception does not exist,"Bug #1242662 in neutron: ""exceptions.Error does not exist""","def add_extension(self, ext):
        # Do nothing if the extension doesn't check out
        if not self._check_extension(ext):
            return
        alias = ext.get_alias()
        LOG.info(_('Loaded extension: %s'), alias)
        if alias in self.extensions:
            raise exceptions.Error(_(""Found duplicate extension: %s"") %
                                   alias)
        self.extensions[alias] = ext","Replace a non-existing exception

Closes-Bug: #1242662

Change-Id: Ie44023c0fd9ba1373ec6e62d9245884b9f719b7e"
951,7dc3c671656974596b9a373c911e5b786d57be35,1387259568,,1.0,10,4,2,2,1,0.591672779,True,1.0,46976.0,8.0,3.0,False,9.0,2135188.0,17.0,3.0,1.0,1256.0,1256.0,1.0,1092.0,1092.0,1.0,539.0,539.0,0.003139717,0.847723705,0.847723705,194,597,1261665,neutron,7dc3c671656974596b9a373c911e5b786d57be35,1,1, was being applied on the incorrect port.,"Bug #1261665 in neutron: ""Midonet plugin: Source NAT not applied""","Steps:
Using the MidoNet plugin:
* Create a private network
* Create a VM on the private network
* Create a router
* Create an interface for the router on the private network
* Create an external network
* Set a router's gateway to the external network
* Ping a publicly routable address from a VM on a network attached the router
Observed:
Source NAT is not applied to the outbound traffic
Expected:
Outbound traffic should be source NATted at the router","Midonet plugin: Fix source NAT

Source NAT rule was being applied on the incorrect port.
It was being applied to the Neutron gateway port, not to
the MidoNet tenant / provider router link port.

Change-Id: Ib818c09adfb6957b7cad4523e5ce1fdffde9590b
Closes-Bug: #1261665"
952,7dd4d98c2873e1868296ef973f0be42cadcb6d6d,1386253229,,1.0,2,2,1,1,1,0.0,True,2.0,4933969.0,11.0,5.0,False,5.0,10623749.0,6.0,4.0,65.0,2714.0,2757.0,48.0,2339.0,2367.0,62.0,2555.0,2596.0,0.009278351,0.376435935,0.382474227,152,554,1258166,nova,7dd4d98c2873e1868296ef973f0be42cadcb6d6d,0,0,Recomendation,"Bug #1258166 in OpenStack Compute (nova): ""N310 check recommends function that doesn't exist""","Check N310 can return ""N310  timeutils.now() must be used instead of datetime.now()"", but timeutils.now() does not exist. Only utcnow() does.","Recommend the right call instead of datetime.now()

Closes-Bug: #1258166
Change-Id: I92b94dbcd0405d8f2bb818ad1c44c4f4934c8bec"
953,7e13cf73f1869599c1218cf58468c21bb22dd5bf,1390919346,,1.0,22,2,2,2,1,0.994984828,True,4.0,498730.0,20.0,5.0,False,26.0,4494.0,47.0,3.0,202.0,723.0,813.0,195.0,554.0,639.0,149.0,577.0,618.0,0.132508834,0.510600707,0.546819788,225,628,1263945,glance,7e13cf73f1869599c1218cf58468c21bb22dd5bf,1,0, Backwards compat support breaks transport_url,"Bug #1263945 in Glance: ""Backwards compat support breaks transport_url""","Passing a fake url to `get_transport` makes it ignore the configured `transport_url`. This blocks any chance of using the new transport_url configuration.
https://github.com/openstack/glance/blob/master/glance/notifier.py#L81","Don't override transport_url with old configs

The backwards compatibility support turned out being a forward
compatibility blocker. This patch attempts to first load the notifier
transport using the transport_url and then, if the transport_url was not
configured, it loads the transport by using the old strategy config
params.

Closes-bug: #1263945

Change-Id: I4c80c64e2d1afaf518886b51a50300a576ee6317"
954,7e18d67cc9c96a14e0ca24a41820441e41457738,1390511017,0.0,1.0,1,2,1,1,1,0.0,True,1.0,95148.0,8.0,4.0,False,141.0,606123.0,595.0,5.0,2016.0,1629.0,3179.0,1640.0,1434.0,2639.0,1918.0,1465.0,2931.0,0.268466704,0.205092334,0.410184667,307,716,1270693,nova,7e18d67cc9c96a14e0ca24a41820441e41457738,0,0,tests,"Bug #1270693 in OpenStack Compute (nova): ""_last_vol_usage_poll was not properly updated""","I found this error in jinkins's log: http://logs.openstack.org/02/67402/4/check/gate-nova-python27/b132ac8/console.html
2014-01-20 02:36:59.295 | ======================================================================
2014-01-20 02:36:59.295 | FAIL: nova.tests.compute.test_compute.ComputeVolumeTestCase.test_poll_volume_usage_with_data
2014-01-20 02:36:59.295 | tags: worker-0
2014-01-20 02:36:59.296 | ----------------------------------------------------------------------
2014-01-20 02:36:59.296 | Empty attachments:
2014-01-20 02:36:59.296 |   stderr
2014-01-20 02:36:59.296 |   stdout
2014-01-20 02:36:59.297 |
2014-01-20 02:36:59.297 | pythonlogging:'': {{{
2014-01-20 02:36:59.297 | INFO [nova.virt.driver] Loading compute driver 'nova.virt.fake.FakeDriver'
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Auditing locally available compute resources
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free ram (MB): 7680
2014-01-20 02:36:59.298 | AUDIT [nova.compute.resource_tracker] Free disk (GB): 1028
2014-01-20 02:36:59.299 | AUDIT [nova.compute.resource_tracker] Free VCPUS: 1
2014-01-20 02:36:59.299 | INFO [nova.compute.resource_tracker] Compute_service record created for fake-mini:fakenode1
2014-01-20 02:36:59.299 | AUDIT [nova.compute.manager] Deleting orphan compute node 2
2014-01-20 02:36:59.300 | }}}
2014-01-20 02:36:59.300 |
2014-01-20 02:36:59.300 | Traceback (most recent call last):
2014-01-20 02:36:59.300 |   File ""nova/tests/compute/test_compute.py"", line 577, in test_poll_volume_usage_with_data
2014-01-20 02:36:59.301 |     self.compute._last_vol_usage_poll)
2014-01-20 02:36:59.301 |   File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2014-01-20 02:36:59.301 |     raise self.failureException(msg)
2014-01-20 02:36:59.302 | AssertionError: _last_vol_usage_poll was not properly updated <1390185067.18>","Make test_poll_volume_usage_with_data more reliable

This test includes 2 calls to time.time().  It assumes that the result
from the second call may be greater than the first.  This isn't
definietly true and we see periodic failures of this test in the gate as
a result.

Change the test to not worry about the specific time value that gets
set.  We set _last_vol_usage_poll to 0 before calling the function under
test.  We really only need to check that it got set to some value after
calling the function under test.  This tweak should make the test pass
consistently.

Change-Id: I6c01d2958cf671c29622562689a15ca1ddea0a3c
Closes-bug: #1270693"
955,7e54641fc1f760423c2f1210787903e17ea1cc06,1402626054,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,974,1413,1329614,nova,7e54641fc1f760423c2f1210787903e17ea1cc06,0,0,Bug in test,"Bug #1329614 in OpenStack Compute (nova): ""test_uri_length_limit follows proxy and hits wrong localhost""","test_uri_length_limit honours HTTP_PROXY (if set).  This means the test queries to http://localhost:$port/ hit the wrong localhost when $HTTP_PROXY is another host, resulting in a meaningless test scenario, and apparent failures with HTTP status codes other than REQUEST_URI_TOO_LARGE (414).","Don't follow HTTP_PROXY when talking to localhost test server

Unlike urllib2, requests honours HTTP_PROXY by default.  This meant
test_uri_length_limit's test queries to http://localhost:$port/ would
hit the wrong localhost when $HTTP_PROXY was another host - resulting in a
meaningless test scenario and apparent failures with HTTP status codes
other than the expected REQUEST_URI_TOO_LARGE (414).

This change forces requests to go direct for those test queries.

Closes-Bug: #1329614
Change-Id: Ida3424ca9a750bd2d3f2e81dc7dfacc2a334adf1"
956,7e91362dbb974df5ee44d346e1d0971479510d4b,1387882126,,1.0,15,9,3,2,1,0.94639463,True,2.0,3828385.0,35.0,12.0,False,3.0,5744148.0,5.0,6.0,44.0,453.0,475.0,36.0,414.0,429.0,22.0,262.0,264.0,0.035276074,0.403374233,0.406441718,224,627,1263881,neutron,7e91362dbb974df5ee44d346e1d0971479510d4b,1,1, fails when more than one port is added/deleted simultaneously ,"Bug #1263881 in neutron: ""ML2 l2-pop Mechanism driver fails when more than one port is added/deleted simultaneously""","If I create more than one VM at a time (with nova boot option --num-instances), sometimes the flooding flow for broadcast, multicast and unknown unicast are not added on certain l2 agents.
And conversely, when I delete more than one VM at a time, the flooding rule are not purge on certain l2 agents when it's necessary.
I made this test on the trunk version with OVS agent.","[ML2] l2-pop MD handle multi create/delete ports

If more than one port is added or removed simultaneously, port db entry
have status BUILD or DOWN and pass to ACTIVE when agent have finish to
configured it.
l2-pop mechanism driver use events port pass to ACTIVE or DOWN to send
fdb entries. In case of port is the first or the last network port on
an agent, the flooding entry need to be add or removed.

This patch fix the method to determine how many ports are active on a
agent by adding filter on status port to be ACTIVE.

Closes-bug: #1263881
Change-Id: I9c1f8bd69dee37bc01a5d42327aa5f737998c5aa"
957,7e95b05b937e632ae8aee82cfa8f6f31835e6316,1410110572,,1.0,10,3,5,3,1,0.972362611,False,,,,,True,,,,,,,,,,,,,,,,,1205,1663,1357432,cinder,7e95b05b937e632ae8aee82cfa8f6f31835e6316,1,1,,"Bug #1357432 in Cinder: ""Accept Transfer race condition""","In accept_transfer() workflow (https://github.com/openstack/cinder/blob/master/cinder/transfer/api.py#L193)
         try:
             # Transfer ownership of the volume now, must use an elevated
             # context.
             self.volume_api.accept_transfer(context,
                                             vol_ref,
                                             context.user_id,
                                             context.project_id)
             self.db.transfer_accept(context.elevated(),
                                     transfer_id,
                                     context.user_id,
                                     context.project_id)
   self.volume_api.accept_transfer() sends out a RPC request (cast) to volume manager and volume driver may do something in backend (e.g. modify account for the volume), but since it's a unblocking RPC cast, this call returns pretty fast and the volume record in DB will be updated.   There can be cases where DB update finishes even before volume manager / driver does their job.  Unfortunately some driver(s) relies on original DB record to do their stuff (SolidFire for example).  Such situation will turn volume into inconsistent state (between backend and cinder DB) and become unusable.
   We may either change accept_transfer() in volume RPC API from CAST to blocking CALL, or we can pass enough original volume state to volume manger so that they don't have to rely on unreliable DB state.","Fix possible race condition for accept transfer

Accept transfer API workflow is currently like this:

  call volume_api.accept_transfer()
    |
    --- RPC cast to volume manager
          |
          --- volume manager calls volume driver accept_transfer()

  update volume's DB record

Given the non-blocking nature of RPC cast, what happens in volume
manager and volume driver can happen in parallel with the DB update.
If volume driver relies on original DB record to do things, then
DB record shouldn't be updated until volume driver finishes its job.

So this patch change volume RPC API accept_transfer() from cast
to call to make sure the workflow is in serialized manner.  Also
elevated the context when volume manager tries to update the DB
record when driver has done accept_transfer().

Change-Id: Ieae52e167aa02967338e0be5d78d570d682faa7a
Closes-bug: #1357432"
958,7ea605df3ac71dc568194bcd5eaf1c115008e1ee,1413029285,,1.0,33,7,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,1385,1855,1378508,neutron,7ea605df3ac71dc568194bcd5eaf1c115008e1ee,1,1,,"Bug #1378508 in neutron: ""KeyError in DHPC RPC when port_update happens.- this is seen when a delete_port event occurs ""","When there is a delete_port event occassionally we are seeing a TRACE in dhcp_rpc.py file.
2014-10-07 12:31:39.803 DEBUG neutron.api.rpc.handlers.dhcp_rpc [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Update dhcp port {u'port': {u'network_id': u'12548499-8387-480e-b29c-625dbf320ecf', u'fixed_ips': [{u'subnet_id': u'88031ffe-9149-4e96-a022-65468f6bcc0e'}]}} from ubuntu. from (pid=4414) update_dhcp_port /opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py:290
2014-10-07 12:31:39.803 DEBUG neutron.openstack.common.lockutils [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Got semaphore ""db-access"" from (pid=4414) lock /opt/stack/neutron/neutron/openstack/common/lockutils.py:168
2014-10-07 12:31:39.832 ERROR oslo.messaging.rpc.dispatcher [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Exception during message handling: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     'update_port')
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher     net_id = port['port']['network_id']
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher KeyError: 'network_id'
2014-10-07 12:31:39.832 TRACE oslo.messaging.rpc.dispatcher
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] Returning exception 'network_id' to caller
2014-10-07 12:31:39.833 ERROR oslo.messaging._drivers.common [req-803de1d2-a128-41f1-8686-2bec72c61f5a None None] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 294, in update_dhcp_port\n    \'update_port\')\n', '  File ""/opt/stack/neutron/neutron/api/rpc/handlers/dhcp_rpc.py"", line 81, in _port_action\n    net_id = port[\'port\'][\'network_id\']\n', ""KeyError: 'network_id'\n""]
2014-10-07 12:31:39.839 DEBUG neutron.context [req-7d40234b-6e11-4645-9bab-8f9958df5064 None None] Arguments dropped when creating context: {u'project_name': None, u'tenant': None} from (pid=4414) __init__ /opt/stack/neutron/neutron/context.py:83","Fix KeyError in dhcp_rpc when plugin.port_update raise exception

KeyError exception is seen because of following reasons

* DhcpRpcCallback._port_action() is called by two functions
   -  DhcpRpcCallback.create_dchp_port()
   -  DhcpRpcCallback.update_dhcp_port()

* When create_dhcp_port() function calls _port_action(), the
  function argument 'port' will have the body as
  {'port': {'network_id': foo_network_id, 'fixed_ips': [..] ...}

* When update_dhcp_port() function calls _port_action(), the
  function argument 'port' will have the body as
  {'id': port_id, 'port': {{'port': {'network_id': foo_network_id,
			            'fixed_ips': [..] ...}}

* If an exception occurs when _port_action() calls plugin.create_port(),
  network id is accessed as
  net_id = port['port']['network_id']

* If an exception occurs when _port_action() calls plugin.update_port(),
  network id is accessed as
  net_id = port['port']['network_id']
  which is causing the KeyError. network_id should have been accessed as
  net_id = port['port']['port']['network_id']

This patch fixes the issue by making the _port_action() take the
same port body. update_dhcp_port() insteading of passing the port_id
and port information in a single argument, it now adds port_id
in the port body itself.

Change-Id: I70b92fa20b421b05ca2053a9a57f62db726f7625
Closes-bug: #1378508"
959,7eb42f18ce8574b28ecba16b478bcfc40dfa2005,1393237843,,1.0,3,3,2,2,1,0.918295834,True,8.0,3283948.0,125.0,36.0,False,24.0,13747.0,33.0,6.0,43.0,1841.0,1852.0,42.0,1584.0,1595.0,38.0,1772.0,1778.0,0.005252525,0.238787879,0.23959596,459,874,1283876,nova,7eb42f18ce8574b28ecba16b478bcfc40dfa2005,1,1,Wrong message,"Bug #1283876 in OpenStack Compute (nova): ""webob.exc.HTTPUnprocessableEntity can't return correct messages""","In nova/api/openstack/compute/contrib/services.py there are codes like:
190             if id == ""disable-log-reason"":
191                 reason = body['disabled_reason']
192                 if not self._is_valid_as_reason(reason):
193                     msg = _('The string containing the reason for disabling '
194                             'the service contains invalid characters or is '
195                             'too long.')
196                     raise webob.exc.HTTPUnprocessableEntity(detail=msg)
But HTTPUnprocessableEntity should use 'explanation' parameter to return the error message.
The source can be referred here:
https://github.com/Pylons/webob/blob/master/webob/exc.py#L885","Change HTTPUnprocessableEntity to HTTPBadRequest

Change HTTPUnprocessableEntity to HTTPBadRequest, and use the
parameter 'explanation' in place of'detail' to show the correct
error message.
This patch fixes this bug.

Change-Id: I6dc2cbe29f24634138ba4c5255b0edd7b8ad6b0e
Closes-Bug: #1283876"
960,7ee80f76674bf4ee37b42a1223eb570138425ddd,1412853624,,1.0,22,24,2,2,1,0.858981037,False,,,,,True,,,,,,,,,,,,,,,,,1369,1839,1376194,cinder,7ee80f76674bf4ee37b42a1223eb570138425ddd,1,0,"Requirements. “Currently the display name and description of the target volume
    is changed to”","Bug #1376194 in Cinder: ""After restore operation , target  volume name getting changing  to  source volume of  backup""","while restoring backup to a volume only data should get updated however target volume name getting changed with  name of backuped  volume source.
ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Name    | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol  |  1   |     None    |   true   |             |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | silver_vol |  1   |    silver   |  false   |             |
+--------------------------------------+-----------+------------+------+-------------+----------+-------------+
ssatya@devstack:~/devstack$  cinder backup-restore a8b2f82c-a6a9-4fd1-b087-18798970ff4d  --volume  a0f016b7-6e6a-4d6f-a17b-1f18b0408158
ssatya@devstack:~/devstack$ cinder list
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+
| 9f673616-3742-49b3-8861-b85706faf763 | available | image_vol |  1   |     None    |   true   |             |
| a0f016b7-6e6a-4d6f-a17b-1f18b0408158 | available | image_vol |  1   |    silver   |   true   |             |
+--------------------------------------+-----------+-----------+------+-------------+----------+-------------+","Fix display name change during backup restore

Currently the display name and description of the target volume
is changed to that in the backup meta-data during backup restore.
This can confuse the end-user, especially in the case of restore
to an existing volume. This patch removes display name and display
description from the list of fields to be copied from the backup
meta-data to update the volume.

Change-Id: I21a0505352ddc8db801f4820d575399a4771b259
Closes-Bug: #1376194"
961,7eedeb2e5db4cfcd8505cc74d85ec625b7069302,1386582643,0.0,1.0,7,12,2,2,1,0.981940787,True,2.0,198303.0,18.0,9.0,False,37.0,418462.0,69.0,6.0,0.0,3922.0,3922.0,0.0,3186.0,3186.0,0.0,2944.0,2944.0,0.000146735,0.432134996,0.432134996,1419,1152748,1152748,nova,7eedeb2e5db4cfcd8505cc74d85ec625b7069302,0,0,“needs to be refactored”,"Bug #1152748 in OpenStack Compute (nova): ""libvirt imagebackend cache code encapsulation needs refactoring""","libvirt.imagebackend.cache needs to be refactored to eliminate the callback to call_if_not_exists().  The only way it could meet all the requirements while remaining a callback is if the os.path.exists() check were removed from the function, which would then require path checks in each of the backend classes where prepare_template() is called.  Ultimately, this is a lot more code than simply adding a backend-specific call to each class.
There's too much risk associated with such a refactor at such a late stage for Grizzly, so we'll look to tackle this for Havana.","Move calls to os.path.exists() in libvirt imagebackend

The checks for the path existence were removed from the base Image class
to avoid redundant checks (some of implementations also perform them and
behave differently depending on the results). Also special handling for
some backends were removed from the base Image class.

Change-Id: Ie6cdf0d6d9801f84fae7a58c1034547611712323
Closes-Bug: #1152748"
962,7ef73d325bac3a57970be1fc0cfe8b0d871f533e,1401694717,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,940,1376,1325481,cinder,7ef73d325bac3a57970be1fc0cfe8b0d871f533e,1,1,“comment misspelled about volume_clear in cinder/volume/driver.py (volume misspelled)”,"Bug #1325481 in Cinder: ""comment misspelled about volume_clear""",comment misspelled about volume_clear in cinder/volume/driver.py (volume misspelled),"Fixed the comment spelling error - voumes to volumes

Closes-bug: #1325481
Change-Id: I38a1d9df98c709d3ca173fa61315ad3f7ca9ffdc"
963,7f3e27ab06a0792181347f0f4feb7d06b546d9ac,1390884705,,1.0,11,7,1,1,1,0.0,True,3.0,435108.0,11.0,5.0,False,7.0,1688102.0,7.0,3.0,187.0,720.0,765.0,186.0,551.0,595.0,162.0,574.0,595.0,0.144503546,0.509751773,0.528368794,341,752,1274365,glance,7f3e27ab06a0792181347f0f4feb7d06b546d9ac,0,0,tests,"Bug #1274365 in Glance: ""FAIL: glance.tests.unit.v2.test_tasks_resource.TestTasksController.test_index_with_sort_key""","Seeing this in the gate-glance-python27 jobs - 8 hits in the last 7 days.
2014-01-30 01:29:36.436 | ======================================================================
2014-01-30 01:29:36.436 | FAIL: glance.tests.unit.v2.test_tasks_resource.TestTasksController.test_index_with_sort_key
2014-01-30 01:29:36.436 | ----------------------------------------------------------------------
2014-01-30 01:29:36.436 | _StringException: Traceback (most recent call last):
2014-01-30 01:29:36.436 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_tasks_resource.py"", line 233, in test_index_with_sort_key
2014-01-30 01:29:36.436 |     self.assertEqual(UUID4, actual[0])
2014-01-30 01:29:36.437 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
2014-01-30 01:29:36.437 |     self.assertThat(observed, matcher, message)
2014-01-30 01:29:36.437 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
2014-01-30 01:29:36.437 |     raise mismatch_error
2014-01-30 01:29:36.437 | MismatchError: !=:
2014-01-30 01:29:36.437 | reference = '6bbe7cc2-eae7-4c0f-b50d-a7160b0c6a86'
2014-01-30 01:29:36.437 | actual    = 'c80a1a6c-bd1f-41c5-90ee-81afedb1d58d'
2014-01-30 01:29:36.437 |
Logstash URL:
http://logstash.openstack.org/#eyJzZWFyY2giOiJcImluIHRlc3RfaW5kZXhfd2l0aF9zb3J0X2tleVwiIEFORCBcInRlc3RfdGFza3NfcmVzb3VyY2UucHlcIiBBTkQgZmlsZW5hbWU6XCJjb25zb2xlLmh0bWxcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM5MTA0NzE4ODMyNywibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIifQ==
Logstash Query:
message:""in test_index_with_sort_key"" AND message:""test_tasks_resource.py"" AND filename:""console.html""","Provide explicit task create and update value in controller tests

Assign an explicit created_at and updated_at datetime value to task DB
fixtures, it be used to fixes race condition in TasksController test
cases.

Closes-Bug: #1274365

Change-Id: I9f13c7e0e97085faac15713342808722a6692178
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
964,7f53a8c91e623c69855caadf6cf65703a7e5b799,1402258607,,1.0,24,26,4,4,1,0.923367079,False,,,,,True,,,,,,,,,,,,,,,,,952,1388,1326926,nova,7f53a8c91e623c69855caadf6cf65703a7e5b799,1,1,"“This only shows up in master so it's a regression in Juno only, this is the patch that broke it:
https://review.openstack.org/#/c/92285/""","Bug #1326926 in OpenStack Compute (nova): ""Conductor passing GlanceImageService instead of nova.image.api.API object to compute_utils.get_image_metadata()""","After image-api partial refactor, some calls still use a glance service instance to call compute_utils.get_instance_metadata which expect the object to have a 'get()' method.
Since that method is not present in GlanceImageService, and exception is thrown and the image metadata cannot be retrieved.
Sample calling  _cold_migration(..)
2014-06-05 15:45:13.138 WARNING nova.compute.utils [req-7a86365f-f01a-4d49-b1c3-595e8dc9bd24 admin admin] [instance: 290d3587-b69a-48d8-b5c0-307259e2f590] Can't access image 40c33532-0aed-4acc-8d7a-2a45698e1f2d: 'GlanceImageService' object has no attribute 'get'","live migrate conductor tasks to use nova.image.API

While the patch (I67e7c9d841e5bad82cdca8f7048eaa57f0cfbb2f) was being
reviewed, live_migrate functionality was moved to the conductor. This
live_migrate task code needed to be updated to refer to the new
nova.image.API object instead of the old nova.glance.GlanceImageService
object, which had a slightly different API.

Change-Id: I7ddb778432643f4b5371d11b675951fc678135c5
Closes-bug: #1326926"
965,7f643ca464194067c70d86efdc262daa9d2d3a92,1394184686,,1.0,5,5,4,4,1,0.960964047,True,2.0,501989.0,20.0,4.0,False,23.0,410654.0,52.0,2.0,72.0,584.0,628.0,72.0,517.0,561.0,63.0,560.0,595.0,0.04246848,0.372262774,0.395487724,561,982,1289230,cinder,7f643ca464194067c70d86efdc262daa9d2d3a92,1,1,typo in code,"Bug #1289230 in Cinder: ""Conversion types is missing in some strings""","Ex.:
             except nexenta.NexentaException as exc:
-                LOG.warning(_('Cannot delete snapshot %(origin): %(exc)s'),
+                LOG.warning(_('Cannot delete snapshot %(origin)s: %(exc)s'),
                             {'origin': origin, 'exc': exc})","Add conversion types in some strings

Some strings have no conversion types
for 'String Formatting Operations'.

Change-Id: I2c6176954643a93898ff2d06d686793fb0977458
Closes-Bug: #1289230"
966,7f6a6cdfc95610cea0a6e4bb01e7de330e1f76d4,1394066337,,1.0,1,1,1,1,1,0.0,True,1.0,66872.0,24.0,2.0,False,2.0,116716.0,3.0,2.0,279.0,1498.0,1650.0,212.0,1285.0,1389.0,111.0,690.0,737.0,0.124031008,0.765227021,0.817275748,538,959,1288492,neutron,7f6a6cdfc95610cea0a6e4bb01e7de330e1f76d4,1,1,typo in code,"Bug #1288492 in neutron: ""Typo in floating ip migration script""","Missing comma here:
https://github.com/openstack/neutron/blob/30f6f2fa4e6e73cf8045febbd1e8d26360714ac5/neutron/db/migration/alembic_migrations/versions/2eeaf963a447_floatingip_status.py#L38","Fix typo in migration script

Recently merged migration script for floating ip is missing
a list item separator.

Change-Id: I6d20cfd584059befd591c856ec1519450030b4b2
Closes-Bug: #1288492"
967,7f7cfc2dc6baa53adbea431aba840cd88eea8040,1413903116,,1.0,55,3,6,3,1,0.586576549,False,,,,,True,,,,,,,,,,,,,,,,,1409,1879,1383757,nova,7f7cfc2dc6baa53adbea431aba840cd88eea8040,1,1,Revert,"Bug #1383757 in OpenStack Compute (nova): ""oslo-incubator copy of request_id needed for grenade juno -> master upgrade""","http://logs.openstack.org/74/128974/1/check/check-grenade-dsvm/6b7d6f3/logs/new/screen-n-api.txt.gz#_2014-10-21_07_12_30_741
https://review.openstack.org/#/c/127057/ caused the issue.","Revert ""Replace outdated oslo-incubator middleware""

This is breaking the Juno -> master grenade path. As we are using the old
api-paste.ini file which requires the oslo-incubator copy of request_id

This breakage didn't happen from Icehouse -> master since the icehouse
api-paste.ini didn't have request_id as a middleware. request_id is only
used as middleware in the V21/V3 API and not the V2 API.

Closes-Bug: #1383757
This reverts commit 641de5666f38532252e91735a1cd2611d96c489a.

Change-Id: I1ca6bddf9d9370529e999efcc7eae60622e19e6f"
968,7f9c6259923747f0629166dc85561b0ab231ff70,1396782505,1.0,1.0,14,1,2,2,1,0.566509507,True,2.0,719732.0,66.0,9.0,False,17.0,3680655.0,20.0,4.0,65.0,1461.0,1470.0,65.0,1219.0,1228.0,56.0,857.0,865.0,0.051865332,0.780709736,0.787989081,749,1176,1303312,neutron,7f9c6259923747f0629166dc85561b0ab231ff70,0,0,"feature ""Allow combined certificate/key files for SSL”","Bug #1303312 in neutron: ""SSL mode with combined cert/keys not supported""","When Neutron WSGI is running in SSL mode, it requires a separate cert file and key file. However, there are cases where these may be combined into one file and neutron currently does not support this mode of operation even though the underlying SSL library does[1].
1. https://docs.python.org/2/library/ssl.html#ssl.wrap_socket","Allow combined certificate/key files for SSL

Allows the ssl_key_file parameter to be excluded
during SSL operation to support combined certificate/key files.

Closes-Bug: #1303312
Change-Id: Ied5c7a7657e0e26eda31305fc96104c6593e9593"
969,7fbb42296970ee30d21bbd66b144a2ab421aa17f,1393035063,,1.0,29,1,4,4,1,0.89232035,True,8.0,3922876.0,63.0,17.0,False,44.0,1330544.0,94.0,4.0,52.0,1670.0,1700.0,48.0,1428.0,1454.0,49.0,1567.0,1595.0,0.006744908,0.211520302,0.21529745,395,808,1279300,nova,7fbb42296970ee30d21bbd66b144a2ab421aa17f,0,0,Give more info,"Bug #1279300 in OpenStack Compute (nova): ""detach boot device volume without warning""","If I use block_device_mapping_v2 api to set a backend volume as boot device for instance. The instance will be launched by this remote volume successfully.
But, in such case, we can also use detach api to force this boot device being detached. So, when we do it, the guestOS of this instance would be damaged, and the whole system would not work normally if do a I/O operation. It seems can not resume.
I think we should give a warning for user at least or even forbiden this operation.","Don't detach root device volume

Detach a root device volume may cause various guest error. Although
we can argue that it's user response to not detach the root device
volume, a check in nova will be more helpful.

We can either give a warning message, or not allow it. This patch
forbids this operation, considering that even in real system, remove
root device does not make much sense.

Closes-Bug: #1279300

DocImpact

Change-Id: I01f086be668b35442a2f139f4d7dc96884a3bda6
Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>"
970,7fd2eea9710ca96415ee7d08349ae981a262f44f,1406641027,,1.0,7,3,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,1113,1568,1349808,cinder,7fd2eea9710ca96415ee7d08349ae981a262f44f,1,0,""" Update ref used for notifications
“","Bug #1349808 in Cinder: ""volume has wrong status in notification-info when it had extended successfully""","Volume's status have update to 'available' in database when volume extended successfully, but in 'notification-info' it still being 'extending'.
2014-07-28 14:45:40.449 3580 INFO cinder.volume.manager [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] volume d514c152-890b-47ae-ba87-2cb47a34fdcb: extended successfully
2014-07-28 14:45:40.513 3580 INFO oslo.messaging._drivers.impl_zmq [req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0 06d7b23a60ab4341bbc8701fb121de26 236be19c86d7461295b122f3ef16cf27 - - -] 'notifications-info' {'event_type': 'volume.resize.end',
 'message_id': 'b59eeffa-cb58-4930-81b0-8086fa92eba3',
 'payload': {'availability_zone': u'nova',
             'created_at': '2014-07-15 03:57:03',
             'display_name': u'vol22',
             'launched_at': '2014-07-15 03:57:12',
             'size': 3,
             'snapshot_id': None,
             'status': u'extending',
             'tenant_id': u'236be19c86d7461295b122f3ef16cf27',
             'user_id': u'06d7b23a60ab4341bbc8701fb121de26',
             'volume_id': u'd514c152-890b-47ae-ba87-2cb47a34fdcb',
             'volume_type': None},
 'priority': 'INFO',
 'publisher_id': 'volume.controller1',
 'request_id': u'req-2391ae35-c1dc-461c-84f9-9dd0ed89bbd0',
 'timestamp': '2014-07-28 06:45:40.512994'}","Update ref used for notifications

When we update a volume status in the db, we need to
use the newly updated volume-ref object in the following
notification method call.  Otherwise we're sending a
notification message with the old/outdated status
information.

Change-Id: I4d92b340cf5e20aaa885377dfb6bd6fc4ed87d69
Closes-Bug: #1349808"
971,8010c8faf9f030d2c0264189f9e6c70e10a093f2,1393226921,,1.0,49,15,2,2,1,0.896038233,True,5.0,3207771.0,99.0,19.0,False,21.0,1789465.0,25.0,7.0,204.0,2042.0,2076.0,202.0,1773.0,1806.0,195.0,1960.0,1986.0,0.026415094,0.264285714,0.267789757,229,632,1264220,nova,8010c8faf9f030d2c0264189f9e6c70e10a093f2,1,1,(ef3b1385cb3708aa4993a149ce24082b4082e347),"Bug #1264220 in OpenStack Compute (nova): ""An internal error happens if passing invalid parameter to ""create flavor_extraspecs"" API""","If passing invalid parameter to ""create flavor_extraspecs"" API, an internal error happens and Traceback is written in log file.
Nova should return BadRequest response instead of internal error.
$ curl -i 'http://10.21.42.109:8774/v2/fd283c7ef47b4f46899403e9ebb1e2ed/flavors/6e05eb08-ef1a-4183-9eb9-5c175060247a/os-extra_specs' [..] -d '{""foo"": {""key01"": ""value01""}}'
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-e8b5120c-e585-4095-a387-d3eed52b7415
Date: Thu, 26 Dec 2013 11:06:46 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}}
$
** The log of nova-api **
2013-12-26 20:06:46.026 ERROR nova.api.openstack [req-e8b5120c-e585-4095-a387-d3eed52b7415 admin demo] Caught error: 'NoneType' object has no attribute 'keys'
2013-12-26 20:06:46.026 TRACE nova.api.openstack Traceback (most recent call last):
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/__init__.py"", line 121, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return req.get_response(self.application)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-12-26 20:06:46.026 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-12-26 20:06:46.026 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 581, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return self.app(env, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return resp(environ, start_response)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 930, in __call__
2013-12-26 20:06:46.026 TRACE nova.api.openstack     content_type, body, accept)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 992, in _process_stack
2013-12-26 20:06:46.026 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/wsgi.py"", line 1073, in dispatch
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/api/openstack/compute/contrib/flavorextraspecs.py"", line 76, in create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/api.py"", line 1487, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     extra_specs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 130, in wrapper
2013-12-26 20:06:46.026 TRACE nova.api.openstack     return f(*args, **kwargs)
2013-12-26 20:06:46.026 TRACE nova.api.openstack   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 4483, in flavor_extra_specs_update_or_create
2013-12-26 20:06:46.026 TRACE nova.api.openstack     filter(models.InstanceTypeExtraSpecs.key.in_(specs.keys())).\
2013-12-26 20:06:46.026 TRACE nova.api.openstack AttributeError: 'NoneType' object has no attribute 'keys'","Fix the validation of flavor_extraspecs v2 API

""create flavor_extraspecs"" v2 API does not validate the data type
of a request body. If invalid parameter is passed, an internal error
happens. If many invalid requests come, a log file would be occupied
with traceback.

In addition, it does not validate the lengths of both key and value of
extra_specs. extra_specs are stored into the instance_type_extra_specs
table, and the key and value are defined as String(255).

This patch fixes the validation code from the viewpoint of data type
and key/value length.

Closes-Bug: #1264220

Change-Id: I195bd5d45a896e9b26dd81dab1e49c9f939b4805"
972,801a37de197cdd98f264caa2bbe9c6ebddec070b,1402666217,,1.0,3,140,8,7,1,0.816481797,False,,,,,True,,,,,,,,,,,,,,,,,966,1404,1328694,nova,801a37de197cdd98f264caa2bbe9c6ebddec070b,1,1,“ This reverts commit 8057b66c0b7ac87863f175d850414bc0ed260ab2.”,"Bug #1328694 in OpenStack Compute (nova): ""FloatingIp pollster spamming n-api logs""","Noticed this here:
http://logs.openstack.org/02/99002/1/check/check-tempest-dsvm-full/5fca6a7/logs/screen-n-api.txt.gz?level=TRACE
Showing up a ton since 6/8:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiQ2F1Z2h0IGVycm9yOiBQYXJlbnQgaW5zdGFuY2VcIiBBTkQgbWVzc2FnZTpcImlzIG5vdCBib3VuZCB0byBhIFNlc3Npb247IGxhenkgbG9hZCBvcGVyYXRpb24gb2YgYXR0cmlidXRlICdmaXhlZF9pcCcgY2Fubm90IHByb2NlZWRcIiBBTkQgdGFnczpcInNjcmVlbi1uLWFwaS50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiNjA0ODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMjQzNDU3MTcwNX0=
Assuming this is the culprit given it merged on 6/8 and is related to floating ip's which is what shows up in that n-api log mess.
https://review.openstack.org/83676","Revert ""Allow admin user to get all tenant's floating IPs""

This reverts commit 6478554f531f6ee2fa86226fbc79dd31e556bc06.

This API change merged as a bug fix and introduced bugs and our API
change policy is more or less to go through the blueprint/nova-specs
review process:

http://lists.openstack.org/pipermail/openstack-dev/2014-June/037536.html

So reverting this so it can go through the blueprint/nova-specs
review process.

There should be minimal impacts to removing this (broken) API since
it merged not too long ago and was added strictly for Ceilometer's
usage, not an external deployer that would be (more) broken by this
API removal.

Closes-Bug: #1328694

Change-Id: I22a711795ea616aabc155a71703c9c794e84a5dc"
973,803d59e9f409a59403b2ce55040acc02e14eee28,1402491567,,1.0,32,3,2,2,1,0.775512658,False,,,,,True,,,,,,,,,,,,,,,,,902,1338,1321381,nova,803d59e9f409a59403b2ce55040acc02e14eee28,1,1,“Commit a141206e9dfd31955b9b31d9e5a7f73bbd8510ca ensured that user defined ports were not deleted when an instance is deleted or rescheduled. The problem with this commit is that it did not reset the 'device_id' and ‘device_owner' of the port.”,"Bug #1321381 in OpenStack Compute (nova): ""VMware: Ports are not cleaned up after reschedule""","On Havana, when an instance errors out and gets rescheduled, attached ports are currently not being cleaned up and is causing NVS/OVS to have multiple ports with the same UUID and MAC.
This is currently occurring with the VC Driver + Neutron and is blocking VMware Minesweeper CI.","Network: ensure that ports are 'unset' when instance is deleted

Commit a141206e9dfd31955b9b31d9e5a7f73bbd8510ca ensured that user defined
ports were not deleted when an instance is deleted or rescheduled. The
problem with this commit is that it did not reset the 'device_id' and
'device_owner' of the port. This leads to port leakage and eventually a tenant
is unable to launch any instances as their port quota has exceeded the
allocated amount.

The solution is for the deallocation to reset the 'device_id' and the
'device_owner' of the port. This is allocated when the port is actually assigned
to an instance.

In the case of minesweeper (which runs on OpenStack), the tenant was leaking ports.
The root cause was that the user defined ports were assigned to a specific instance,
which was deleted, as a result of a failed reschedule (which is legitimate).

When an instance is deleted all artificats of that instance should be reset on the
port.

Change-Id: Ie1516c299ce6d79faf2d5103a14d6d8931d3ede2
Closes-bug: #1321381"
974,804f5de841b410c3e6a2ff30756546f2a086b112,1409803553,,1.0,29,4,2,2,1,0.999337504,False,,,,,True,,,,,,,,,,,,,,,,,1304,1770,1368381,cinder,804f5de841b410c3e6a2ff30756546f2a086b112,1,1,,"Bug #1368381 in Cinder: ""Export target objects hang in Datera backend""","For every export in the Datera driver, there is an export object associated with it.
Problem: When a detach operation happens, the export object hangs around.
To Reproduce: Attach a Datera volume to a virtual machine. Then detach it and notice in the backend the object is still there.
Expected Behavior: If the lun that exists in the export object is removed disassociated, the export object should be destroyed.","Destroy Datera export target after detach

Since there is only one lun associated with each target export object in
the Datera backend, we will destroy the target export object after
detaching the volume so they don't build up. This patch also fixes some
missing values for formatted strings.

Closes-Bug: #1368381
Change-Id: Id44ac4bb385ab39af50865a295938861ca977fa3"
975,80b14a54710dad659fde3e319f8b0cf0e673acdd,1410212559,,1.0,252,2,10,5,1,0.759251532,False,,,,,True,,,,,,,,,,,,,,,,,1254,1714,1364279,cinder,80b14a54710dad659fde3e319f8b0cf0e673acdd,1,0,“The new cinder pools submission breaks any cinder operation that involves <host> as input parameter”,"Bug #1364279 in Cinder: ""Cinder manage now requires a pool name, which administrators have no way to discover""","The new cinder pools submission breaks any cinder operation that involves <host> as input parameter. I tested cinder manage/unmanage feature taking the latest code. cinder manage breaks if host not given in the form host@backend#pool. For eg the following breaks:
cinder manage --source-name vol1 openstack1@iscsi
but this succeeds
cinder manage --source-name vol1 openstack1@iscsi#pool1
The knowledge of pool is only limited to the backend and hence any operation that involves <host> as input should not be burdened to also include pool name with it. This should be looked into at high priority.","Allow scheduler pool information to be retrieved

With pool support added to Cinder, now we are kind of in an awkward
situation where we require admin to input exact location for volumes
to-be managed (imported) or migrated, which must have pool info, but
there is no way to find out what pools are there for backends except
looking at the scheduler log.  That causes bad user experience, and
thus is a bug from UX POV.

This change simply adds a new admin-api extension to allow admin to
fetch all the pool information from scheduler cache (memory), which
closes the gap for end users.

This extension provides two level of pool information: names only or
detailed information:

Pool name only:
GET http://CINDER_API_ENDPOINT/v2/TENANT_ID/scheduler-stats/get_pools

Detailed Pool info:
GET http://CINDER_API_ENDPOINT/v2/TENANT_ID/scheduler-stats/get_pools
\?detail\=True

Closes-bug: #1364279

Change-Id: I445d4e472c83db2f2d8db414de139c87d09f8fda"
976,80df9f5ec53cc71c1ec51a8590921ae5b776ec22,1404993538,1.0,1.0,39,2,5,5,1,0.779133431,False,,,,,True,,,,,,,,,,,,,,,,,1053,1499,1340159,nova,80df9f5ec53cc71c1ec51a8590921ae5b776ec22,1,1,,"Bug #1340159 in OpenStack Compute (nova): ""Resize to zero disk flavor is not allowed""","When old flavor's root_gb is not equal 0 and new flavor's root_gb is 0,    the resize() in nova.compute.api will raise CannotResizeDisk.
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2368
    def resize(self, context, instance, flavor_id=None,
        if not flavor_id:
            LOG.debug(""flavor_id is None. Assuming migration."",
                      instance=instance)
            new_instance_type = current_instance_type
        else:
            new_instance_type = flavors.get_flavor_by_flavor_id(
                    flavor_id, read_deleted=""no"")
            if (new_instance_type.get('root_gb') == 0 and
                current_instance_type.get('root_gb') != 0):
                reason = _('Resize to zero disk flavor is not allowed.')
                raise exception.CannotResizeDisk(reason=reason)","Catch CannotResizeDisk exception when resize to zero disk

When old flavor's root_gb is not equal 0 and new flavor's root_gb is 0,
the resize() in nova.compute.api will raise CannotResizeDisk.

Move up the new_instance_type check before using.

Closes-Bug: #1340159

Change-Id: I4abf93530cf919af50a88d6049019fb745547257"
977,80eb1faf1b9466176a0fbf1f8a8b18cb500c6811,1395958913,0.0,1.0,37,6,2,2,1,0.99022469,True,2.0,324927.0,51.0,9.0,False,30.0,260318.0,51.0,6.0,107.0,1313.0,1329.0,107.0,1139.0,1155.0,100.0,821.0,833.0,0.09683605,0.788111218,0.799616491,693,1119,1298658,neutron,80eb1faf1b9466176a0fbf1f8a8b18cb500c6811,1,1,,"Bug #1298658 in neutron: ""Stale external gateway devices left behind""","This is a follow on to https://bugs.launchpad.net/neutron/+bug/1244853.
I found today that the same problem can happen with external gateway devices.  Those should be identified and removed in a manner similar to the fix before the other bug.","Delete duplicate external devices in router namespace

When a router's gateway is removed during L3-agent restart, the agent
will fail to delete the old external device.  This device should be
identified and removed as soon as possible.

Change-Id: Ifd3e6ca009242138bea4a098e3fde258aeaa391e
Closes-Bug: #1298658"
978,80ffc9b8b38293c05c9d48cb615b24fd7255a850,1392394945,,1.0,15,17,3,2,1,0.755975243,True,5.0,6825054.0,34.0,10.0,False,47.0,929023.6667,90.0,4.0,88.0,3214.0,3253.0,74.0,2734.0,2765.0,71.0,2302.0,2329.0,0.00978128,0.3128651,0.31653308,412,827,1280363,nova,80ffc9b8b38293c05c9d48cb615b24fd7255a850,0,0,Replace exceptions (refactoring),"Bug #1280363 in OpenStack Compute (nova): ""Replace exception re-raises with excutils.save_and_reraise_exception() in the Hyper-V driver""",A few methods of the Hyper-V driver ops classes use a catch / raise pattern that replaces original exceptions with specific domain exceptions inherited from vmutils.HyperVUtilsException(). The original exception is thus masqueraded and trackable only by analysing the logs.,"Replaces exception re-raising in Hyper-V

In a few cases in the Hyper-V driver classes, generic exceptions
are getting caught, logged and a domain specific exception is raised.

This creates issues in identifying the exception cause programmatically
(e.g. in unit tests).

This patch enforces the usage of excutils.save_and_reraise_exception()

Closes-Bug: #1280363

Change-Id: I27cc54941c5f592ea567ccaa8fa369362da1427f"
979,81348368c70cd39c6241e7da6d33629e577494f5,1407207970,,1.0,45,45,6,3,2,0.546350532,False,,,,,True,,,,,,,,,,,,,,,,,1145,1600,1352595,nova,81348368c70cd39c6241e7da6d33629e577494f5,1,0,"""In libvirt driver directory, rbd.py confict with global rbd library
    which is imported in rbd.py,”","Bug #1352595 in OpenStack Compute (nova): ""nova boot fails when using rbd backend""","Trace ends with:
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3]  File ""/opt/stack/nova/nova/virt/libvirt/rbd.py"", line 238, in exists
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3]     except rbd.ImageNotFound:
TRACE nova.compute.manager [instance: c1edd5bf-ba48-4374-880f-1f5fa2f41cd3] AttributeError: 'module' object has no attribute 'ImageNotFound'
It looks like the above module tries to do a ""import rbd"" and ends up importing itself again instead of the global library module.
A quick fix would be renaming the file to rbd2.py and changing the references in driver.py and imagebackend.py, but maybe there is a better solution?","Rename rbd.py to rbd_utils.py in libvirt driver directory

In libvirt driver directory, rbd.py confict with global rbd library
which is imported in rbd.py, so we rename rbd.py to rbd_utils.py.

Change-Id: Ib62e430e678fe09c4a8475a636a8ecc11a194f5c
Closes-Bug: #1352595"
980,8143d814f2b355b1a2a7a1bd92920641e4d0fb43,1390894257,,1.0,34,21,2,2,1,0.376019851,True,3.0,7243355.0,10.0,5.0,False,30.0,1145004.0,67.0,5.0,194.0,942.0,947.0,193.0,760.0,765.0,164.0,602.0,602.0,0.146147033,0.534100974,0.534100974,318,727,1272136,glance,8143d814f2b355b1a2a7a1bd92920641e4d0fb43,0,0,tests,"Bug #1272136 in Glance: ""Case test_get_index_sort_updated_at_desc failed sometimes""","I have seen this failure several times. And it doesn't happen always, so it seems like a race condition issue. And until, I just saw it on py27, so not sure if it's existed in py26.
2014-01-23 15:12:33.910 | FAIL: glance.tests.unit.v2.test_registry_client.TestRegistryV2Client.test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.910 | ----------------------------------------------------------------------
2014-01-23 15:12:33.911 | _StringException: Traceback (most recent call last):
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/unit/v2/test_registry_client.py"", line 268, in test_get_index_sort_updated_at_desc
2014-01-23 15:12:33.911 |     unjsonify=False)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/glance/tests/utils.py"", line 472, in assertEqualImages
2014-01-23 15:12:33.911 |     self.assertEqual(images[i]['id'], value)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 324, in assertEqual
2014-01-23 15:12:33.911 |     self.assertThat(observed, matcher, message)
2014-01-23 15:12:33.911 |   File ""/home/jenkins/workspace/gate-glance-python27/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 414, in assertThat
2014-01-23 15:12:33.911 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2014-01-23 15:12:33.911 | MismatchError: !=:
2014-01-23 15:12:33.911 | reference = u'db4ddeb5-edd6-4557-b635-6ecb4e5265a0'
2014-01-23 15:12:33.912 | actual    = '406c995a-70e0-4010-a6eb-9dff61a2d2a7'
http://logs.openstack.org/19/67019/2/check/gate-glance-python27/783d216/console.html","Provide explicit image create value in Registry v2 Client test

Assign an explicit created_at datetime value to image db fixtures,
it be used to fixes race condition in Registry v2 Client test cases.

Closes-Bug: #1272136
Change-Id: Ib9f1c7072c6ac828479c6a2e5bddd0f3ad057653
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
981,815dd8c4ea026b4064a853c7ad5bab47afec06b6,1394220645,,1.0,44,4,2,2,1,0.738284866,True,5.0,1108525.0,92.0,4.0,False,1.0,1259.0,10.0,3.0,42.0,953.0,956.0,42.0,789.0,792.0,34.0,335.0,338.0,0.037634409,0.361290323,0.364516129,562,983,1289236,neutron,815dd8c4ea026b4064a853c7ad5bab47afec06b6,0,0,Change the except. Exception very narrow,"Bug #1289236 in neutron: ""BigSwitch: HTTPException type caught is too narrow""","The BigSwitch plugin currently determines if it needs to reconnect by checking for an httplib ImproperConnectionState exception. However, this exception is too narrow and does not cover the other httplib exceptions that indicate a reconnection is necessary (e.g. NotConnected).
It should just catch httplib.HTTPExceptions.","BigSwitch: Widen range of HTTPExceptions caught

In order to trigger a reconnect, the Big Switch
plugin was only checking for httplib.ImproperConnectionState
which doesn't include relevant exceptions that
indicate a reconnect is necessary. This patch
adjusts it to reconnect on any HTTPException.

Closes-Bug: #1289236
Change-Id: I4b389172dc4a53b0381e40005d8421f71b7d8522"
982,8185d1b5db421441ebba128f23f840a42f9bf050,1385493022,,1.0,70,11,2,2,1,0.98665549,True,2.0,706241.0,12.0,9.0,False,24.0,2455592.0,57.0,8.0,476.0,1430.0,1490.0,470.0,1262.0,1321.0,460.0,1124.0,1177.0,0.385774059,0.941422594,0.985774059,29,341,1244257,cinder,8185d1b5db421441ebba128f23f840a42f9bf050,1,1,,"Bug #1244257 in Cinder: ""KeyError from storwize driver when 'host' is not provided to terminate_connection""","When user tries to delete an instance, if the nova compute driver for the instance's host is down, the nova api code goes into _local_delete().  The following code creates a fake ""connector"" without a 'host' key/vale.
        for bdm in bdms:
            if bdm['volume_id']:
                # NOTE(vish): We don't have access to correct volume
                #             connector info, so just pass a fake
                #             connector. This can be improved when we
                #             expose get_volume_connector to rpc.
                connector = {'ip': '127.0.0.1', 'initiator': 'iqn.fake'}
                self.volume_api.terminate_connection(context,
                                                     bdm['volume_id'],
                                                     connector)
When code flow reaches the cinder driver, which expects a 'host', a ""KeyError: 'host'"" exception is raised, and deletion of the instance fails.
The failure in deletion is as expected since the nova driver is not up. This issue is requesting a modification in nova api code, so that if the nova compute server is not available, a more meaningful error message (""nova compute server is not available"") could be returned, before getting to the cinder code.","Fix Storwize terminate_connection with no host

Nova may pass a connector to Cinder with no 'host' field, which was
causing a KeyError in the Storwize driver. This patch resolves this case
by doing the following:
1. If the volume is mapped to only 1 host, unmap it
2. If the volume was not mapped or mapped to multiple hosts, print a
   warning but don't raise an exception

Change-Id: I0ec1c24adbdfbcf1c4868b4981a2e2618d4b411c
Closes-Bug: #1244257"
983,819f28a0b8863bd18f8a14491b5966c8b2723432,1395913154,,2.0,495,27,7,5,1,0.587809362,False,,,,,True,,,,,,,,,,,,,,,,,711,1137,1300546,glance,819f28a0b8863bd18f8a14491b5966c8b2723432,0,0,,"Bug #1300546 in Glance: ""config.generator could not handle split configs""","Current config.generator could not handle split configs for the different services within a project, all configurations be collected and save to a single large template file. But for most project, the code repo contains more then one service, like nova repo/project contains nova-api, nova-compute. So the single template file is hard to be used/maintained for separated service. IMO, it will be cool if config.generator could generate separated configs based on service instead of project.
Example of ""split configs"" is glance-api.conf versus glance-registry.conf","Enabling separated sample config file generation

As a common approach most projects used now and Oslo preferred, this
change enabled sample configuration file generation mechanism for
each Glance services.

This change, as an enhancement, allows generating separated sample
configuration files for each Glance major services, e.g.:

    etc/glance-api.conf.sample
    etc/glance-cache.conf.sample
    etc/glance-manage.conf.sample
    etc/glance-registry.conf.sample
    etc/glance-scrubber.conf.sample

It is different than I94d486d6686815c45705a7a9b00fb26062e1eb63
which only supports generating an unified sample configuration
file to including all Glance available options.

This mechanism  not only can help auditing by packager, milestone
maintainer or developer as a function (testenv) of tox, but also
those separated configuration files could make deployment be easy.
And it helps keeping sample configuration files be update with
code change.

The change added ""genconfigs"" function as a tox testenv (-egenconfigs),
and it dependes on oslo-config-generator function of oslo.config.

The change doesn't introduce those sample files Glance repo, so
next step is to investigate if we can generate them in gate
automatically when a change was merged.

Related-Change-Id: I15686708fc9460948a58cfea3d18dae40ba1fda9
Related-Change-Id: Iae31856d5886ee78786972d80c7c103c3460a2b3
Related-Change-Id: I76043b08e2872867e5af2a5ac902e4d092fda5c8
Closes-Bug: #1300546
Closes-Bug: #1361963

Change-Id: Ibe03a3fe80b96ca32acb1a6bea7e38e6075951bb
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
984,819f28a0b8863bd18f8a14491b5966c8b2723432,1395913154,,2.0,495,27,7,5,1,0.587809362,False,,,,,True,,,,,,,,,,,,,,,,,1238,1697,1361963,glance,819f28a0b8863bd18f8a14491b5966c8b2723432,1,0,"""glance should do same change as well.”","Bug #1361963 in Glance: ""No default control_exchange configuration prompt in glance-api.conf ""","In current default glance-api.conf, messaging configurations as below, but actually 'rabbit_notification_exchange = glance' and 'qpid_notification_exchange = glance' do not impact topic consumer_queue creation. because Oslo .messaging uses 'control_exchange' as queue name, default value is 'openstack'.  other component such as cinder has written ''control_exchange=cinder'' into cinder conf.  glance should do same change as well.
# Messaging driver used for 'messaging' notifications driver
# rpc_backend = 'rabbit'
# Configuration options if sending notifications via rabbitmq (these are
# the defaults)
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False
# Configuration options if sending notifications via Qpid (these are
# the defaults)
qpid_notification_exchange = glance
qpid_notification_topic = notifications
qpid_hostname = localhost
qpid_port = 5672
qpid_username =
qpid_password =
qpid_sasl_mechanisms =
qpid_reconnect_timeout = 0
qpid_reconnect_limit = 0
qpid_reconnect_interval_min = 0
qpid_reconnect_interval_max = 0
qpid_reconnect_interval = 0","Enabling separated sample config file generation

As a common approach most projects used now and Oslo preferred, this
change enabled sample configuration file generation mechanism for
each Glance services.

This change, as an enhancement, allows generating separated sample
configuration files for each Glance major services, e.g.:

    etc/glance-api.conf.sample
    etc/glance-cache.conf.sample
    etc/glance-manage.conf.sample
    etc/glance-registry.conf.sample
    etc/glance-scrubber.conf.sample

It is different than I94d486d6686815c45705a7a9b00fb26062e1eb63
which only supports generating an unified sample configuration
file to including all Glance available options.

This mechanism  not only can help auditing by packager, milestone
maintainer or developer as a function (testenv) of tox, but also
those separated configuration files could make deployment be easy.
And it helps keeping sample configuration files be update with
code change.

The change added ""genconfigs"" function as a tox testenv (-egenconfigs),
and it dependes on oslo-config-generator function of oslo.config.

The change doesn't introduce those sample files Glance repo, so
next step is to investigate if we can generate them in gate
automatically when a change was merged.

Related-Change-Id: I15686708fc9460948a58cfea3d18dae40ba1fda9
Related-Change-Id: Iae31856d5886ee78786972d80c7c103c3460a2b3
Related-Change-Id: I76043b08e2872867e5af2a5ac902e4d092fda5c8
Closes-Bug: #1300546
Closes-Bug: #1361963

Change-Id: Ibe03a3fe80b96ca32acb1a6bea7e38e6075951bb
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
985,81f9256747377f1b9488b407a2f33d251ffd1d8f,1390871926,,1.0,6,6,2,2,1,0.650022422,True,6.0,5950574.0,86.0,25.0,False,8.0,81638.0,7.0,4.0,23.0,2810.0,2823.0,19.0,2362.0,2372.0,23.0,1890.0,1903.0,0.003344482,0.26351728,0.265328874,335,745,1273803,nova,81f9256747377f1b9488b407a2f33d251ffd1d8f,1,1,Trying to do something wrong,"Bug #1273803 in OpenStack Compute (nova): ""The pci manager try to modify the pci device list""","Currently the ObjectList is mostly immutable, i.e. although the items in the list is changable, but the list itself should not be add or remove.
However the PCI manager use a ObjectList to track all the devices in the host and may add/remove, this is not correct. We should not use the Object List but a simple list to track all the devices.","Remove PciDeviceList usage in pci manager

Pci manager keeps a PciDeviceList and even try to append
object to the list. This is not correct, as the ObjectList
is designed as immutable, although the objects in the list is
mutable. We should simply use a list to keep devices.

Closes-Bug: #1273803

Change-Id: Ie80b6f5f47a1d2d2e20aadb4779be142c17b5b38"
986,81fc3967bca0f154d72d5de7875b646e1b6c47c8,1385134358,,1.0,57,47,19,13,1,0.849974533,True,8.0,15640561.0,79.0,26.0,False,273.0,64805.42105,1786.0,6.0,135.0,2919.0,2923.0,135.0,2350.0,2354.0,130.0,1877.0,1880.0,0.019678534,0.282109058,0.282559712,1763,1251261,1251261,nova,81fc3967bca0f154d72d5de7875b646e1b6c47c8,1,1,,"Bug #1251261 in OpenStack Compute (nova): ""finish_revert_migration does not include context as parameter""","When finish_revert_migration is called, the caller of
finish_revert_migration already includes context as a parameter,
but finish_revert_migration did not reuse this parameter and still
re-generate the context inside finish_revert_migration, we should
add context as a new parameter for finish_revert_migration so that
the functions inside of it can reuse context when needed.
For the function _create_domain_and_network in libvirt/driver.py,
it set context as an optional parameter, but context isn't really
an optional parameter for this method. We should always pass a
context down because it might be needed somewhere inside
_create_domain_and_network().","Add context as parameter for two libvirt APIs

When finish_revert_migration is called, the caller of
finish_revert_migration already includes context as a parameter,
but finish_revert_migration did not reuse this parameter and still
re-generate the context inside finish_revert_migration, we should
add context as a new parameter for finish_revert_migration so that
the functions inside of it can reuse context when needed.

For the function _create_domain_and_network in libvirt/driver.py,
it set context as an optional parameter, but context isn't really
an optional parameter for this method. We should always pass a
context down because it might be needed somewhere inside
_create_domain_and_network().

The patch includes two parts:
1) Add context as a new parameter for finish_revert_migration
2) Set context as a required parameter for _create_domain_and_network

Change-Id: I72d131fc8dce903400e3080d51ba0fc0a1320751
Closes-Bug: #1251261"
987,82283790b7c5ba6ae0e561ee3302d7fed7983267,1390158486,,1.0,2,2,1,1,1,0.0,True,2.0,209760.0,11.0,5.0,False,12.0,1460347.0,26.0,4.0,15.0,1415.0,1428.0,8.0,1237.0,1243.0,2.0,1239.0,1239.0,0.002227171,0.920564217,0.920564217,305,714,1270624,cinder,82283790b7c5ba6ae0e561ee3302d7fed7983267,1,1,Bug in log,"Bug #1270624 in Cinder: ""UnboundLocalError in TgtAdm.update_iscsi_target in brick""","If some error occurs on tgt-admin call, update_iscsi_target fails with UnboundLocalError instead of expected ISCSITargetUpdateFailed because of debug output.
Traceback (most recent call last):
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
    **args)
  File ""/opt/stack/new/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
    return getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 345, in create_volume
    _run_flow_locked()
  File ""/opt/stack/new/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
    retval = f(*args, **kwargs)
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 340, in _run_flow_locked
    _run_flow()
  File ""/opt/stack/new/cinder/cinder/volume/manager.py"", line 336, in _run_flow
    flow_engine.run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 51, in wrapper
    return f(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 118, in run
    self._run()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 128, in _run
    self._revert(misc.Failure())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 81, in _revert
    misc.Failure.reraise_if_any(failures.values())
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 487, in reraise_if_any
    failures[0].reraise()
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 494, in reraise
    six.reraise(*self._exc_info)
  File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 36, in _execute_task
    result = task.execute(**arguments)
  File ""/opt/stack/new/cinder/cinder/volume/flows/create_volume/__init__.py"", line 1450, in execute
    model_update = self.driver.create_export(context, volume_ref)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 595, in create_export
    return self._create_export(context, volume)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 626, in _create_export
    volume_path, chap_auth)
  File ""/opt/stack/new/cinder/cinder/volume/drivers/lvm.py"", line 447, in _create_tgtadm_target
    old_name=old_name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 187, in create_iscsi_target
    self.update_iscsi_target(name)
  File ""/opt/stack/new/cinder/cinder/brick/iscsi/iscsi.py"", line 253, in update_iscsi_target
    LOG.debug(""StdOut from tgt-admin --update: %s"" % out)
UnboundLocalError: local variable 'out' referenced before assignment","Fix UnboundLocalError in TgtAdm.update_iscsi_target

Closes-Bug: #1270624
Change-Id: I74a057a9603c5d6aefaec6db62228352b739df56"
988,822f3069cd485215e863bc142b0e05d7bd1d52e6,1400055013,,1.0,9,9,3,2,1,0.965633607,True,2.0,312010.0,31.0,10.0,False,179.0,561247.3333,772.0,5.0,16.0,2000.0,2004.0,16.0,1668.0,1672.0,10.0,1675.0,1675.0,0.001379137,0.210130391,0.210130391,887,1323,1319615,nova,822f3069cd485215e863bc142b0e05d7bd1d52e6,0,0,"Refactoring, future bug “We should avoid to assign builtin function with other value.”","Bug #1319615 in OpenStack Compute (nova): ""Don't shadow builtin function type""",We should avoid to assign builtin function with other value. That case will lead calling type failure in other place.,"vmware:Don't shadow builtin function type

We should avoid to assign builtin function with other value.

Closes-Bug: #1319615
Change-Id: I9c52524a16b6a74ad76466473338bf86414f0c50"
989,825321cd7af9fc033d05f7bfbf730b50e4c6533c,1403900607,,1.0,39,9,8,3,1,0.947475935,False,,,,,True,,,,,,,,,,,,,,,,,1021,1464,1335315,nova,825321cd7af9fc033d05f7bfbf730b50e4c6533c,1,0,"""old-world style instance and failing on the task_state attribute access.”","Bug #1335315 in OpenStack Compute (nova): ""Cells: set_admin_password doesn't work after an objects update""","Set admin password fails in the child cell service with the following:
2014-06-24 20:45:08.403 29591 DEBUG nova.openstack.common.policy [req- None] Rule compute
:set_admin_password will be now enforced enforce /opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/openstack/common/
policy.py:288
2014-06-24 20:45:08.404 29591 ERROR nova.cells.messaging [req- None] Error processing mes
sage locally: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging Traceback (most recent call last):
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 200, in _process_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 1289, in _process_message_locally
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ells/messaging.py"", line 692, in run_compute_api_method
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 201, in wrapped
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 191, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 172, in inner
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging File ""/opt/rackstack/806.0/nova/lib/python2.6/site-packages/nova/c
ompute/api.py"", line 2712, in set_admin_password
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging instance.task_state = task_states.UPDATING_PASSWORD
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'task_state'
2014-06-24 20:45:08.404 29591 TRACE nova.cells.messaging","Cells: Update set_admin_password for objects

Cells code was calling the compute/api set_admin_password method with an
old-world style instance and failing on the task_state attribute access.
This adds set_admin_password to the list of methods handled via the
cells RPC redirect method which is objects compatible.

Change-Id: I12e212260ed62dc9380202de31fda928a9498dcb
Closes-Bug: #1335315"
990,825499fffc7a320466e976d2842e175c2d158c0e,1382053099,,1.0,11,5,2,2,1,0.69621226,True,10.0,3375324.0,61.0,27.0,False,15.0,664692.0,22.0,6.0,20.0,3160.0,3174.0,20.0,2841.0,2855.0,4.0,2247.0,2250.0,0.000784314,0.352627451,0.353098039,1711,1241625,1241625,nova,825499fffc7a320466e976d2842e175c2d158c0e,1,1,“Enable non-ascii characters in flavor names”,"Bug #1241625 in OpenStack Compute (nova): ""flavor name is restricted to ascii characters""",Flavor name is restricted to [a-zA-Z0-9_.- ] during create. This will not allow characters from languages other than english.,"Enable non-ascii characters in flavor names

The current implementation provided under commit e6b42d made it so that
flavor names did not have special characters (e.g. *&^%$ etc. except for
-._ and space) however it also limited names to only ascii letters.

This patch makes it so that special characters are still not allowed,
except the ones currently allowed, but it allows the use of non-ascii
characters so that flavor names can be created in locales other than
English.

Change-Id: I056451a5e8b7a49f86e0a11d0011393dc769db34
Closes-bug: #1241625"
991,825a90bdb4ff2b6c08ef6717d350cc8b9d654120,1408603876,,1.0,69,2361,21,9,1,0.709752396,False,,,,,True,,,,,,,,,,,,,,,,,1125,1580,1350387,neutron,825a90bdb4ff2b6c08ef6717d350cc8b9d654120,0,0,Refactoring “The Open vSwitch and Linuxbridge plugins are being removed from the Neutron tree in Juno.”,"Bug #1350387 in neutron: ""Remove the Cisco Nexus monolithic plugin from the code tree""","The Open vSwitch and Linuxbridge plugins are being removed from the Neutron tree in Juno.
See https://bugs.launchpad.net/neutron/+bug/1323729
The Cisco Nexus monolithic plugin does not work without the Open vSwitch plugin, so it also needs to be removed from the tree. The Cisco monolithic plugin contains code for both the Nexus hardware switches and the N1KV virtual switch. The N1KV code will remain in the tree (it does not depend on the OVS plugin).
Note: the Cisco Nexus is now supported in Neutron via the Nexus mechanism driver in the ML2 plugin.","Remove the Cisco Nexus monolithic plugin

The Cisco Nexus monolithic plugin does not work without the Open
vSwitch plugin. The Open vSwitch plugin is scheduled to be removed
as per #1323729. This patch removes the Nexus Hardware switch
related plugin code. The N1KV virtual switch related code will
still remain in the tree as it doesn't depend on Open vSwitch
plugin.

Closes-Bug: #1350387
Change-Id: I2542e92c25e9280e975c3903afb9114e850a966a"
992,82683feff64b5b0dff36f983526421e608f13d94,1401298635,,1.0,9,5,3,3,1,0.817581734,False,,,,,True,,,,,,,,,,,,,,,,,932,1368,1324194,neutron,82683feff64b5b0dff36f983526421e608f13d94,1,1,“This is caused by a missing network_id in the port body”,"Bug #1324194 in neutron: ""keyerror while updating dhcp port""","When an update for a dhcp port fails, a keyerror on missing network_id while attempting to log a trace masks the underlying exception. An example is here:
http://logs.openstack.org/91/94891/1/check/check-tempest-dsvm-neutron/c87b4fc/logs/screen-q-svc.txt.gz?level=TRACE
This is because on update, the dhcp agent only sends either this:
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L770
or this:
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/dhcp.py#L787
This is somewhat a corner case, but it would be good to address the issue to see what actually went wrong.","Fix KeyError exception while updating dhcp port

This is caused by a missing network_id in the port body.
This patch adds it so that a warning message can be traced
correctly. Wording is slightly tweaked to ensure it applies
to the right context.

Closes-bug: #1324194

Change-Id: I4b67b5c3584aa33278eb0e9e879ca338ba0ca8b0"
993,826c4520d093f5e0809a08e10b5d18bf69adc698,1351377390,0.0,,160,0,2,2,1,0.963167245,True,2.0,7051.0,10.0,5.0,False,0.0,0.0,0.0,4.0,950.0,1477.0,2214.0,887.0,1466.0,2144.0,3.0,186.0,186.0,0.020725389,0.968911917,0.968911917,1868,1358524,1358524,Cinder,826c4520d093f5e0809a08e10b5d18bf69adc698,0,0,“Improve Cinder API internal cache interface”,"Bug #1358524 in Cinder: ""Improve Cinder API cache implementation""","The current cache implementation found in cinder.api.openstack.wsgi is too simple.
Only a bunch of generic methods are provided:
- cache_resource(self, resource_to_cache, id_attribute='id', name=None)
- cached_resource(self, name=None):
- cached_resource_by_id(self, resource_id, name=None):
As you can see, the name is optional. Current API code never explicitly provides the resource type name. This could be problematic is we try to cache 2 types of resources during the same request.
Furthermore, the documentation provided in cinder.api.openstack.wsgi is wrong:
    Different resources types might need to be cached during the same
    request, they can be cached using the name parameter. For example:
        Controller 1:
            request.cache_resource(db_volumes, 'volumes')
            request.cache_resource(db_volume_types, 'types')
        Controller 2:
            db_volumes = request.cached_resource('volumes')
            db_type_1 = request.cached_resource_by_id('1', 'types')
The second parameter of cache_resource is id_attribute, not name.
To improve the situation, it has been suggested to take the Nova's implementation instead.
The Nova's implementation provides a better interface. Each resource type has a dedicated method which makes the resource type cached explicitly mentioned.
An example of such implementation for Cinder would be:
- cache_db_volumes(volumes) vs cache_resource(volumes, name='volumes')
- cache_db_volume(volume) vs cache_resource(volumes, name='volumes')
- get_db_volumes() vs cached_resource('volumes')
- get_db_volume(id) vs cached_resource_by_id(id, name='volumes')
This interface makes it clear that a volume is added or retrieved from the cache. A side-effect is that code will be shorter.
The proposed implementation will be backward compatible for people with out-of-tree extensions by preserving existing methods and still using the same variable to store the cached resources.","Add VolumeHostAttribute API extension

Expose the host on which a volume resides through a new API
extension. This is only presented to Admins by default. This
can be controlled with the 'volume_host_attribute' policy
rule.

Fixes bug 1035350.

Change-Id: I0a74a0dfbd78e853219150fbe0d3fba77c6f9bb6"
994,82716b4ac836024b968c76db75be8a92ede0e226,1412863546,,1.0,14,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1370,1840,1376199,cinder,82716b4ac836024b968c76db75be8a92ede0e226,1,1,,"Bug #1376199 in Cinder: ""cinder backup service should change backup & volume status  in absence of swift service""","stopped one of swift service [s-object]
Take back up of any existing volume
observed exception in c-bak service logs however status of backup remains in 'creating' only
commands :
stop s-object service
ssatya@autojuno:~/juno/devstack$ cinder backup-create b7d8b000-d69c-4411-ab00-a3911c64ecdc  --name test_backup2
+-----------+--------------------------------------+
|  Property |                Value                 |
+-----------+--------------------------------------+
|     id    | c19ecea3-a57c-49b4-94ed-22854a031f53 |
|    name   |             test_backup2             |
| volume_id | b7d8b000-d69c-4411-ab00-a3911c64ecdc |
+-----------+--------------------------------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     |      None     |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+------------+------+------+-------------+----------+-------------+
|                  ID                  |   Status   | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available  | test |  1   |     None    |  false   |             |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | backing-up | test |  1   |     None    |  false   |             |
+--------------------------------------+------------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
| 74c5a098-b2a8-4e80-b82d-18441396280e | available | test |  1   |     None    |  false   |             |
| b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test |  1   |     None    |  false   |             |
+--------------------------------------+-----------+------+------+-------------+----------+-------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct  1 13:50:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
ssatya@autojuno:~/juno/devstack$ date
Wed Oct  1 13:55:27 IST 2014
ssatya@autojuno:~/juno/devstack$ cinder backup-list
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
|                  ID                  |              Volume ID               |   Status  |     Name     | Size | Object Count |   Container   |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+
| c19ecea3-a57c-49b4-94ed-22854a031f53 | b7d8b000-d69c-4411-ab00-a3911c64ecdc |  creating | test_backup2 |  1   |     None     | volumebackups |
| df49c43a-0ee8-43ad-8254-5c8526922d73 | b7d8b000-d69c-4411-ab00-a3911c64ecdc | available | test_backup  |  1   |      2       | volumebackups |
+--------------------------------------+--------------------------------------+-----------+--------------+------+--------------+---------------+","Truncate fail_reason to column length

During create_backup failure handling, backup_update fails with
DataError (""Data too long for column"") if the fail_reason is
greater than 255 characters. As a result, backup status is stuck
in 'creating' state. This patch avoids the problem by truncating
fail_reason to 255 characters before update.

Closes-Bug: #1376199
Change-Id: If0d616b81d3869f7ea110caab8cf4140cf5c5c9e"
995,82821b407ce7a226918758876abca46b727d9706,1392369117,,1.0,20,1,2,2,1,0.591672779,True,14.0,2960469.0,147.0,47.0,False,155.0,323615.0,475.0,4.0,0.0,1650.0,1650.0,0.0,1370.0,1370.0,0.0,1588.0,1588.0,0.000135888,0.215926077,0.215926077,348,759,1274758,nova,82821b407ce7a226918758876abca46b727d9706,1,1,,"Bug #1274758 in OpenStack Compute (nova): ""Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'""","Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/periodic_task.py"", line 182, in run_periodic_tasks
TRACE nova.openstack.common.periodic_task     task(self, context)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5049, in update_available_resource
TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/compute/resource_tracker.py"", line 300, in update_available_resource
TRACE nova.openstack.common.periodic_task     resources = self.driver.get_available_resource(self.nodename)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3943, in get_available_resource
TRACE nova.openstack.common.periodic_task     stats = self.host_state.get_host_stats(refresh=True)
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5016, in get_host_stats
TRACE nova.openstack.common.periodic_task     self.update_status()
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5052, in update_status
TRACE nova.openstack.common.periodic_task     data[""vcpus_used""] = self.driver.get_vcpu_used()
TRACE nova.openstack.common.periodic_task   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3626, in get_vcpu_used
TRACE nova.openstack.common.periodic_task     total += len(vcpus[1])
TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object has no attribute '__getitem__'
TRACE nova.openstack.common.periodic_task
http://logs.openstack.org/51/63551/6/gate/gate-tempest-dsvm-postgres-full/4860441/logs/screen-n-cpu.txt.gz?level=TRACE#_2014-01-30_22_38_29_401
Seen in the gate
logstash query: message:""TypeError: 'NoneType' object has no attribute '__getitem__'"" AND filename:""logs/screen-n-cpu.txt""","Fixes NoneType vcpu list returned by Libvirt driver

* libvirt dom.vcpu may return a NoneType if vcpus dont exist

Change-Id: Iee10c2559ceea636bc4896595962e3470228ad35
Closes-Bug: #1274758"
996,8299e80ad437d86925484366c29a01fa30c344ed,1412699577,,1.0,18,3,3,3,1,0.846795401,False,,,,,True,,,,,,,,,,,,,,,,,1382,1852,1378389,nova,8299e80ad437d86925484366c29a01fa30c344ed,1,1,,"Bug #1378389 in OpenStack Compute (nova): ""os-interface:show will not handle PortNotFoundClient exception from neutron""","The os-interface:show method in the v2/v3 compute API is catching a NotFound(NovaException):
http://git.openstack.org/cgit/openstack/nova/tree/nova/api/openstack/compute/contrib/attach_interfaces.py?id=2014.2.rc1#n67
But when using the neutronv2 API, if you get a port not found it's going to raise up a PortNotFoundClient(NeutronClientException), which won't be handled by the NotFound(NovaException) in the compute API since it's not the same type of exception.
http://git.openstack.org/cgit/openstack/nova/tree/nova/network/neutronv2/api.py?id=2014.2.rc1#n584
This bug has two parts:
1. The neutronv2 API show_port method needs to return nova exceptions, not neutron client exceptions.
2. The os-interfaces:show v2/v3 APIs need to handle the exceptions (404 is handled, but neutron can also raise Forbidden/Unauthorized which the compute API isn't handling).","Handle Forbidden error from network_api.show_port in os-interface:show

We can get a 403 back from the neutronv2 API when calling
network_api.show_port so handle that in the compute API extension.

This also updates the v3 API code to match the v2 API code to put the
original error message for the 404 into the HTTPNotFound error.

Closes-Bug: #1378389

Change-Id: I7afc832f26481dc44f1c500b6121e2060f36c63a"
997,831e6013544326e32a1e252917301618c9ea7626,1381388407,,1.0,108,16,4,4,1,0.600534341,True,6.0,2341695.0,53.0,19.0,False,27.0,87263.0,44.0,3.0,3.0,1645.0,1647.0,3.0,1440.0,1442.0,1.0,1580.0,1580.0,0.000319234,0.25235435,0.25235435,1660,1237611,1237611,nova,831e6013544326e32a1e252917301618c9ea7626,1,1, ,"Bug #1237611 in OpenStack Compute (nova): ""flavor name with only white spaces should return 400 error""","When user passes only white spaces to flavor name, it creates flavor successfully.  Since name is a mandatory parameter, it should restrict user from passing white spaces. Also leading and trailing white spaces should be removed before saving it to the backend similar to the instance name.
{
    ""flavor"": {
        ""name"": ""   ""
        ""ram"": 1024,
        ""vcpus"": 2,
        ""disk"": 10,
        ""id"": ""10"",
        ""os-flavor-access:is_public"": false
    }
}
For example
name = ""   "" 		  #not allowed
name = ""extra large"" 	  #allowed
name = ""  extra large  "" #allowed, but leading and trailing white spaces will be trimmed before saving it to the backend.
Actual output: HTTP/1.1 200 OK
Expected  output: HTTP/1.1 400 Bad Request","Flavor name should not contain only white spaces

Returns 400 Bad request instead of 200 Ok if flavor name
contains only whitespaces.
Flavor name containing any leading and trailing whitespaces will be
trimmed before validation and before saving it to the backend.

Added unit test cases for both v2 and v3 api's. For v3 api instead of
stubbing flavors 'create' method stubbed db's 'flavor_create' method.
Also removed redundant code of stubbing 'flavor_create' method.

For example
name = ""  ""              #not allowed
name = ""extra large""     #allowed
name = "" extra large ""   #allowed, but leading and trailing whitespaces
                         #will be trimmed (same as instance name)

Closes-Bug: #1237611

Change-Id: Ie68be299030a878a752dbfdcdfe5094f8427598a"
998,8328fc46d783f4ec9286eededafa91afae89cba0,1388085157,,1.0,16,4,1,1,1,0.0,True,3.0,27959.0,9.0,4.0,False,4.0,138933.0,5.0,2.0,1.0,1213.0,1214.0,1.0,1039.0,1040.0,0.0,1059.0,1059.0,0.00078125,0.828125,0.828125,231,634,1264360,cinder,8328fc46d783f4ec9286eededafa91afae89cba0,1,0,Mysql specific,"Bug #1264360 in Cinder: ""Downgrading cinder schema fails when running 018_add_qos_specs.py""","Downgrading the cinder schema fails when running  018_add_qos_specs.py under MySQL.  The upgrade path of this schema patch adds the foreign key volume_types_ibfk_1 on table volume_types, and the downgrade does not correspondingly remove it before attempting to drop the qos_specs_id column.","Bugfix missing foreign key removal for mysql

Downgrading the cinder schema fails when running 018_add_qos_specs.py
under MySQL. The upgrade path of this schema patch adds the foreign
key volume_types_ibfk_1 on table volume_types, and the downgrade does
not correspondingly remove it before attempting to drop the
qos_specs_id column.  This update removes the foreign key when
the engine is mysql prior to dropping the column.

Change-Id: Ibd3b35ad3b0bd41ad04ab7aeeb28c3ba7e5d255d
Closes-Bug: #1264360"
999,834e1f2150c6f2afd92036e6b7f53afee4006682,1403809148,,1.0,10,9,2,2,1,0.74248757,False,,,,,True,,,,,,,,,,,,,,,,,1014,1457,1334774,glance,834e1f2150c6f2afd92036e6b7f53afee4006682,1,0,"“upload_utils is concatenating str and Message objects which doesn't work
on python 2, so change the str to a unicode object.”","Bug #1334774 in Glance: ""test_quota unit tests failing with UnicodeError from oslo gettextutils""","I'm seeing something like this in many unrelated changes:
2014-06-26 15:55:57.452 | FAIL: glance.tests.unit.test_quota.TestImageLocationQuotas.test_add_too_many_image_locations
2014-06-26 15:55:57.452 | tags: worker-0
2014-06-26 15:55:57.452 | ----------------------------------------------------------------------
2014-06-26 15:55:57.452 | Traceback (most recent call last):
2014-06-26 15:55:57.452 |   File ""glance/tests/unit/test_quota.py"", line 568, in test_add_too_many_image_locations
2014-06-26 15:55:57.452 |     self.assertIn('Attempted: 2, Maximum: 1', str(exc))
2014-06-26 15:55:57.452 |   File ""glance/openstack/common/gettextutils.py"", line 333, in __str__
2014-06-26 15:55:57.452 |     raise UnicodeError(msg)
2014-06-26 15:55:57.452 | UnicodeError: Message objects do not support str() because they may contain non-ascii characters. Please use unicode() or translate() instead.
http://logs.openstack.org/63/102863/1/check/gate-glance-python27/6ad16a3/console.html
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5pY29kZUVycm9yOiBNZXNzYWdlIG9iamVjdHMgZG8gbm90IHN1cHBvcnQgc3RyKCkgYmVjYXVzZSB0aGV5IG1heSBjb250YWluIG5vbi1hc2NpaSBjaGFyYWN0ZXJzLiBQbGVhc2UgdXNlIHVuaWNvZGUoKSBvciB0cmFuc2xhdGUoKSBpbnN0ZWFkLlwiIEFORCB0YWdzOmNvbnNvbGUgQU5EIHByb2plY3Q6XCJvcGVuc3RhY2svZ2xhbmNlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDM4MDQ5ODkwMDZ9
There is a gettextutils sync here: https://review.openstack.org/#/c/91047/
But that didn't hit this in the check queue, but hit it after being approved.","Fix lazy translation UnicodeErrors

upload_utils is concatenating str and Message objects which doesn't work
on python 2, so change the str to a unicode object.

test_quota.py was casting Message objects to str which fails for the
same reason, so rather than cast to str, cast using six.text_type.

See oslo commit 2cfc1a78d8063cf20083cf7df796d730a576551c for history and
details on why the Message object doesn't support __str__ casting.

This is a small part of a larger effort that the oslo team will be
rolling out with graduation and adoption of the oslo-i18n library.

Closes-Bug: #1334774

Change-Id: I9d3f29c661f1afffabbbbb499f48d37ef5a8fcee"
1000,83a11d406eb421285a7fffd3ce853394211ac397,1396423057,0.0,1.0,40,34,3,2,1,0.680721339,True,7.0,521819.0,118.0,17.0,False,12.0,39597.0,13.0,4.0,217.0,506.0,614.0,201.0,424.0,518.0,174.0,303.0,376.0,0.162790698,0.282790698,0.350697674,724,1150,1301363,neutron,83a11d406eb421285a7fffd3ce853394211ac397,0,0,Feature “It makes sense to refactor the functional tests to provide a base class which all these classes can inherit from”,"Bug #1301363 in neutron: ""Refactor functional tests to create base class for agent functional tests""",I'm writing additional functional tests which require sudo. There is also additional shared code from some existing agent tests which would be nice to use. It makes sense to refactor the functional tests to provide a base class which all these classes can inherit from.,"Add common base class for agent functional tests

Refactor the functional tests a bit to add a common base class for agent
tests. This is needed an upcoming commit which adds a functional test
for VXLAN version checking.

Closes-Bug: #1301363

Change-Id: I438e401cbfb40e5557e5125f2a409bac5c1fd1c2"
1001,83a3e77e63851c290e2f849db6db555cffb1ef41,1382768565,,1.0,11,37,2,2,1,0.650022422,True,1.0,1704821.0,11.0,3.0,False,2.0,848375.0,2.0,2.0,377.0,908.0,1101.0,355.0,843.0,1022.0,121.0,387.0,419.0,0.245472837,0.780684105,0.845070423,1716,1242447,1242447,neutron,83a3e77e63851c290e2f849db6db555cffb1ef41,0,0,“deprecated”,"Bug #1242447 in neutron: ""X_TENANT_ID or X_ROLE in keystone auth are not deprecated""","In neutron/auth.py, X_TENANT_ID and X_ROLE are used to extract context information.
They are now deprecated and X_PROJECT_ID and X_ROLES are recommended instead.
https://github.com/openstack/python-keystoneclient/blob/master/keystoneclient/middleware/auth_token.py","Remove deprecated fields in keystone auth middleware

This commit makes the following changes:

Use X_PROJECT_ID instead of X_TENANT_ID:
  In the latest keystone auth_token middleware it is recommended to
  use X_PROJECT_ID instead of X_TENANT_ID. X_PROJECT_ID is supported
  in auth_token middleware for both keystone v2 and v3 API.
  The corresponding change was done in keystoneclient before Grizzly
  release, so we don't need to take care of X_TENANT_ID now.

USE X_ROLES instead of X_ROLE:
  X_ROLES is now recommended.
  X_ROLE exists just for diablo backward compatibility.

Remove X_TENANT and X_USER:
  X_TENANT and X_USER are for diablo backward compatibility.
  We no longer need take care of them in Icehouse Neutron.

Change-Id: Ie714b1323ff8c44dbee66b54e683226cf675b104
Closes-Bug: #1242447"
1002,83ab44f6ffb8533dd53d5eb385f99f62d23a6056,1409046203,,1.0,77,23,6,5,1,0.67648974,False,,,,,True,,,,,,,,,,,,,,,,,1233,1692,1361517,nova,83ab44f6ffb8533dd53d5eb385f99f62d23a6056,1,1,,"Bug #1361517 in OpenStack Compute (nova): ""Nova prompt wrong message when boot from a error status volume""","1. create a cinder from an existed image
cinder create 2 --display-name hbvolume-newone --image-id 9769cbfe-2d1a-4f60-9806-16810c666d7f
2. set the created volume with error status
cinder reset-state --state error 76f5e521-d45f-4675-851e-48f8e3a3f039
3. boot a vm from the created volume
nova boot --flavor 2 --block-device-mapping vda=76f5e521-d45f-4675-851e-48f8e3a3f039:::0 device-mapping-test2 --nic net-id=231eb787-e5bf-4e65-a822-25d37a84eab8
# cinder list
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |   Display Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
| 21c50923-7341-49ba-af48-f4a7e2099bfd | available |       None      |  1   |     None    |  false   |             |
| 76f5e521-d45f-4675-851e-48f8e3a3f039 |   error   |    hbvolume-2   |  2   |     None    |   true   |             |
| 92de3c7f-9c56-447a-b06a-a5c3bdfca683 | available | hbvolume-newone |  2   |     None    |   true   |             |
+--------------------------------------+-----------+-----------------+------+-------------+----------+-------------+
#RESULTS
it reports ""failed to get the volume""
ERROR (BadRequest): Block Device Mapping is Invalid: failed to get volume 76f5e521-d45f-4675-851e-48f8e3a3f039. (HTTP 400)
#Expected Message:
report the status of the volume is not correct to boot a VM","Fix incorrect exception when bdm with error state volume

When boot from an error state volume, compute api will raise
InvalidBDMVolume exception which is incorrect. It should raise
InvalidVolume exception with appropriate prompt message.

Note that the v2 API extension is not updated to handle the
InvalidVolume exception since the API already handles the
base exception Invalid. A unit test is added to verify the
coverage.

Change-Id: I9dbc3f68e68b6d452077bc47717ad0fafd5640ac
Closes-Bug: #1361517
Co-authored-by: Matt Riedemann <mriedem@us.ibm.com>"
1003,83b37aeb5da8af6e305098a7e698d058f431f332,1405655603,1.0,1.0,14,1,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,1077,1527,1343778,nova,83b37aeb5da8af6e305098a7e698d058f431f332,1,1,“API missing decorator expected_errors.”,"Bug #1343778 in OpenStack Compute (nova): ""V3 servers core API missing decorator expected_errors""","Most of v3 servers core API missing decorator expected_errors.
All the v3 api should be under the protection of expected_errors","Add decorator expected_errors to V3 servers core

All v3 api should use expected_errors to prevent unexpected exception
raised. This patch add decorator expected_errors for v3 servers core.

Change-Id: Ia1fa09d6cf68a65c85ac5fd2aff9b9202d76e7ca
Closes-Bug: #1343778"
1004,840599bc2d496874e5e15639afa643fed1bbe3bb,1377572579,,1.0,37,26,4,3,1,0.721667016,True,4.0,3592206.0,32.0,11.0,False,15.0,1049170.0,23.0,4.0,322.0,2097.0,2253.0,321.0,1912.0,2067.0,319.0,1984.0,2138.0,0.054832077,0.340130226,0.366518163,1487,1216929,1216929,nova,840599bc2d496874e5e15639afa643fed1bbe3bb,1,1, ,"Bug #1216929 in OpenStack Compute (nova): ""when ephemeral_gb and swap is default value 0, the response is wrong in v3 flavor api""","when I port flaovr tempest test in nova v3. I find when create flavor with default value of ephemeral_gb and swap (both of them are 0). the response is """".  I think it's bug. I look into the code find that the issue is
swap"": flavor.get(""swap"") or """",
""ephemeral"": flavor.get(""ephemeral_gb"") or """",
it's a logic bug in nova/api/openstack/compute/views/flavors.py, I think.
the tempest log is the following:
2013-08-26 21:45:10.222 31704 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-26 21:45:10.222 31704 DEBUG tempest.common.rest_client [-] Request Body: {""flavor"": {""disk"": 10, ""vcpus"": 1, ""ram"": 512, ""name"": ""test_flavor_832042179"", ""id"": 1663317675}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-26 21:45:10.628 31704 INFO tempest.common.rest_client [-] Response Status: 200
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Mon, 26 Aug 2013 13:45:10 GMT', 'content-length': '369', 'content-type': 'application/json', 'x-compute-request-id': 'req-7ba21477-fda1-41f0-a782-3d58b05f293a'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-26 21:45:10.628 31704 DEBUG tempest.common.rest_client [-] Response Body: {""flavor"": {""name"": ""test_flavor_832042179"", ""links"": [{""href"": ""http://192.168.1.101:8774/v3/flavors/1663317675"", ""rel"": ""self""}, {""href"": ""http://192.168.1.101:8774/flavors/1663317675"", ""rel"": ""bookmark""}], ""ram"": 512, ""ephemeral"": """", ""disabled"": false, ""vcpus"": 1, ""swap"": """", ""os-flavor-access:is_public"": true, ""rxtx_factor"": 1.0, ""disk"": 10, ""id"": ""1663317675""}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314","Fix V3 API flavor returning empty string for attributes

Fixes cases where in the V3 API the ephemeral, swap and vcpus
attributes for flavors were returned as the empty string
when they had a value of 0. Also removes default setting for
the disabled attribute as it is always set in the db.

Fixes up some associated testcase data as it was using data which
could not exist in the db.

Change-Id: I90f48956411f792037b1cd744c4fa8d287f9e752
Closes-Bug: #1216929"
1005,842b2abfe76dede55b3b61ebaad5a90c356c5ace,1385388025,0.0,1.0,13,165,5,2,1,0.731385073,True,4.0,7327163.0,40.0,23.0,False,151.0,8495.2,441.0,1.0,303.0,605.0,889.0,263.0,395.0,645.0,296.0,438.0,717.0,0.04444777,0.065698893,0.107452858,74,471,1254727,nova,842b2abfe76dede55b3b61ebaad5a90c356c5ace,0,0,We are able to increase the min required libvirt to 0.9.11,"Bug #1254727 in OpenStack Compute (nova): ""Increase min required libvirt to 0.9.11 to allow use of libvirt python on PyPI""","Based on the data in this wiki
https://wiki.openstack.org/wiki/LibvirtDistroSupportMatrix
and discussions on the dev & operator mailing lists
  http://lists.openstack.org/pipermail/openstack-dev/2013-November/019767.html
  http://lists.openstack.org/pipermail/openstack-operators/2013-November/003748.html
We are able to increase the min required libvirt to 0.9.11
This will allow us to switch to using the (soon to be released) standalone libvirt python binding from PyPI, as well as removing some old compat code.","Increase min required libvirt to 0.9.11

Increase the min required libvirt version to 0.9.11 since
we require that for libvirt-python from PyPI to build
successfully. Kill off the legacy CPU model configuration
and legacy OpenVSwitch setup code paths only required by
libvirt < 0.9.11

Closes-Bug: #1254727
DocImpact
Change-Id: Ibe8d2117e1246e4097d1bedeadcd6d99618f8400"
1006,84368554d8a13a421297298de40693956af25fcd,1394217684,,1.0,17,2,2,2,1,0.74248757,True,1.0,103255.0,21.0,3.0,False,1.0,877.0,7.0,3.0,39.0,1242.0,1245.0,39.0,1035.0,1038.0,31.0,667.0,670.0,0.034519957,0.720604099,0.723840345,557,978,1289134,neutron,84368554d8a13a421297298de40693956af25fcd,1,1,bad formating error,"Bug #1289134 in neutron: ""BigSwitch: Cfg.error call formatted incorrectly""","In an exception handling case a config error is raised; however, the call is currently formatted incorrectly. It passes the details as a second parameter rather than performing the string formatting in place.
It also refers to strerror, which may not be present on all exception types that could be raised during the certificate retrieval.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L358","BigSwitch: Fix cfg.Error format in exception

Fixes an incorrectly formatted call to cfg.Error
in an exception handling routine in the Big Switch
server manager module.

Closes-Bug: #1289134
Change-Id: I9a1137d7395412ff6f061aacbe85f9b26269a1da"
1007,844ae433c17ee3956a8c83ed693ca44c8e3c38de,1391624113,1.0,1.0,1,1,1,1,1,0.0,True,2.0,4946082.0,21.0,8.0,False,10.0,136589.0,13.0,4.0,18.0,1208.0,1209.0,18.0,1039.0,1040.0,18.0,596.0,597.0,0.025032938,0.786561265,0.787878788,366,778,1276409,neutron,844ae433c17ee3956a8c83ed693ca44c8e3c38de,1,1,typo in a message ,"Bug #1276409 in neutron: ""Fix error message typo in PLUMgrid Plugin""","PLUMgrid Plugin needs to fix error message typo in ""_network_admin_state"" function where
""Network Admin State Validation Falied"" needs to have typo fixed.","Fix error message typo

 * Fix error message typo in ""_network_admin_state""
   function where ""Network Admin State Validation Falied""
   should be changed to ""Network Admin State Validation Failed""

Change-Id: I767e93c300250b4422e3980f799862ceb976c951
Closes-Bug: #1276409
Signed-off-by: Fawad Khaliq <fawad@plumgrid.com>"
1008,844df860c38ac38550b8d1739fd53131cd7fd864,1389926112,,1.0,6,4,2,2,1,0.721928095,True,2.0,3908890.0,53.0,14.0,False,148.0,104649.0,450.0,10.0,53.0,4647.0,4672.0,49.0,3476.0,3499.0,53.0,3596.0,3621.0,0.00761851,0.507477427,0.511004515,299,707,1270008,nova,844df860c38ac38550b8d1739fd53131cd7fd864,1,1,Add a exception to handle a bug,"Bug #1270008 in OpenStack Compute (nova): ""periodic tasks will be invalid if a qemu process becomes to defunct status""","I am using stable havana nova.
I got this exception while I delete my kvm instance, but the qemu process of this instance become to 'defunct' status by some unknown reason(may be a qemu/kvm bug), and then the periodic task stopped unexpectly everytime, then the resources of this compute node will never be reported, because of this exception below, I think we should handle this exception while running periodic task.
2014-01-16 15:53:28.421 47954 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     task(self, context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 5617, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 281, in update_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     resources = self.driver.get_available_resource(self.nodename)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4275, in get_available_resource
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     stats = self.host_state.get_host_stats(refresh=True)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5350, in get_host_stats
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     self.update_status()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 5386, in update_status
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     data[""vcpus_used""] = self.driver.get_vcpu_used()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 3949, in get_vcpu_used
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     vcpus = dom.vcpus()
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rv = execute(f,*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     rv = meth(*args,**kwargs)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 2222, in vcpus
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task     if ret == -1: raise libvirtError ('virDomainGetVcpus() failed', dom=self)
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task libvirtError: cannot get CPU affinity of process 62279: No such process
2014-01-16 15:53:28.421 47954 TRACE nova.openstack.common.periodic_task
and the exception while I delete this instance:
2014-01-16 15:13:26.640 47954 ERROR nova.openstack.common.rpc.amqp [req-03ed9463-0740-4423-bf1e-2334ed29ee5c 9537af4d80e546409b670673f9a81388 3179fc9d69d747b4a06f27a6d2334050] Exception during message handling
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     **args)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 400, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     payload)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 290, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     pass
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 276, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 341, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 318, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 305, in decorated_function
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2128, in terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     do_terminate_instance(instance, bdms, clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2120, in do_terminate_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     reservations=reservations)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/hooks.py"", line 105, in inner
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = f(*args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2091, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     user_id=user_id)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2063, in _delete_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1984, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     requested_networks)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1974, in _shutdown_instance
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     context=context, clean_shutdown=clean_shutdown)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 883, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     self._destroy(instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 838, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     instance=instance)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 810, in _destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     virt_dom.destroy()
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 179, in doit
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 139, in proxy_call
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = execute(f,*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77, in tworker
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     rv = meth(*args,**kwargs)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 760, in destroy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp     if ret == -1: raise libvirtError ('virDomainDestroy() failed', dom=self)
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp libvirtError: Failed to terminate process 61945 with SIGKILL: Device or resource busy
2014-01-16 15:13:26.640 47954 TRACE nova.openstack.common.rpc.amqp","libvirt: handle exception while get vcpu info

If an exception is raised while get a libvirt domain's vcpu info,
the update_available_resource periodic task will be failed, which
will result in the resource of this compute node will never be
reported.

This patch add an exception handling to avoid this situation.
Closes-bug: #1270008

Change-Id: I69109402416989fdaa421f8dbc72953bd067c407"
1009,8479c212b79e61aa9b694ec1368a1073898a09d5,1404381074,,1.0,20,2,2,2,1,0.845350937,False,,,,,True,,,,,,,,,,,,,,,,,1024,1467,1335959,neutron,8479c212b79e61aa9b694ec1368a1073898a09d5,1,0,"""This most likely is due to a recent commit: https://bugs.launchpad.net/neutron/+bug/1316190""","Bug #1335959 in neutron: ""Neutron dhcp agent variable 'mode' referenced before an assignment""",UnboundLocalError: local variable 'mode' referenced before assignment. This most likely is due to a recent commit: https://bugs.launchpad.net/neutron/+bug/1316190,"Omit mode keyword when spawning dnsmasq with some ipv6 subnets

Fix UnboundLocalError for certain cases of ipv6 subnets

Closes-Bug: #1335959
Change-Id: I5e4b94d9ba1b13b72e2300b53d5638968e0d3401"
1010,8496dad34797104482a5a76d809b72569adcd794,1391490335,,1.0,0,3,1,1,1,0.0,True,2.0,211575.0,20.0,5.0,False,31.0,481245.0,102.0,1.0,169.0,205.0,347.0,169.0,198.0,340.0,158.0,152.0,284.0,0.139351446,0.134092901,0.249780894,361,772,1276011,glance,8496dad34797104482a5a76d809b72569adcd794,0,0,Remove duplicate code,"Bug #1276011 in Glance: ""Duplicate type defination in v2 images schema""","For example, see https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L664 and https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L666","Remove duplicate type defination of v2 images schema

Closes-Bug: #1276011

Change-Id: I53565fcaaf9fde064fb28c63bf00a4a7a0f602eb"
1011,84a797ec8c801057fef7a3c30ab74426e4c3fbdc,1382990983,,1.0,37,37,6,5,1,0.661746519,True,17.0,6222201.0,105.0,39.0,False,207.0,601769.1667,1209.0,1.0,13.0,910.0,922.0,13.0,886.0,898.0,13.0,891.0,903.0,0.00216819,0.138144649,0.140003097,1558,1225659,1225659,nova,84a797ec8c801057fef7a3c30ab74426e4c3fbdc,1,1, ,"Bug #1225659 in OpenStack Compute (nova): ""Wrong handling of Instance expected_task_state""","When Instance.save called the expected_task_state parameter passed to it in order to specify what is instance expected task state.
There many cases when we would expect that there no task state for the vm (task_state=None in db). The expected_task_state=None is overwritten by the default substitution in the save() arguments and due to this not passed correctly to the db.instance_update_and_get_original method call.","Wrong handling of Instance expected_task_state

When Instance.save called the expected_task_state parameter can be
passed to it in order to specify what is the expected task state.
There many cases when we would expect that there no task state
for the vm (task_state=None in db). Currently the
expected_task_state=None is overwritten by the default substitution in
the save() arguments and due to this not passed correctly to the
db.instance_update_and_get_original method call.

Closes-Bug: #1225659

Change-Id: I84fd2f7e2c50c5f34c84f713d6afc040524a8078"
1012,84dfaa8a87cce660aa20619bd93263e645bbb2d0,1401284236,,1.0,19,3,3,3,1,0.905618518,False,,,,,True,,,,,,,,,,,,,,,,,930,1366,1324120,neutron,84dfaa8a87cce660aa20619bd93263e645bbb2d0,1,1,“Problem is the line https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L1012. A flat network will return an object instead of 0”,"Bug #1324120 in neutron: ""NSX: exception when creating a flat network""","2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session   File ""/usr/lib/python2.7/dist-packages/MySQLdb/converters.py"", line 97, in Instance2Str
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session     return d[o.__class__](o, d)
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session RuntimeError: maximum recursion depth exceeded
2014-05-28 05:38:02.696 TRACE neutron.openstack.common.db.sqlalchemy.session
Problem is the line https://github.com/openstack/neutron/blob/master/neutron/plugins/vmware/plugins/base.py#L1012. A flat network will return an object instead of 0","NSX: fix bug for flat provider network

The flat provider network would cause an exception when writing to
database. This is due to the fact that the DB expected an integer
and received an object instead.

Change-Id: Ib9d38711c0c2ef16d8bf74bfae44864a1bc272b1
Closes-bug: #1324120"
1013,85068cc9f68aa704895ffe5ac185bb9cf7d05e2d,1389131022,,1.0,67,20,3,2,1,0.442251206,True,33.0,1619666.0,144.0,83.0,False,145.0,13089.0,442.0,5.0,142.0,1551.0,1551.0,141.0,1338.0,1338.0,133.0,1474.0,1474.0,0.019186712,0.211197022,0.211197022,264,668,1266711,nova,85068cc9f68aa704895ffe5ac185bb9cf7d05e2d,1,1,,"Bug #1266711 in OpenStack Compute (nova): ""AttributeError: virConnect instance has no attribute 'registerCloseCallback'""","During Jenkins tests I got this error two times with a different patchset
ft1.8205: nova.tests.virt.libvirt.test_libvirt.LibvirtNonblockingTestCase.test_connection_to_primitive_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{WARNING [nova.virt.libvirt.driver] URI test:///default does not support events: internal error: could not initialize domain event timer}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_libvirt.py"", line 7570, in test_connection_to_primitive
    jsonutils.to_primitive(connection._conn, convert_instances=True)
  File ""nova/virt/libvirt/driver.py"", line 678, in _get_connection
    wrapped_conn = self._get_new_connection()
  File ""nova/virt/libvirt/driver.py"", line 664, in _get_new_connection
    wrapped_conn.registerCloseCallback(
  File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/eventlet/tpool.py"", line 172, in __getattr__
    f = getattr(self._obj,attr_name)
AttributeError: virConnect instance has no attribute 'registerCloseCallback'","Fix bug with not implemented virConnect.registerCloseCallback

The unit test test_connection_to_primitive fails many times due to
""virConnect instance has no attribute 'registerCloseCallback'"".

registerCloseCallback has been added since original(community's)
python-libvirt v1.0.0, and v1.2.0 also contains the method.
However ubuntu cloud's v1.0.1 does not contain the method, and current
version check of python-libvirt does not work.
This patch tries to operate the method and catch TypeError exception if
the method does not exist instead of the version check.

Co-Authored-By: Sahid Orentino Ferdjaoui <sahid.ferdjaoui@cloudwatt.com>
Closes-Bug: #1266711
Change-Id: I4ec9ff9a684639ae5b146f400c90115c83afcda7"
1014,85239cc81440d9e5a4aee3c0961c96a4197ad939,1393272652,2.0,1.0,166,199,14,5,2,0.781693147,True,6.0,118183.0,33.0,12.0,False,2.0,131980.8,2.0,3.0,208.0,1546.0,1567.0,200.0,1315.0,1336.0,198.0,1385.0,1401.0,0.136021873,0.947368421,0.958304853,471,886,1284338,cinder,85239cc81440d9e5a4aee3c0961c96a4197ad939,0,0,refactoring code,"Bug #1284338 in Cinder: ""fibre channel zone manager config options aren't in a group""","The new Fibre Channel Zone Manager settings in the cinder.conf aren't in their own group.  Also the zone fabric settings don't use a group.
The fabric settings use a dynamic name to group the values together instead of using the config groups
For example,
  the current fc fabric config options looked like
fc_fabric_address_BRCD_NAME1=some address
fc_fabric_user_BRCD_NAME1=some user
...
It should be
[BRCD_NAME1]
fc_fabric_address=some address
fc_fabric_user=some user","Update FibreChannel Zone Manager config

This patch changes the way the config options are
layed out in the cinder.conf to follow the
standards for grouping settings together.

Instead of using dynamic config names to group
settings together by fabric name, we look for
fabric groups based upon the fc_fabric_names.
We also create a new fc-zone-manager group
for all of the global fczm settings.

Change-Id: I91c8a05adec2fb12f664ca0df6007465c1a460e0
Closes-Bug: #1284338
DocImpact"
1015,85332012dede96fa6729026c2a90594ea0502ac5,1384280742,,1.0,40,5,2,2,1,0.836640742,True,16.0,11662695.0,89.0,30.0,False,20.0,1531111.5,25.0,9.0,529.0,2518.0,2621.0,521.0,2326.0,2429.0,474.0,1861.0,1954.0,0.072254335,0.283236994,0.297383632,1752,1250580,1250580,nova,85332012dede96fa6729026c2a90594ea0502ac5,0,0,"Refactoring, “will enable reuse of the already set up client”","Bug #1250580 in OpenStack Compute (nova): ""Excessive calls to keystone from neutron glue code""","This has been noticed before, it just came up again with comments in https://bugs.launchpad.net/neutron/+bug/1250168
[12:23] <dims> salv-orlando, for 1250168, where are the keystone calls being made from? (you indicated a large percent of api calls are keystone in the last comment)
[12:35] <salv-mobile> It looks like all calls in admin context trigger a POST to keystone as the admon token is not cachex
[12:35] <salv-mobile> Cached
[12:36] <salv-mobile> I think this was intentional even if I know do not recall the precise reason
[12:40] <dims> salv-mobile, which file? in python-neutronclient?
[12:41] <salv-mobile> I am afk i think it's either nova.network.neutronv2.api or nova.network.neutronv2
[12:41] <dims> salv-mobile, thx, will take a peek
[12:42] <salv-mobile> The latter would be __init_.py of course","Cache Neutron Client for Admin Scenarios

Store a thread local copy of Neutron client for use with admin
tokens, this will prevent excessive calls to keystone to mint
new tokens and will enable reuse of the already set up client
for subsequent calls in this thread.

Change-Id: I6346281ca3516c31b195ce71942786d724e5e9df
Closes-Bug: #1250580"
1016,853b5c9b24a169ec609253637adf093862db35b6,1404210879,,1.0,186,7,4,4,1,0.878808323,False,,,,,True,,,,,,,,,,,,,,,,,683,1109,1297362,glance,853b5c9b24a169ec609253637adf093862db35b6,1,1,wrong behaviour in http codes,"Bug #1297362 in Glance: ""Glance v2: HTTP 404s are returned for unallowed methods""","Requests for many resources in Glance v2 will return a 404 if the request is using an unsupported HTTP verb for that resource. For example, the /v2/images resource does exist but a 404 is returned when attempting a DELETE on that resource. Instead, this should return an HTTP 405 MethodNotAllowed response.","Changes HTTP response code for unsupported methods

Requests for resources with an unsupported HTTP method now return a HTTP
response 405 (Method Not Allowed) or 501 (Not Implemented) rather than a
404 (Not Found) for everything.

For example, attempting to DELETE on /v2/images will now return a 405
instead of a 404 and will provide a response header 'Allow' that lists
the valid methods for the resource.

Attempting to use NON_EXISTENT_METHOD on /v2/images will now return a
501.

Attempting to GET on /v2/non_existent_resource will, as expected, return
a 404.

Fixed for v1 and v2.

Change-Id: I5406f8ee423d3d5e66c56a26a7009b4f438a7e0f
Closes-Bug: #1297362"
1017,854a5ac0bec932130eac0ac1ca1e679cc6487c93,1392664230,,1.0,1,1,1,1,1,0.0,True,3.0,529757.0,71.0,16.0,False,2.0,11147038.0,2.0,6.0,0.0,920.0,920.0,0.0,818.0,818.0,0.0,507.0,507.0,0.00122399,0.621787026,0.621787026,418,833,1280597,neutron,854a5ac0bec932130eac0ac1ca1e679cc6487c93,0,0,typo in commentary,"Bug #1280597 in neutron: ""Fix typo in service_drivers.ipsec""","neutron.services.vpn.service_drivers.ipsec has typo in its comment.
""""""Retuns the vpnservices on the host.""""""  - should be  """"""Returns the vpnservices on the host.""""""","Fix typo in service_drivers.ipsec

Closes-Bug: #1280597

Change-Id: If2238d66af4db1b0134bbd6407b2229b4af363ee"
1018,85d0ace169be513c30b09e13a35fa7d912f5b380,1384939358,0.0,1.0,32,1,2,2,1,0.439496987,True,2.0,2357758.0,30.0,16.0,False,187.0,15550.0,1021.0,7.0,314.0,3337.0,3631.0,291.0,2864.0,3139.0,5.0,2378.0,2381.0,0.000904432,0.358607175,0.359059391,1728,1245719,1245719,nova,85d0ace169be513c30b09e13a35fa7d912f5b380,1,1, ,"Bug #1245719 in OpenStack Compute (nova): ""RBD backed instance can't shutdown and restart""","Version: Havana w/ Ubuntu Repos. with Ceph for RBD.
When creating Launching a instance with ""Boot from image (Creates a new volume)"" this creates the instance fine and all is well however if you shutdown the instance I can't turn it back on again.
I get the following error in the nova-compute.log when trying to power on an shutdown instance.
#######################################################################################
2013-10-29 00:48:33.859 2746 WARNING nova.compute.utils [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] [instance: cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09] Can't access image $
2013-10-29 00:48:34.040 2746 WARNING nova.virt.libvirt.vif [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Deprecated: The LibvirtHybridOVSBridgeDriver VIF driver is now de$
2013-10-29 00:48:34.578 2746 ERROR nova.openstack.common.rpc.amqp [req-89bbd72f-2280-4fac-802a-1211ec774980 27106b78ceac4e389558566857a7875f 464099f86eb94d049ed1f7b0f0144275] Exception during message handling
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1832, in start_instance
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     self._power_on(context, instance)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1819, in _power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1948, in power_on
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     self._hard_reboot(context, instance, network_info, block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1903, in _hard_reboot
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 4318, in get_instance_disk_info
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     dk_size = int(os.path.getsize(path))
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/genericpath.py"", line 49, in getsize
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp     return os.stat(filename).st_size
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp OSError: [Errno 2] No such file or directory: '/var/lib/nova/instances/cc370f6d-4be0-4cd3-9f20-bf86f5ad7c09/disk'
2013-10-29 00:48:34.578 2746 TRACE nova.openstack.common.rpc.amqp
#######################################################################################
On Closer inspection It seems the libvirt.xml file for the instance gets all screwed up.
This is the libvirt.xml file for this instance before shutdown.
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""network"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source protocol=""rbd"" name=""volumes/volume-5abadeb8-49e9-4628-a54d-d742d6f2e012"">
        <host name=""10.100.96.10"" port=""6789""/>
        <host name=""10.100.96.11"" port=""6789""/>
        <host name=""10.100.96.12"" port=""6789""/>
      </source>
      <auth username=""volumes"">
        <secret type=""ceph"" uuid=""13a673af-ff80-3036-8310-4c72f566673d""/>
      </auth>
      <target bus=""virtio"" dev=""vda""/>
      <serial>5abadeb8-49e9-4628-a54d-d742d6f2e012</serial>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################
All looks fine here i can see the ceph mons etc.
Next i'll shutdown the instance and as expected the file stays the same now this is the file if i try to power the instance back on again.
you'll notice all of the details regarding the disk have changed and now it thinks its a qcow2 disk located on the local hard drive... how does this happen?
#######################################################################################
<domain type=""kvm"">
  <uuid>dc9749cd-0002-41c9-ac55-5f691637146a</uuid>
  <name>instance-00000004</name>
  <memory>524288</memory>
  <vcpu>1</vcpu>
  <sysinfo type=""smbios"">
    <system>
      <entry name=""manufacturer"">OpenStack Foundation</entry>
      <entry name=""product"">OpenStack Nova</entry>
      <entry name=""version"">2013.2</entry>
      <entry name=""serial"">4c4c4544-0053-3210-8032-b6c04f5a5931</entry>
      <entry name=""uuid"">dc9749cd-0002-41c9-ac55-5f691637146a</entry>
    </system>
  </sysinfo>
  <os>
    <type>hvm</type>
    <boot dev=""hd""/>
    <smbios mode=""sysinfo""/>
  </os>
  <features>
    <acpi/>
    <apic/>
  </features>
  <clock offset=""utc"">
    <timer name=""pit"" tickpolicy=""delay""/>
    <timer name=""rtc"" tickpolicy=""catchup""/>
  </clock>
  <cpu mode=""host-model"" match=""exact""/>
  <devices>
    <disk type=""file"" device=""disk"">
      <driver name=""qemu"" type=""qcow2"" cache=""none""/>
      <source file=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <interface type=""bridge"">
      <mac address=""fa:16:3e:45:17:d4""/>
      <model type=""virtio""/>
      <source bridge=""qbr125aa659-01""/>
      <target dev=""tap125aa659-01""/>
    </interface>
    <serial type=""file"">
      <source path=""/var/lib/nova/instances/dc9749cd-0002-41c9-ac55-5f691637146a/console.log""/>
    </serial>
    <serial type=""pty""/>
    <input type=""tablet"" bus=""usb""/>
    <graphics type=""vnc"" autoport=""yes"" keymap=""en-us"" listen=""10.100.32.10""/>
  </devices>
</domain>
#######################################################################################","Include image block device maps in info

In cases for example soft/hard reboot, we were just getting volume and
snapshot block device maps. For block devices with a source type image,
Nova would fall back on a local disk location and fail.

This change allows us to include block devices with a source of an
image, so that we correctly get the right the location of the block
device.

Closes-Bug: #1245719
Change-Id: I1f726293e85183d4b84c05b635a7c606a092992f"
1019,85ddbde058d8bda0b938eb7a45ef73519a831b3b,1392404357,0.0,1.0,4,1,2,1,1,0.970950594,True,1.0,1641107.0,63.0,5.0,False,4.0,163169.0,6.0,4.0,264.0,1722.0,1722.0,244.0,1440.0,1440.0,197.0,691.0,691.0,0.24535316,0.857496902,0.857496902,270,674,1267101,neutron,85ddbde058d8bda0b938eb7a45ef73519a831b3b,1,1, request timeout error,"Bug #1267101 in neutron: ""nicira: Lock wait timeout exceeded on port operations""","Stacktrace here:
http://paste.openstack.org/show/60785/
Occurence here:
http://162.209.83.206/logs/58017/16/logs/screen-q-svc.txt.gz","Fix request timeout errors during calls to NSX controller

Sometimes two correlated exception traces are observed in
the server log for the Neutron Server backed by NSX:
RequestTimeout (The nsx request has timed out) and
OperationalError (Lock wait timeout exceeded). This is
generally described by Guru Salvatore Orlando as the,
and I quote, the ""infamous eventlet-mysql deadlock"".

This patch tries to address the issue by adding a
cooperative yield in the nsx client code (it’s a good idea
to call sleep(0) occasionally in any case) and also by
avoiding the unnecessary spawning of another Greenthread
within a call that is already executed in Greenthred
itself.

Closes-bug: #1267101
Related-bug: #1279497

Change-Id: I8e298468fb730f11a66fbd4211121ee7d3e2a548"
1020,861d67120227f7a52ff3f9ef7225b84a9ac26647,1390254200,,1.0,2,1,1,1,1,0.0,True,4.0,5578126.0,65.0,23.0,False,9.0,5471948.0,13.0,5.0,25.0,3196.0,3201.0,25.0,2627.0,2632.0,21.0,3024.0,3025.0,0.003089454,0.424799888,0.424940317,308,717,1270811,nova,861d67120227f7a52ff3f9ef7225b84a9ac26647,0,0,tests,"Bug #1270811 in OpenStack Compute (nova): ""Wrong option is used in test_availability.py""","I found a testcase bug in nova/tests/api/openstack/compute/plugins/v3/test_availability_zone.py
     def test_create_instance_with_availability_zone(self):
         def create(*args, **kwargs):
             self.assertIn('availability_zone', kwargs)
             return old_create(*args, **kwargs)
         old_create = compute_api.API.create
         self.stubs.Set(compute_api.API, 'create', create)
         image_href = '76fa36fc-c930-4bf3-8c8a-ea2a2420deb6'
         flavor_ref = 'http://localhost/v3/flavors/3'
         body = {
             'server': {
                 'name': 'config_drive_test',
                 'image_ref': image_href,
                 'flavor_ref': flavor_ref,
                 'metadata': {
                     'hello': 'world',
                     'open': 'stack',
                 },
               ★  'availability_zone': ""nova"",★
             },
         }
         req = fakes.HTTPRequestV3.blank('/v3/servers')
         req.method = 'POST'
         req.body = jsonutils.dumps(body)
         req.headers[""content-type""] = ""application/json""
         admin_context = context.get_admin_context()
         service1 = db.service_create(admin_context, {'host': 'host1_zones',
                                          'binary': ""nova-compute"",
                                          'topic': 'compute',
                                          'report_count': 0})
         agg = db.aggregate_create(admin_context,
                 {'name': 'agg1'}, {'availability_zone': 'nova'})★
         db.aggregate_host_add(admin_context, agg['id'], 'host1_zones')
The correct request for availability-zone option is 'os-availability-zone:availability_zone'.","Fix availability-zone option miss when creates an instance

In test_availability_zone.py the availability-zone option is
used as 'availability_zone' which should be modified to
'os-availability-zone:availability_zone'.

Change-Id: I765d07b699ecb425dadccb22af834b11f52ff5b0
Closes-Bug: #1270811"
1021,863ae9f484089aa5d9e576255b39288da82c1100,1379105804,,1.0,45,14,3,3,1,0.651097601,True,4.0,997784.0,26.0,10.0,False,73.0,246817.0,185.0,3.0,127.0,1086.0,1100.0,126.0,988.0,1001.0,114.0,942.0,949.0,0.115461847,0.946787149,0.953815261,1557,1225194,1225194,cinder,863ae9f484089aa5d9e576255b39288da82c1100,1,1,Wrong logic,"Bug #1225194 in Cinder: ""LVM clear_volume should raise exc. if volume_clear opt is invalid""","Currently if a user configures volume_clear='non_existent_volume_clearer' in cinder.conf, the LVM driver will silently delete a volume and not wipe it.
Instead, the delete operation should fail, leaving the volume in the 'error_deleting' state.  This prevents a user from believing their volumes are being wiped when they are not due to misconfiguration.","LVM volume_clear: error on unexpected inputs

Currently if a user configures
volume_clear='non_existent_volume_clearer' in cinder.conf,
the LVM driver will silently delete a volume and not wipe it.

Instead, the delete operation should fail, leaving the volume
in the 'error_deleting' state.

Also fail if the volume reference does not contain either a
'size' or 'volume_size' field.

Closes-Bug: #1225194
Change-Id: I78fec32d7d5aeaa8e2deeac43066ca5e2e26d9ca"
1022,86591a24d48fe19be427b5a8981821fcdb85bb37,1407502251,,1.0,405,32,4,2,1,0.912864408,False,,,,,True,,,,,,,,,,,,,,,,,357,768,1275682,cinder,86591a24d48fe19be427b5a8981821fcdb85bb37,0,0, volume retype to be supported,"Bug #1275682 in Cinder: ""vmware: volume retype to be supported for vmdk driver""","https://blueprints.launchpad.net/cinder/+spec/volume-retype
Volume retype is a new driver API that is introduced in IceHouse release. This allows the user to manually change the volume type associated with a volume.
The vmdk driver uses a volume-type to associate vSphere storage profile with the volume. The vmdk driver needs to handle a volume type change.","VMware: Implement retype for VMDK driver

This patch adds support for retype operation in the VMDK driver.

The VMDK driver uses the extra spec keys 'vmware:vmdk_type' and
'vmware:storage_profile' to determine the disk provisioning type
and storage policy of the backing VM. Currently it is not possible
to change the disk provisioning type or storage policy after volume
creation. With retype support, user can change the disk provisioning
type and storage policy by changing the volume's type to another type
with extra spec keys set to desired values.

Closes-Bug: #1275682
Partial-Bug: #1288254

Change-Id: If4b868af7b91373b2b36fd5cd2f0dda12d604d99"
1023,867b131e09a43ba7ed36267dcdec751abd652f14,1378766820,1.0,1.0,52,3,2,2,1,0.892121281,True,4.0,604160.0,22.0,9.0,False,13.0,116922.0,28.0,3.0,125.0,509.0,570.0,124.0,506.0,566.0,112.0,480.0,529.0,0.115306122,0.490816327,0.540816327,1531,1222907,1222907,cinder,867b131e09a43ba7ed36267dcdec751abd652f14,1,1, ,"Bug #1222907 in Cinder: ""GlusterFS clone from snapshot may select wrong source file""","_copy_volume_from_snapshot appears to select its source file using _local_path_volume but needs to resolve source file by using the snap_info metadata.
Found this while filling in gaps in the unit test code, will add tests to cover this.","GlusterFS: Copy snap from correct source file

The GlusterFS driver's copy_volume_from_snapshot was previously
not parsing the snapshot information to determine which backing
file to copy from, instead copying from the active file at all
times.  This would result in the wrong data being supplied when
a volume is cloned from a snapshot.

Add a test for the copy_volume_from_snapshot method.

Closes-Bug: #1222907
Change-Id: Ib829ca1a6812b61845f3b2eb9c5507779fa6ec15"
1024,86a0adefe326116ae42c0e01a8fa2c2849bfa2c2,1406676898,,1.0,20,1,2,2,1,0.453716339,False,,,,,True,,,,,,,,,,,,,,,,,1119,1574,1350089,neutron,86a0adefe326116ae42c0e01a8fa2c2849bfa2c2,1,1,,"Bug #1350089 in neutron: ""Distributed Router Gateway Clear does not delete the CSNAT Interface Ports for that router.""","When Distributed Routers are created with Interface Ports and after a Gateway is set to the router,  the plugin will create the ""csnat"" interface ports with ""device_owner"" as ""router_csnat_interface"".
These ports should be deleted when the Gateway is cleared  or when the interfaces are removed from the  particular router.
In the current plugin code, these ""interface"" ports are not deleted when a Gateway is cleared. But the ports are deleted when the ""router interfaces are removed"".
This needs to be fixed.
Since we don't clean up the ports there may be an odd chance of having unused ports in the ""Service-node"".","Fix-DVR Gateway clear doesn't delete csnat port

When a gateway is set to a distributed router,
the router checks for the interfaces associated
with the router and based on the number of
interfaces the router creates ""csnat"" interface
ports that would be used by the SNAT service in
the Service Node.

When a gateway is cleared, the plugin should
delete the ""csnat"" interface ports. In the
current code, it is deleting the port and
re-creating the port with a different id.

A check need to be made before it creates a new
port to make sure that the router has a valid
gateway port.

This is a bug and this patch fixes this issue.

Change-Id: I84f1795360b3693a025b5fa3454bf9efc7e503ae
Closes-Bug: #1350089"
1025,86a4bd7d9b34057f04ebbe2d2131f033d33d082b,1407168136,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1143,1598,1352428,nova,86a4bd7d9b34057f04ebbe2d2131f033d33d082b,1,1,,"Bug #1352428 in OpenStack Compute (nova): ""HyperV ""Shutting Down"" state is not mapped""","The method which gets VM related information can fail if the VM is in an intermediary state such as ""Shutting down"".
The reason is that some of the Hyper-V specific vm states are not defined as possible states.
This will result into a key error as shown bellow:
http://paste.openstack.org/show/90015/","Fixes Hyper-V vm state issue

The method which gets VM related information can fail if the
vm is in an intermediary state such as ""Shutting down"".
The reason is that some of the Hyper-V specific vm states
are not defined as possible states.

This patch assures that a valid state is always returned by
this method.

Change-Id: I64e95f6b537e83a30dd64474f63f140e0e8e7373
Closes-Bug: #1352428"
1026,86d1007c2078a0ad80e49088b37660466b1a5cb3,1396882180,,1.0,23,1,4,4,1,0.895426041,True,2.0,1020863.0,14.0,6.0,False,35.0,2373086.0,47.0,4.0,52.0,1481.0,1497.0,50.0,1252.0,1266.0,44.0,1173.0,1181.0,0.005807201,0.15150342,0.152535811,752,1179,1303591,nova,86d1007c2078a0ad80e49088b37660466b1a5cb3,1,1,,"Bug #1303591 in OpenStack Compute (nova): ""InvalidAggregateAction exception is not handled""","When an aggregate with 'host' attribute not empty is deleted, InvalidAggregateAction exception will be raised, but this exception
is not handled.
$ nova --os-compute-api-version 3 aggregate-delete agg5
ERROR (ClientException): Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InvalidAggregateAction'> (HTTP 500) (Request-ID: req-9a8500ae-379a-4121-b217-7e7ea6188ad0)
2014-04-07 23:56:16.347 ERROR nova.api.openstack.extensions [req-f7c09203-a681-496c-a84e-18fb3d2e3659 admin demo] Unexpected exception in API method
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 472, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/aggregates.py"", line 155, in delete
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     self.api.delete_aggregate(context, id)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     payload)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     six.reraise(self.type_, self.value, self.tb)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     return f(self, context, *args, **kw)
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 3363, in delete_aggregate
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions     reason='not empty')
2014-04-07 23:56:16.347 TRACE nova.api.openstack.extensions InvalidAggregateAction: Cannot perform action 'delete' on aggregate 3. Reason: not empty.","Catch InvalidAggregateAction when deleting an aggregate

When an aggregate with 'host' attribute not empty is deleted,
InvalidAggregateAction exception will be raised, but this exception
is not handled.

Closes-Bug: #1303591
Change-Id: I4ef0c4a6008acea7e9769e93de79451cfbc0a0fa"
1027,86fc539a23b0b2cdd6e0803aa3a3cb10fc5758a6,1407486458,,1.0,12,7,2,2,1,0.949452015,False,,,,,True,,,,,,,,,,,,,,,,,1164,1621,1354272,cinder,86fc539a23b0b2cdd6e0803aa3a3cb10fc5758a6,1,1,Minor issues in comments,"Bug #1354272 in Cinder: ""EMC: Fix minor issue in VNX driver and unit tests""","There were some minor issues in initial commit of VNX Direct Driver
Juno Update (https://review.openstack.org/#/c/104413/), such as typo,
unclear config option help message and missing period.
This bug is to track these issues.","EMC: Fix minor issue in VNX driver and unit tests

There were some minor issues in initial commit of VNX Direct Driver
Juno Update (https://review.openstack.org/#/c/104413/), such as typo,
unclear config option help message and missing period.

This change fix these issues.

Change-Id: I82bac9695c8138cff858013b3affeb0557dfcc02
Closes-Bug: #1354272"
1028,872420efdb8e6e945cd2fe06994136b8c2ee153a,1319665344,0.0,,1630,32,14,11,3,0.692665725,True,5.0,517349.0,18.0,7.0,False,7.0,96714.92857,7.0,2.0,7.0,3.0,7.0,7.0,3.0,7.0,7.0,3.0,7.0,0.571428571,0.285714286,0.571428571,1845,1318333,1318333,Swift,872420efdb8e6e945cd2fe06994136b8c2ee153a,1,0,Depend when they decided the logging standards. But I think this is caused by the evolution,"Bug #1318333 in OpenStack Object Storage (swift): ""debug level logs should not be translated""","According to the OpenStack translation policy, available at https://wiki.openstack.org/wiki/LoggingStandards, debug messages should not be translated. Like mentioned in several changes in Nova by garyk this is to help prioritize log translation.","Expiring Objects Support

Please see the doc/source/overview_expiring_objects.rst for
more detail.

Change-Id: I4ab49e731248cf62ce10001016e0c819cc531738"
1029,8727745562d63149ad16f1ef07cbedcf506076da,1387734146,1.0,1.0,90,1,4,2,1,0.764103902,True,6.0,5423537.0,49.0,18.0,False,143.0,545886.0,458.0,3.0,1.0,1176.0,1177.0,0.0,1027.0,1028.0,1.0,1117.0,1118.0,0.000289101,0.161607401,0.161751951,202,605,1261978,nova,8727745562d63149ad16f1ef07cbedcf506076da,1,0,specific machine type ,"Bug #1261978 in OpenStack Compute (nova): ""Default machine type for libvirt does not work with ARM""","Today Nova does not pass a specific machine type in the libvirt XML configuration file, resulting in the use of a default machine type. In some cases the operator may want to use a non-default machine type. For example, with ARM by default the machine type is integratorcp, but users need to use the virt model or others such as vexpress-a15 with KVM","Support setting a machine type to enable ARMv7/AArch64 guests to boot

By default the libvirt driver does not specify a machine type. On ARM systems
that results in a default type which will fail. This change provides two ways
to have Nova specify a machine type in the libvirt configuration:

1) The image can be tagged with the property ""hw_machine_type=<type>""
2) If Nova detects an armv7 or aarch64 processor it will use vexpress-a15 or
   virt machine types respectively (this can be overridden with an image
   property)

Change-Id: I8aadfcf50e35a1370783ea795f80b29c1a71abff
Closes-Bug: #1261978
DocImpact"
1030,872d1982716acf426d21dcac838102e9479408f7,1386281095,0.0,1.0,75,15,3,2,1,0.872799089,True,1.0,76155.0,8.0,5.0,False,5.0,4524307.0,5.0,5.0,19.0,1157.0,1157.0,19.0,1044.0,1044.0,14.0,523.0,523.0,0.024630542,0.860426929,0.860426929,1433,1190615,1190615,neutron,872d1982716acf426d21dcac838102e9479408f7,0,0,Test files,"Bug #1190615 in neutron: ""Improve unit test coverage for Cisco plugin common code""","Improve unit test coverage for ...
quantum/plugins/cisco/common/cisco_constants	77	0	0	0	0	100%
quantum/plugins/cisco/common/cisco_credentials_v2	30	6	0	4	0	82%
quantum/plugins/cisco/common/cisco_exceptions	37	0	0	2	0	100%
quantum/plugins/cisco/common/cisco_faults	34	34	0	0	0	0%
quantum/plugins/cisco/common/cisco_utils	15	15	0	0	0	0%
quantum/plugins/cisco/common/config	23	2	0	8	1	84%","Improve unit test coverage for Cisco plugin common code

Closes-Bug: #1190615

This fix improves the unit test coverage for the Cisco Nexus plugin
common modules (directory neutron/plugins/cisco/common) from:
--- cisco_credentials_v2.py 82%
--- cisco_faults.py 0%
--- config.py 84%
To:
--- cisco_credentials_v2.py 100%
--- cisco_faults.py 76%
--- config.py 100%

Change-Id: Id1ccfee1317309039fab83f622ced58efc3b2ae7"
1031,8748a3ee68e60778c90b8b83181bc28a7e8fe9d1,1405957314,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1085,1535,1346372,neutron,8748a3ee68e60778c90b8b83181bc28a7e8fe9d1,1,1,,"Bug #1346372 in neutron: ""The default value of quota_firewall_rule should not be -1""","the default value of ""quota_firewall_rule"" is ""-1"", and this means unlimited. There will be potential security issue if openstack admin do not modify this default value.
A bad tenant User can create unlimited firewall rules to ""attack"" network node, in the backend, we will have a large number of iptables rules. This will make the network node crash or very slow.
So I suggest we use another number but not ""-1"" here.","The default value of quota_firewall_rule should not be -1

A bad tenant User can create unlimited firewall rules to
""attack"" the network node, so I modify the default value to 100.

Change-Id: I485c24cb1a7ed77dee81356fe6d95276808a47d4
Closes-Bug: #1346372"
1032,87710cc8a2654d872adb74d4fa4856c9beb41fbd,1393408113,,1.0,206,206,4,1,1,0.924988077,True,4.0,3053311.0,28.0,8.0,False,22.0,3172299.75,35.0,1.0,0.0,303.0,303.0,0.0,208.0,208.0,0.0,201.0,201.0,0.00154321,0.311728395,0.311728395,1827,1285040,1285040,Swift,87710cc8a2654d872adb74d4fa4856c9beb41fbd,0,0,"""assertEquals is deprecated"" ","Bug #1285040 in OpenStack Object Storage (swift): ""assertEquals is deprecated, need use assertEqual""","assertEquals is deprecated in Python 2.7 , need drop it
http://docs.python.org/2/library/unittest.html#deprecated-aliases","AssertEquals is deprecated, use assertEqual partI

assertEquals is deprecated in Python 2.7, need drop it.
http://docs.python.org/2/library/unittest.html#deprecated-aliases

Change-Id: Ieec505887f740b99b11ef40878ba3ee7247b78a8
Closes-Bug: #1285040"
1033,878a82bf547464f934b1b49896886e3abd8f6387,1394613884,0.0,1.0,1,1,1,1,1,0.0,True,1.0,21958.0,10.0,3.0,False,50.0,43550.0,117.0,3.0,1825.0,2398.0,3758.0,1523.0,2002.0,3066.0,931.0,2326.0,2795.0,0.122987596,0.307073106,0.368962787,593,1017,1291238,nova,878a82bf547464f934b1b49896886e3abd8f6387,0,0,It’s not a bug,"Bug #1291238 in OpenStack Compute (nova): ""VMware: VC drivers reports deprectaion message for ESX driver""","When using the VC driver the following message is received:
2014-03-12 01:42:32.889 INFO nova.openstack.common.periodic_task [-] Skipping periodic task _periodic_update_dns because its interval is negative
2014-03-12 01:42:32.940 INFO nova.virt.driver [-] Loading compute driver 'vmwareapi.VMwareVCDriver'
2014-03-12 01:42:32.979 WARNING nova.virt.vmwareapi.driver [-] The VMware ESX driver is now deprecated and will be removed in the Juno release. The VC driver will remain and continue to be supported.","VMware: ensure that deprecation does not appear for VC driver

Commit d041b95b8977e77a51e6031c38f0011b44d3266a added the
deprecation warning for the ESX driver. This warning should
not appear for the VC driver.

Change-Id: Ide7b47afbd9e7e4c66a4cdc621fbf511b9c3c953
Closes-bug: #1291238"
1034,878f767b9e1cdc2b57858c41d973dd087f37bbaa,1393864535,,1.0,73,36,5,2,1,0.976367,True,12.0,6886207.0,168.0,53.0,False,20.0,84945.6,36.0,1.0,94.0,731.0,746.0,94.0,731.0,746.0,91.0,726.0,739.0,0.012273212,0.096985059,0.098719317,722,1148,1301340,nova,878f767b9e1cdc2b57858c41d973dd087f37bbaa,0,0,"feature “We should create helper to remove this duplicate code and help to implement new filters based on aggregates""","Bug #1301340 in OpenStack Compute (nova): ""Remove duplicate code with aggregate filters""","Some filters are using the same logic to handle per-aggregate options. We should create helper to remove this duplicate code and help to implement new filters based on aggregates
Filters that needs to be addressed:
 * AggregateRamFilter
 * AggregateCoreFilter,
 * AggregateTypeAffinityFilter","Add helpers to create per-aggregate filters

This patchset introduce a new file of utility methods
to deal with aggregate metadata.
As well it updates related filters to use this new logic.
* AggregateRamFilter
* AggregateCoreFilter,
* AggregateTypeAffinityFilter

Closes-Bug: #1301340
Change-Id: I5e4aebe9e61cfbdc7b9e6b5de0788fc6670eb936"
1035,8797aeaf14352a353ebfd1ed011f63a5b2fa3447,1383473679,,1.0,14,5,2,2,1,0.998000884,True,4.0,13722412.0,50.0,30.0,False,43.0,61245.0,91.0,7.0,1501.0,3237.0,3297.0,1345.0,2830.0,2882.0,667.0,2296.0,2356.0,0.102642901,0.352950215,0.362169637,49,377,1247556,nova,8797aeaf14352a353ebfd1ed011f63a5b2fa3447,1,1,,"Bug #1247556 in OpenStack Compute (nova): ""VMware: rescue with IDE disks fails""",IDE disks are not hot addable. They will cannot be added when a VM is running.,"VMware: fix rescue with disks are not hot-addable

Some disks, for example IDE, cannot be attached to an instance
when the instance is running.

Change-Id: I3360066ea6406564caa328ec052b9e3ca9d63dc6
Closes-Bug: #1247556"
1036,87cd23419bdd5724e742eda21bc23876b558e0d1,1404362105,,1.0,8,2,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,1037,1481,1337088,cinder,87cd23419bdd5724e742eda21bc23876b558e0d1,1,1,,"Bug #1337088 in Cinder: ""Volume detach will be attempted if volume stats is ""in-use"" OR attach_status is ""attached""""","In begin_detaching, the detach will be attempted if volume state is ""in-use"" OR attach_status is ""attached"" but the error message states that detach will only be attempted if volume state is ""in-use"" AND attach_status is ""attached""","Fix begin_detach logic

Fix begin_detach logic so it checks that a volume is in-use AND
attached.

Change-Id: I6f0e584070a5fc02c55c26ea92cb8b0c2a2cda3e
Closes-Bug: #1337088"
1037,8825c9c74a67d1cef28b6d2d18d5fbaf40e36f51,1353015285,1.0,,429,262,20,10,2,0.820060117,True,12.0,15456625.0,64.0,14.0,False,34.0,191111.6,117.0,2.0,1.0,89.0,90.0,1.0,89.0,90.0,1.0,83.0,84.0,0.006872852,0.288659794,0.29209622,1721,1242644,1242644,Swift,8825c9c74a67d1cef28b6d2d18d5fbaf40e36f51,1,1, ,"Bug #1242644 in OpenStack Object Storage (swift): ""proxy-server Error: character mapping must return integer, None or unicode:""","Hi,
I run tempest on a freshly installed packages in debian wheezy.
And tests around tempurl fail to compute the hmac in the tempurl middleware
(One of test that failed: tempest.api.object_storage.test_object_temp_url.ObjectTempUrlTest.test_put_object_using_temp_url)
The problem seems due that the HMAC key is 'unicode' instead of 'str'.
Here the error from the proxy-server daemon:
Oct 21 07:26:40 packages-va-wheezy-havana proxy-server Error: character mapping must return integer, None or unicode:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/catch_errors.py"", line 37, in handle_request
    resp = self._app_call(env)
  File ""/usr/lib/python2.7/dist-packages/swift/common/wsgi.py"", line 388, in _app_call
    resp = self.app(env, self._start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/healthcheck.py"", line 57, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/proxy_logging.py"", line 268, in __call__
    iterable = self.app(env, my_start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/memcache.py"", line 67, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/swob.py"", line 1188, in _wsgify_self
    return func(self, Request(env))(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/slo.py"", line 458, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/ratelimit.py"", line 266, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/formpost.py"", line 330, in __call__
    return self.app(env, start_response)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 278, in __call__
    hmac_vals = self._get_hmacs(env, temp_url_expires, keys)
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 381, in _get_hmacs
    for key in keys]
  File ""/usr/lib/python2.7/dist-packages/swift/common/middleware/tempurl.py"", line 404, in _get_hmac
    env['PATH_INFO']), sha1).hexdigest()
  File ""/usr/lib/python2.7/hmac.py"", line 133, in new
    return HMAC(key, msg, digestmod)
  File ""/usr/lib/python2.7/hmac.py"", line 72,
Regards,
sileht","Enhance log msg to report referer and user-agent

Enhance internally logged messages to report referer and user-agent.

Pass the referering URL and METHOD between internal servers (when
known), and set the user-agent to be the server type (obj-server,
container-server, proxy-server, obj-updater, obj-replicator,
container-updater, direct-client, etc.) with the process PID. In
conjunction with the transaction ID, it helps to track down which PID
from a given system was responsible for initiating the request and
what that server was working on to make this request.

This has been helpful in tracking down interactions between object,
container and account servers.

We also take things a bit further performaing a bit of refactoring to
consolidate calls to transfer_headers() now that we have a helper
method for constructing them.

Finally we performed further changes to avoid header key duplication
due to string literal header key values and the various objects
representing headers for requests and responses. See below for more
details.

====

Header Keys

There seems to be a bit of a problem with the case of the various
string literals used for header keys and the interchangable way
standard Python dictionaries, HeaderKeyDict() and HeaderEnvironProxy()
objects are used.

If one is not careful, a header object of some sort (one that does not
normalize its keys, and that is not necessarily a dictionary) can be
constructed containing header keys which differ only by the case of
their string literals. E.g.:

   { 'x-trans-id': '1234', 'X-Trans-Id': '5678' }

Such an object, when passed to http_connect() will result in an
on-the-wire header where the key values are merged together, comma
separated, that looks something like:

   HTTP_X_TRANS_ID: 1234,5678

For some headers in some contexts, this is behavior is desirable. For
example, one can also use a list of tuples which enumerate the multiple
values a single header should have.

However, in almost all of the contexts used in the code base, this is
not desirable.

This behavior arises from a combination of factors:

   1. Header strings are not constants and different lower-case and
      title-case header strings values are used interchangably in the
      code at times

      It might be worth the effort to make a pass through the code to
      stop using string literals and use constants instead, but there
      are plusses and minuses to doing that, so this was not attempted
      in this effort

   2. HeaderEnvironProxy() objects report their keys in "".title()""
      case, but normalize all other key references to the form
      expected by the Request class's environ field

      swob.Request.headers fields are HeaderEnvironProxy() objects.

   3. HeaderKeyDict() objects report their keys in "".lower()"" case,
      and normalize all other key references to "".lower()"" case

      swob.Response.headers fields are HeaderKeyDict() objects.

Depending on which object is used and how it is used, one can end up
with such a mismatch.

This commit takes the following steps as a (PROPOSED) solution:

   1. Change HeaderKeyDict() to normalize using "".title()"" case to
      match HeaderEnvironProxy()

   2. Replace standard python dictionary objects with HeaderKeyDict()
      objects where possible

      This gives us an object that normalizes key references to avoid
      fixing the code to normalize the string literals.

   3. Fix up a few places to use title case string literals to match
      the new defaults

Change-Id: Ied56a1df83ffac793ee85e796424d7d20f18f469
Signed-off-by: Peter Portante <peter.portante@redhat.com>"
1038,8831f3d0e2e2a81a6b406cc9c8bf89bc15989065,1396445483,,1.0,57,29,5,4,1,0.852519285,True,15.0,3500623.0,164.0,42.0,False,212.0,2454918.6,1191.0,1.0,14.0,962.0,973.0,12.0,961.0,971.0,6.0,956.0,959.0,0.00090568,0.123819382,0.12420753,773,1200,1305399,nova,8831f3d0e2e2a81a6b406cc9c8bf89bc15989065,1,1,,"Bug #1305399 in OpenStack Compute (nova): ""Cannot unshelve instance with user volumes""","the steps to reproduce:
1. boot an instance with user volume: nova boot --flavor 11 --image cirros --block-device-mapping /dev/vdb=a6118113-bce9-4e0f-89ce-d2aecb0148f8 test_vm1
2. shelve the instance: nova shelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
3. unshelve the instance: nova unshelve 958b6615-1a02-46a7-a0cf-a4b253f1b9de
the instance will be in task_state of ""unshelving"", and the error message in log file is:
[-] Exception during message handling: Invalid volume: status must be 'available'
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3593, in unshelve_instance
    do_unshelve_instance()
  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3592, in do_unshelve_instance
    filter_properties, node)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3617, in _unshelve_instance
    block_device_info = self._prep_block_device(context, instance, bdms)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1463, in _prep_block_device
    instance=instance)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1442, in _prep_block_device
    self.driver, self._await_block_device_map_created) +
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 364, in attach_block_devices
    map(_log_and_attach, block_device_mapping)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 362, in _log_and_attach
    bdm.attach(*attach_args, **attach_kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 44, in wrapped
    ret_val = method(obj, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/virt/block_device.py"", line 218, in attach
    volume_api.check_attach(context, volume, instance=instance)
  File ""/opt/stack/nova/nova/volume/cinder.py"", line 229, in check_attach
    raise exception.InvalidVolume(reason=msg)
InvalidVolume: Invalid volume: status must be 'available'","Allow to unshelve instance booted from volume

When we shelve an instance, volumes are not detached from cinder to kept
them 'In-Use' in cinder
But when you unshelve this instance, nova ask cinder the reattach theses
volumes. This fails, because volume cannot be attached twice.

This patch permits when we ask the libvirt DriverBlockDevice to attach a
device to the instance to bypass the cinder attachement code that is not needed
when we unshelve an instance, because the cinder volume is kept 'In-use'
during the 'shelved' state.

Co-Authored-By: Sahid Orentino Ferdjaoui <sahid.ferdjaoui@cloudwatt.com>
Closes-bug: #1305399
Change-Id: I780a9407feeb48ecd3e295508ce3e6bc3b09d3e6"
1039,884b08e1b20a2f80fb6eba6f0c7405979b914817,1394940733,,1.0,11,4,1,1,1,0.0,True,4.0,514188.0,67.0,16.0,False,5.0,1685442.0,5.0,7.0,0.0,1342.0,1342.0,0.0,1144.0,1144.0,0.0,774.0,774.0,0.001026694,0.795687885,0.795687885,1482,1215270,1215270,neutron,884b08e1b20a2f80fb6eba6f0c7405979b914817,1,1,Missing code,"Bug #1215270 in neutron: ""metadata namespace_proxy missing help information for options""","When i use `neutron-ns-metadata-proxy  --help`, some option doesn't have help text. It is because in neutron/agent/metadata/namespace_proxy.py:
opts = [
        cfg.StrOpt('network_id'),
        cfg.StrOpt('router_id'),
        cfg.StrOpt('pid_file'),
        cfg.BoolOpt('daemonize', default=True),
        cfg.IntOpt('metadata_port',
                   default=9697,
                   help=_(""TCP Port to listen for metadata server ""
                          ""requests."")),
    ]","Added config value help text in ns metadata proxy

Change-Id: I9d59e902cf9ed89e29d93b29912c87f2bad73c7c
Closes-Bug: #1215270"
1040,88861df2b95a9c66a1a9d1969ad9e560786bb848,1383412434,,1.0,79,18,4,2,1,0.88283803,True,6.0,13573831.0,59.0,29.0,False,49.0,165960.0,107.0,8.0,1408.0,3539.0,4591.0,1263.0,2833.0,3746.0,574.0,3233.0,3469.0,0.088393543,0.497156034,0.533435819,48,376,1247427,nova,88861df2b95a9c66a1a9d1969ad9e560786bb848,1,1,,"Bug #1247427 in OpenStack Compute (nova): ""VMware: vmrescue broken when config drive is used""","Traceback (most recent call last):
File ""/opt/stack/nova/nova/compute/manager.py"", line 2727, in rescue_instance
rescue_image_meta, admin_password)
File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 688, in rescue
_vmops.rescue(context, instance, network_info, image_meta)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1026, in rescue
None, None, network_info)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 523, in spawn
cookies)
File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 566, in _create_config_drive
extra_md=extra_md)
File ""/opt/stack/nova/nova/api/metadata/base.py"", line 145, in __init__
obj_base.obj_to_primitive(instance))
File ""/opt/stack/nova/nova/conductor/api.py"", line 302, in get_ec2_ids
return self._manager.get_ec2_ids(context, instance)
File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 456, in get_ec2_ids
instance=instance_p)
File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
return self._invoke(self.proxy.call, ctxt, method, **kwargs)
File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
return cast_or_call(ctxt, msg, **self.kwargs)
File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
result = rpc.call(context, real_topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
return _get_impl().call(CONF, context, topic, msg, timeout)
File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
rpc_amqp.get_connection_pool(conf, Connection))
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 574, in call
rv = list(rv)
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 539, in __iter__
raise result
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'
Traceback (most recent call last):
File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
**args)
File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
result = getattr(proxyobj, method)(ctxt, **kwargs)
File ""/opt/stack/nova/nova/conductor/manager.py"", line 521, in get_ec2_ids
ec2_ids['instance-id'] = ec2utils.id_to_ec2_inst_id(instance['uuid'])
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 193, in id_to_ec2_inst_id
return id_to_ec2_id(instance_id)
File ""/opt/stack/nova/nova/api/ec2/ec2utils.py"", line 181, in id_to_ec2_id
return template % int(instance_id)
ValueError: invalid literal for int() with base 10: 'bd915bb5-6cae-4d8d-8755-3f6583713eff-rescue'","VMware: bug fix for VM rescue when config drive is configured

The addition of the config drive feature broke the VM rescue. The
cause of the problem was setting the instance UUID to the name of
the rescue instance. This must be a UUID! The solution
is to pass the instance name to the spawn method instead of writing
this on the instance UUID.

Change-Id: Ia8494b0c8099753b666bda18a19c4c9a3f764616
Closes-bug: #1247427"
1041,88b7380d0e7c398780d2bb20abd1936e7c879665,1393095581,,1.0,11,3,1,1,1,0.0,True,6.0,885659.0,59.0,19.0,False,8.0,2721611.0,8.0,5.0,41.0,1778.0,1812.0,41.0,1555.0,1589.0,9.0,1687.0,1690.0,0.001348618,0.227646662,0.228051247,419,834,1280826,nova,88b7380d0e7c398780d2bb20abd1936e7c879665,1,1,failure with msgs,"Bug #1280826 in OpenStack Compute (nova): ""config generator fails when project enables lazy messages""","When lazy message translation is enabled in Nova, the check_update.sh calls generate_sample.sh, which uses a copy of oslo's config/generator.py which produces the following message:
CRITICAL nova [-] TypeError: Message objects do not support addition.
The config/generator.py module installs i18n without lazy enabled (named parameter 'lazy' not specified):
gettextutils.install('nova')
To gather information about the projects options, it loads the project modules looking for entry points.   When these modules are loaded, they may contain code to enable lazy.   In the case of Nova this is the nova/cmds/__init__.py which calls:
gettextutils.enable_lazy()
This means that the messages returned with information for the entry points are lazy enabled.  Thus when config/generator.py tries to work with the help message for the option associated with the Nova modules:
opt_help += ' (' + OPT_TYPES[opt_type] + ')'
it fails because opt_help is a gettextutils.Message instance, which doesn't support addition.","Sync latest config file generator from oslo-incubator

The fix to the config file generator to support lazy messages
is the main addition of interest because it is needed before
lazy messages can be enabled in Nova.

Commits included:

  1a6dfb956 Sanitize FQDN in config generator
  e839886cc Config generator fails with lazy messages
  763eedff6 Fix DictOpt support in config sample generator

Note that these commits do not touch any other files (beyond
oslo testcases).

Partially implements blueprint i18n-messages
Closes-Bug: #1280826

Change-Id: I2174139ba3767d05924747cbefa377ba8780103d"
1042,88d91252eca1f8734e316a6e4b9ebc3a977bee44,1408954808,,1.0,14,3,2,2,1,0.936667382,False,,,,,True,,,,,,,,,,,,,,,,,1229,1688,1361097,nova,88d91252eca1f8734e316a6e4b9ebc3a977bee44,1,1,,"Bug #1361097 in OpenStack Compute (nova): ""Compute exception text never present when max sched attempt reached""","When scheduling VMs and the retry logic kicks in, the failed compute exception text is saved to be displayed for triaging purposes in the conductor/scheduler logs.  When the conductor tries to display the exception text when the maximum scheduling attempts have been reached, the exception always shows 'None' for the exception text.
Snippet from scheduler_utils.py...
 msg = (_('Exceeded max scheduling attempts %(max_attempts)d '
'for instance %(instance_uuid)s. '
'Last exception: %(exc)s.')
% {'max_attempts': max_attempts,
'instance_uuid': instance_uuid,
'exc': exc})
That is, 'exc' is erroneously ALWAYS None in this case.","Preserve exception text during schedule retries

This fix preserves the exception text (as intended) when the number
of scheduling retries exceeds the maximum attempts.  We don't pop
the exception message anymore in _log_compute_error so that the
final ""max attempts"" error message has an opportunity to dump it
as well.

This defect caused the exception text (from the compute process) to
ALWAYS be None, thus making it difficult to triage compute errors.

Change-Id: I5c7cac1d79dc583cca523270bb0a96d3353ca1f6
Closes-Bug: #1361097"
1043,893e10b0add63b364eb6d92715967c45028fa692,1381570298,,1.0,4,2,2,2,1,0.918295834,True,3.0,251786.0,23.0,11.0,False,2.0,2073345.0,3.0,4.0,45.0,685.0,710.0,43.0,627.0,650.0,29.0,221.0,230.0,0.066964286,0.495535714,0.515625,1596,1229954,1229954,neutron,893e10b0add63b364eb6d92715967c45028fa692,1,1,"“Below command can't work because list element ""allowed_address_pairs"" hasn't been updated into the varaiable “attr.PLURALS""""","Bug #1229954 in neutron: ""updating address pairs with xml doesn't work""","Works fine: neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1
Generates:
    [{u'ip_address': u'10.0.0.1'}]
neutron port-update 493199d2-d792-4017-b015-894e85a8e32a --allowed-address-pairs list=true type=dict ip_address=10.0.0.1  --request-format xml
Generates:
    {'allowed_address_pair': {'ip_address': '10.0.0.1'}}
2013-09-24 14:10:34.021 19471 ERROR neutron.api.v2.resource [-] update failed
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 460, in update
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 589, in prepare_request_body
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     attr_vals['validate'][rule])
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/extensions/allowedaddresspairs.py"", line 53, in _validate_allowed_address_pairs
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource     raise AllowedAddressPairsMissingIP()
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource AllowedAddressPairsMissingIP: AllowedAddressPair must contain ip_address
2013-09-24 14:10:34.021 19471 TRACE neutron.api.v2.resource","Updating address pairs with xml doesn't work

Below command can't work because list element
""allowed_address_pairs"" hasn't been updated
into the varaiable ""attr.PLURALS""
""neutron port-update <port-uuid> \
--allowed-address-pairs list=true type=dict \
ip_address=10.0.0.1 --request-format xml""

Change-Id: I0d7dcca5f4848bc968f5c86fbeb46569c70c8fcd
Closes-Bug: #1229954"
1044,897a4b754daae840100e63433d928bb62a81fbdc,1401506232,,1.0,0,12,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,937,1373,1325148,nova,897a4b754daae840100e63433d928bb62a81fbdc,0,0,“Remove useless”,"Bug #1325148 in OpenStack Compute (nova): ""Remove useless codes for server_group""","Some codes about 'extract_members' from 'create_server_group' is written in original bp/patch's reviewing (https://review.openstack.org/#/c/62557/29).
But as the design variation, it can't be specified members in create API (see it in PS29 vs PS30).
But some codes are still kept after this feature merged.
So it's necessary to remove them.","Remove useless codes for server_group

Some codes about 'extract_members' from 'create_server_group'
is written in original bp/patch's reviewing
(I650a8f191dea5eab5b4b1828f0b9f65e33edea2a).

But as the design variation, it can't be specified members
in create API (see it in PS29 vs PS30).
But some useless codes are still kept after this feature merged.
So it's necessary to remove them.

Change-Id: I087c8389f2c735a36d656b44c6a3ecfe3a4aa5a3
Closes-Bug: #1325148"
1045,89b01ca24ba95e36568352b4e5c7abf0cd04e4af,1399069528,1.0,1.0,9,7,1,1,1,0.0,True,4.0,468761.0,51.0,15.0,False,8.0,2539521.0,10.0,6.0,806.0,1961.0,1961.0,664.0,1624.0,1624.0,299.0,1005.0,1005.0,0.249169435,0.835548173,0.835548173,851,1281,1315097,neutron,89b01ca24ba95e36568352b4e5c7abf0cd04e4af,0,0,Optimization “Optimize querying for security groups”,"Bug #1315097 in neutron: ""get_security_groups_on_port takes a long time slowing down neutron""","In Havana neutron, there is a _get_security_groups_on_port(...) which checks that all security groups on port belong to tenant. This method is called every time a port is created. This method calls get_security_groups(...) and with 'service' tenant, this results in getting security groups of all tenants from the database, which is time consuming if there are hundreds of tenants. get_security_groups(...) then applies filter on the security groups returned from db query to select security groups associated with the port. In our environment with around 700 tenants, we saw this method take > 30 seconds and also cause slowdown of other API request processing on this worker instance. This can be made more efficient by supplying 'id' filter so that the database query itself filters on the tenant-id. In our environment, this patch resulted in the method taking 0.1 seconds, down from 30 seconds.","Optimize querying for security groups

In the check for all security groups on port belong to tenant,
add a filter to get security groups for the tenant which are in
common with the security groups of the port.

Change-Id: I66f31755525fca37f9dbce6fb43e475791f82495
Closes-Bug: #1315097"
1046,89c04904416270d3c306d430f443a7127c5fc206,1412698431,,1.0,25,9,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1292,1758,1367771,glance,89c04904416270d3c306d430f443a7127c5fc206,1,1,,"Bug #1367771 in Glance: ""glance-manage db load_metadefs will fail if DB is not empty""","To insert data into DB 'glance-manage db load_metadefs' uses IDs for namespaces which are generated by built-in function in Python - enumerate:
for namespace_id, json_schema_file in enumerate(json_schema_files, start=1):
For empty database it works fine, but this causes problems when there are already metadata namespaces in database. The problem is that when there are already metadata definitions in DB then every invoke of glance-manage db load_metadefs leads to IntegrityErrors because of duplicated IDs.
There are two approaches to fix this:
1. Ask for a namespace just after inserting it. Unfortunately in current implementation we need to do one more query.
2. When this go live - https://review.openstack.org/#/c/120414/ - then we won't need to do another query, because ID is available just after inserting a namespace to DB (namespace.save(session=session)).","Use ID for namespace generated by DB

In current implementation ID that is used in namespace to
insert data to DB is generated by built-in function - enumerate.
This causes problems with loading the metadata when there are already
namespaces in DB.

This patch removes 'enumerate' and asks for namespace ID
generated by database.

Change-Id: I235c6310077526cafb898ac007c3601b4d66c9fe
Closes-Bug: #1367771
Co-Authored-By: Bartosz Fic <bartosz.fic@intel.com>
Co-Authored-By: Pawel Koniszewski <pawel.koniszewski@intel.com>"
1047,89e76a8afae19c5f66d532538607aedaac59722a,1408474871,,1.0,56,17,3,3,1,0.755313952,False,,,,,True,,,,,,,,,,,,,,,,,1213,1672,1358636,neutron,89e76a8afae19c5f66d532538607aedaac59722a,0,0,Bug in test,"Bug #1358636 in neutron: ""test_add_list_remove_router_on_l3_agent fails with Conflict""","Test tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent fails for job gate-tempest-dsvm-neutron-full with traceback:
ft335.1: tempest.api.network.admin.test_l3_agent_scheduler.L3AgentSchedulerTestXML.test_add_list_remove_router_on_l3_agent[gate,smoke]_StringException: Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
2014-08-18 15:31:39,865 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 200 POST http://127.0.0.1:5000/v2.0/tokens
2014-08-18 15:31:40,148 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 201 POST http://127.0.0.1:9696/v2.0/routers 0.282s
2014-08-18 15:31:40,259 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:test_add_list_remove_router_on_l3_agent): 409 POST http://127.0.0.1:9696/v2.0/agents/47dd83c6-f92d-40d9-8601-5a38b6b9eda0/l3-routers 0.109s
2014-08-18 15:31:40,714 535 INFO     [tempest.common.rest_client] Request (L3AgentSchedulerTestXML:_run_cleanups): 204 DELETE http://127.0.0.1:9696/v2.0/routers/fd7d082c-71db-4fa0-bd0c-0b31acef9b1a 0.450s
}}}
Traceback (most recent call last):
  File ""tempest/api/network/admin/test_l3_agent_scheduler.py"", line 66, in test_add_list_remove_router_on_l3_agent
    self.agent['id'], router['router']['id'])
  File ""tempest/services/network/xml/network_client.py"", line 218, in add_router_to_l3_agent
    resp, body = self.post(uri, str(common.Document(router)))
  File ""tempest/services/network/network_client_base.py"", line 73, in post
    return self.rest_client.post(uri, body, headers)
  File ""tempest/common/rest_client.py"", line 219, in post
    return self.request('POST', url, extra_headers, headers, body)
  File ""tempest/common/rest_client.py"", line 431, in request
    resp, resp_body)
  File ""tempest/common/rest_client.py"", line 485, in _error_checker
    raise exceptions.Conflict(resp_body)
Conflict: An object with that identifier already exists
Details: {'message': 'The router fd7d082c-71db-4fa0-bd0c-0b31acef9b1a has been already hosted by the L3 Agent 47dd83c6-f92d-40d9-8601-5a38b6b9eda0.', 'type': 'RouterHostedByL3Agent', 'detail': {}}
There is log of test results:  http://logs.openstack.org/43/97543/10/gate/gate-tempest-dsvm-neutron-full/8f09c74/logs/testr_results.html.gz","Fix l3 agent scheduling logic to avoid unwanted failures

In case router is being added to l3 agent which is already hosting
the router it is fine to let such a request to succeed.
This patch also adds a check for unnecessary scheduling that might happen
twice in described case and lead to unwanted messages in the logs.

Change-Id: Id104b36ba7e1e6f6a9378ee600c33e9962230521
Closes-Bug: #1358636"
1048,89f25623a4345b5044a58cff36e7fd103c6b88e8,1384817539,0.0,2.0,107,170,4,3,1,0.743393372,True,18.0,11737935.0,170.0,64.0,False,24.0,1523016.0,40.0,12.0,235.0,1595.0,1638.0,220.0,1430.0,1459.0,165.0,520.0,523.0,0.300180832,0.942133816,0.94755877,141,542,1257815,neutron,89f25623a4345b5044a58cff36e7fd103c6b88e8,1,1,,"Bug #1257815 in neutron: ""Internal server error while deleting subnet(can not find the rows  in ipavailabilityranges  table)""","when trying to delete a subnet, sometimes the following error comes out.
Icehouse, Database  in use is DB2, but I guess it might happen for other databases too.
================================
2013-12-04 03:49:48.275 26604 TRACE neutron.plugins.ml2.plugin
2013-12-04 03:49:48.277 26604 ERROR neutron.api.v2.resource [req-e8e78c50-25b0-4e19-b5f0-796041d7b464 f53f4f5b40154ad6b1ec1ac08f88ecf2 b93ff0
8b44da407185a26033768101f5] NT-C3C9C57 delete failed
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 443
, in delete_network
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.delete_subnet(context, subnet.id)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 530, in delete_subnet
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     break
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449,in __exit__
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.commit()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self._prepare_impl()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340,in _prepare_impl
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     self.session.flush()
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 545, in _wrap
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource     raise exception.DBError(e)
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource DBError: (Error) ibm_db_dbi::Error:
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource Error 1: [IBM][CLI Driver][DB2/LINUXX8664] SQL0100W  No row was found for FETCH,UPDATE or DELETE; or the result of a query is an empty table.  SQLSTATE=02000 SQLCODE=100
2013-12-04 03:49:48.277 26604 TRACE neutron.api.v2.resource  'DELETE FROM ipavailabilityranges WHERE ipavailabilityranges.allocation_pool_id= ? AND ipavailabilityranges.first_ip = ? AND ipavailabilityranges.last_ip = ?' (('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.2', '10.100.0.2'), ('e376f33d-a224-4468-91eb-82191e158726', '10.100.0.4', '10.100.0.14'))","Simplify ip allocation/recycling to relieve db pressure

I found that multiple calls to delete_port can pile up on the
_recycle_ip operation.  This patch simplifies this operation.  It
reduces the _recycle_ip operation to a single row delete in the ip
allocations table and doesn't touch the availability table.

To acheive the recycling of ips in a pool, this code runs a more
complex operation of rebuilding the availability table when it is
exhausted.  Only one API process will perform this more expensive
operation and others waiting for allocation will immediately benefit.
The amortized cost of this operation is much less than the cumulative
cost of running the more expensive _recycle_ip operation for every
port delete.

IP allocation behaves a bit differently with this patch.  Instead of
giving out the first IP available in a pool, the entire pool will be
allocated before wrapping around and recycling ip addresses that have
been released.  This is a desirable feature as it puts ip addresses in
a sort of quarantine after they are released.  It is easier to
distinguish newly allocated ips from old ones.

Change-Id: Ia55b66128de9986e075b0f87acc401d211cd91d3
Closes-Bug: #1252506
Closes-Bug: #1257815"
1049,89f25623a4345b5044a58cff36e7fd103c6b88e8,1384817539,0.0,2.0,107,170,4,3,1,0.743393372,True,18.0,11737935.0,170.0,64.0,False,24.0,1523016.0,40.0,12.0,235.0,1595.0,1638.0,220.0,1430.0,1459.0,165.0,520.0,523.0,0.300180832,0.942133816,0.94755877,1777,1252506,1252506,neutron,89f25623a4345b5044a58cff36e7fd103c6b88e8,1,1, ,"Bug #1252506 in neutron: ""_recycle_ip slow, multiple port deletes begin timing out""","I've found that port delete fails if there are a number of port delete/create operations happening at once.
It seems to depend on database load but I've seen as few as about 8 parallel port deletes begin to timeout waiting for the appropriate locks inside of the _recycle_ip method in neutron/db/db_base_plugin_v2.py.","Simplify ip allocation/recycling to relieve db pressure

I found that multiple calls to delete_port can pile up on the
_recycle_ip operation.  This patch simplifies this operation.  It
reduces the _recycle_ip operation to a single row delete in the ip
allocations table and doesn't touch the availability table.

To acheive the recycling of ips in a pool, this code runs a more
complex operation of rebuilding the availability table when it is
exhausted.  Only one API process will perform this more expensive
operation and others waiting for allocation will immediately benefit.
The amortized cost of this operation is much less than the cumulative
cost of running the more expensive _recycle_ip operation for every
port delete.

IP allocation behaves a bit differently with this patch.  Instead of
giving out the first IP available in a pool, the entire pool will be
allocated before wrapping around and recycling ip addresses that have
been released.  This is a desirable feature as it puts ip addresses in
a sort of quarantine after they are released.  It is easier to
distinguish newly allocated ips from old ones.

Change-Id: Ia55b66128de9986e075b0f87acc401d211cd91d3
Closes-Bug: #1252506
Closes-Bug: #1257815"
1050,8a08a3cb47d0dd69d4aa2e8fa661d04054fe95ae,1412728718,,1.0,48,54,3,3,1,0.475727628,False,,,,,True,,,,,,,,,,,,,,,,,1390,1860,1378952,neutron,8a08a3cb47d0dd69d4aa2e8fa661d04054fe95ae,0,0,It’s a proposed feature,"Bug #1378952 in neutron: ""Updating ipv6 modes is problematic""","See
http://lists.openstack.org/pipermail/openstack-dev/2014-October/047978.html","Disable PUT for IPv6 subnet attributes

In Juno we are not ready for allowing the IPv6 attributes on a subnet
to be updated after the subnet is created, because:
- The implementation for supporting updates is incomplete.
- Perceived lack of usefulness, no good use cases known yet.
- Allowing updates causes more complexity in the code.
- Have not tested that radvd, dhcp, etc. behave OK after update.

Therefore, for now, we set 'allow_put' to False for the two IPv6
attributes, ipv6_ra_mode and ipv6_address_mode. This prevents the
modes from being updated via the PUT:subnets API.

Closes-bug: #1378952

Change-Id: Id6ce894d223c91421b62f82d266cfc15fa63ed0e"
1051,8a34fc3d48c467aa196f65eed444ccdc7c02f19f,1385581476,,1.0,1,1,1,1,1,0.0,True,4.0,5067867.0,32.0,25.0,False,134.0,39540.0,381.0,8.0,48.0,3781.0,3791.0,47.0,3222.0,3232.0,43.0,2685.0,2694.0,0.006542751,0.399405204,0.400743494,1574,1227027,1227027,nova,8a34fc3d48c467aa196f65eed444ccdc7c02f19f,1,1,“In the following commit:“,"Bug #1227027 in OpenStack Compute (nova): ""[OSSA 2014-001] Insecure directory permissions with snapshot code (CVE-2013-7048)""","In the following commit:
commit 46de2d1e2d0abd6fdcd4da13facaf3225c721f5e
Author: Rafi Khardalian <email address hidden>
Date:   Sat Jan 26 09:02:19 2013 +0000
    Libvirt: Add support for live snapshots
    blueprint libvirt-live-snapshots
There was the following chunk of code
         snapshot_directory = CONF.libvirt_snapshots_directory
         fileutils.ensure_tree(snapshot_directory)
         with utils.tempdir(dir=snapshot_directory) as tmpdir:
             try:
                 out_path = os.path.join(tmpdir, snapshot_name)
-                snapshot.extract(out_path, image_format)
+                if live_snapshot:
+                    # NOTE (rmk): libvirt needs to be able to write to the
+                    #             temp directory, which is owned nova.
+                    utils.execute('chmod', '777', tmpdir, run_as_root=True)
+                    self._live_snapshot(virt_dom, disk_path, out_path,
+                                        image_format)
+                else:
+                    snapshot.extract(out_path, image_format)
Making the temporary directory 777 does indeed give QEMU and libvirt permission to write there, because it gives every user on the whole system permission to write there. Yes, the directory name is unpredictable since it uses 'tempdir', this does not eliminate the security risk of making it world writable though.
This flaw is highlighted by the following public commit which makes the mode configurable, but still defaults to insecure 777.
https://review.openstack.org/#/c/46645/","Enforce permissions in snapshots temporary dir

Live snapshots creates a temporary directory where libvirt driver
creates a new image from the instance's disk using blockRebase.
Currently this directory is created with 777 permissions making this
directory accessible by all the users in the system.

This patch changes the tempdir permissions so they have the o+x
flag set, which is what libvirt needs to be able to write in it and

Closes-Bug: #1227027
Change-Id: I767ff5247b4452821727e92b668276004fc0f84d"
1052,8a37d04bfda7a583d02a3fac262f4f9242d7ed76,1380019328,0.0,1.0,40,20,3,2,1,0.986053956,True,6.0,322411.0,27.0,4.0,False,7.0,1665493.333,10.0,2.0,0.0,859.0,859.0,0.0,793.0,793.0,0.0,292.0,292.0,0.00265252,0.777188329,0.777188329,1592,1229655,1229655,neutron,8a37d04bfda7a583d02a3fac262f4f9242d7ed76,1,1,"“dns nameserver information  was not being passed down to the midonet client API.""","Bug #1229655 in neutron: ""host routes and dns name server not set in midonet plugin""","When creating a subnet in the Midonet plugin, the dns name servers and host routes are not set.","Add host routes and dns nameservers to Midonet DHCP

In the Midonet plugin, the host routes and dns nameserver information
was not being passed down to the midonet client API. This fix addresses
this by passing down the correct information.

Change-Id: I142ad4ceccdcf8b0e13db55fa54513f82995efc5
Closes-Bug: #1229655"
1053,8a50755b9df445a07140f385f1ff32db20bf683b,1407624361,,1.0,11,26,3,2,1,0.85611193,False,,,,,True,,,,,,,,,,,,,,,,,640,1066,1293938,nova,8a50755b9df445a07140f385f1ff32db20bf683b,0,0,Changing in the code for the tests,"Bug #1293938 in OpenStack Compute (nova): ""nova.image.glance unit tests should not use fake glance service""","The unit tests in nova.tests.image.test_glance unfortunately make use of a faked glance image service in nova.tests.image.fakes. What this does is actually mask a number of problems and makes it harder to assert for specific behavior in the real glanceclient client classes.
The unit tests should be rewritten to simply mock out the very specific code boundaries where tested calls interact with the glanceclient client classes, and that's it. Unit tests should just test one little unit of code, not giant swathes of dependent code.","Remove final use of glance_stubs

Removes the final piece of glance_stubs from the image unit tests.

Change-Id: I0db3b6c83edaf91466e85d423ce75b3e75fd3517
Closes-bug: #1293938"
1054,8a50e13a31e99148cebd5580f77cb92b8808fffc,1405087748,1.0,1.0,13,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1058,1504,1340696,neutron,8a50e13a31e99148cebd5580f77cb92b8808fffc,1,0,“After migrating to oslo.messaging we don't need to have debug level in oslo.messaging but it's good to have communication logged.” (Change in the enviroment),"Bug #1340696 in neutron: ""rpc communication is not logged in debug""","Currently even though service is running in with debug=True, rpc messages are not logged. Previously in icehouse we could see those logs. It helps with debugging when we can see the communication between agents and server.","Log methods using rpc communcation

In Icehouse was used rpc library from oslo-incubator. Because the code
wasn't mature enough it was running in DEBUG mode. After migrating to
oslo.messaging we don't need to have debug level in oslo.messaging but
it's good to have communication logged.

Closes-Bug: #1340696
Change-Id: I4f68545053912f96affc1bbcd64fcd7efe8d18c0"
1055,8a8b0ba9abcf7237dc4a7d8ddfc739939c6e9164,1385458877,,1.0,1,1,1,1,1,0.0,True,1.0,261861.0,11.0,3.0,False,31.0,565.0,49.0,3.0,16.0,2260.0,2273.0,16.0,1549.0,1562.0,6.0,2138.0,2141.0,0.001044776,0.319253731,0.319701493,84,481,1255023,nova,8a8b0ba9abcf7237dc4a7d8ddfc739939c6e9164,0,0,tests,"Bug #1255023 in OpenStack Compute (nova): ""Supplement 'os-migrateLive' in actions list in test_admin_actions.py""","Action ""os-migrateLive"" not defined in actions list in test_actions_with_non_existed_instance func.
It should not test the action ""os-migrateLive"".","Supplement 'os-migrateLive' in actions list

The test for actions with non existed instance missing the action
'os-migrateLive'.

Change-Id: Ia36c369d60bea9500cc5a64be7971e60cbe79377
Closes-Bug: #1255023"
1056,8ab6fd6d7e521a3692f57542e5c5c5d513d57ccc,1396390619,,1.0,7,8,2,2,1,0.970950594,True,3.0,637150.0,66.0,9.0,False,31.0,882.0,54.0,4.0,109.0,1707.0,1727.0,109.0,1459.0,1479.0,102.0,876.0,895.0,0.096351731,0.820392891,0.838166511,717,1143,1301042,neutron,8ab6fd6d7e521a3692f57542e5c5c5d513d57ccc,1,1,,"Bug #1301042 in neutron: ""Routers may never be torn down if router_delete_namespaces is False""","The code recently added https://review.openstack.org/#/c/30988/ very nicely cleans out stale routers assuming that self.conf.router_delete_namespaces is true.
The problem is that automatic namespace deletion is still a bit unstable because of problems with the kernel and the iproute utility.  So, many users may not have self.conf.router_delete_namespaces set to True.  In this case, all of the advantages added by the above mentioned patch don't help us.
The problem arises if a router gets deleted or moved to another agent while the L3 agent is down.  When the L3 agent comes back up, it will not touch the router and the router will continue to function as if it were never deleted.","Clean out namespaces even if we don't delete namespaces

Even when we don't enable namespace deletion, we still want to run the
code that cleans out the namespaces so that the devices get unplugged,
etc.  Otherwise, routers deleted while the agent is down will continue
to operate as if they were never deleted.

The trade-off to consider here is that if there are many stale
namespaces this will slow down the restart of the L3 agent.  The best
option is to get namespace deletion working correctly.  However, where
that has not been worked out yet, this patch provides the cleaning
service for deleted routers.

Change-Id: Ic7b4608a23c4d9530f521d5faff3f8526200b92e
Closes-Bug: #1301042
Related-Bug: #1052535"
1057,8b6bc869394bdcbe9cbead8bda8d45be75d2874b,1378443614,,1.0,3,3,1,1,1,0.0,True,6.0,1522412.0,39.0,23.0,False,100.0,13955.0,254.0,8.0,5.0,3022.0,3026.0,5.0,2761.0,2765.0,1.0,2126.0,2126.0,0.000336134,0.357478992,0.357478992,1523,1221527,1221527,nova,8b6bc869394bdcbe9cbead8bda8d45be75d2874b,1,1,“Exception message that is different from the expected” . Duplicate.,"Bug #1221527 in OpenStack Compute (nova): ""Exception message that is different from the expected""","When an exception ProjectUserQuotaNotFound occurs, the message ""Quota
could not be found"" was obtained.  I think that expecting ""Quota for
user XXX in project YYY could not be found."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 480, in test_get_by_project_and_user_with_wrong_resource
    'fake_user', 'wrong_resource')
  File ""/opt/stack/nova/nova/quota.py"", line 1066, in get_by_project_and_user
    user_id, resource)
  File ""/opt/stack/nova/nova/tests/test_quota.py"", line 247, in get_by_project_and_user
    user_id=user_id)
ProjectUserQuotaNotFound: Quota for user fake_user in project test_project could not be found.
As a similar problem, when an exception ConsoleTypeUnavailable occurs,
the message ""Unacceptable parameters."" was obtained.  I think that
expecting ""Unavailable console type XXX."" as follows.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 5020, in test_get_spice_console_unavailable
    conn.get_spice_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2155, in get_spice_console
    ports = get_spice_ports_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2153, in get_spice_ports_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='spice')
ConsoleTypeUnavailable: Unavailable console type spice.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 4979, in test_get_vnc_console_unavailable
    conn.get_vnc_console(instance_ref)
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2135, in get_vnc_console
    port = get_vnc_port_for_instance(instance['name'])
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2133, in get_vnc_port_for_instance
    raise exception.ConsoleTypeUnavailable(console_type='vnc')
ConsoleTypeUnavailable: Unavailable console type vnc.","Fixes unexpected exception message in ProjectUserQuotaNotFound

When an exception ProjectUserQuotaNotFound occurs, the message ""Quota
could not be found"" was obtained. This is because the name of the
variable to override is incorrect. There is a similar bug in
ConsoleTypeUnavailable and ISERTargetNotFoundForVolume.

Change-Id: If709aff55b9042eaca49aaebddc31c366681a36c
Closes-Bug: #1221527"
1058,8b9d6f6fedbbd47932cd672f51d4db7031724e84,1379024207,1.0,1.0,37,3,3,3,1,0.928883995,True,14.0,582475.0,72.0,23.0,False,135.0,25964.66667,525.0,2.0,1.0,1055.0,1056.0,1.0,956.0,957.0,1.0,981.0,982.0,0.000331675,0.162852405,0.163018242,1550,1224712,1224712,nova,8b9d6f6fedbbd47932cd672f51d4db7031724e84,1,1,“this does seem to be problematic”,"Bug #1224712 in OpenStack Compute (nova): ""Cannot get ComputeNodeStat by DB utility of compute_node_get_all()""","When there is hypervisor gets removed, the compute_node_get_all() will not return stat for new added hypervisors.
In the following codes of compute_node_get_all() of nova/db/sqlalchemy, it assume all the record in compute_node_stats should have a matched compute node. However in current implementation of nova conductor API of compute_node_delete(), the records in compute_node_stats is not deleted. Therefore when a hypervisor gets removed, there is no node matching the record of compute_node_stats which belongs to the removed hypervisor in following codes. As a result, all the nodes will be set with 'stats' of [].
    # Join ComputeNode & ComputeNodeStat manually.
    # NOTE(msdubov): ComputeNode and ComputeNodeStat map 1-to-Many.
    #                Running time is (asymptotically) optimal due to the use
    #                of iterators (itertools.groupby() for ComputeNodeStat and
    #                iter() for ComputeNode) - we handle each record only once.
    compute_nodes.sort(key=lambda node: node['id'])
    compute_nodes_iter = iter(compute_nodes)
    for nid, nsts in itertools.groupby(stats, lambda s: s['compute_node_id']):
        for node in compute_nodes_iter:
            if node['id'] == nid:
                node['stats'] = list(nsts)
                break
            else:
                node['stats'] = []
    return compute_nodes
We need enhance either nova conductor API to clean up all the record related with instance.","Prune node stats at compute node delete time

This commit addresses the situation in which a compute node is deleted, but
its compute node stats still remain in the database as **not** deleted. This
causes ill side effects in compute_node_get_all when it's retrieving host stats
as it doesn't expect there to be compute node stats for which there is no
corresponding compute node (i.e., causing some nodes' stats to be empty).

As such, when a compute node is deleted, its stats should also be implicitly
deleted.

The new test case that's been created fails without the code changes, which
illustrates the problem that compute node stats are empty when they should
not be.

Also included is a simple DB migration script that will update old stats that
were not marked soft-deleted as they should have been.

Change-Id: Ief0f7cf1a506e71898b5a45a0513d34167432d67
Closes-Bug: #1224712"
1059,8babd6a99014ccaf51d955769eaec085e037cc76,1394573377,1.0,1.0,97,1,2,2,1,0.371232327,True,3.0,68372.0,47.0,20.0,False,52.0,497418.5,97.0,6.0,382.0,2273.0,2641.0,164.0,2077.0,2229.0,102.0,1522.0,1611.0,0.013599155,0.201082651,0.212833377,588,1010,1291014,nova,8babd6a99014ccaf51d955769eaec085e037cc76,1,1,Attribute error,"Bug #1291014 in OpenStack Compute (nova): ""Nova boot fails: AttributeError: is_public""","[req-d4c97a98-2b0e-419e-83d6-0e88332a699a account1 account1] [instance: 036a26b1-7fe2-4d56-b7a2-4781e8cad696] Error: is_public
Traceback (most recent call last):
  File ""/opt/nova/nova/compute/manager.py"", line 1254, in _build_instance
    set_access_ip=set_access_ip)
  File ""/opt/nova/nova/compute/manager.py"", line 394, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/nova/nova/compute/manager.py"", line 1655, in _spawn
    LOG.exception(_('Instance failed to spawn'), instance=instance)
  File ""/opt/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/nova/nova/compute/manager.py"", line 1652, in _spawn
    block_device_info)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2230, in spawn
    admin_pass=admin_password)
  File ""/opt/nova/nova/virt/libvirt/driver.py"", line2538, in _create_image
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 184, in cache
    *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 310, in create_image
    prepare_template(target=base, max_size=size, *args, **kwargs)
  File ""/opt/nova/nova/openstack/common/lockutils.py"", line 249, in inner
    return f(*args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/imagebackend.py"", line 174, in fetch_func_sync
    fetch_func(target=target, *args, **kwargs)
  File ""/opt/nova/nova/virt/libvirt/utils.py"", line 654, in fetch_image
    max_size=max_size, imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 108, in fetch_to_raw
    imagehandler_args=imagehandler_args)
  File ""/opt/nova/nova/virt/images.py"", line 98, in fetch
    fetched_to_local = handler.is_local()
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""/opt/nova/nova/openstack/common/fileutils.py"", line 98, in remove_path_on_error
    remove(path)
  File ""/opt/nova/nova/virt/images.py"", line 71, in _remove_image_on_exec
    image_path):
  File ""/opt/nova/nova/virt/imagehandler/__init__.py"", line 154, in handle_image
    img_locs = image_service.get_locations(context, image_id)
  File ""/opt/nova/nova/image/glance.py"", line 307, in get_locations
    if not self._is_image_available(context, image_meta):
  File ""/opt/nova/nova/image/glance.py"", line 446, in _is_image_available
    if image.is_public or context.is_admin:
  File ""/usr/local/lib/python2.7/dist-packages/warlock/model.py"", line 72, in __getattr__
    raise AttributeError(key)
AttributeError: is_public","Ensure is_image_available handles V2 Glance API

The V2 Glance API uses warlock to validate returned models
from calls such as GET /images/{id}. The get_locations() call in
the Nova Glance image driver was using the V2 Glance API call
to GET /images/{id} (which is correct, since image locations only
exist in the V2 API), but the GlanceImageService._is_image_available()
method was checking the is_public attribute of the supplied image model.

This caused warlock to throw an exception since is_public is not
an attribute on the V2 API's returned image model in glanceclient.
This change adds some checks that can handle both the V2 and V1 returned
image models from glanceclient and a bunch of unit tests to thoroughly
exercise the is_image_available method.

Change-Id: I35b5af8331c5972bd89e1a674091fa2e3bde28a7
Closes-bug: #1291014"
1060,8bd9b9a63fd104ada41ae1e449e83da337068157,1392910235,,1.0,97,64,1,1,1,0.0,True,8.0,4322152.0,73.0,25.0,False,5.0,1218231.0,19.0,5.0,2.0,722.0,723.0,2.0,639.0,640.0,2.0,677.0,678.0,0.00206754,0.467263956,0.467953136,400,814,1279703,cinder,8bd9b9a63fd104ada41ae1e449e83da337068157,1,1,bugs in log messages…? I dont think it’s a bug.. but is typo in code,"Bug #1279703 in Cinder: ""fixup ceph backup driver log messages""","This is to fix the log messges as per Jay's comment in https://review.openstack.org/#/c/51900/ - ""I think it is important that we be doing all LOG.* messages in a sentence format that ends with a '.' . This makes it easier to read through the messages when debugging.""","ceph backup driver: improve log messages

Added additional information to log messages where helpful and tidied
code.

Change-Id: Ie7c54ab970223c6993254069d5d419e1ee90bedb
Closes-Bug: #1279703"
1061,8c161b6a4b0a7617ee224b23ada0a368e97eaae7,1404340434,,1.0,69,26,2,2,1,0.999920071,False,,,,,True,,,,,,,,,,,,,,,,,1036,1480,1336970,glance,8c161b6a4b0a7617ee224b23ada0a368e97eaae7,1,0,“commit cb93eb60abf10af6fbb1ee438a6e8a51853e6788 changed the VMware store to use chunked encoding when the size is not known”,"Bug #1336970 in Glance: ""VMware store to use content-length when available""","commit cb93eb60abf10af6fbb1ee438a6e8a51853e6788 changed the VMware store to use chunked encoding when the size is not known (or zero).
We still need to use Content-Length when we know the image size.","VMware store: Use the Content-Length if available

Change I579084460e7f61ab4042632d17ec0f045fa6f5af changed the VMware
store to use chunked encoding to upload data to the underlying backend.
We should do that only if the image size is not provided or zero and not
all the time.

This patch addressed the issue by checking the image_size before upload.

Change-Id: If348be7fd24fd05146b0c86bef45b79f600cc0f7
Closes-Bug: #1336970"
1062,8c359e0d0cf40b82b07ed1a30f6bfc00369892a6,1387779296,1.0,1.0,6,15,1,1,1,0.0,True,2.0,1728357.0,14.0,6.0,False,21.0,308126.0,44.0,5.0,186.0,2452.0,2526.0,186.0,2134.0,2208.0,176.0,1543.0,1607.0,0.025578035,0.223121387,0.232369942,217,620,1263602,nova,8c359e0d0cf40b82b07ed1a30f6bfc00369892a6,0,0,tests,"Bug #1263602 in OpenStack Compute (nova): ""Do not use contextlib.nested if only mock one function""","There are some test cases in test_compute_mgr.py using contextlib.nested to mock up functions even there is only one function.
We should use mock.patch.object directly if only mock one function.
def test_init_instance_sets_building_error(self):
        with contextlib.nested(  <<<<< No need use nested here
            mock.patch.object(self.compute, '_instance_update')
          ) as (
            _instance_update,
          ):
            instance = instance_obj.Instance(self.context)
            instance.uuid = 'foo'
            instance.vm_state = vm_states.BUILDING
            instance.task_state = None
            self.compute._init_instance(self.context, instance)
            call = mock.call(self.context, 'foo',
                             task_state=None,
                             vm_state=vm_states.ERROR)
            _instance_update.assert_has_calls([call])","Do not use contextlib.nested if only mock one function

There are some test cases in test_compute_mgr.py using contextlib.nested
to mock up functions even there is only one function needs to be mocked.

We should use mock.patch.object directly if only mock one function.

Change-Id: I30f05d0dcb442d426358b673f0e638ace8b85e0e
Closes-Bug: #1263602"
1063,8c985874c7885f31871204d3f83ce547fefc5fb6,1382010080,,2.0,17,77,6,2,1,0.718482134,True,8.0,15589256.0,46.0,26.0,False,144.0,11.0,433.0,4.0,8.0,2035.0,2041.0,8.0,1771.0,1777.0,7.0,1930.0,1935.0,0.001256676,0.303330192,0.304115614,1,313,1242819,nova,8c985874c7885f31871204d3f83ce547fefc5fb6,1,1,Optimize the code,"Bug #1242819 in OpenStack Compute (nova): ""Remove unnecessary steps for cold snapshots""","When creating cold snapshots we first stop the instance, create a snapshot of that instance, extract the snapshot to a file, delete the snapshot and bring the instance back up.
If the instance is stopped, then there's no need to create a snapshot because there's no concurrent writer, so the snapshot can be extracted directly and save us from the two unnecessary steps (creation and deletion of a snapshot).","Remove unnecessary steps for cold snapshots

Up until now when we created cold snapshots we were stopping the
instance, create an internal snapshot, extract the snapshot to a file
and then delete the internal snapshot before bringing up the instance.

If the instance is shut down, there's no concurrent writer, so the image
can be directly extracted without taking an internal snapshot first,
because the snapshot and the current state are the same.

In this patch the creation and deletion of the internal snapshot are
removed to eliminate the extra steps and optimize the creation of
snapshots a bit.

Closes-Bug: #1242819
Closes-Bug: #1247185

Change-Id: I429fa2e1e26aea321eecbf14edd11263fff525ae"
1064,8c985874c7885f31871204d3f83ce547fefc5fb6,1382010080,,2.0,17,77,6,2,1,0.718482134,True,8.0,15589256.0,46.0,26.0,False,144.0,11.0,433.0,4.0,8.0,2035.0,2041.0,8.0,1771.0,1777.0,7.0,1930.0,1935.0,0.001256676,0.303330192,0.304115614,46,374,1247185,nova,8c985874c7885f31871204d3f83ce547fefc5fb6,1,0,if qemu-img >= 0.12.1,"Bug #1247185 in OpenStack Compute (nova): ""libvirt extract snapshot fails if qemu-img >= 0.12.1""","Running stable/havana tempest against havana 2013.2 nova with this test:
tempest/tempest/api/compute/images/test_images_oneserver.py:test_create_second_image_when_first_image_is_being_saved
Using the libvirt driver on x86_64 RHEL 6.5 fails with this:
http://paste.openstack.org/show/50398/
Essentially: Stderr: ""convert: invalid option -- 's'\n""
This is the code:
https://github.com/openstack/nova/blob/2013.2/nova/virt/libvirt/utils.py#L549
Looks like it's not a new change, it's just incompatible with the latest qemu-img:
https://github.com/openstack/nova/commit/b216ed51914986087ea7dee57bc29904fda001a0
Looks like in the latest qemu-img the snapshot was moved into it's own sub-command:
[root@rhel62 ~]# qemu-img --help
qemu-img version 0.12.1, Copyright (c) 2004-2008 Fabrice Bellard
usage: qemu-img command [command options]
QEMU disk image utility
Command syntax:
  check [-f fmt] filename
  create [-f fmt] [-o options] filename [size]
  commit [-f fmt] [-t cache] filename
  convert [-c] [-p] [-f fmt] [-t cache] [-O output_fmt] [-o options] [-S sparse_size] filename [filename2 [...]] output_filename
  info [-f fmt] filename
  snapshot [-l | -a snapshot | -c snapshot | -d snapshot] filename
  rebase [-f fmt] [-t cache] [-p] [-u] -b backing_file [-F backing_fmt] filename
  resize filename [+ | -]size","Remove unnecessary steps for cold snapshots

Up until now when we created cold snapshots we were stopping the
instance, create an internal snapshot, extract the snapshot to a file
and then delete the internal snapshot before bringing up the instance.

If the instance is shut down, there's no concurrent writer, so the image
can be directly extracted without taking an internal snapshot first,
because the snapshot and the current state are the same.

In this patch the creation and deletion of the internal snapshot are
removed to eliminate the extra steps and optimize the creation of
snapshots a bit.

Closes-Bug: #1242819
Closes-Bug: #1247185

Change-Id: I429fa2e1e26aea321eecbf14edd11263fff525ae"
1065,8cc59d2084a8b1129929a162b753d4ca407018e7,1390187437,,1.0,1,1,1,1,1,0.0,True,2.0,3173681.0,39.0,18.0,False,117.0,152689.0,278.0,6.0,13.0,4126.0,4128.0,13.0,3493.0,3495.0,11.0,3154.0,3154.0,0.001685867,0.443242484,0.443242484,213,616,1263204,nova,8cc59d2084a8b1129929a162b753d4ca407018e7,0,0,Refactoring code. Friendly messages,"Bug #1263204 in OpenStack Compute (nova): ""Exceptions that get returned via REST api should have more user friendly messages ""","Exceptions in nova/exception.py that get returned via REST api should have more user friendly (non-admin and non-dev) messages.
For example
 class InstanceRecreateNotSupported(Invalid):
     msg_fmt = _('Instance recreate is not implemented by this virt driver.')
Assuming that exception gets returned via the REST API, a user shouldn't have to know what a 'virt driver' is, that is a backend concept that we should be hiding.
Instead this exception should say something like 'Instance recreate is not supported'","Make exception message more friendly

Exceptions that get returned via REST api
should have more user friendly message,
users shouldn't have to know backend concept.
more detail,see bug.

Change-Id: Idc20680e46c36fa3441838177ce61b6f5e1d2979
Closes-Bug: #1263204"
1066,8cd2b890710ba6e53884f43b5d6ce095672732a4,1394753366,,1.0,12,1,2,2,1,0.995727452,True,6.0,3624948.0,64.0,20.0,False,150.0,6030.0,589.0,4.0,32.0,1334.0,1355.0,28.0,1114.0,1131.0,32.0,1038.0,1059.0,0.00434382,0.136764512,0.139528761,485,903,1285259,nova,8cd2b890710ba6e53884f43b5d6ce095672732a4,0,0,Change in requirements,"Bug #1285259 in OpenStack Compute (nova): ""After evacuate origin host still report a runing vm""","After evacuate a host with one instance to a target host it still report there is an instance in that hypervisor
Pre evacuate report:
$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 2     |
| current_workload     | 0     |
| disk_available_least | 22    |
| free_disk_gb         | 50    |
| free_ram_mb          | 3860  |
| local_gb             | 50    |
| local_gb_used        | 0     |
| memory_mb            | 4948  |
| memory_mb_used       | 1088  |
| running_vms          | 1     |
| vcpus                | 3     |
| vcpus_used           | 1     |
+----------------------+-------+
$ nova hypervisor-list
+----+---------------------+
| ID | Hypervisor hostname |
+----+---------------------+
| 1  | jmolle-Controller   |
| 2  | jmolle-Node1        |
+----+---------------------+
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property                  | Value                                                 |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch             | x86_64                                                |
| cpu_info_features         | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model            | Westmere                                              |
| cpu_info_topology_cores   | 1                                                     |
| cpu_info_topology_sockets | 2                                                     |
| cpu_info_topology_threads | 1                                                     |
| cpu_info_vendor           | Intel                                                 |
| current_workload          | 0                                                     |
| disk_available_least      | 10                                                    |
| free_disk_gb              | 25                                                    |
| free_ram_mb               | 3378                                                  |
| host_ip                   | 192.168.41.101                                        |
| hypervisor_hostname       | jmolle-Controller                                     |
| hypervisor_type           | QEMU                                                  |
| hypervisor_version        | 1000000                                               |
| id                        | 1                                                     |
| local_gb                  | 25                                                    |
| local_gb_used             | 0                                                     |
| memory_mb                 | 3954                                                  |
| memory_mb_used            | 576                                                   |
| running_vms               | 1                                                     |
| service_host              | jmolle-Controller                                     |
| service_id                | 4                                                     |
| vcpus                     | 2                                                     |
| vcpus_used                | 1                                                     |
+---------------------------+-------------------------------------------------------+
$ nova hypervisor-servers jmolle-Controller
+--------------------------------------+-------------------+---------------+---------------------+
| ID                                   | Name              | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 1             | jmolle-Controller   |
+--------------------------------------+-------------------+---------------+---------------------+
But after evacuate the instanse we get:
$ nova hypervisor-stats
+----------------------+-------+
| Property             | Value |
+----------------------+-------+
| count                | 2     |
| current_workload     | 1     |
| disk_available_least | 22    |
| free_disk_gb         | 50    |
| free_ram_mb          | 3796  |
| local_gb             | 50    |
| local_gb_used        | 0     |
| memory_mb            | 4948  |
| memory_mb_used       | 1152  |
| running_vms          | 2     |
| vcpus                | 3     |
| vcpus_used           | 2     |
+----------------------+-------+
here we see that now there are 2 running instances instead of one
and if we use show command we get:
$ nova hypervisor-show jmolle-Controller
+---------------------------+-------------------------------------------------------+
| Property                  | Value                                                 |
+---------------------------+-------------------------------------------------------+
| cpu_info_arch             | x86_64                                                |
| cpu_info_features         | [""rdtscp"", ""hypervisor"", ""x2apic"", ""ss"", ""ds"", ""vme""] |
| cpu_info_model            | Westmere                                              |
| cpu_info_topology_cores   | 1                                                     |
| cpu_info_topology_sockets | 2                                                     |
| cpu_info_topology_threads | 1                                                     |
| cpu_info_vendor           | Intel                                                 |
| current_workload          | 0                                                     |
| disk_available_least      | 10                                                    |
| free_disk_gb              | 25                                                    |
| free_ram_mb               | 3378                                                  |
| host_ip                   | 192.168.41.101                                        |
| hypervisor_hostname       | jmolle-Controller                                     |
| hypervisor_type           | QEMU                                                  |
| hypervisor_version        | 1000000                                               |
| id                        | 1                                                     |
| local_gb                  | 25                                                    |
| local_gb_used             | 0                                                     |
| memory_mb                 | 3954                                                  |
| memory_mb_used            | 576                                                   |
| running_vms               | 1                                                     |
| service_host              | jmolle-Controller                                     |
| service_id                | 4                                                     |
| vcpus                     | 2                                                     |
| vcpus_used                | 1                                                     |
+---------------------------+-------------------------------------------------------+
we also see 1 running instance
but if we list servers we get 0
$ nova hypervisor-servers jmolle-Controller
+----+------+---------------+---------------------+
| ID | Name | Hypervisor ID | Hypervisor Hostname |
+----+------+---------------+---------------------+
+----+------+---------------+---------------------+
and the instance was evacuate to the other host correctly
$ nova hypervisor-servers jmolle-Node1
+--------------------------------------+-------------------+---------------+---------------------+
| ID                                   | Name              | Hypervisor ID | Hypervisor Hostname |
+--------------------------------------+-------------------+---------------+---------------------+
| a3c291e5-05b0-43fc-b16b-121bf36f30c0 | instance-00000001 | 2             | jmolle-Node1        |
+--------------------------------------+-------------------+---------------+---------------------+
I think this is a nova bug due to the inconcistency of hypervisor-show and hypervisor-servers nova commands","Not count disabled compute node for statistics

No server will be scheduled to disabled compute service, thus we
should not count the corresponding compute node information.

It's arguable if we should count for 'down' service, since service
may be marked down because of communication error. If we do want to
exclude the down service, we need passing the information from caller
because the up/down state is not kepts in database, and it means compute
and cell api changes.

Closes-Bug: #1285259

Change-Id: I5e3e71ef30683c5eb5cc4462f58fa5f29d7c3f4b"
1067,8ce85e4fd40e9def337daadfdc92aca7b064efff,1399549121,,1.0,41,16,2,2,1,0.856405239,True,1.0,914708.0,19.0,3.0,False,17.0,6377720.0,29.0,1.0,10.0,655.0,662.0,10.0,601.0,608.0,10.0,638.0,645.0,0.006823821,0.396401985,0.400744417,824,1252,1311058,cinder,8ce85e4fd40e9def337daadfdc92aca7b064efff,1,1,,"Bug #1311058 in Cinder: ""CINDER retype doesn't work with the volumes with no volume-type""","""cinder retype""  seems to work perfectly while retyping a volume having a initial volume-type i.e. while volume is created with volume-type.
but it gets stuck at 'retyping'  state while retyping a volume having a volume-type = None (for the volumes created with no volume-type)
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
|                  ID                                                            |   Status    |  Name  | Size   | Volume Type  | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f  | available   | None    |  1       |     None           |  false         |                         |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
openstack@ubuntu:~$ cinder type-list
+------------------------------------------------+---------------+
|                  ID                                                            |          Name     |
+------------------------------------------------+---------------+
| ef8ad765-cea2-4fba-9162-a10d73a6c823  |          abcd        |
+------------------------------------------------+---------------+
openstack@ubuntu:~$ cinder retype  f508dacd-845d-4d09-b69a-6412ec93213f   abcd
openstack@ubuntu:~$ cinder list
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
|                  ID                                                            |   Status    |  Name  | Size   | Volume Type  | Bootable | Attached to |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+
| f508dacd-845d-4d09-b69a-6412ec93213f  | retyping   | None    |  1       |     None           |  false        |                         |
+------------------------------------------------+-----------+--------+------+---------------+-----------+--------------+","Fix retyping volume that has volume type None

Modified volume_types_diff to work when one volume type is None.

Fixed _fix_encryption_specs so that the new dictionary is returned.

Change-Id: Iaba3032317f4d0649c917fb92403c4e9146fe3e9
Closes-Bug: #1311058"
1068,8cf394b896e3644ff51edf6a0d462501fb6e6843,1382886861,,1.0,13,6,2,2,1,0.981940787,True,10.0,14145551.0,189.0,63.0,False,16.0,522015.0,18.0,11.0,3.0,957.0,957.0,3.0,909.0,909.0,2.0,433.0,433.0,0.005976096,0.864541833,0.864541833,33,345,1244860,neutron,8cf394b896e3644ff51edf6a0d462501fb6e6843,1,1,,"Bug #1244860 in neutron: ""Two DHCP ports on same network due to cleanup failure""","On a network, ""neutron port-list --network_id <net-id> --device_owner 'network:dhcp'"" shows there are two ports.  This is checked from the mysql database:
mysql> select * from ports where tenant_id='abcd' and device_owner='network:dhcp' and network_id='7d2e3d47-396d-4867-a2b0-0311465a8454';
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| tenant_id      | id                                   | name | network_id                           | mac_address       | admin_state_up | status | device_id                                                                     | device_owner |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
| abcd | 3d6a7627-6af9-4fb6-9cf6-591c1373d349 |      | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:60:83:3f |              1 | ACTIVE | dhcp4fff1f08-9922-5c44-b6f8-fd9780f48512-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
| abcd | a4c0eb19-407e-4970-90a8-0128259fb048 |      | 7d2e3d47-396d-4867-a2b0-0311465a8454 | fa:16:3e:e1:1b:8f |              1 | ACTIVE | dhcpce80c236-6a89-571d-970b-a1d4bb787827-7d2e3d47-396d-4867-a2b0-0311465a8454 | network:dhcp |
+----------------+--------------------------------------+------+--------------------------------------+-------------------+----------------+--------+-------------------------------------------------------------------------------+--------------+
2 rows in set (0.00 sec)
However, the ""neutron dhcp-agent-list-hosting-net 7d2e3d47-396d-4867-a2b0-0311465a8454 shows only one DHCP-server running.
This problem is observed in an environment with 4 nodes running dhcp-agents.  The neutron API server and the DHCP agents are NOT running on the same node.
What happened is that error occurred when the DHCP server is being ""moved"" from DHCP-agentA running on nodeA to DHCP-agentB running on nodeB.  The sequence is
  neutron dhcp-agent-network-remove <agentA> <net-id> (1)
  neutron dhcp-agent-network-add <agentB> <net-id>  (2)
Right before or during the time step 1 is done, nodeA was rebooted.  So the DHCP-port ws never removed.  When nodeA came back and the DHCP-agent restarted, it didn't do the unplug of the dhcp port device.  THe DHCP agent also failed to make the release_dhcp_port RPC call to the API-server to have the port deleted from mysql.","Delete DHCP port without DHCP server on a net node

A DHCP-network was deleted from one host using neutron
dhcp-agent-network-remove and then added to another host
using neutron dhcp-agent-network-add command. While the
dhcp-agent-network-remove command was in progress, the
host crashed.  As a result, the removal of the DHCP-network
was partially done. The network was disassociated from
the agent in mysql. However, the agent never made the
release_dhcp_port RPC call to delete the port -- even
after the agent restarted. The end result is that there
are two DHCP ports for the same network. One of these
is found on the host that is no longer hosting the
dhcp-server.

This fix make the DHCP agent invoke the release_dhcp_port
RPC call on a stale network whose dnsmasq process is not
running (not active). Before this change, the RPC call is
made on a stale network only when the dnsmasq process is
running.

Closes-Bug: #1244860
Change-Id: Ie0bafdac698810b5455550c306c6a75ddf91d9bb"
1069,8d0abf2db701803eaf11c0a8ff4d759b0da5dc39,1384467607,,1.0,205,184,5,2,1,0.527155278,True,3.0,7609063.0,55.0,20.0,False,2.0,4806442.2,4.0,5.0,15.0,958.0,959.0,15.0,893.0,894.0,11.0,445.0,445.0,0.022099448,0.821362799,0.821362799,1704,1241098,1241098,neutron,8d0abf2db701803eaf11c0a8ff4d759b0da5dc39,1,1, ,"Bug #1241098 in neutron: ""ML2 Cisco Nexus MD: Create pre/post DB event handlers""",Split ML2 cisco nexus event handers for update and delete into precommit (called during DB transactions) and postcommit (called after DB transactions) methods.,"ML2 Cisco Nexus MD: Create pre/post DB event handlers

Split ML2 cisco nexus event handers for update and delete
into precommit (called during DB transactions) and postcommit
(called after DB transactions) methods.

Also fixes some unit tests that were incorrectly accessing
context managers without using the ""with"" statement.

Closes-Bug: #1241098
Change-Id: I59b046342706230222c1be39d13a455ca5a884ea"
1070,8d2b250105564bc3288bf01881dfdc7f48012708,1394975661,,1.0,0,1,1,1,1,0.0,True,1.0,133659.0,18.0,7.0,False,48.0,402284.0,85.0,7.0,389.0,4534.0,4887.0,168.0,3730.0,3874.0,107.0,3312.0,3396.0,0.014182534,0.435062377,0.446093237,627,1053,1292984,nova,8d2b250105564bc3288bf01881dfdc7f48012708,0,0,remove log msg,"Bug #1292984 in OpenStack Compute (nova): ""Log message ""fetching image %s from glance"" is written in wrong place""","In nova.image.glance:
def get_remote_image_service(context, image_href):
    """"""Create an image_service and parse the id from the given image_href.
    The image_href param can be an href of the form
    'http://example.com:9292/v1/images/b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3',
    or just an id such as 'b8b2c6f7-7345-4e2f-afa2-eedaba9cbbe3'. If the
    image_href is a standalone id, then the default image service is returned.
    :param image_href: href that describes the location of an image
    :returns: a tuple of the form (image_service, image_id)
    """"""
    # Calling out to another service may take a while, so lets log this
    LOG.debug(_(""fetching image %s from glance"") % image_href)
    #NOTE(bcwaldon): If image_href doesn't look like a URI, assume its a
    # standalone image ID
    if '/' not in str(image_href):
        image_service = get_default_image_service()
        return image_service, image_href
    try:
        (image_id, glance_host, glance_port, use_ssl) = \
            _parse_image_ref(image_href)
        glance_client = GlanceClientWrapper(context=context,
                host=glance_host, port=glance_port, use_ssl=use_ssl)
    except ValueError:
        raise exception.InvalidImageRef(image_href=image_href)
    image_service = GlanceImageService(client=glance_client)
    return image_service, image_id
Clearly the LOG.debug() message above is incorrect. The method does not fetch an image at all. It just returns an ImageService object.","Remove bad log message in get_remote_image_service

There was a LOG.debug() message in the
nova.image.glance.get_remote_image_service() call that was incorrect and
misleading:

LOG.debug(_(""fetching image %s from glance"") % image_href)

This function actually does not fetch any image from Glance -- rather,
it is just used to get the default image service object or an image
service object for use with a remote Glance server.

Change-Id: I2b71c2c52d6593ad4677767a6f2bb40c399fe9eb
Closes-bug: #1292984"
1071,8d4312a2c9b020cec1241fde0b9a1fb29af5e6f6,1392445497,0.0,1.0,73,73,16,9,1,0.859408655,True,2.0,481582.0,15.0,5.0,False,70.0,250298.1875,277.0,3.0,2.0,437.0,437.0,2.0,376.0,376.0,2.0,391.0,391.0,0.002595156,0.339100346,0.339100346,415,830,1280522,glance,8d4312a2c9b020cec1241fde0b9a1fb29af5e6f6,0,0,tests,"Bug #1280522 in Glance: ""Replace assertEqual(None, *) with assertIsNone in tests""","Replace assertEqual(None, *) with assertIsNone in tests to have
more clear messages in case of failure.","Modify assert statement when comparing with None

Replace assertEqual(None, *) with assertIsNone in glance's
tests to have more clear messages in case of failure.

Also, replace assertTrue(* is not None) with assertIsNotNone
for the same reason.

Change-Id: If41a71bd750057c7136b03bef94c04517092134c
Closes-Bug: #1280522"
1072,8d6e9cf0fc564942dce14ce8f133c5d0ee06c4f4,1404185479,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1005,1447,1334086,cinder,8d6e9cf0fc564942dce14ce8f133c5d0ee06c4f4,1,0,“Commit 971a63bd9cf675a00bce1244ec101577b5c17cac has added a new parameter 'optional_args' to execute method of QuotaReserveTask and QuotaCommitTask. So where those Task used need change to provide the new parameter.”,"Bug #1334086 in Cinder: ""Import existing volume failed due to missing 'optional_args' arguments""","Import existing volume failed with below ERROR:
2014-06-24 13:04:54.407 110362 ERROR oslo.messaging.rpc.dispatcher [req-7aff78e5-92c2-4864-8388-7884068e3360 86341fc3271b4e2da241dd3b603c52f5 0807a39b955048ed88c123c9b3a0485b - - -] Exception during message handling: taskflow.patterns.linear_flow.Flow: volume_manage_existing_manager; 7 requires ['optional_args'] but no other entity produces said requirements
Commit 971a63bd9cf675a00bce1244ec101577b5c17cac has added a new parameter 'optional_args' to execute method of QuotaReserveTask and QuotaCommitTask. So where those Task used need change to provide the new parameter.
This commit has changed volume create task flow as below to provide the new parameter.  We need the samiliar change to get_flow method in cinder\volume\flows\manager\manage_existing.py
diff --git a/cinder/volume/api.py b/cinder/volume/api.py
index 2fd65bf..8e4201c 100644
--- a/cinder/volume/api.py
+++ b/cinder/volume/api.py
@@ -174,6 +174,7 @@ class API(base.Base):
             'scheduler_hints': scheduler_hints,
             'key_manager': self.key_manager,
             'backup_source_volume': backup_source_volume,
+            'optional_args': {'is_quota_committed': False}
         }
Commit comment of b5c17cac for reference:
     Made provision for providing optional arguments
    The 'quota_committed' attribute of 'RequestContext' object is
    a transient property, so it will not be saved in the taskflow
    persistent storage. The updated value of 'quota_committed'
    attribute will not be available while resuming/reverting the
    flow, if cinder api-service is down/stopped after committing
    the quota.
    Since this 'quota_committed' attribute is not used anywhere
    in cinder project other than in create-volume taskflow api, so
    removed 'quota_committed' from RequestContext and made
    provision to pass it as an optional argument which will be
    passed to api-flow via create_what dictionary, in order to
    make it persistent and use it as and when needed.","Add optional_args to fix Volume Import failure

Commit 971a63bd9cf675a00bce1244ec101577b5c17cac added a new
parameter 'optional_args' to execute method of QuotaReserveTask
and QuotaCommitTask. So the new parameter should be needed where
those Tasks are used. This changed volume create task flow to
provide the new parameter. We need the samiliar change to get_flow
method in cinder/volume/flows/manager/manage_existing.py

Trigger EMC VNX CI

Change-Id: Ifed7bd146eec82ba35c2b2c19e9bacd2259ae3ed
Closes-Bug: #1334086"
1073,8d88ee7411d43f148b45d0a145fe32a75765a3ac,1377802255,,1.0,16,1,2,2,1,0.522559375,True,8.0,3460405.0,54.0,20.0,False,12.0,45551.0,13.0,4.0,646.0,888.0,888.0,623.0,826.0,826.0,200.0,219.0,219.0,0.782101167,0.856031128,0.856031128,1495,1218556,1218556,neutron,8d88ee7411d43f148b45d0a145fe32a75765a3ac,1,0,“Some distributions (e.g. openSUSE) have udev rules installed by default” Change environment,"Bug #1218556 in neutron: ""veth pair connecting between physical and integration bridge down after ovs agent restart""","Sometimes after restarting the  openvswitch-agent the veth pair that connects the physical bridge with the integration bridge doesn't come up correctly. (Which of cause disconnects any running VM instance from the network)
# /etc/init.d/openstack-neutron-openvswitch-agent restart
# ip addr show
[..]
83: phy-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether 3a:6c:d6:a4:1c:89 brd ff:ff:ff:ff:ff:ff
84: int-br-eth1: <BROADCAST,MULTICAST> mtu 1500 qdisc pfifo_fast state DOWN qlen 1000
    link/ether a2:12:2a:e5:b8:e4 brd ff:ff:ff:ff:ff:ff
[..]
I was able to reproduce this problem on openSUSE 12.3 and SLES 11. Ubuntu seems to be unaffected by this.
Doing a manual ""ip link set up dev <device>"" on both ends of the veth pair fixes the problem. (until another restarted might bring it back)
I think I was able to track this down to a race condition between udev  (and its network rules) and the ip commands that the openvswitch-agent during startup. Among other things the agent does this during startup:
ip link delete  int-br-fixed
ip link add int-br-fixed type veth peer  name phy-br-fixed
ip link set int-br-fixed up
ip link set phy-br-fixed up
The ip link delete and ip link add command cause several udev events to be fired. However on my system the processing of the udev rules takes so long that the ""remove"" events are not completely processed before the ip link add command is started. Which causes the interface to be down after the above commands completed.
A possible fix for this is to call ""udevadm settle"" after the ip link delete call.
I will upload a draft patch for review shortly.","Avoid race with udev during ovs agent startup

After taking down the veth link between the physical bridge and the integration
bridge call udevadm settle to wait for any udev events to be completely
processed by the operating system before recreating the veth pair.

Some distributions (e.g. openSUSE) have udev rules installed by default that
call e.g. ifdown <interface> during the remove event. If that is processed
after the ovs agent already brought up the veth pair again the veth pair's
link will be down after the agent completed startup and networking will be
broken for all VM instances.

Change-Id: I95520ea96a9804c5261a0c994bbca137535cc37c
Closes-Bug: #1218556"
1074,8dc60e928d35ac29eef62a2d7963059edac55577,1379394690,,1.0,1,1,1,1,1,0.0,True,2.0,145759.0,12.0,4.0,False,5.0,38996.0,5.0,4.0,41.0,532.0,567.0,41.0,527.0,562.0,8.0,498.0,500.0,0.008973081,0.497507478,0.499501496,1560,1226190,1226190,cinder,8dc60e928d35ac29eef62a2d7963059edac55577,1,1, “The correct description should be 'Enable monkey patching’”. Bug in the Comments,"Bug #1226190 in Cinder: ""monkey_patch config option description is wrong""","The ""monkey_patch"" BoolOpt is described as ""Whether to log monkey patching"" but appears to act as ""Enable monkey patching"".
Option definition:  http://git.openstack.org/cgit/openstack/cinder/tree/cinder/common/config.py?id=e34ab12#n189
Use: http://git.openstack.org/cgit/openstack/cinder/tree/cinder/utils.py?id=e34ab12#n593","fix wrong desciption of monkey_patch config

The corrent desciption should be 'Enable monkey patching'

Change-Id: I1ef14be75c1e46fa703bc8da1a68487e16ebd035
Closes-Bug: #1226190"
1075,8e31122d36ce5c9d367696921ed92c50cb062b5f,1405037758,,1.0,9,5,2,1,1,0.985228136,False,,,,,True,,,,,,,,,,,,,,,,,1076,1526,1343750,neutron,8e31122d36ce5c9d367696921ed92c50cb062b5f,1,1,"“ This commit fixes the regression.”,”    The recently merged DVR change (https://review.openstack.org/#/c/102332/)
    assumes that port_id is always a complete uuid “","Bug #1343750 in neutron: ""RYU CI fails with PortNotFound""","recently merged ""RPC additions to support DVR"" change make ofagent CI fail.
https://review.openstack.org/#/c/102332/
http://180.37.183.32/ryuci/32/102332/38/check/check-tempest-dsvm-ofagent/d88f439/logs/screen-q-svc.txt.gz
2014-07-18 03:53:27.765 16087 ERROR oslo.messaging.rpc.dispatcher [req-a13fe10b-aae5-458a-a44f-564563368b56 ] Exception during message handling: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/rpc.py"", line 191, in update_device_up
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     l3plugin.dvr_vmarp_table_update(rpc_context, port_id, ""add"")
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 439, in dvr_vmarp_table_update
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     port_dict = self._core_plugin._get_port(context, port_id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 109, in _get_port
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher     raise n_exc.PortNotFound(port_id=id)
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher PortNotFound: Port 7d526916-5c could not be found
2014-07-18 03:53:27.765 16087 TRACE oslo.messaging.rpc.dispatcher","Fix DVR regression for ofagent

Background:
    ML2 plugin sometimes uses truncated port uuids.
    For example, in the case of ofagent and linuxbridge,
    if port id is 804ceaa1-0e3e-11e4-b537-08606e7f74e7,
    an agent would send ""tap804ceaa1-0e"" to the plugin.
    ML2 plugin's _device_to_port_id() would restore it to
    ""804ceaa1-0e"".  While it's still truncated, ML2 plugin's
    get_port() handles that by using ""startswith"".

The recently merged DVR change (https://review.openstack.org/#/c/102332/)
assumes that port_id is always a complete uuid (it's the case
for openvswitch) and fails to handle the above mentioned case.
This commit fixes the regression.

Change-Id: I9c0845be606969068ab5d13c0165e76760378500
Closes-Bug: #1343750"
1076,8e3dc81837ddb76dff5267ff5fbbd750b20fc31f,1395083208,,1.0,23,8,3,2,1,0.955452426,True,3.0,200548.0,27.0,6.0,False,21.0,1859112.0,54.0,3.0,712.0,3534.0,3902.0,641.0,2749.0,3070.0,700.0,3409.0,3769.0,0.091958547,0.447330447,0.494555949,636,1062,1293750,nova,8e3dc81837ddb76dff5267ff5fbbd750b20fc31f,1,1,Bug. Object doesnt have attribute,"Bug #1293750 in OpenStack Compute (nova): ""Cells: AttributeError: 'dict' object has no attribute 'disable_terminate'""","When a delete is issued for an instance that doesn't have a cell_name in the db, a delete is broadcast to all cells. As that message passes through the cells rpc layer the instance is converted from an object to a dict. This causes a problem when it gets to the cell since the delete methods expect to receive an object.
2014-03-13 15:21:49.717 31063 ERROR nova.cells.messaging [req-d2d5f4ea-4010-405a-b52a-8f19d3991498 10110789 5877036] Error processing message locally: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1291, in _process_message_locally
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1101, in instance_delete_everywhere
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self.compute_api.delete(message.ctxt, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return function(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 216, in _wrapped
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return fn(self, context, instance, *args, **kwargs)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging return f(self, context, instance, *args, **kw)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1710, in delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging self._delete_instance(context, instance)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1700, in _delete_instance
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging task_state=task_states.DELETING)
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging File ""/opt/rackstack/615.4/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 1395, in _delete
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging if instance.disable_terminate:
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'disable_terminate'
2014-03-13 15:21:49.717 31063 TRACE nova.cells.messaging","Cells: Send instance object for instance_delete_everywhere

The delete method in the compute api expects an instance object to be
passed in. So the instance object needs to not be to_primitived in the
instance_delete_everywhere rpc method.

Change-Id: I620ec57bc28ba11335758455c7cac0bfa63f5b37
Closes-bug: #1293750"
1077,8e9f00a19dab98e5cfc7ca32beb9f17ebb5bc1bb,1400214092,1.0,1.0,119,26,5,4,1,0.681352062,False,,,,,True,,,,,,,,,,,,,,,,,578,999,1290486,neutron,8e9f00a19dab98e5cfc7ca32beb9f17ebb5bc1bb,1,1,,"Bug #1290486 in neutron: ""neutron-openvswitch-agent does not recreate flows after ovsdb-server restarts""","The DHCP requests were not being responded to after they were seen on the undercloud network interface.  The neutron services were restarted in an attempt to ensure they had the newest configuration and knew they were supposed to respond to the requests.
Rather than using the heat stack create (called in devtest_overcloud.sh) to test, it was simple to use the following to directly boot a baremetal node.
    nova boot --flavor $(nova flavor-list | grep ""|[[:space:]]*baremetal[[:space:]]*|"" | awk '{print $2}) \
          --image $(nova image-list | grep ""|[[:space:]]*overcloud-control[[:space:]]*|"" | awk '{print $2}') \
          bm-test1
Whilst the baremetal node was attempting to pxe boot a restart of the neutron services was performed.  This allowed the baremetal node to boot.
It has been observed that a neutron restart was needed for each subsequent reboot of the baremetal nodes to succeed.","Reprogram flows when ovs-vswitchd restarts

When OVS is restarted, by default it will not reprogram flows which were
programmed. For the case of the OVS agent, this means a restart will cause
all traffic to be switched using the NORMAL action. This is undesirable for
a number of reasons, including obvious security reasons.

This change provides a way for the agent to check if a restart of ovs-vswitchd
has happened in the main agent loop. If a restart of ovs-vswitchd is detected,
the agent will run through the setup of the bridges on the host and reprogram
flows for all the ports connected.

DocImpact
This changes adds a new table (table 23) to the integration bridge, with a
single 'drop' flow. This is used to monitor OVS restarts and to reprogram
flows from the agent.

Change-Id: If9e07465c43115838de23e12a4e0087c9218cea2
Closes-Bug: #1290486"
1078,8eb142748d9f63a0e4d5902f55bb66281d0df615,1379082628,,1.0,5,1,2,2,1,0.918295834,True,3.0,701561.0,15.0,5.0,False,1.0,4943080.0,1.0,2.0,1.0,739.0,739.0,1.0,707.0,707.0,1.0,266.0,266.0,0.006116208,0.816513761,0.816513761,1555,1224981,1224981,neutron,8eb142748d9f63a0e4d5902f55bb66281d0df615,1,1,“due to a string concatenation bug”,"Bug #1224981 in neutron: ""ncs mechanism driver uses wrong URL on full sync""","The URL used in the ""full sync"" operation of the ML2 mechanism driver is incorrect due to a string concatenation bug.","Fix URL used in NCS mechanism driver sync_full() operation

The URL was incorrect due to a simple argument-passing bug.

Change-Id: I2fabacdb2838022a1d187ceb70cb090c15457b7c
Closes-Bug: #1224981"
1079,8eb31484547b3cfc6c42e06e5d0124501b6339ef,1396661384,,1.0,23,1,4,3,1,0.907472795,True,1.0,291499.0,14.0,3.0,False,75.0,540211.0,225.0,2.0,76.0,860.0,895.0,76.0,557.0,592.0,69.0,623.0,652.0,0.057995029,0.516984258,0.541010771,682,1108,1297358,glance,8eb31484547b3cfc6c42e06e5d0124501b6339ef,0,0,Change in requirements,"Bug #1297358 in Glance: ""Glance v1: HTTP HEAD on /images/detail returns 500""","Attempting a HEAD request on the /images/detail resource results in a 500 response. This should ideally be an HTTP 405 response.
This issue does not occur when doing a HEAD on /images for some reason
curl -i -X HEAD -H ""X-Auth-Token: $AUTH_TOKEN"" -H 'Content-Type: application/json' -H 'User-Agent: python-glanceclient' http://localhost:9292/v1/images/detail
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Tue, 25 Mar 2014 15:18:34 GMT
Connection: close
The traceback for the error can be found here: http://paste.openstack.org/show/74261/","Do not allow HEAD images/detail

Currently, a HEAD images/detail request goes through the images/{id}
route which leads to a 500 response status.
This HEAD method should be handled separetely in order to return a
405 Method Not Allowed response and suggest GET as an allowed method.

Change-Id: Ic83e819189bcff7c4735af476217a9ed1e4f41b8
Closes-Bug: #1297358"
1080,8f0f6ca82fcbec153f992fd2347b6c4fd9a9eb25,1392066188,,1.0,43,8,2,2,1,0.873981048,True,1.0,80786.0,22.0,4.0,False,21.0,617505.0,92.0,3.0,593.0,826.0,1209.0,466.0,734.0,1009.0,242.0,298.0,418.0,0.313548387,0.385806452,0.540645161,387,800,1278581,neutron,8f0f6ca82fcbec153f992fd2347b6c4fd9a9eb25,1,1,It fails. can't reassociate floating ip,"Bug #1278581 in neutron: ""NSX plugin can't reassociate floating ip to different IP on same port""","the NSX plugin is unable to associate a floating IP to a different internal address on the same port where it's currently associated.
How to reproduce:
- create a port with two IPs on the same subnet (just because the fip needs to go through the same router)
- associate a floating IP with IP #1
- associate the same floating IP with IP#2
- FAIL!
This happens with this new test being introduced in tempest: https://review.openstack.org/#/c/71251","NSX plugin: fix floatingip re-association

The NSX plugin does not allow to reassociate a floating IP to
a different internal IP address on the same port where it's
currently associated.
This patch fixes this behaviour and adds a unit test to ensure
re-association on the same port with a different IP is possible.

A few tweaks to the unit test aux functions were necessary to
accomodate the newly introduced unit test.

Change-Id: Iafbc3c54ebc4509ca75155ef138cc6da869df7bd
Closes-Bug: #1278581"
1081,8f112b270af0965a6f9aafbd83976c1ad22314f8,1398285868,,1.0,8,3,2,2,1,0.945660305,True,2.0,141480.0,12.0,4.0,False,48.0,3453631.0,109.0,2.0,18.0,933.0,946.0,18.0,820.0,833.0,10.0,902.0,907.0,0.006913891,0.567567568,0.570710245,827,1256,1311277,cinder,8f112b270af0965a6f9aafbd83976c1ad22314f8,1,1,,"Bug #1311277 in Cinder: ""cinder list with metadata filter doesn't work""","cinder list does not work with --metadata filtered
>cinder list
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| b67cffdd-e573-4cdc-8362-dea6b1fe49a9 | available |    test2     |  1   |     None    |  false   |             |
| cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 | available |     test     |  1   |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
one of them has the readonly metadata attribute setted
 cinder show test
+--------------------------------+--------------------------------------+
|            Property            |                Value                 |
+--------------------------------+--------------------------------------+
|          attachments           |                  []                  |
|       availability_zone        |                 nova                 |
|            bootable            |                false                 |
|           created_at           |      2014-04-21T18:34:40.000000      |
|      display_description       |                 None                 |
|          display_name          |                 test                 |
|           encrypted            |                False                 |
|               id               | cd2880ae-cfd2-4c9e-8e73-2083d3eb3a90 |
|            metadata            |        {u'readonly': u'True'}        |
|     os-vol-host-attr:host      |          jmolle-Controller           |
| os-vol-mig-status-attr:migstat |                 None                 |
| os-vol-mig-status-attr:name_id |                 None                 |
|  os-vol-tenant-attr:tenant_id  |   55088aa5b5054b878b11d765e960c459   |
|              size              |                  1                   |
|          snapshot_id           |                 None                 |
|          source_volid          |                 None                 |
|             status             |              available               |
|          volume_type           |                 None                 |
+--------------------------------+--------------------------------------+
cinder show test2
+--------------------------------+--------------------------------------+
|            Property            |                Value                 |
+--------------------------------+--------------------------------------+
|          attachments           |                  []                  |
|       availability_zone        |                 nova                 |
|            bootable            |                false                 |
|           created_at           |      2014-04-22T16:19:01.000000      |
|      display_description       |                 None                 |
|          display_name          |                test2                 |
|           encrypted            |                False                 |
|               id               | b67cffdd-e573-4cdc-8362-dea6b1fe49a9 |
|            metadata            |                  {}                  |
|     os-vol-host-attr:host      |          jmolle-Controller           |
| os-vol-mig-status-attr:migstat |                 None                 |
| os-vol-mig-status-attr:name_id |                 None                 |
|  os-vol-tenant-attr:tenant_id  |   55088aa5b5054b878b11d765e960c459   |
|              size              |                  1                   |
|          snapshot_id           |                 None                 |
|          source_volid          |                 None                 |
|             status             |              available               |
|          volume_type           |                 None                 |
+--------------------------------+--------------------------------------+
But if I try to get the list with the metadata readonly as True it return that no volume has it setted
cinder list --metadata readonly=True
+----+--------+--------------+------+-------------+----------+-------------+
| ID | Status | Display Name | Size | Volume Type | Bootable | Attached to |
+----+--------+--------------+------+-------------+----------+-------------+
+----+--------+--------------+------+-------------+----------+-------------+
It is spected that volume test is listed (it has the readonly attribute setted)","Cinder list does not filter admin metadata.

For example, if you want to filter by 'readonly' attribute,
you do not get results.
This patch adds the admin metadata for filtering.

Change-Id: Ia2a72776e649abd82160cb13ad63489cc760b190
Closes-Bug: #1311277"
1082,8f1c1b1a3be563db173d8096b59bcfa3964179ed,1389935038,,1.0,3,1,1,1,1,0.0,True,3.0,1075739.0,17.0,6.0,False,30.0,11108252.0,51.0,2.0,2.0,406.0,408.0,2.0,311.0,313.0,0.0,0.0,0.0,0.016393443,0.016393443,0.016393443,1672,1238366,1238366,glance,8f1c1b1a3be563db173d8096b59bcfa3964179ed,1,0,“Keep the date and content of man page update to date”,"Bug #1238366 in Glance: ""Keep the date and content of man page update to date""",See https://github.com/openstack/glance/blob/master/doc/source/man/glanceapi.rst,"Update all the glance manpages

Finish updating the glance manpages:
  - Update and modernize all files and references
  - Add a new manpage for glance-replicator
  - Move the common configuration options to common files for
    easier maintenance and less redundancy

Change-Id: If2cbcc30f2761b187038f39324c7698de4eb2ab9
Closes-Bug: #1238366"
1083,8f299f5545e19b36533742458de43649d41080ed,1393508346,,1.0,4,4,4,3,1,1.0,True,5.0,7217810.0,97.0,34.0,False,81.0,680623.0,251.0,2.0,1.0,2255.0,2256.0,0.0,1972.0,1972.0,0.0,1363.0,1363.0,0.000134012,0.182792817,0.182792817,685,1111,1297685,nova,8f299f5545e19b36533742458de43649d41080ed,1,1,Problem introduced by commit 114109dbf4094ae6b6333d41c84bebf6f85c4e48,"Bug #1297685 in OpenStack Compute (nova): ""Spaces in SSH public key comment breaks cloud-init""","A Nova generated public SSH key contains the comment 'Generated by Nova'. Earlier versions of cloud-init (prior to 0.7.2) can't properly handle spaces in key comments and as a result, fail to disable the root account (if configured to do so).
Earlier Nova versions (Essex and older) didn't have spaces in the comment.
Problem introduced by commit: 114109dbf4094ae6b6333d41c84bebf6f85c4e48
cloud-init bug report: https://bugs.launchpad.net/ubuntu/+source/cloud-init/+bug/1220273","Remove spaces from SSH public key comment

An SSH key generated by Nova contains the comment 'Generated by Nova'. Older
versions (prior to 0.7.2) of cloud-init trip over the spaces in the comment
and as a result of that the key injected into the root account is not disabled
by cloud-init. Yes, it's a cloud-init bug but it's also a regression in the
sense that older OpenStack installations (Essex and older) don't contain
spaces in Nova generated key comments and thus older cloud-init's are not
affected in these environments.

This patch replaces the spaces with dashes, i.e., 'Generated-by-Nova'.

Spaces were introduced by commit: 114109dbf4094ae6b6333d41c84bebf6f85c4e48
Ubuntu cloud images with cloud-init < 0.7.2: 10.04 LTS and 12.04 LTS
cloud-init bug report: https://bugs.launchpad.net/ubuntu/+source/cloud-init/+bug/1220273

Closes-Bug: #1297685
Change-Id: I1761f61dfbba58be98351ae4a51884b03268cf09"
1084,8f8a8a6ffe14cf7bb461dc26d178bf8acc8ddf12,1406649435,,1.0,3,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1118,1573,1349936,cinder,8f8a8a6ffe14cf7bb461dc26d178bf8acc8ddf12,1,1,It performs additional filtering not done in the original solution (63f5798e8795f80f3612fb0699eb668e20ae321c),"Bug #1349936 in Cinder: ""Listing volumes has poor performance (due to glance metadata)""","Listing volumes with a non-admin user has poor performance.
This is caused by the query built to retrieve glance metadata associated to a volume which is sub-optimal: all rows of volume_glance_metadata table are returned.","Fix glance metadata SQL query performance

The query built to retrieve glance metadata associated to a volume
was sub-optimal: all rows of volume_glance_metadata were returned.

Fix it by properly joining the 2 tables with volume_id field.

Closes-bug: #1349936
Change-Id: Ic09414de769e71f8b8f99113838af48d8520e187"
1085,8f8b6e656a6ef06bf0b99068a07a7d194783fc9b,1402412823,,1.0,20,2,2,2,1,0.574635698,False,,,,,True,,,,,,,,,,,,,,,,,955,1393,1327476,nova,8f8b6e656a6ef06bf0b99068a07a7d194783fc9b,1,1,"""Commit 59a6cf233b538d6666740de4796fce25ed8265aa added code to handle
    serializing non-str exception traceback objects, but didn't account for
    unicode. “","Bug #1327476 in OpenStack Compute (nova): ""AttributeError: 'unicode' object has no attribute 'tb_frame'""","Saw this in CI -> http://logs.openstack.org/11/98511/2/check/check-grenade-dsvm/5d60567/logs/new/screen-n-cpu.txt.gz?level=ERROR
2014-06-06 23:00:29.533 ERROR oslo.messaging._drivers.common [req-4ae02289-de3e-4700-968c-7611980e0346 ServerActionsTestXML-67143133 ServerActionsTestXML-1032052660] Returning exception 'unicode' object has no attribute 'tb_frame'
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 602, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 119, in wrapper
    kwargs['exc_tb'] = ''.join(traceback.format_tb(exc_tb))
  File ""/usr/lib/python2.7/traceback.py"", line 76, in format_tb
    return format_list(extract_tb(tb, limit))
  File ""/usr/lib/python2.7/traceback.py"", line 95, in extract_tb
    f = tb.tb_frame
AttributeError: 'unicode' object has no attribute 'tb_frame'
 to caller
2014-06-06 23:00:29.534 ERROR oslo.messaging._drivers.common [req-4ae02289-de3e-4700-968c-7611980e0346 ServerActionsTestXML-67143133 ServerActionsTestXML-1032052660] ['Traceback (most recent call last):\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply\n    incoming.message))\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch\n    return self._do_dispatch(endpoint, method, ctxt, args)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch\n    result = getattr(endpoint, method)(ctxt, **new_args)\n', '  File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped\n    payload)\n', '  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped\n    return f(self, context, *args, **kw)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 286, in decorated_function\n    pass\n', '  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 336, in decorated_function\n    function(self, context, *args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/compute/utils.py"", line 437, in __exit__\n    exc_tb=exc_tb, want_result=False)\n', '  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 121, in wrapper\n    return fn.__get__(None, cls)(*args, **kwargs)\n', '  File ""/opt/stack/new/nova/nova/objects/base.py"", line 144, in wrapper\n    args, kwargs)\n', '  File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 355, in object_class_action\n    objver=objver, args=args, kwargs=kwargs)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call\n    wait_for_reply=True, timeout=timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 89, in _send\n    timeout=timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 386, in send\n    return self._send(target, ctxt, message, wait_for_reply, timeout)\n', '  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 379, in _send\n    raise result\n', 'AttributeError: \'unicode\' object has no attribute \'tb_frame\'\nTraceback (most recent call last):\n\n  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 602, in _object_dispatch\n    return getattr(target, method)(context, *args, **kwargs)\n\n  File ""/opt/stack/new/nova/nova/objects/instance_action.py"", line 119, in wrapper\n    kwargs[\'exc_tb\'] = \'\'.join(traceback.format_tb(exc_tb))\n\n  File ""/usr/lib/python2.7/traceback.py"", line 76, in format_tb\n    return format_list(extract_tb(tb, limit))\n\n  File ""/usr/lib/python2.7/traceback.py"", line 95, in extract_tb\n    f = tb.tb_frame\n\nAttributeError: \'unicode\' object has no attribute \'tb_frame\'\n\n']
2014-06-06 23:01:15.378 ERROR nova.virt.libvirt.driver [req-a0da2719-553b-4654-8c45-215b49ce1d3f ServerActionsTestJSON-1623844969 ServerActionsTestJSON-1569101731] An error occurred while trying to launch a defined domain with xml: <domain type='qemu'>
I feel like this is the remote end of the error?  Or maybe it's an oslo bug, idk.","Handle string types for InstanceActionEvent exc_tb serialization

Commit 59a6cf233b538d6666740de4796fce25ed8265aa added code to handle
serializing non-str exception traceback objects, but didn't account for
unicode.  This change uses six.string_types to handle str and unicode
objects for the exc_tb argument.

Adds a new unit test for the unicode case and firms up two existing
tests for how traceback.format_tb is mocked (or shouldn't be in the case
of exc_tb being a str).

Closes-Bug: #1327476

Change-Id: Icc10b62a3f65610c50e86c0b366c2b70b1a0932d"
1086,8f8be6815e6a7d26165ee6f185821597e56cb740,1384360777,,1.0,47,24,1,1,1,0.0,True,5.0,946078.0,27.0,11.0,False,7.0,5758.0,15.0,3.0,2.0,571.0,571.0,2.0,549.0,549.0,2.0,542.0,542.0,0.002604167,0.471354167,0.471354167,1761,1251126,1251126,cinder,8f8be6815e6a7d26165ee6f185821597e56cb740,0,0,Bug in test files,"Bug #1251126 in Cinder: ""test_get_dss_rp in test_vmware_vmdk.py not testing the positive case as intended""","The test ""test_get_dss_rp"" in test_vmware_vmdk.py is not testing the positive case as intended. Rather, it tests the negative case (no datastores) which is already covered by ""test_get_dss_rp_without_datastores"".","To fix test_get_dss_rp in test_vmware_vmdk.py

The test ""test_get_dss_rp"" in test_vmware_vmdk.py is not testing
the positive case as intended. Rather, it tests the negative case
(no datastores) which is already covered by
""test_get_dss_rp_without_datastores"".

Change-Id: I5b4802e22218217e0338fc721fc0ce1efeaee4f1
Closes-Bug: #1251126"
1087,8f932311da19ea9de7ba1b344484ccdb748f5786,1383309433,,1.0,23,3,2,2,1,0.995727452,True,2.0,11987205.0,24.0,16.0,False,187.0,144678.0,991.0,9.0,225.0,2705.0,2805.0,220.0,2531.0,2627.0,222.0,1804.0,1902.0,0.034307692,0.277692308,0.292769231,1439,1195947,1195947,nova,8f932311da19ea9de7ba1b344484ccdb748f5786,1,1, ,"Bug #1195947 in OpenStack Compute (nova): ""VM re-scheduler mechanism will cause BDM-volumes conflict""","Due to re-scheduler mechanism, when a user tries to
 create (in error) an instance using a volume
 which is already in use by another instance,
the error is correctly detected, but the recovery code
 will incorrectly affect the original instance.
Need to raise exception directly when the situation above occurred.
------------------------
------------------------
We can create VM1 with BDM-volumes (for example, one volume we called it “Vol-1”).
But when the attached-volume (Vol-1..) involved in BDM parameters to create a new VM2, due to VM re-scheduler mechanism, the volume will change to attach on the new VM2 in Nova & Cinder, instead of raise an “InvalidVolume” exception of “Vol-1 is already attached on VM1”.
In actually, Vol-1 both attached on VM1 and VM2 on hypervisor. But when you operate Vol-1 on VM1, you can’t see any corresponding changes on VM2…
I reproduced it and wrote in the doc. Please check the attachment for details~
-------------------------
I checked on the Nova codes, the problem is caused by VM re-scheduler mechanism:
Now Nova will check the state of BDM-volumes from Cinder now [def _setup_block_device_mapping() in manager.py]. If any state is “in-use”, this request will fail, and trigger VM re-scheduler.
According to existing processes in Nova, before VM re-scheduler, it will shutdown VM and detach all BDM-volumes in Cinder for rollback [def _shutdown_instance() in manager.py]. As the result, the state of Vol-1 will change from “in-use” to “available” in Cinder. But, there’re nothing detach-operations on the Nova side…
Therefore, after re-scheduler, it will pass the BDM-volumes checking in creating VM2 on the second time, and all VM1’s BDM-volumes (Vol-1) will be possessed by VM2 and are recorded in Nova & Cinder DB. But Vol-1 is still attached on VM1 on hypervisor, and will also attach on VM2 after VM creation success…
---------------
Moreover, the problem mentioned-above will occur when “delete_on_termination” of BDMs is “False”. If the flag is “True”, all BDM-volumes will be deleted in Cinder because the states are already changed from “in-use” to “available” before [def _cleanup_volumes() in manager.py].
(P.S. Success depends on the specific implementation of Cinder Driver)
Thanks~","Prevent rescheduling on block device failure

Due to a race condition - it is possible for more instances to race for
the same volume. In such a scenario, the one that fails will get
rescheduled, and in the process detach the volume of a successful
instance.

To prevent this, this patch makes nova not reschedule on block device
failures. This is actually reasonable behaviour as block device failures
are rarely related to the compute host itself and so rescheduling is not
usually useful.

This bug does not exist in the new boot code in the manager which will
be used once remove-cast-to-schedule-run-instance bp lands (see
Iefab71047996b7cc08107794d5bc628c11680a70). However, it is now clear
that this will not be merged for Icehouse, so this patch is a
""forward port"" of a patch we already applied to stable/havana.

Closes-bug: #1195947

Change-Id: I6b68965ac65cdb0e1da3b44e83428f056b1693aa"
1088,8fa190c0d4ede0acc8ed4a462a5ebb751380143c,1394691234,1.0,1.0,30,14,2,2,1,0.994030211,True,4.0,1749092.0,85.0,27.0,False,161.0,126169.0,500.0,3.0,59.0,3498.0,3516.0,59.0,2724.0,2742.0,57.0,3373.0,3389.0,0.007644655,0.444708053,0.446816924,606,1030,1291805,nova,8fa190c0d4ede0acc8ed4a462a5ebb751380143c,1,1,Bug after change to list,"Bug #1291805 in OpenStack Compute (nova): ""Don't change list to tuple when get info from libvirt""","In the libvirt.driver, we now use the code like this:
(state, _max_mem, _mem, _cpus, _t) = virt_dom.info()
if the libvirt add new variables in the domain info, the code will be failed.
the error will like this :
 File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 974, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 882, in _init_instance
    drv_state = self._get_power_state(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 990, in _get_power_state
    return self.driver.get_info(instance)[""state""]
  File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3462, in get_info
    (state, max_mem, mem, num_cpu, cpu_time) = virt_dom.info()
ValueError: too many values to unpack","Use the list when get information from libvirt

If the libvirt adds new elements to the domain info struct or others,
when use the new libvirt, there will be a valueError (too many values
to unpack). When we get information from libvirt, we should use the
original variable format. This change is for compatibility with upcoming
versions of libvirt which change this signature.

Change-Id: I0271a260a53fe8f5fc17b2934ebe3a3c9ee0c130
Closes-Bug: #1291805"
1089,8fb023ed53aff31f37ebebb94aca4f8e2a188342,1396412682,,1.0,35,7,5,3,1,0.895174737,True,10.0,3536125.0,157.0,18.0,False,20.0,105378.8,29.0,5.0,6.0,450.0,451.0,6.0,366.0,367.0,6.0,249.0,250.0,0.006529851,0.233208955,0.234141791,712,1138,1300570,neutron,8fb023ed53aff31f37ebebb94aca4f8e2a188342,1,1,"""This problem is brought by the patch:  https://review.openstack.org/#/c/72565/""","Bug #1300570 in neutron: ""dhcp_agent fails in RPC communication with neutron-server under Metaplugin""","This problem occurs when ml2 plugin runs under Metaplugin.
error log of dhcp_agent is as follows:
---
2014-03-28 18:57:17.062 ERROR neutron.agent.dhcp_agent [req-9c53d7a6-d850-42de-896f-184827b33bfd None None] Failed reporting state!
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Traceback (most recent call last):
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/dhcp_agent.py"", line 564, in _report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     self.state_rpc.report_state(ctx, self.agent_state, self.use_call)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/agent/rpc.py"", line 72, in report_state
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     return self.call(context, msg, topic=self.topic)
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent   File ""/opt/stack/neutron/neutron/openstack/common/rpc/proxy.py"", line 129, in call
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent     exc.info, real_topic, msg.get('method'))
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent Timeout: Timeout while waiting on RPC response - topic: ""q-plugin"", RPC method: ""report_state"" info: ""<unknown>""
2014-03-28 18:57:17.062 TRACE neutron.agent.dhcp_agent
---
This problem is brought by the patch:
 https://review.openstack.org/#/c/72565/
because ml2 plguin does not become to open RPC connection at plugin initialization.","Add support for multiple RPC workers under Metaplugin

Metaplugin needs a fix to support multiple RPC workers properly
because a plugin which supports multiple RPC workers cannot
initialize RPC connections at plugin initialization.

Closes-Bug: #1300570
Change-Id: I584f70abb8969054cd4edc8f914d00f6be930bab"
1090,8fb62fe8274ef4a8b7e2a811d4c55999246cdb5f,1410890355,,1.0,40,25,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1326,1794,1370184,nova,8fb62fe8274ef4a8b7e2a811d4c55999246cdb5f,0,0,"Refactoring “is out-of-date, and was’","Bug #1370184 in OpenStack Compute (nova): ""Ironic driver states file out-of-date""","The current ironic states file, nova/virt/ironic/ironic_states.py, is out-of-date, and was recently updated in ironic with this change:
https://review.openstack.org/118467
Ideally, we should keep these in sync to prevent confusion.","Update ironic states and documentation

This adds doc strings for the various states, and updates the documentation
header to reflect the current state of states.

In addition, this removes the following unused states:
  - INIT
  - BUILDING
  - SUSPEND

Change-Id: I917f1819066520a81cfaac1c9e884a1bf359d59c
Closes-Bug: #1370184"
1091,8fb69a4633e87446eadd7f0173a0168f78604cbe,1402494418,,1.0,39,2,2,2,1,0.600608575,False,,,,,True,,,,,,,,,,,,,,,,,947,1383,1326256,neutron,8fb69a4633e87446eadd7f0173a0168f78604cbe,1,1,,"Bug #1326256 in neutron: ""Dnsmasq config files syntax issue when dhcp_domain is empty""","A previous fix as already been done to manage an empty dhcp_domain:
https://bugs.launchpad.net/neutron/+bug/1099625
Hence in case of an empty dhcp_domain  it remains a bug on Dnsmasq config files.
With an empty dhcp_domain Dnsmasq launch command line will be:
 dnsmasq --no-hosts --no-resolv --strict-order --bind-interfaces --interface=tapbdb96782-bb --except-interface=lo --pid-file=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/pid --dhcp-hostsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/host --addn-hosts=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/addn_hosts --dhcp-optsfile=/var/lib/neutron/dhcp/c80662aa-9550-44e6-97f5-a1628b3fca0e/opts --leasefile-ro --dhcp-range=set:tag0,20.0.0.0,static,86400s --dhcp-lease-max=256 --conf-file=
""addn_hosts"" file contains:
20.0.0.3	host-20-0-0-3. host-20-0-0-3
20.0.0.4	host-20-0-0-4. host-20-0-0-4
""host"" file contains:
fa:16:3e:bf:e1:e4,host-20-0-0-3.,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4.,20.0.0.4
=> for both ""addn_hosts"" and ""host"" files the hostname (2nd parameter) is ended with an extra dot char.
(it should be ""host-20-0-0-3"" instead of ""host-20-0-0-3."" )
So generated files should be:
""addn_hosts"" file:
20.0.0.3	host-20-0-0-3 host-20-0-0-3
20.0.0.4	host-20-0-0-4 host-20-0-0-4
""host"" file:
fa:16:3e:bf:e1:e4,host-20-0-0-3,20.0.0.3
fa:16:3e:5f:88:81,host-20-0-0-4,20.0.0.4","Dnsmasq config files syntax issue when dhcp_domain is empty

When using dhcp-agent with the following property dhcp_domain=""""
addn_hosts and host files genererated for Dnsmasq have wrong syntax
concerning hostname parameter (dot char at the end).

As described in RFC-952 the hostname grammar is as follows:
<hname> ::= <name>*["".""<name>]
<name>  ::= <let>[*[<let-or-digit-or-hyphen>]<let-or-digit>]
Hence it can't be conclude with a dot char.

Dnsmasq process is waiting for a hostname (2nd parameter) in the
following files:
""addn_hosts"" file contains:
   20.0.0.3 host-20-0-0-3. host-20-0-0-3
""host"" file contains:
   fa:16:3e:bf:e1:e4,host-20-0-0-3.,20.0.0.3

With the patch you will get:
""addn_hosts"":
   20.0.0.3 host-20-0-0-3 host-20-0-0-3
""host"":
   fa:16:3e:bf:e1:e4,host-20-0-0-3,20.0.0.3

Change-Id: I4c10169019becaed6b2968b74f03ef356244a057
Closes-Bug: #1326256"
1092,8fcd1b2c73dad8e2a7b4d299da270934fd5328cc,1406289340,,1.0,54,28,10,10,1,0.619158378,False,,,,,True,,,,,,,,,,,,,,,,,1105,1557,1348629,nova,8fcd1b2c73dad8e2a7b4d299da270934fd5328cc,1,1,It does not meet req “bogus”,"Bug #1348629 in OpenStack Compute (nova): ""Baremetal driver reports bogus vm_mode of 'baremetal'""","The Baremetal driver reports a 'vm_mode' of 'baremetal' for supported instance types. This is bogus because the baremetal driver is running OS using the native machine ABI, which is represented by vm_mode.HVM","virt: use compute.vm_mode constants and validate vm mode type

Where we have hardcoded vm modes, use compute.vm_mode constants.
Where we get vm modes from external systems, validate them against
the list of acceptable names.

The baremetal/ironic drivers are reporting a bogus vm mode of
'baremetal' for supported instances which is confusing the vm
mode with the Nova driver type. These drivers use the native
ABI for their architecture, so should be reporting 'hvm' as
the vm mode, which indicates an unmodified native OS ABI.

On the other side, the ImagePropertiesFilter will canonicalize
the hvtype it fetches from image metadata, so that 'baremetal'
gets remapped to 'hvm' during comparison.

Closes-bug: #1348629
Change-Id: Ibda7c5ab0759aee672870d84974ccdea25f14407"
1093,8ff06df88b1f979f5a67f9d55fa0e828a6e73d04,1394747336,,1.0,10,2,2,2,1,0.650022422,True,2.0,91222.0,26.0,6.0,False,150.0,314023.0,588.0,3.0,78.0,3198.0,3253.0,77.0,2285.0,2340.0,39.0,2945.0,2961.0,0.00526801,0.387988937,0.390096141,615,1040,1292285,nova,8ff06df88b1f979f5a67f9d55fa0e828a6e73d04,1,0,version sql or related to a special db,"Bug #1292285 in OpenStack Compute (nova): ""equal_any() DB API helper produces incorrect SQL query""","Given an attribute name and a list of values equal_any() is meant to produce a WHERE clause which returns rows for which the column (denoted by an attribute of an SQLAlchemy model) is equal to ANY of passed values that involves using of SQL OR operator. In fact, AND operator is used to combine equality expressions.
E.g. for a model:
class Instance(BaseModel):
    __tablename__ = 'instances'
   id = sa.Column('id', sa.Integer, primary_key=True)
   ...
   task_state = sa.Column('task_state', sa.String(30))
using of equal_any():
  q = model_query(context, Instance).
  constraint = Constraint({'task_state': equal_any('error', 'deleting')})
  q = constraint.apply(Instance, q)
will produce:
SELECT * from instances
WHERE task_state = 'error' AND task_state = 'deleting'
instead of expected:
SELECT * from instances
WHERE task_state = 'error' OR task_state = 'deleting'","Fix equal_any() DB API helper

equal_any() query helper is meant to produce a WHERE clause combining
a few conditions with OR operator in order to select the rows for
which the given column is equal to ANY of passed values. In fact,
it uses AND operator instead of OR, which means that an incorrect
WHERE clause will be returned, if more than one value is passed to
equal_any().

This is caused by the fact that the or_() function from SQLAlchemy
expression language is used incorrectly: it accepts unpacked
instances of BinaryExpression, but a list of ones is passed.
Unfortunately, in 0.7.x and 0.8.x branches of SQLAlchemy passing
a list of values to or_() won't raise an exception immidiately,
and the result will be an iterable, which explains why we didn't
see equal_any() was actually broken before. Besides, equal_any() is
rarely used in our code base and the test didn't cover the case when
more than 1 value is passed to the function.

This effectively fixes the error when running Nova unit tests
with SQLAlchemy 0.9.x releases.

Closes-Bug: #1292285

Change-Id: If4c0f1d3e016e2affb0d49293c62ca9df0d033f7"
1094,8ff170dc95bf3101fe38a2624e941bfa3b7c1138,1400540327,,1.0,41,6,2,2,1,0.607171655,False,,,,,True,,,,,,,,,,,,,,,,,885,1321,1319182,nova,8ff170dc95bf3101fe38a2624e941bfa3b7c1138,1,1,,"Bug #1319182 in OpenStack Compute (nova): ""Pausing a rescued instance should be impossible""","In the following commands, 'vmtest' is a freshly created virtual machine.
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | ACTIVE
$ nova rescue vmtest
+-----------+--------------+
| Property  | Value
+-----------+--------------+
| adminPass | 2ZxvzZULT4sr
+-----------+--------------+
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | RESCUE
$ nova pause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | PAUSED
$ nova unpause vmtest
$ nova show vmtest | grep -E ""(status|task_state)""
| OS-EXT-STS:task_state                | -
| status                               | ACTIVE
Here, we would want the vm to be in the 'RESCUE' state, as it was before being paused.
$ nova unrescue vmtest
ERROR (Conflict): Cannot 'unrescue' while instance is in vm_state active (HTTP 409) (Request-ID: req-34b8004d-b072-4328-bbf9-29152bd4c34f)
The 'unrescue' command fails, which seems to confirm that the VM was no longer being rescued.
So, two possibilities:
1) When unpausing, the vm should go back to 'rescued' state
2) Rescued vms should not be allowed to be paused, as is indicated by this graph: http://docs.openstack.org/developer/nova/devref/vmstates.html
Note that the same issue can be observed with suspend/resume instead of pause/unpause, and probably other commands as well.
WDYT ?","VM in rescue state must have a restricted set of actions

Right now it is possible to pause, suspend and stop a VM in state RESCUED,
so after the state is changed, it's not possible to trigger unrescue anymore
since the original state is lost.

This patch remove vm_states.RESCUED as valid state from stop,
pause and suspend actions.

The vm_states devref is also updated to reflect this change including the
current reboot flow.( vm_states.RESCUED cannot be rebooted as per
today code)

DocImpact
Closes-Bug: #1319182
Co-Authored-By: Cyril Roelandt <cyril.roelandt@enovance.com>
Change-Id: I531dea5a5499bf93c24bea37850d562134dee281"
1095,9014f66fce619af6d72823b3fdb0ecf148582649,1395169913,,1.0,2,11,2,2,1,0.779349837,True,1.0,156641.0,19.0,5.0,False,3.0,573989.0,6.0,4.0,52.0,440.0,481.0,52.0,385.0,426.0,44.0,315.0,349.0,0.04509018,0.316633267,0.350701403,644,1070,1294308,neutron,9014f66fce619af6d72823b3fdb0ecf148582649,1,1,,"Bug #1294308 in neutron: ""ML2 bigswitch driver modifies PortContext.current""","The ML2 bigswitch mechanism driver's create_port_postcommit() and update_port_postcommit() methods pass the PortContext to a _prepare_port_for_controller() method that modifies the port dictionary accessed as PortContext.current. This modified port dictionary is currently returned by ML2 as the result of the port create or update operation, but the changes do not get persisted in the DB. The fix for bug 1276391 is likely to result in these changes no longer being returned to the client. Mechanism drivers are not supposed to modify this port dictionary, except by calling PortContext.set_binding() from within bind_port(). It does not appear that this mechanism driver actually binds ports. If the port dictionary needs to be modified before being passed to the bigswitch controller, it should be copied first.
Also, the TestBigSwitchMechDriverPortsV2.test_port_vif_details() unit test asserts that the returned binding:vif_type is the modified value, so this test will also need to be changed or eliminated as part of this fix.","ML2 BigSwitch: Don't modify parent context

Makes a copy of the port context before changing
it in preparation for the backend controller so
other drivers are not affected. Also removes a
UT that was exercising direct modification of VIF
details in the port context, which is not allowed
by the ML2 plugin.

Closes-Bug: #1294308
Change-Id: I47281dcd23c022813b8b6eda0a3d39c4482277b9"
1096,901d676b8bcbb3e731ba44a4e574e9ae998f486e,1389087418,,1.0,156,39,7,6,1,0.603818284,True,11.0,12094964.0,199.0,39.0,False,15.0,4366755.0,24.0,4.0,54.0,1000.0,1014.0,54.0,910.0,924.0,47.0,521.0,531.0,0.071641791,0.779104478,0.794029851,134,535,1257354,neutron,901d676b8bcbb3e731ba44a4e574e9ae998f486e,1,0,"Since the old L3 mixin has been moved as a service plugin, the metering service plugin doesn't respect anymore…","Bug #1257354 in neutron: ""Metering doesn't anymore respect the l3 agent binding""","Since the old L3 mixin has been moved as a service plugin, the metering service plugin doesn't respect anymore the l3 agent binding. So instead of using the cast rpc method it uses the fanout_cast method.","Fix Metering doesn't respect the l3 agent binding

This patch fix the issue by changing the call to
find the plugin which handles the l3 which is now
the l3_router service plugin instead of the old mixin.

Also change the unit tests to use the l3 service plugin
instead of the l3 mixin and refactor the rpc callbacks
part.

Co-Authored-By: Ala Rezmerita <ala.rezmerita@cloudwatt.com>
Closes-bug: #1257354
Change-Id: Ide26f825005fa63cd3fcc75fa91fffb947e0be7a"
1097,9026db874634a78b8cdd3abb45a75a37f6ededfe,1397092567,,1.0,39,15,3,2,1,0.617694849,True,7.0,2764312.0,28.0,3.0,False,10.0,769575.0,11.0,1.0,12.0,91.0,100.0,12.0,70.0,79.0,4.0,3.0,7.0,0.003184713,0.002547771,0.005095541,778,1205,1306032,cinder,9026db874634a78b8cdd3abb45a75a37f6ededfe,1,0,"""When creating a volume from a snapshot on Windows”","Bug #1306032 in Cinder: ""Volumes created from snapshots on Windows are unaccessible""","When creating a volume from a snapshot on Windows, a shadow copy volume is exported as an iSCSI disk. The issue is that this export is readonly and cannot be mounted to instances. As this export cannot be modified, to make the new volume usable, the solution would be to copy the image to a new path and import it as the desired volume.","Fixes cinder volume from snapshot on Windows

When creating a volume from a snapshot on Windows, a shadow copy
volume is exported as an iSCSI disk. The issue is that this export
is readonly and cannot be mounted to instances.

As this export cannot be modified, to make the new volume usable,
the according image must be moved to a new path and imported .

Closes-Bug: #1306032

Change-Id: I3a936d30fdd7875059dc56c2681f453757c2605f"
1098,903e2a8cd1dd9169048d1ad9dd8a566b2ae52395,1408645570,,1.0,210,58,2,2,1,0.93137384,False,,,,,True,,,,,,,,,,,,,,,,,938,1374,1325184,neutron,903e2a8cd1dd9169048d1ad9dd8a566b2ae52395,0,0,Bug in test,"Bug #1325184 in neutron: ""add unit tests for the ODL MechanismDriver""","All the operations (create, update or delete) haven't been covered by unit tests.
Bug #1324450 about the delete operations would have been caught.","Add unit tests covering single operations to ODL

This commit adds the remaining test cases (create and update
operations) to fully cover sync_single_resource. It also defines the
filter_* methods as static or class methods and removes their duplicate
arguments.

Change-Id: I6b6153ea577bd685576690559b1c389490ee525d
Closes-Bug: #1325184"
1099,903ebc2b71bbfafc29fb42eebbd87ce7e04605f4,1410876480,1.0,1.0,99,15,4,4,1,0.878693829,False,,,,,True,,,,,,,,,,,,,,,,,1322,1790,1369984,nova,903ebc2b71bbfafc29fb42eebbd87ce7e04605f4,1,1,,"Bug #1369984 in OpenStack Compute (nova): ""NUMA topology checking will not check if instance can fit properly.""","When testing weather the instance can fit into the host topology will currently not take into account the number of cells hte instance has, and will only claim matching cells and pass an instance if the matching cells fit.
So for example a 4 NUMA cell isntance would pass the claims test on a 2 NUMA cell host, as long as the first 2 cells fit, without considering that the whole instance will not actually fit.","Fix NUMA fit testing in claims and filter class

This patch makes sure that the overall topology is considered when
checking wather a NUMA instance can fit on a NUMA host, as previously we
would only check the corresponding cells without considering the overall
topology.

It adds a method for checking it on the VirtNUMAHostTopology class and
make sure it's called when checking for placement in the scheduler and
confirming claims on compute hosts.

We also split out the method for getting the topology from a host
as it can be reused in the scheduler now.

Change-Id: I038c81a5241bf1fd2ce37f7eff69f89ecedf59ef
Closes-bug: #1369984"
1100,9047165b7fed691eb89ef7322b7f4fe9b453d184,1401583177,,1.0,54,54,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,939,1375,1325246,nova,9047165b7fed691eb89ef7322b7f4fe9b453d184,0,0,Bug in test,"Bug #1325246 in OpenStack Compute (nova): ""test_objects.test_versions fails with different hash but code is unchanged""","After rebasing with community I'm seeing this failure, but I haven't changed the Network object or any of it's base classes.
======================================================================
FAIL: nova.tests.objects.test_objects.TestObjectVersions.test_versions
tags: worker-0
----------------------------------------------------------------------
Empty attachments:
  stderr
  stdout
pythonlogging:'': {{{
INFO [migrate.versioning.api] 215 -> 216...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 216 -> 217...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 217 -> 218...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 218 -> 219...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 219 -> 220...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 220 -> 221...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 221 -> 222...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 222 -> 223...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 223 -> 224...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 224 -> 225...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 225 -> 226...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 226 -> 227...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 227 -> 228...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 228 -> 229...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 229 -> 230...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 230 -> 231...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 231 -> 232...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 232 -> 233...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 233 -> 234...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 234 -> 235...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 235 -> 236...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 236 -> 237...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 237 -> 238...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 238 -> 239...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 239 -> 240...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 240 -> 241...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 241 -> 242...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 242 -> 243...
INFO [migrate.versioning.api] done
INFO [migrate.versioning.api] 243 -> 244...
INFO [migrate.versioning.api] done
}}}
Traceback (most recent call last):
  File ""nova/tests/objects/test_objects.py"", line 941, in test_versions
    self._test_versions_cls(obj_name)
  File ""nova/tests/objects/test_objects.py"", line 937, in _test_versions_cls
    'has been bumped, and then update this hash') % obj_name)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/opt/stack/nova/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = '1.1-faba26d0290395456f9a040584c4364b'
actual    = '1.1-6d5f3c575cfc4b25db53ee5f071207ce'
: Network object has changed; please make sure the version has been bumped, and then update this hash
======================================================================
FAIL: process-returncode
tags: worker-0
----------------------------------------------------------------------
Binary content:
  traceback (test/plain; charset=""utf8"")
Ran 2 tests in 3.262s (+2.164s)
FAILED (id=100, failures=2)
error: testr failed (1)
Looks like it's also happening in the check queue:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiTmV0d29yayBvYmplY3QgaGFzIGNoYW5nZWQ7IHBsZWFzZSBtYWtlIHN1cmUgdGhlIHZlcnNpb24gaGFzIGJlZW4gYnVtcGVkLCBhbmQgdGhlbiB1cGRhdGUgdGhpcyBoYXNoXCIgQU5EIChidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNiBPUiBidWlsZF9uYW1lOmdhdGUtbm92YS1weXRob24yNykgQU5EIHRhZ3M6Y29uc29sZSIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiMTcyODAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTQwMTU2MjMwMzg3Mn0=
10 hits in 48 hours, all failures, check queue only but different changes.","Remove variability from object change detection unit test

Insure hashes of object representations are consistent by normalizing
data using sorted lists (as compared to dictionaries whose keys and
values are listed in an arbitrary, variable, dependent order:
https://docs.python.org/2/library/stdtypes.html#dict.items).

Closes-Bug: #1325246

Change-Id: I1344a820263683a024818d181ec6ee08e5e18984"
1101,907bf41afbdb9f565c45a535f637c8928d0be52a,1395145999,,1.0,26,16,2,2,1,0.940285959,True,11.0,1948484.0,159.0,35.0,False,4.0,543426.0,4.0,9.0,1109.0,1388.0,1389.0,946.0,1203.0,1204.0,651.0,844.0,845.0,0.655935614,0.850100604,0.85110664,607,1032,1291915,neutron,907bf41afbdb9f565c45a535f637c8928d0be52a,1,1,,"Bug #1291915 in neutron: ""neutron-netns-cleanup script doesn't work in icehouse/havana, code is broken""","1st) Some configuration options are not registered on the tool, but they're used in neutron.agent.linux.dhcp  during execution
$ neutron-netns-cleanup --debug --force --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/dhcp_agent.ini --config-file  /etc/neutron/plugins/ml2/ml2_conf.ini
2014-03-12 14:55:44.791 INFO neutron.common.config [-] Logging enabled!
2014-03-12 14:55:44.792 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list'] from (pid=1785) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-03-12 14:55:45.001 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'list']
Exit code: 0
Stdout: 'qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2\nqrouter-acc5f724-a169-4ffc-9e81-f00d43954509\nqrouter-5ed23337-9538-4994-823f-c64720506e54\n'
Stderr: '' from (pid=1785) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 14:55:47.006 ERROR neutron.agent.linux.dhcp [-] Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
Error importing interface driver 'neutron.agent.linux.interface.OVSInterfaceDriver': no such option: ovs_use_veth
2nd) When we try to destroy a network, there's a dependency on the .namespace attribute of the network, that wasn't before.
Stderr: '' from (pid=1969) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-03-12 15:08:53.048 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-65cb66de-82d0-407c-aa23-2c544528f0d2
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 131, in destroy_namespace
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     kill_dhcp(conf, namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 86, in kill_dhcp
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     dhcp_driver.disable()
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     self.device_manager.destroy(self.network, self.interface_name)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 814, in destroy
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util     self.driver.unplug(device_name, namespace=network.namespace)
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util AttributeError: 'FakeNetwork' object has no attribute 'namespace'
2014-03-12 15:08:53.048 TRACE neutron.agent.netns_cleanup_util
3rd) This error will happen because no plugin rpc connection is provided,
and that's used in /opt/stack/neutron/neutron/agent/linux/dhcp.py as self.plugin.release_dhcp_port
2014-03-13 12:00:07.880 ERROR neutron.agent.netns_cleanup_util [-] Error unable to destroy namespace: qdhcp-388a37af-556d-4f4c-98b4-0ba41f944e32
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util Traceback (most recent call last):
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 132, in destroy_namespace
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     kill_dhcp(conf, namespace)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/netns_cleanup_util.py"", line 87, in kill_dhcp
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     dhcp_driver.disable()
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 181, in disable
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     self.device_manager.destroy(self.network, self.interface_name)
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util   File ""/opt/stack/neutron/neutron/agent/linux/dhcp.py"", line 816, in destroy
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util     self.plugin.release_dhcp_port(network.id,
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util AttributeError: 'NoneType' object has no attribute 'release_dhcp_port'
2014-03-13 12:00:07.880 TRACE neutron.agent.netns_cleanup_util","fixes broken neutron-netns-cleanup

Some configuration parameters used in neutron.agent.linux.utils
were missing. The namespace attribute in the FakeNetwork object
was missing, and used in neutron.agent.linux.dhcp. Also, the
plugin object was missing for release_dhcp_port operation.

We provide a fake plugin object to accept current and any future
plugin calls as this is meant to be an standalone tool that
should work without any RPC connectivity.

FakeNetwork was switched for neutron.agent.linux.dhcp.NetModel
to follow any future changes in NetModel.

Two wrong called_once_with_args calls without assert were fixed.

Change-Id: Ia51ea9bd4c8eea6b250858964ad5286c933702e0
Closes-Bug: #1291915
Partial-Bug: #1297875"
1102,9082273305b0b9c117eb677a0e34c2ffde4e66f0,1410024722,,1.0,222,130,7,7,1,0.55450028,False,,,,,True,,,,,,,,,,,,,,,,,1277,1740,1366371,cinder,9082273305b0b9c117eb677a0e34c2ffde4e66f0,1,1,0,"Bug #1366371 in Cinder: ""Volume type needs to be provided when creating a consistency group""","When creating a consistency group, the scheduler will find a backend that
supports all input volume types. If volume types are not provided, the
default_volume_type in cinder.conf will be used, however, this could cause
inconsistent behavior in a user environment where the default_volume_type
is defined in some places but not in others. This fix removed the use of
default_volume_type for CG creation, added a check to verify that
volume types are provided when creating a CG.
When creating a volume and adding it to a CG, we need to make sure a
volume type is provided as well.","Volume types need to be specified when creating CG

When creating a consistency group, the scheduler will find a backend that
supports all input volume types. If volume types are not provided, the
default_volume_type in cinder.conf will be used, however, this could cause
inconsistent behavior in a user environment where the default_volume_type
is defined in some places but not in others. This fix removed the use of
default_volume_type for CG creation, added a check to verify that volume
types are provided when creating a CG.

When creating a volume and adding it to a CG, we need to make sure a
volume type is provided as well.

Change-Id: I078d4fdd8d92529e853be16272ad74d1e130f712
Closes-Bug: #1366371"
1103,90bbed483618505620346255d0f1f3eeb539fea0,1409767634,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1262,1722,1365065,cinder,90bbed483618505620346255d0f1f3eeb539fea0,1,1,,"Bug #1365065 in Cinder: ""Restore to new volume ends up on the wrong host""","cinder/backup/api.py in the case of a restore to a new volume, creates the volume and then casts the restore request to the backup service. Unfortunately it casts it to the backup host name, not the volume host name, and since the lvm driver uses localpath in the restore code, this means the volume isn't found and the restore fails (lvm, multi-node case only)
i.e. the existing:
        self.backup_rpcapi.restore_backup(context,
                                          backup['host'],
                                          backup['id'],
                                          volume_id)
needs to be:
        self.backup_rpcapi.restore_backup(context,
                                          extract_host_from_volume(volume['host']),
                                          backup['id'],
                                          volume_id)","During a restore send the restore request to the right host

For restore from LVM volumes to work, the restore must be done
on the host that serves the volume, since local_path is used
in the restore code.

Change-Id: Iad1fb5588b8365a0a165754389340bca3f1830b5
Closes-Bug: #1365065"
1104,90fedbe44ca6bfccce5d71465532fbdc85ee3814,1404577074,,1.0,61,22,2,2,1,0.994863108,False,,,,,True,,,,,,,,,,,,,,,,,972,1411,1329546,neutron,90fedbe44ca6bfccce5d71465532fbdc85ee3814,1,1,,"Bug #1329546 in neutron: ""Upon rebuild instances might never get to Active state""","VMware mine sweeper for Neutron (*) recently showed a 100% failure rate on tempest.api.compute.v3.servers.test_server_actions
Logs for two instances of these failures are available at [1] and [2]
The failure manifested as an instance unable to go active after a rebuild.
A bit of instrumentation and log analysis revealed no obvious error on the neutron side - and also that the instance was actually in ""running"" state even if its task state was ""rebuilding/spawning""
N-API logs [3] revealed that the instance spawn was timing out on a missed notification from neutron regarding VIF plug - however the same log showed such notification was received [4]
It turns out that, after rebuild, the instance network cache had still 'active': False for the instance's VIF, even if the status for the corresponding port was 'ACTIVE'. This happened because after the network-vif-plugged event was received, nothing triggered a refresh of the instance network info. For this reason, the VM, after a rebuild, kept waiting for an even which obviously was never sent from neutron.
While this manifested only on mine sweeper - this appears to be a nova bug - manifesting in vmware minesweeper only because of the way the plugin synchronizes with the backend for reporting the operational status of a port.
A simple solution for this problem would be to reload the instance network info cache when network-vif-plugged events are received by nova. (But as the reporter knows nothing about nova this might be a very bad idea as well)
[1] http://208.91.1.172/logs/neutron/98278/2/413209/testr_results.html
[2] http://208.91.1.172/logs/neutron/73234/34/413213/testr_results.html
[3] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=WARNING#_2014-06-06_01_46_36_219
[4] http://208.91.1.172/logs/neutron/73234/34/413213/logs/screen-n-cpu.txt.gz?level=DEBUG#_2014-06-06_01_41_31_767
(*) runs libvirt/KVM + NSX","Do not mark device as processed if it wasn't

Currently treat_devices_added_or_updated in the OVS agent skips
processing devices which disappeared from the integration bridge
during the agent loop.
This is fine, however the agent should not mark these devices as
processed. Otherwise they won't be processed, should they appear
again on the bridge.

This patch ensures these devices are not added to the current
device set.

The patch also changes treat_devices_added_or_updated. The
function now will return the list of skipped devices and not
anymore a flag signalling whether a resync is required.
With the current logic a resync would be required if retrieval
of device details fails. With this change, the function
treat_devices_added_or_updated will raise in this case and the
exception will be handled in process_network_ports.

For the sake of consistency, this patch also updates the
similar function treat_ancillary_devices_added in order to
use the same logic.

Finally, this patch amends an innaccurate related comment.

Closes-Bug: #1329546

Change-Id: Icc744f32494c7a76004ff161536316924594fbdb"
1105,911fe582ec68a1a2df0abb2cb11352c9106842c2,1378794924,,1.0,12,1,2,2,1,0.619382195,True,5.0,2126512.0,36.0,16.0,False,17.0,1129258.0,45.0,6.0,6.0,3565.0,3569.0,6.0,3055.0,3059.0,2.0,2545.0,2545.0,0.000501672,0.425752508,0.425752508,1532,1223198,1223198,nova,911fe582ec68a1a2df0abb2cb11352c9106842c2,1,1,"“in some cases the message is incorrect.""","Bug #1223198 in OpenStack Compute (nova): ""Invalid exception message in create_volume_from_image.""","When the PowerVMFileTransferFailed occurs in
PowerVMLocalVolumeAdapter.create_volume_from_image, in some cases the
message is incorrect.","Fixes the usage of PowerVMFileTransferFailed class

If it fails in the process of ""Calculate file size in multiples of 512
bytes"", the exception.PowerVMFileTransferFailed message is illegal.
""KeyError: u'file_path'"" is occurs when there is no argument.

Change-Id: I15a7467bbc103f683d82949dc0892210cd63c4b6
Closes-Bug: #1223198"
1106,913f45bc16dd2c35f7be772d5867f9208064498a,1395325473,,1.0,9,28,1,1,1,0.0,True,1.0,1316645.0,21.0,5.0,False,6.0,1157602.0,15.0,4.0,1141.0,1308.0,1314.0,976.0,1087.0,1093.0,678.0,731.0,737.0,0.672277228,0.724752475,0.730693069,662,1088,1295448,neutron,913f45bc16dd2c35f7be772d5867f9208064498a,0,0,tests,"Bug #1295448 in neutron: ""Big Switch Restproxy unit test unnecessarily duplicates tests""","The VIF type tests currently have separate classes that all extend the ports test class. This means in addition to testing the VIF changing logic, it's unnecessarily exercising a lot of code that is not impacted by the VIF type.","De-duplicate unit tests for ports in Big Switch

This eliminates the separate classes that existed
for the VIF type testing. Each class was going
through all of the port tests when none of the
logic those tests were exercising was affected by
the VIF types.

This cut the number of tests run in test_restproxy_plugin
by close to 44% without a reduction in code coverage.

Closes-Bug: #1295448
Change-Id: I8c84e1599568fad36b6dd3d51e4ba8a3c012c6c5"
1107,916aec717c2c1a6096e2f1884e4ad81a30ae70e6,1405322326,,1.0,39,15,7,6,1,0.852603212,False,,,,,True,,,,,,,,,,,,,,,,,1062,1509,1341459,nova,916aec717c2c1a6096e2f1884e4ad81a30ae70e6,1,1,,"Bug #1341459 in OpenStack Compute (nova): ""block device isn't resized after swap to larger volume""","libvirt support swap volume. But if the new volume is larger than the old one, the block device isn't resized. The instance can't see the extra space.
$ nova show vm3
+--------------------------------------+----------------------------------------------------------------+
| Property                             | Value                                                          |
+--------------------------------------+----------------------------------------------------------------+
| OS-DCF:diskConfig                    | AUTO                                                           |
| OS-EXT-AZ:availability_zone          | nova                                                           |
| OS-EXT-SRV-ATTR:host                 | os3                                                            |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | os3                                                            |
| OS-EXT-SRV-ATTR:instance_name        | instance-00000039                                              |
| OS-EXT-STS:power_state               | 1                                                              |
| OS-EXT-STS:task_state                | -                                                              |
| OS-EXT-STS:vm_state                  | active                                                         |
| OS-SRV-USG:launched_at               | 2014-07-14T01:43:31.000000                                     |
| OS-SRV-USG:terminated_at             | -                                                              |
| accessIPv4                           |                                                                |
| accessIPv6                           |                                                                |
| config_drive                         |                                                                |
| created                              | 2014-07-14T01:43:23Z                                           |
| flavor                               | m1.nano (42)                                                   |
| hostId                               | c8e8cab21e9e22dbc3779fd171e77f44940ba1c81161dc114ba4ad85       |
| id                                   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e                           |
| image                                | cirros-0.3.2-x86_64-uec (da82a342-aeac-407a-bf9d-cf28bf68dc6b) |
| key_name                             | -                                                              |
| metadata                             | {}                                                             |
| name                                 | vm3                                                            |
| net1 network                         | 10.0.0.66                                                      |
| os-extended-volumes:volumes_attached | [{""id"": ""756d0869-2ef2-4537-90c8-66df9657135f""}]               |
| progress                             | 0                                                              |
| security_groups                      | sg1, default                                                   |
| status                               | ACTIVE                                                         |
| tenant_id                            | fdbb1e8f23eb40c89f3a677e2621b95c                               |
| updated                              | 2014-07-14T06:34:57Z                                           |
| user_id                              | 158d3c971e244f479593c86ff751bf8f                               |
+--------------------------------------+----------------------------------------------------------------+
$ cinder  list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d | available | vol3 |  2   |     None    |  false   |                                      |
| 756d0869-2ef2-4537-90c8-66df9657135f |   in-use  | vol1 |  1   |     None    |  false   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 |  1   |     None    |  false   |                                      |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Then login guest OS:
$ sudo fdisk -l
.....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot      Start         End      Blocks   Id  System
/dev/vdc1            2048     2097151     1047552   83  Linux
Swap the volume to larger one.
$ nova volume-update vm3 756d0869-2ef2-4537-90c8-66df9657135f 13097504-5b0c-4581-b1a5-9e05f616b89d
vm3 attached with the vol3
$ cinder list
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| 13097504-5b0c-4581-b1a5-9e05f616b89d |   in-use  | vol3 |  2   |     None    |  false   | ccda09b7-6c50-40b0-ba7c-0c5c3f0cbb7e |
| 756d0869-2ef2-4537-90c8-66df9657135f | available | vol1 |  1   |     None    |  false   |                                      |
| f0da7609-486d-49b1-bdf3-029bcbe56268 | available | vol2 |  1   |     None    |  false   |                                      |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
Check the guest again:
$ sudo fdisk -l
....
Disk /dev/vdc: 1073 MB, 1073741824 bytes
9 heads, 8 sectors/track, 29127 cylinders, total 2097152 sectors
Units = sectors of 1 * 512 = 512 bytes
Sector size (logical/physical): 512 bytes / 512 bytes
I/O size (minimum/optimal): 512 bytes / 512 bytes
Disk identifier: 0xd6091017
   Device Boot      Start         End      Blocks   Id  System
/dev/vdc1            2048     2097151     1047552   83  Linux
The device size isn't changed.","Resize block device after swap to larger volume

After swap to larger volume, instance's block device should be
resized. Otherwise, the guest can't utilize the extra space.

This patch make the libvirt driver resize block device after mirror
the volume. And add new parameter 'resize_to' to virt driver, that
is used to indicate the new size.

Change-Id: Ib4d65e8812c7d3c28100155124218c75a94e16e7
Closes-Bug: #1341459"
1108,91ddf85abb8a516cfa2da346b393aa7234660f6c,1395446620,,1.0,223,41,5,2,1,0.85503532,True,19.0,1508034.0,179.0,60.0,False,63.0,121554.8,188.0,4.0,1852.0,4302.0,4302.0,1537.0,3495.0,3495.0,952.0,3305.0,3305.0,0.124428777,0.43164904,0.43164904,660,1086,1295381,nova,91ddf85abb8a516cfa2da346b393aa7234660f6c,1,1,Fix the insstance resize bug,"Bug #1295381 in OpenStack Compute (nova): ""VMware: resize operates on orig VM and not clone""","The resize operation when using the VCenter driver ends up resizing the original VM and not the newly cloned VM.
To recreate:
1) create a new VM from horizon using default debian image.  I use a flavor of nano.
2) wait for it to complete and go active
3) click on resize and choose a flavor larger than what you used originally.  i then usually choose a flavor of small.
4) wait for horizon to prompt you to confirm or revert the migration.
5) Switch over to vSphere Web Client.  Notice two VMs for your newly created instance.  One with a UUID name and the other with a UUID-orig name.  ""-orig"" indicating the original.
6) Notice the original has be resized (cpu and mem are increased, disk is not, but that's a separate bug) and not the new clone.  This is problem #1.
7) Now hit confirm in horizon.  It works, but the logs contain a warning: ""The attempted operation cannot be performed in the current state (Powered on)."".  I suspect its attempting to destroy the orig VM, but the orig was the VM resized and powered on, so it fails.  This is problem #2.
Results in a leaked VM.","VMware: Fixes the instance resize problem

The fix includes separating out methods for
associating/disassociating a vsphere vm from the
openstack instance. Modifying the resize workflow
to use the above mentioned methods.

Closes-Bug: #1295381

Change-Id: I92acdd5cd00f739d504738413d3b63a2e17f2866"
1109,9271e300a55f8aa16ff867fff71adadb383849b8,1391579990,1.0,1.0,821,127,9,3,1,0.815452446,True,6.0,295725.0,38.0,12.0,False,107.0,970962.0,305.0,4.0,951.0,2349.0,2349.0,754.0,2026.0,2026.0,889.0,2249.0,2249.0,0.122572648,0.309874673,0.309874673,324,733,1272500,nova,9271e300a55f8aa16ff867fff71adadb383849b8,1,0,newer version,"Bug #1272500 in OpenStack Compute (nova): ""Newer version of SQLite (>=3.7.16) cause some of our tests to fail""","After upgrading my machine to fedora 20 I noticed that some of our tests were failing. Then with a bit more investigation I saw that it was related to the version of the sqlite.
The problem seems to be already fixed in oslo by the review https://review.openstack.org/#/c/61405 . We need to sync the new version of the db.sqlalchemy module from oslo to ironic in order to get it fixed in Ironic as well.
LOG:
(venv)[lucasagomes@lucasagomes ironic]$ rpm -q sqlite
sqlite-3.8.2-1.fc20.x86_64
Example of broken test:
FAIL: ironic.tests.conductor.test_manager.ManagerTestCase.test_start_registers_driver_names
tags: worker-5
----------------------------------------------------------------------
Empty attachments:
  pythonlogging:''
  stdout
stderr: {{{
ironic/openstack/common/db/sqlalchemy/session.py:486: DeprecationWarning: BaseException.message has been deprecated as of Python 2.6
  m = _DUP_KEY_RE_DB[engine_name].match(integrity_error.message)
}}}
Traceback (most recent call last):
  File ""ironic/tests/conductor/test_manager.py"", line 73, in test_start_registers_driver_names
    self.service.start()
  File ""ironic/conductor/manager.py"", line 106, in start
    'drivers': self.drivers})
  File ""ironic/objects/__init__.py"", line 28, in wrapper
    result = fn(*args, **kwargs)
  File ""ironic/db/sqlalchemy/api.py"", line 521, in register_conductor
    conductor.save()
  File ""ironic/openstack/common/db/sqlalchemy/models.py"", line 53, in save
    session.flush()
  File ""ironic/openstack/common/db/sqlalchemy/session.py"", line 551, in _wrap
    raise exception.DBError(e)
DBError: (IntegrityError) UNIQUE constraint failed: conductors.hostname u'INSERT INTO conductors (created_at, updated_at, hostname, drivers) VALUES (?, ?, ?, ?)' ('2014-01-24 19:43:43.362929', '2014-01-24 19:43:43.362563', 'test-host', '[""fake3"", ""fake4""]')","Sync latest db.sqlalchemy from oslo-incubator

The solution to bug 1272500 (Unit tests failing with sqllite 3.8.2-1)
is to use a newer version of db.sqlalchemy from oslo-incubator.
This sync introduces a few incompatible changes that also had to
be fixed for the unit tests to pass:

-_extra_keys in Models must be a property
-Additional utcnow calls were added, which broke some unit tests
that were incorrectly mocking timeutils.utcnow.  timeutils
provides an override method that allows fake times to be returned
in unit tests.

Change-Id: I99685c8a1086fa4e1354f1fa0a715d88d0468dc6
Closes-Bug: #1272500
Co-Authored-By: Ben Nemec <bnemec@redhat.com>"
1110,9283379849906f74047e47a679326e08e923fecc,1403871579,,1.0,25,5,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,981,1420,1330981,nova,9283379849906f74047e47a679326e08e923fecc,1,1,,"Bug #1330981 in OpenStack Compute (nova): ""Cannot attach volumes to LXC instances""","Volumes cannot be attach to any LXC instances
since it's root device cannot be parsed properly.
This later causes a failure in device name generation in get_next_device_name(),
when attempting to generate a name for the attached volume.
The generated device name, will not be recognized by get_dev_prefix_for_disk_bus()
when trying to select a disk bus, nor libvirt will be able to attach the volume
with an unrecognized device name.
When creating a LXC instance, the /dev/nbd1 or /dev/loop0 devices will be saved as
instance root device in _create_domain()
Later, when attaching the volume, block_device.match_device will be called from compute_utils.get_next_device_name(),
The formed device will be named as /dev/na (for /dev/nbdX)
Which will not be recognized in blockinfo.get_dev_prefix_for_disk_bus()
Even if it will be recognized, libvirt wont be able to attach a volume named /dev/na","libvirt: saving the lxc rootfs device in instance metadata

Currently, nbd/loop device, to which lxc image has been
connected, is being saved as instance root device, which causing
the attached bug.
Saving the device name in instance system metadata instead.

Closes-Bug: #1330981
Change-Id: I3a0533a89ea8af61349118a39b822f4af146cc04"
1111,9294de441e684a81f6e802ba0564083f1ad319d6,1408686973,,1.0,7,66,2,1,1,0.993357028,False,,,,,True,,,,,,,,,,,,,,,,,1231,1690,1361366,neutron,9294de441e684a81f6e802ba0564083f1ad319d6,0,0,Bug in test,"Bug #1361366 in neutron: ""all one convergence IPv6 tests have to be skipped explicitly""","The One Convergence plugin doesn't currently support IPv6 so every new IPv6 test has to be explicitly skipped in the plugin's tests. This is a burden to IPv6 developers. As an interim until v6 support is added, a way to skip IPv6 tests by default should be added.","One Convergence: Skip all tests with 'v6' in name

Adds a check to the test setup for the One Convergence tests
to look for v6 in the name of the test and skip it if present.
This prevents IPv6 developers from having to explicitly skip
each new IPv6 test in the One Convergence unit tests until the
plugin gains IPv6 support.

Closes-bug: #1361366
Change-Id: I5c17d7290302597d5c85ef2c1df95f4f71e1a69b"
1112,92ae5ac0df07a68b9c7c8f58740e2d7b2e2959d0,1409907090,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1273,1735,1365901,cinder,92ae5ac0df07a68b9c7c8f58740e2d7b2e2959d0,1,0,“inder-api ran into hang loop in python2.6”,"Bug #1365901 in Cinder: ""cinder-api ran into hang loop in python2.6""","cinder-api ran into hang loop in python2.6
#cinder-api
...
...
snip...
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
Exception RuntimeError: 'maximum recursion depth exceeded in __subclasscheck__' in <type 'exceptions.AttributeError'> ignored
Exception AttributeError: ""'GreenSocket' object has no attribute 'fd'"" in <bound method GreenSocket.__del__ of <eventlet.greenio.GreenSocket object at 0x4e052d0>> ignored
...
...
snip...","remove object in wsgi LOG.info

Method __init__ in Server class records log for wsgi server name, host
and port using its __dict__ which includes a socket object. i18n message
will deep copy each item's value in __dict__. In python2.6, deep copy
the socket object will raise ""Exception RuntimeError"" and can not be
caught. This makes cinder-api run into a hang loop. This patch uses the
related properties instead of __dict__ object to fix the problem.

Closes-Bug: #1365901
Change-Id: Ia6ac51f4849d369c54ac88b1587741a2d2beb40b"
1113,92b4c34e05770e8b3c31459158a018bdd5449c3e,1380083193,3.0,1.0,27,3,2,2,1,0.836640742,True,2.0,183433.0,14.0,3.0,False,10.0,2262072.5,20.0,2.0,333.0,1020.0,1173.0,315.0,915.0,1060.0,81.0,322.0,339.0,0.215789474,0.85,0.894736842,1602,1230184,1230184,neutron,92b4c34e05770e8b3c31459158a018bdd5449c3e,1,1,“Missing allowed addr pairs support in NEC plugin.”,"Bug #1230184 in neutron: ""allowed addr pairs support in nec plugin""","Missing allowed addr pairs support in NEC plugin.
It is needed to support VRRP. I hope it is a part of the release while I understand it is a bit late to the race.
It took some time to investigate the failure in XML security group RPC test (bug 1229954) and bug 1230083 that OVS plugin is not loaded properly.","Allowed Address Pairs support in NEC plugin

Closes-Bug: #1230184
Change-Id: I84d4dda9f88f7bbb524e173b0274db013f69b7e3"
1114,92d92c7fe1d4db4a5c263d4499eae64568765047,1399255544,,1.0,16,14,4,4,1,0.687639683,True,1.0,40035.0,18.0,5.0,False,68.0,767883.0,141.0,4.0,474.0,4373.0,4578.0,470.0,3057.0,3260.0,452.0,3935.0,4119.0,0.057132047,0.4964056,0.519611553,863,1296,1316167,nova,92d92c7fe1d4db4a5c263d4499eae64568765047,0,0,Bug in test,"Bug #1316167 in OpenStack Compute (nova): ""mock.assert_not_called() is not a thing""","We have 14 hits of tests using mock.assert_not_called() which is not a real method for mocks:
nova
nova
tests
api
openstack
test_common.py
395: href_link_mock.assert_not_called()
compute
test_compute_utils.py (2 matches)
764: mock_log.warning.assert_not_called()
775: mock_log.warning.assert_not_called()
image
test_glance.py (10 matches)
548: img.assert_not_called()
556: img.assert_not_called()
690: trans_from_mock.assert_not_called()
708: is_avail_mock.assert_not_called()
709: trans_from_mock.assert_not_called()
754: trans_from_mock.assert_not_called()
791: is_avail_mock.assert_not_called()
792: trans_from_mock.assert_not_called()
848: trans_from_mock.assert_not_called()
920: trans_from_mock.assert_not_called()
virt
baremetal
test_nova_baremetal_deploy_helper.py
339: self.m_mp.assert_not_called()
https://code.google.com/p/mock/issues/detail?id=159
We should be using self.assertFalse(mock.called) instead.","Fix calls to mock.assert_not_called()

There is no assert_not_called() method in mock, as described here:

https://code.google.com/p/mock/issues/detail?id=159

Fix these calls to use the mock.called attribute instead.

Closes-Bug: #1316167

Change-Id: I865463244ebfc45e1f940c74e1afbb9084930bb3"
1115,92e058d6800e1cb33a0359cdc5c079b55612525c,1386754513,,1.0,2,1,1,1,1,0.0,True,1.0,531302.0,20.0,3.0,False,16.0,1105206.0,25.0,3.0,29.0,569.0,588.0,29.0,525.0,544.0,14.0,520.0,524.0,0.012087027,0.419822724,0.423045931,172,574,1259867,cinder,92e058d6800e1cb33a0359cdc5c079b55612525c,1,1,,"Bug #1259867 in Cinder: ""Raise KeyError exception while generating a WSGI response based on the exception""","1. Create snapshot metadata with a too long key('a'*260)
    On the _update_snapshot_metadata() in snapshot_metadata.py, it will raise HTTPRequestEntityTooLarge(413) exception in this case. So  the 413 exception expected to raise.  But we got the server fault(500)
2. The KeyError exception raised while generating a WSGI response based on the exception, because headers no 'Retry-After' attribute in this case.
   if code == 413:
            retry = self.wrapped_exc.headers['Retry-After']
            fault_data[fault_name]['retryAfter'] = retry","Fix KeyError while generating a WSGI response

The KeyError exception raised while generating a WSGI response based on
the 413 exception. Because there may be no ""Retry-After"" attribute in the
exception headers.

Change-Id: I89eb1878acc2195775be17fbcba8088f2e00bac1
Closes-Bug: #1259867"
1116,93012915a3445a8ac8a0b30b702df30febbbb728,1412794160,,1.0,192,92,3,2,1,0.899463729,False,,,,,True,,,,,,,,,,,,,,,,,1389,1859,1378866,neutron,93012915a3445a8ac8a0b30b702df30febbbb728,1,1,,"Bug #1378866 in neutron: ""leftover router ports""","During testing, we've found some instances of leftover router ports.  The ports are not properly cleaned up when a router is removed.  The user is able to manually remove the ports to work around the issue.","Add database relationship between router and ports

Add an explicit schema relationship between a router and its ports. This
change ensures referential integrity among the entities and prevents orphaned
ports.

Change-Id: I09e8a694cdff7f64a642a39b45cbd12422132806
Closes-Bug: #1378866"
1117,9335ffd7c3eaad0d66d7d19e7760ae12476a5ea2,1385040508,1.0,1.0,32,3,2,2,1,0.775512658,True,6.0,2475125.0,39.0,22.0,False,7.0,2248507.0,10.0,7.0,556.0,2321.0,2321.0,518.0,2108.0,2108.0,126.0,527.0,527.0,0.225978648,0.939501779,0.939501779,1780,1252806,1252806,neutron,9335ffd7c3eaad0d66d7d19e7760ae12476a5ea2,1,1, ,"Bug #1252806 in neutron: ""unable to add allow all ingress traffic security group rule""","The following rule is unable to be installed:
$ neutron security-group-rule-create --direction ingress default
409-{u'NeutronError': {u'message': u'Security group rule already exists. Group id is 29dc1837-75d3-457a-8a90-14f4b6ea6db9.', u'type': u'SecurityGroupRuleExists', u'detail': u''}}
The reason for this is when the db query is done it passes this in as a filter:
{'tenant_id': [u'577a2f0c78fb4e36b76902977a5c1708'], 'direction': [u'ingress'], 'ethertype': ['IPv4'], 'security_group_id': [u'0fb10163-81b2-4538-bd11-dbbd3878db51']}
and the remote_group_id is wild carded thus it matches this rule:
[ {'direction': u'ingress',
  'ethertype': u'IPv4',
  'id': u'8d5c3429-f4ef-4258-8140-5ff3247f9dd6',
  'port_range_max': None,
  'port_range_min': None,
  'protocol': None,
  'remote_group_id': None,
  'remote_ip_prefix': None,
  'security_group_id': u'0fb10163-81b2-4538-bd11-dbbd3878db51',
  'tenant_id': u'577a2f0c78fb4e36b76902977a5c1708'}]","Fix unable to add allow all IPv4/6 security group rule

Previously, if one tried to add a rule to allow all ingress ipv4 neutron
would respond that the rule was already part of the security group.
This happened as the filter for querying existing rules uses a wildcard for
remote_group_id thus returning a false match. This patch addresses
this issue.

Change-Id: I0320013a3869d25fb424995354721929465d2848
Closes-bug: #1252806"
1118,933603ed8523493d0693f02f62fef6d427de421f,1386255006,0.0,1.0,107,15,6,2,1,0.71951068,True,25.0,6838086.0,175.0,63.0,False,53.0,235303.3333,133.0,6.0,1537.0,3623.0,4780.0,1350.0,2825.0,3801.0,688.0,3334.0,3657.0,0.101413011,0.490874301,0.53841625,153,555,1258179,nova,933603ed8523493d0693f02f62fef6d427de421f,1,1,Scalability scenario,"Bug #1258179 in OpenStack Compute (nova): ""VMware: timeouts due to nova-compute stuck at 100% when using deploying 100 VMs""","When there are 100's of VM deployed there are problems with nova compute. This is due to the fact that each interaction with the VM;s via get_vm_ref reads all of the VM's ont he system and then filters by the UUID. The filtering is done on the client side.
There are specific API's that optimize this search - http://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.wssdk.apiref.doc%2Fvim.SearchIndex.html more specifically FindAllByUuid","VMware: optimize instance reference access

Fix bug causing nova-compute CPU to spike to 100%.

When there are hundreds of VMs running each time a VM is referenced
all of the VMs in the system will be read by nova-compute and then
filtered according to the UUID.

This is addressed by using an API (FindAllByUuid) which reads only
the specific VM. When a VM is created the config spec will be updated
with the UUID of the VM - that is, the field 'instanceUuid' will be
set. The search is later done on this field.

If the search fails then the old code will be invoked - this ensures
backward compatibility with running VM's. Thus all VM's created
without the 'instanceUuid' set will not be affected.

In addition to optimizing the search we also cache the VM reference.
This ensures that additional calls for the specific VM do not need
to query the backend for the reference.

Change-Id: I00d6c29f46b06d082cf3af0369a69147a3376341
Closes-bug: #1258179"
1119,933a7c01ffa78810b283aa3c3b32d75e4c4504bb,1411509940,,1.0,10,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1372,1842,1376349,cinder,933a7c01ffa78810b283aa3c3b32d75e4c4504bb,0,0,Feature “Warn at driver startup if the configuration option should be set to True”,"Bug #1376349 in Cinder: ""NetApp ESeries driver: warn if 'use_multipath_for_image_xfer'  is not set to True""","ESeries filers require appropriate multipath/DMMP configuration on the host running cinder volume process for volume attach and image transfers to work reliably and efficiently, and the cinder.conf option 'use_multipath_for_image_xfer' should be set to True.
Here we will commit a fix to log a warning on driver startup if Eseries backend is being used and 'use_multipath_for_image_xfer' is not set to True.","Eseries warn if multipath is not set for img xfer

Warn at driver startup if the configuration option
""use_multipath_for_image_xfer"" has not been set to ""True"".  Eseries filers
require appropriate multipath/DMMP configuration on the host running the
cinder volume process for image transfers to work reliably.

Change-Id: Ib79a23a5a9527b254f155a9f4ea022d4387a4a13
Closes-Bug: #1376349"
1120,93bc58765f393536c640494ec129bf39dbdc30c4,1386002516,,1.0,1,1,1,1,1,0.0,True,2.0,132590.0,22.0,10.0,False,64.0,73448.0,144.0,5.0,575.0,1537.0,2064.0,535.0,1165.0,1653.0,171.0,1297.0,1430.0,0.025458851,0.192125518,0.211811723,118,517,1256981,nova,93bc58765f393536c640494ec129bf39dbdc30c4,1,1,Typo in code,"Bug #1256981 in OpenStack Compute (nova): ""network_device_mtu should be IntOpt""",network_device_mtu should be IntOpt,"network_device_mtu should be IntOpt

Change-Id: Id178cde23779c144c6dfd90c9eb1c4767395f143
Closes-bug: #1256981"
1121,93f29574c416aebcd1bd7527d3007664aa073766,1396456623,,1.0,10,19,4,3,1,0.943559844,True,3.0,316204.0,34.0,11.0,False,26.0,3232527.0,39.0,6.0,423.0,3572.0,3768.0,400.0,2942.0,3119.0,415.0,2607.0,2799.0,0.05380238,0.337299534,0.362131402,728,1154,1301515,nova,93f29574c416aebcd1bd7527d3007664aa073766,0,0,"refactoring “Reduce logging in scheduler""","Bug #1301515 in OpenStack Compute (nova): ""reduce logging in the scheduler to improve performance""","The current debug logs in the scheduler are at critical points in the code, and are causing performance issues.
After the DB, the scheduler is spending more time doing logging, than anything else.
This was discovered using the test_performance_check_select_destination unit test, and modifying it to look at when there are around 200 hosts, which is still quite a modest size.","Reduce logging in scheduler

The logging in the scheduler is what takes most of the time,
once you are doing no DB calls.

This ensures the servicegroup is_up and retry_filter only log
on failures, which is more in line with the other filters.

Using the test ""test_performance_check_select_destination""
modifying it to use 200 hosts, the request time went from
70ms to under 10ms.

We could reduce logging further, but this minimal change makes
a massive impact. The next steps would require a config value
to disable all debug logging in the scheduler.

Closes-Bug: #1301515

Change-Id: Ia4c7e506e8df9c560c349e2ebec3f5111aa0cb0d"
1122,94090e8760da324d6586b5d45b7c339457e33647,1382436343,,1.0,29,17,2,2,2,0.666578358,True,5.0,2674845.0,50.0,15.0,False,12.0,1646457.5,23.0,2.0,44.0,512.0,535.0,44.0,446.0,469.0,26.0,505.0,510.0,0.036635007,0.686567164,0.693351425,1616,1232787,1232787,swift,94090e8760da324d6586b5d45b7c339457e33647,1,1, ,"Bug #1232787 in OpenStack Object Storage (swift): ""Bulk Delete should use POST not DELETE""","The DELETE verb applies to a single resource, and doesn't define any semantics for the body.
http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.7
The swift Bulk Delete command affects multiple resources specified in a DELETE body.
http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.bulk
While Bulk Delete is a welcome operation, its usage of DELETE is unusual: affecting multiple resources and relying on reading content.
More typically, such an operation employs POST (or PUT), which folks including api-craft usually agree is the best ""catch-all"" verb for behaviors such as those affecting multiple resources.  That's the TL;DR; of the thread below.
https://groups.google.com/forum/#!searchin/api-craft/Regarding$20Bulk$20actions/api-craft/wY-W1NdZDRs/7YDwMhCR608J
Note that this topic isn't nasal or abstract.  The current behavior is unsupported using the built-in java http client.  Even if third-party libraries can work around this behavior, it is probably best to not be a snowflake wrt http verb semantics where possible!
http://stackoverflow.com/questions/9100776/http-delete-with-request-body-issues","Use POST in bulk-delete

The DELETE verb applies to a single resource, and doesn't define any
semantics for the body.

http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html#sec9.7

The swift Bulk Delete command affects multiple resources specified in a
DELETE body.

http://docs.openstack.org/developer/swift/misc.html#module-swift.common.middleware.bulk

While Bulk Delete is a welcome operation, its usage of DELETE is
unusual: affecting multiple resources and relying on reading content.

More typically, such an operation employs POST (or PUT), which folks
including api-craft usually agree is the best ""catch-all"" verb for
behaviors such as those affecting multiple resources. That's the TL;DR;
of the thread below.

https://groups.google.com/forum/#!searchin/api-craft/Regarding$20Bulk$20actions/api-craft/wY-W1NdZDRs/7YDwMhCR608J

Note that this topic isn't nasal or abstract. The current behavior is
unsupported using the built-in java http client. Even if third-party
libraries can work around this behavior, it is probably best to not be a
snowflake wrt http verb semantics where possible!

http://stackoverflow.com/questions/9100776/http-delete-with-request-body-issues

DocImpact

Closes-Bug: #1232787
Change-Id: I0fc74c85618fe4dd7ff5e7f9756c7f6f67aa0465"
1123,946ab2d6a518501f150a59d86428df1e986c6028,1382968265,,1.0,31,24,3,3,1,0.484547739,True,1.0,2145288.0,10.0,6.0,False,7.0,2939055.0,26.0,6.0,142.0,1033.0,1052.0,142.0,968.0,987.0,101.0,444.0,449.0,0.201980198,0.881188119,0.891089109,1723,1245388,1245388,neutron,946ab2d6a518501f150a59d86428df1e986c6028,0,0,Test files,"Bug #1245388 in neutron: ""LBaaS UT: use constants vs magic numbers for http error codes""",Shouldn't use magic numbers for http error codes in unit tests: use codes from webop.exc like webob.exc.HTTPConflict.code,"LBaaS UT: use constants vs magic numbers for http error codes

Closes-Bug: #1245388
Change-Id: I4192b577182891f6d777a5a5eac237c9961e41bb"
1124,948ff4f3d0a159f1aed9fab65d205ede845b3eb9,1408112130,,1.0,8,13,3,2,1,0.828249471,False,,,,,True,,,,,,,,,,,,,,,,,1203,1661,1357372,nova,948ff4f3d0a159f1aed9fab65d205ede845b3eb9,1,1,“CVE-2014-8750“,"Bug #1357372 in OpenStack Compute (nova): ""[oss-security] [OSSA 2014-035] Nova VMware driver may connect VNC to another tenant's console (CVE-2014-8750)""","When spawning some instances,  nova VMware driver could have a race condition in VNC port allocation. Although the get_vnc_port function has a lock it not guarantee that the whole vnc port allocation process is locked, so another instance could receive the same port if it requests the VNC port before nova has finished the vnc port allocation to another VM.
If the instances with the same VNC port are allocated in same host it could lead to a improper access to the instance console.
Reproduce the problem: Launch  two or more instances at same time. In some cases one instance could execute the get_vnc_port and pick a port but before this instance has finished the _set_vnc_config another instance could execute get_vnc_port and pick the same port.
How often this occurs: unpredictable.","VMware: prevent race condition with VNC port allocation

When spawning some instances, nova VMware driver could have a race condition
in VNC port allocation. This fix ensures that the lock is done on the
actual setting in the VM configuration spec.

Co-authored-by: Marcio Roberto Starke <marcio.starke@serpro.gov.br>

Change-Id: I70fab021bbf2df418df53e5f47e19cf16dbe45ac
Closes-bug: #1357372"
1125,94a3b83f9f1fd52a78b9d49b32ddfae40182f852,1397763925,,1.0,26,6,2,2,1,0.974489403,True,3.0,2975498.0,40.0,12.0,False,212.0,14482.0,1197.0,3.0,1857.0,1090.0,2923.0,902.0,1084.0,1966.0,1740.0,1077.0,2793.0,0.222463583,0.137745975,0.357015078,812,1240,1309154,nova,94a3b83f9f1fd52a78b9d49b32ddfae40182f852,1,1,,"Bug #1309154 in OpenStack Compute (nova): ""Detach Volume Ordering Has Bad User Experience""","Currently when detaching a volume we do:
1. driver.detach() -> detaches the volume in the hypervisor
2. cinder.detach() -> marks the volume detached in cinder
3. bdm.delete() -> stops the nova side from reporting an attached volume.
This leads to bad UX for two reasons:
a. If the cinder detach fails, the bdm still exists so nova reports a volume attached even when the hypervisor
    has detached it.
b. There is a window where cinder reports the volume available but nova still thinks the attachment exists
I propose we reverse the order of 2. and 3.
This reverses a. so that a detach fail will show the volume as attached in cinder but nova will not show it. The nova side more accurately reflects what nova knows about.
This reverses b. so that there is a window where nova has removed the attachment but cinder still reports it in use. This is a fairly minor difference, but leads to a nicer experience when you are detaching and re-attaching the same volume because it can be reattached once it becomes available.
This also may help with https://bugs.launchpad.net/nova/+bug/1172695","Reverse order of cinder.detach() and bdm.delete()

The user experience for detaching a volume is better if the bdm
is destroyed as soon as the driver has detached the volume instead
of waiting until after we have updated cinder.

Change-Id: I6f3de25f7737db270af4ba41d7451ae95f3950f5
Closes-Bug: #1309154"
1126,94c1ad5c0eb0fd574f659a9477b222a7d6c84c4b,1383722795,,1.0,4,2,2,2,1,0.918295834,True,2.0,1658590.0,31.0,7.0,False,20.0,1034562.0,33.0,4.0,4.0,2293.0,2296.0,4.0,1606.0,1609.0,3.0,2149.0,2151.0,0.000613027,0.329501916,0.329808429,64,392,1248424,nova,94c1ad5c0eb0fd574f659a9477b222a7d6c84c4b,1,1,,"Bug #1248424 in OpenStack Compute (nova): ""RequestContext initialization failed in nova""","RequestContext initialization failed in nova because of the following error:
                   ""TypeError: 'in <string>' requires string as left operand, not NoneType""
My operations as follows:
1.Call keystone api to create a service without type.
2.Call keystone api to add a endpoint with the service.
3.Call nova api to list servers.
Then the error TypeError: 'in <string>' requires string as left operand, not NoneType"" has been thrown.","Fixes RequestContext initialization failure.

RequestContext initialization failed in nova because of the following
error:
""TypeError: 'in <string>' requires string as left operand, not NoneType""

It must traverses in tuple not in string when find the ""volume"" service_catalog.

Change-Id: Ie0de50fcafbeabf4698696add610d4e6804f45eb
Closes-Bug: #1248424"
1127,9501fa954b4d724b665207c2a0188e92f691fdfb,1378471728,,1.0,25,2,2,2,1,0.605186577,True,3.0,2730913.0,26.0,7.0,False,29.0,135170.5,67.0,3.0,209.0,3080.0,3245.0,209.0,2508.0,2673.0,121.0,2803.0,2880.0,0.020473234,0.47054875,0.483470381,1525,1221646,1221646,nova,9501fa954b4d724b665207c2a0188e92f691fdfb,1,1,“access_ipv4 should be access_ip_v4”,"Bug #1221646 in OpenStack Compute (nova): ""v3 server's rebuild with access_ip_* doesn't work for xml format""","if node.hasAttribute(""access_ipv4""):
            rebuild[""access_ip_v4""] = node.getAttribute(""access_ip_v4"")
        if node.hasAttribute(""access_ipv6""):
            rebuild[""access_ip_v6""] = node.getAttribute(""access_ip_v6"")
access_ipv4 should be access_ip_v4
We can't merge this patch https://review.openstack.org/#/c/41349/
So we need fix current code.","Fix v3 server rebuild deserializer checking with wrong access_ip key

When rebuild server with xml format, the xml deserializer code
are checking with wrong key. So rebuilding with access_ips doesn't
make any effect. So correct the wrong key 'access_ipv4', 'access_ipv6'
to 'access_ip_v4', 'access_ip_v6'.

Change-Id: I8fccf3727f193bf9142dc814a76a32d5ab1a1b0a
Closes-bug: #1221646"
1128,951bae39e704b04414ebb711b425631cf2d46be5,1392091743,,1.0,52,15,6,6,1,0.865255649,True,9.0,7215766.0,141.0,35.0,False,69.0,218733.0,226.0,3.0,18.0,203.0,212.0,18.0,201.0,210.0,15.0,189.0,195.0,0.00218281,0.025920873,0.026739427,316,725,1271821,nova,951bae39e704b04414ebb711b425631cf2d46be5,0,0,No bug. ‘So we need to add a protection ‘,"Bug #1271821 in OpenStack Compute (nova): ""Target 'host' in 'evacuate' should not be the original server's host""","'Evacuate' function aims to help administrator/operator to evacuate servers if this compute node fails.
So we need to add a protection here, the target host should not be the original host.","Target host in evacuate can't be the original one

'Evacuate' function aims to help administrator/operator to evacuate
servers if this compute node fails.

If no protection, the API returns a successful response,
but the vm_state will be changed to ERROR after nova-compute started,
due to the 'InstanceExists' exception raised from 'rebuild_instance()'.

So we need to add verification here, the target host should not be the
original host.

Change-Id: Ic468cd57688b370a18cacfc6e0844a8201eb9ab3
Closes-Bug: #1271821"
1129,9532e4ed969e47826801b14aff40dd9e356f2677,1381396422,1.0,1.0,20,2,2,2,1,0.574635698,True,2.0,15165511.0,57.0,18.0,False,156.0,506970.0,758.0,7.0,15.0,2261.0,2261.0,15.0,1814.0,1814.0,8.0,2036.0,2036.0,0.001435865,0.324984046,0.324984046,1666,1237868,1237868,nova,9532e4ed969e47826801b14aff40dd9e356f2677,1,1,fix missing host when unshelving”,"Bug #1237868 in OpenStack Compute (nova): ""Fail to suspend a unshelved server""","After unshelved a shelved server(status: ACTIVE), it fails to suspend the server like the following:
$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID                                   | Name | Status            | Task State | Power State | Networks         |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | SHELVED_OFFLOADED | None       | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
$ nova unshelve vm01
$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 152a323d-6309-48ef-9c4a-2b02aaf1a58c | vm01 | ACTIVE | None       | Running     | private=10.0.0.2 |
+--------------------------------------+------+--------+------------+-------------+------------------+
$ nova suspend vm01
ERROR: Unable to process the contained instructions (HTTP 422) (Request-ID: req-5c8edaf3-abb7-4d00-af96-e8a6d9777910)
$","fix missing host when unshelving

An instance has the hostname of the host on which it is running.
The hostname is deleted when shelving. However, the hostname is not
recreated when unshelving.

This patch sets the hostname when unshelving.

Change-Id: I2e431718198321c46b9335e6fb7ab7be3943fab6
Closes-Bug: #1237868"
1130,9569b2fe58d0e836071992f545886ca985d5ace8,1409042591,0.0,1.0,86,30,3,3,1,0.813519129,False,,,,,True,,,,,,,,,,,,,,,,,1234,1693,1361545,neutron,9569b2fe58d0e836071992f545886ca985d5ace8,1,1,"This is tricky! The code works with one configuration, but when there is another one it requires changes. The bug is solved by generalizing the code. If the requirements at the time of writing the original code were just to use that configuration, then there is no BIC. My understanding is that when writing the code, the coder should have been aware, so I am more inclined towards a BIC! (42061fc9cb26558baef52541eb4a02d66575f1c1)","Bug #1361545 in neutron: ""dhcp agent shouldn't spawn metadata-proxy for non-isolated networks""","The ""enable_isolated_metadata = True"" options tells DHCP agents that for each network under its care, a neutron-ns-metadata-proxy process should be spawned, regardless if it's isolated or not.
This is fine for isolated networks (networks with no routers and no default gateways), but for networks which are connected to a router (for which the L3 agent spawns a separate neutron-ns-metadata-proxy which is attached to the router's namespace), 2 different metadata proxies are spawned. For these networks, the static routes which are pushed to each instance, letting it know where to search for the metadata-proxy, is not pushed and the proxy spawned from the DHCP agent is left unused.
The DHCP agent should know if the network it handles is isolated or not, and for non-isolated networks, no neutron-ns-metadata-proxy processes should spawn.","Don't spawn metadata-proxy for non-isolated nets

If the configuation option ""enable_isolated_metadata = True"" for the
DHCP agent is set, the neutron-ns-metadata-proxy process is spawned
for all networks, regardless if they are isolated or not. In case
the network is not isolated (ie. connected to a neutron router), the
L3 agent also spawns a proxy process, and the DHCP's proxy is left
unused. This patch adds a check prior to the spawning of new proxies:
if a network is not isolated, no proxy is spawned.

Change-Id: I9bdb8c3d37997b22435bca33ec47a67db08efa51
Closes-bug: #1361545"
1131,95b3dd1ea114b1460c9d18b8a74bbc44ccf4b066,1405333114,,1.0,5,6,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1857,1341518,1341518,Neutron,95b3dd1ea114b1460c9d18b8a74bbc44ccf4b066,1,1,"""Migration 5446f2a45467_set_server_default try to set incorrect default boolean value on Integer column cisco_network_profiles.""","Bug #1341518 in neutron: ""Incorrect default value on integer column""","Migration 5446f2a45467_set_server_default try to set incorrect default boolean value on Integer column cisco_network_profiles.
http://paste.openstack.org/show/86383/","Fix incorrect default paramater in migration

Migration 5446f2a45467_set_server_default try to set incorrect default
boolean value on Integer column cisco_network_profiles.

Closes-bug: #1341518

Change-Id: I594a6811af23fced7105252f47d72232bf37184c"
1132,95d81a0b03703f47d457f26acecb36d7a10e66cb,1398748273,,1.0,1,1,1,1,1,0.0,True,1.0,3610.0,2.0,,True,,,,,,,,,,,,,,,,,845,1274,1314007,glance,95d81a0b03703f47d457f26acecb36d7a10e66cb,1,1,,"Bug #1314007 in Glance: ""run_tests.sh failed by  ""No module named tools"" exception""","$ ./run_tests.sh
No virtual environment found...create one? (Y/n) y
Traceback (most recent call last):
  File ""tools/install_venv.py"", line 29, in <module>
    from tools import install_venv_common as install_venv
ImportError: No module named tools
Change ""Enable H304 check"" (I099ed65db9b42223eaa4b66a3a5c6113d1cc56fe) causes it.","To fixes import error for run_tests.sh

Change-Id: Iacce978001750c9cb8f398dba78361dee22ed2fb
Closes-bug: #1314007
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
1133,963ad71af4750e28745b6de262da11816b403801,1405547040,,1.0,3,3,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1071,1520,1342919,nova,963ad71af4750e28745b6de262da11816b403801,1,1,,"Bug #1342919 in OpenStack Compute (nova): ""instances rescheduled after building network info do not update the MAC""","This is weird - Ironic has used the mac from a different node (which quite naturally leads to failures to boot!)
nova list | grep spawn
| 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | ci-overcloud-NovaCompute3-zmkjp5aa6vgf  | BUILD  | spawning   | NOSTATE     | ctlplane=10.10.16.137 |
 nova show 6c364f0f-d4a0-44eb-ae37-e012bbdd368c | grep hyperv
 | OS-EXT-SRV-ATTR:hypervisor_hostname  | b07295ee-1c09-484c-9447-10b9efee340c                     |
 neutron port-list | grep 137
 | 272f2413-0309-4e8b-9a6d-9cb6fdbe978d |                    | 78:e7:d1:23:90:0d | {""subnet_id"": ""a6ddb35e-305e-40f1-9450-7befc8e1af47"", ""ip_address"": ""10.10.16.137""} |
ironic node-show b07295ee-1c09-484c-9447-10b9efee340c | grep wait
 | provision_state        | wait call-back                                                         |
ironic port-list | grep 78:e7:d1:23:90:0d  # from neutron
| 33ab97c0-3de9-458a-afb7-8252a981b37a | 78:e7:d1:23:90:0d |
ironic port-show 33ab97c0-3de9-458a-afb7-8252a981
+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | 69dc8c40-dd79-4ed6-83a9-374dcb18c39b                      |  # Ruh-roh, wrong node!
| uuid       | 33ab97c0-3de9-458a-afb7-8252a981b37a                      |
| extra      | {u'vif_port_id': u'aad5ee6b-52a3-4f8b-8029-7b8f40e7b54e'} |
| created_at | 2014-07-08T23:09:16+00:00                                 |
| updated_at | 2014-07-16T01:23:23+00:00                                 |
| address    | 78:e7:d1:23:90:0d                                         |
+------------+-----------------------------------------------------------+
ironic port-list | grep 78:e7:d1:23:9b:1d  # This is the MAC my hardware list says the node should have
| caba5b36-f518-43f2-84ed-0bc516cc89df | 78:e7:d1:23:9b:1d |
# ironic port-show caba5b36-f518-43f2-84ed-0bc516cc
+------------+-----------------------------------------------------------+
| Property   | Value                                                     |
+------------+-----------------------------------------------------------+
| node_uuid  | b07295ee-1c09-484c-9447-10b9efee340c                      |  # and tada right node
| uuid       | caba5b36-f518-43f2-84ed-0bc516cc89df                      |
| extra      | {u'vif_port_id': u'272f2413-0309-4e8b-9a6d-9cb6fdbe978d'} |
| created_at | 2014-07-08T23:08:26+00:00                                 |
| updated_at | 2014-07-16T19:07:56+00:00                                 |
| address    | 78:e7:d1:23:9b:1d                                         |
+------------+-----------------------------------------------------------+","Deallocate the network if rescheduling for Ironic

Ironic (and any other driver that limits the allowed MAC addresses
on an instance will fail if the rescheduled instance runs on a node
that has a different constraint for MAC addresses. To deal with this
we deallocate the network (rather than e.g. trying to lazy update it)
because Nova can't know what the implications of leaving the network
allocated are *when MAC limits are in place* - and we expect them to
be in place when external systems are constrained, so being
conservative is a good idea.

However the code to trigger this looked at dhcp options (which drivers
can safely update pre-boot) not MAC addresses (which can cause conflicts
in Neutron if left in-place).

Change-Id: I59e748db7a943d5a36d75ef63b2a1ec458c58937
Closes-Bug: #1342919"
1134,965542bfac90194bd032e5e6aeb6a507dcb11088,1384296767,2.0,1.0,63,10,2,2,1,0.865693994,True,8.0,7244663.0,276.0,36.0,False,19.0,12062.0,20.0,6.0,38.0,1222.0,1229.0,38.0,1133.0,1140.0,35.0,449.0,456.0,0.067039106,0.837988827,0.851024209,1754,1250644,1250644,neutron,965542bfac90194bd032e5e6aeb6a507dcb11088,1,1, ,"Bug #1250644 in neutron: ""dhcp_release not called causing new VMs to fail DHCP""","I've found some situations where dhcp_release is not called when a port is deleted.  When this happens, dnsmasq refused to give out the IP to a new port when the IP address gets recycled.  The result is that the VM with the new port cannot get its IP address on boot.
There are a few conceivable scenarios that lead to this.  I will attempt to describe some in the comments.","Use information from the dnsmasq hosts file to call dhcp_release

Certain situations can cause the DHCP agent's local cache to get out
of sync with the leases held internally by dnsmasq.  This method of
detecting when to call dhcp_release is idempotent and not dependent on
the cache.  It is more robust.

Change-Id: I4eafd9cfb94a77a2f0229f89de5483dad23725cf
Closes-Bug: #1250644"
1135,965e2c359477b7445b36f84ad1186f541cb831ca,1393881361,,1.0,2,2,2,2,1,1.0,True,2.0,128760.0,9.0,2.0,False,41.0,929974.0,152.0,2.0,61.0,772.0,779.0,61.0,570.0,577.0,55.0,614.0,617.0,0.047457627,0.521186441,0.523728814,499,918,1286375,glance,965e2c359477b7445b36f84ad1186f541cb831ca,0,0,Feature: Add OVA container format ,"Bug #1286375 in Glance: ""Add OVA container format""",An OVA package is a tar archive usually containing an OVF directory inside it. Nova needs to be able to differentiate OVF and OVA based on the container format in order to extract the relevant information from it.,"Add the OVA container format

An OVA package is a tar archive usually containing an OVF directory
inside it. Nova needs to be able to differentiate OVF and OVA based on
the container format in order to extract the relevant information from
it.
This patch adds the OVA container format to the Glance configuration.

Closes-Bug: #1286375

Change-Id: I5034d2943907823a9296e7a5fb41a28f2d92ec5a"
1136,966dbb5368d710e2652b18f5b9cb4295f58598da,1397849546,,1.0,1,1,1,1,1,0.0,True,1.0,516265.0,30.0,10.0,False,14.0,1216614.0,20.0,7.0,317.0,1896.0,1972.0,294.0,1628.0,1683.0,249.0,1012.0,1046.0,0.216076059,0.87554019,0.904926534,811,1239,1309144,neutron,966dbb5368d710e2652b18f5b9cb4295f58598da,1,1,,"Bug #1309144 in neutron: ""TypeError while building routers""","This stacktrace:
2014-04-17 09:37:06.460 10212 ERROR neutron.openstack.common.loopingcall [req-512e4936-b85a-4019-bc2c-e1b8faf4c8c5 None] in dynamic looping call
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall Traceback (most recent call last):
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/openstack/common/loopingcall.py"", line 123, in _inner
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     idle = self.f(*self.args, **self.kw)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 647, in _synchronize_state
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     scan_missing=scan_missing)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/plugins/vmware/common/sync.py"", line 401, in _synchronize_lrouters
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     filters=filters)
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 195, in _get_collection
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     items = [dict_func(c, fields) for c in query]
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall   File ""/opt/stack/neutron/neutron/db/l3_db.py"", line 106, in _make_router_dict
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall     nw_id = router.gw_port['network_id']
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall TypeError: 'NoneType' object has no attribute '__getitem__'
2014-04-17 09:37:06.460 10212 TRACE neutron.openstack.common.loopingcall
has been observed during this run:
http://208.91.1.172/logs/neutron/82709/9/412384/","gw_port should be set as lazy='join'

This forces the object to be ready ahead of accessing time.
The change in loading behavior is also beneficial while listing
routers, as reduces the amount of queries being issued. Tests
show a performance gain of 15-30% for router-list times.

As Mr. Aaron Rosen is used to saying: Woot woot!!

Closes-bug: #1309144

Change-Id: Ibae261b91de26ca7c50e3f6c80e9be2a799785a3"
1137,96e0eb23a458f068f540d98132ee261053c45585,1379385536,,1.0,21,2,2,2,1,0.965636133,True,15.0,906583.0,104.0,42.0,False,17.0,776834.0,19.0,3.0,5.0,1357.0,1360.0,5.0,1230.0,1233.0,3.0,113.0,114.0,0.011627907,0.331395349,0.334302326,1565,1226366,1226366,neutron,96e0eb23a458f068f540d98132ee261053c45585,1,1,,"Bug #1226366 in neutron: ""Raise an exception if starting neutron-l3-agent without creating a router""","Version : Havana
OS : RHEL
Running Neutron services  : neutron-server, neutron-openvswitch-agent, neutron-l3-agent
No routers being created, then the following exception will be raised.
2013-09-16 08:51:32.820 23737 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/common/rpc.py"", line 44, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_rpc_base.py"", line 54, in sync_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     l3plugin.auto_schedule_routers(context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/db/l3_agentschedulers_db.py"", line 241, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self, context, host, router_ids)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/scheduler/l3_agent_scheduler.py"", line 113, in auto_schedule_routers
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     context.session.add(binding)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 449, in __exit__
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self.commit()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 361, in commit
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self._prepare_impl()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/session.py"", line 340, in _prepare_impl
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     self.session.flush()
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 542, in _wrap
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp DBError: (IntegrityError) (1452, 'Cannot add or update a child row: a foreign key constraint fails (`ovs_neutron`.`routerl3agentbindings`, CONSTRAINT `routerl3agentbindings_ibfk_1` FOREIGN KEY (`router_id`) REFERENCES `routers` (`id`) ON DELETE CASCADE)') 'INSERT INTO routerl3agentbindings (id, router_id, l3_agent_id) VALUES (%s, %s, %s)' ('28570d41-e7ab-4ca5-bed5-922bfbc154fc', '', '0214005e-8876-4bb1-a3b5-68f629b6e987')
2013-09-16 08:51:32.820 23737 TRACE neutron.openstack.common.rpc.amqp
The root cause is that the router_ids transferred is defined as [''], which is not None, but have an invalid empty value, the DB can't take this value in.","Raise an exception if no router_id provided

IF both service neutron-l3-agent and neutron-server are up,
but no router id configured in /etc/neutron/l3_agent.ini, an
exception will be raised on DB as ""DBError: IntegrityError"",
because the variable router_ids has a default '' value that
doesn't match the DB grammar.

    * Check router id is specified in _init_() of l3 when
      not using namespace.
    * Move part of checking config params actions to new function
      _check_config_params()
    * Add corresponding unit tests.

Closes-Bug: #1226366

Change-Id: I905f8a4061c5b250782e025681dfefcc41c8c03c"
1138,9710357f246e670bd3cacb153cae245941362c54,1409357193,,1.0,62,12,2,2,1,0.303374836,False,,,,,True,,,,,,,,,,,,,,,,,1391,1861,1378964,cinder,9710357f246e670bd3cacb153cae245941362c54,1,1,,"Bug #1378964 in Cinder: ""GPFS should snap the glance image file only the image format is 'raw'""","GPFS 'copy_on_write' mode works only when creating a volume from an image, with image format being 'raw'.
When the image format is other than 'raw' like for example 'qcow2', the GPFS driver copies the image file to the volume by converting it to 'raw' format.
But, currently during this operation as well, GPFS driver is snap'ing the glance image, making it a clone parent, with no child associated.
Though this does not break any functionality, this is a unnecessary operation and needs to be avoided.","Fix unnecessary snap of glance image, with non-raw images

When the glance image format is not raw, the glance image stored on
GPFS should not be snapped. Because, we copy the image to cinder volume
repository converting the image format to 'raw'. So, no copy_on_write
is supported.

Change-Id: I9cc3ffb2f36b65876b05ecb9f4654461be4516b7
Closes-Bug: #1378964"
1139,9730c185847e1cc3fe0ced10f98a14a112e23a07,1395443840,2.0,1.0,4,67,14,11,1,0.9229703,True,4.0,801452.0,80.0,14.0,False,45.0,397152.5,146.0,6.0,519.0,1300.0,1566.0,420.0,1176.0,1367.0,262.0,697.0,779.0,0.256835938,0.681640625,0.76171875,669,1095,1295887,neutron,9730c185847e1cc3fe0ced10f98a14a112e23a07,0,0,test,"Bug #1295887 in neutron: ""unit test contextmanager: trying to delete resource hides original errors""","In unit tests, resource contextmanagers such as network(), subnet() try to delete themselves after returning from yield even if an   exception occurs. However when an exception occurs, there is a case  where deletion fails. In this case original exception will be hidden and it makes difficult to debug test failures.
Before each time starts, resources like database entries will be recreated, so there is no need to try to delete resources even when an exception occurs.
For example, there is a test with programming error below:
    def test_create_dummy(self):
        with self.network() as network:
            port_res = self._create_port(self.fmt, network['network']['id'])
            port = self.deserialize(self.fmt, port_res)
            # --> Some programming error!!!!
            self.assertEqual(20, hoge)
            self._delete('ports', port['port']['id'])
When running this unit tests, we will get the following error. It is hard to understand.
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 816, in test_create_dummy
    self._delete('ports', port['port']['id'])
  File ""/usr/lib/python2.7/contextlib.py"", line 35, in __exit__
    self.gen.throw(type, value, traceback)
  File ""neutron/tests/unit/test_db_plugin.py"", line 534, in network
    self._delete('networks', network['network']['id'])
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/ubuntu/neutron/.tox/py27/local/lib/python2.7/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 409 != 204
It is better we have the original exception:
Traceback (most recent call last):
  File ""neutron/tests/unit/test_db_plugin.py"", line 809, in test_create_dummy
    self.assertEqual(20, hoge)
NameError: global name 'hoge' is not defined","UT: do not hide an original error in test resource ctxtmgr

In unit tests, resource contextmanagers such as network(), subnet()
try to delete themselves after returning from yield even if an
exception occurs. However when an exception occurs, there is a case
where deletion fails. In this case original exception will be hidden
and it makes difficult to debug test failures.

Before each test starts, resources like database entries will be
recreated, so there is no need to try to delete resources even
when an exception occurs. This commit removes try-finally clause
from resource contextmanagers to make original errors visible.

Closes-Bug: #1295887
Change-Id: Ia844d2aa2c9fc036e643068c5284f64798963ee3"
1140,9795c3e9fdc1f6253f3099414e379367b50587dc,1380187655,,1.0,5,3,1,1,1,0.0,True,38.0,4009597.0,226.0,73.0,False,22.0,3044757.0,28.0,4.0,65.0,1890.0,1928.0,65.0,1633.0,1671.0,64.0,1829.0,1866.0,0.01060705,0.298629243,0.304667102,1472,1212915,1212915,nova,9795c3e9fdc1f6253f3099414e379367b50587dc,0,0,Test files,"Bug #1212915 in OpenStack Compute (nova): ""nova api ""show"" do not fetch error message of vm""","Nova api ""show"" do not fetch the error message of vm in havana.
In /nova/nova/api/openstack/compute/views/servers.py, fault = instance.get(""fault"", None), cannot get the fault message.
I tried instance[""fault""], If the vm in error state, it get the error message.
So i think it's some lazy load problem of the instance object and instance fault.","Fix loading instance fault in servers view

If VM schedule failed, then VM will be set to ERROR state,
but ""nova show"" does not show the fault of the VM.

The fix use lazy load to get instance fault if the instance
went to ERROR state.

Closes-Bug: #1212915

Change-Id: If286fd0176be8307c50b2a7c3534fff33fa5c10f"
1141,979ec03f0be170b7b83fb8c9b6d76c678d8fca66,1395966043,,1.0,1,1,1,1,1,0.0,True,1.0,38654.0,33.0,3.0,False,1.0,877253.0,13.0,3.0,58.0,861.0,910.0,58.0,704.0,753.0,50.0,472.0,515.0,0.048850575,0.453065134,0.494252874,694,1120,1298699,neutron,979ec03f0be170b7b83fb8c9b6d76c678d8fca66,1,1,Wrong order of lines :/,"Bug #1298699 in neutron: ""BigSwitch: Servermanager references possible missing attr""","The server manager module references an attribute of a variable that could either be an HTTPSConnection object or None. If it's None, the attribute reference will fail.","BigSwitch: Move attr ref after error check

Change in Big Switch server manager module:
Moves an attribute reference to a line after
the error check that validates the object is
not None.

Closes-Bug: #1298699
Change-Id: I5f9abf3b456d5066e90c05bc6b3aa5adcecb7943"
1142,97c0723cfe7ffa1dbc6c278110029df755df8045,1394216807,,1.0,12,1,2,2,1,0.779349837,True,1.0,25805.0,16.0,3.0,False,1.0,48936.0,6.0,2.0,38.0,1241.0,1244.0,38.0,1034.0,1037.0,30.0,666.0,669.0,0.033477322,0.720302376,0.723542117,556,977,1289132,neutron,97c0723cfe7ffa1dbc6c278110029df755df8045,1,1,,"Bug #1289132 in neutron: ""BigSwitch: Unfriendly error for server misconfiguration""","If the server port is misconfigured, the plugin dies with an unfriendly error when starting the BigSwitch plugin.
e.g. ValueError: invalid literal for int() with base 10: 'a'","BigSwitch: Fix error for server config check

Raises config error when a port is incorrectly
configured for a server instead of getting a
ValueError when python tries to convert it to
an integer.

Closes-Bug: #1289132
Change-Id: Ifec0bec5640a4a57859f2673c46c610f1d21c36c"
1143,97f81734c030f7f704369b37e1fe122c7040b9a6,1405789452,,1.0,1,61,4,4,1,0.853712847,False,,,,,True,,,,,,,,,,,,,,,,,1075,1525,1343689,nova,97f81734c030f7f704369b37e1fe122c7040b9a6,1,1,"“ Revert ""Add missing image to instance booted from volume""
    This reverts commit c3191cf0ba5ad3dc2df8da2a2bf5c9d270fde9d9.”","Bug #1343689 in OpenStack Compute (nova): ""tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern failing: ""Caught error: 'image_id'""""","Not certain where the bug is yet, but filing here to have a recheck target.
Noticed two separate patches fail gating in different jobs but on the same test (tempest.scenario.test_volume_boot_pattern.TestVolumeBootPattern):
http://logs.openstack.org/30/104730/1/gate/gate-tempest-dsvm-neutron/975e16b/
http://logs.openstack.org/28/102628/4/check/check-grenade-dsvm-partial-ncpu/d2e1673/
Common error in n-api seems to be:
2014-07-17 23:33:31.537 ERROR nova.api.openstack [req-cd950aba-e19c-4c6b-918b-bdfd41d2afce TestVolumeBootPattern-908565911 TestVolumeBootPattern-1581069975] Caught error: 'image_id'
which just started popping up today:
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiQ2F1Z2h0IGVycm9yOiAnaW1hZ2VfaWQnXCIgQU5EIHRhZ3M6XCJzY3JlZW4tbi1hcGkudHh0XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjE0MDU2NDE3OTMwMzl9","Revert ""Add missing image to instance booted from volume""

This reverts commit c3191cf0ba5ad3dc2df8da2a2bf5c9d270fde9d9.

The change needs to be thought out some more, i.e. why isn't
compute_api.get_all just loading up the image_ref value on the instance
object rather than calling a helper method from the servers extension to
the instance object which just calls back to compute_api, seems very
roundabout, plus isn't it a v3 API issue also?

Conflicts:
	nova/api/openstack/compute/servers.py

Due to: 826aed0 Use oslo.i18n

Change-Id: I6abbfdfa786c3d98065c969e7f9d7d5830caf7e8
Closes-Bug: #1343689"
1144,9834639454103ee2ba980628f0bc67ad0baa94ff,1389050767,1.0,1.0,17,5,2,2,1,0.976020648,True,2.0,398750.0,20.0,8.0,False,14.0,22789.0,60.0,3.0,1.0,1353.0,1353.0,1.0,1186.0,1186.0,1.0,1198.0,1198.0,0.001531394,0.918070444,0.918070444,1747,1250249,1250249,cinder,9834639454103ee2ba980628f0bc67ad0baa94ff,1,1, ,"Bug #1250249 in Cinder: ""3PAR: Snapshot stuck in Error_Deleting status""","3PAR Snapshot stuck in Error_Deleting status
Scenario
1. Create a volume vol1
2. Snapshot the volume vol1_snap
3. Create volume from snapshot vol1_snap_to_vol2
4. Try to delete snapshot
Talked to walt, we should be able to recover from this and return status to availalbe after sometime, instead of always being stuck in Error_Deleting status
2013-11-11 15:18:29.135 ERROR cinder.openstack.common.rpc.amqp [req-c8a80e4e-3679-42c6-919e-f82ab06a0970 0d4a18c85ff447d7b0dce7017793ce9c 0982d8dd597b40fb96b67e199137b06d] Exception during message handling
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/utils.py"", line 820, in wrapper
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 425, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     {'status': 'error_deleting'})
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 413, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.driver.delete_snapshot(snapshot_ref)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 154, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.common.delete_snapshot(snapshot)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 982, in delete_snapshot
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     self.client.deleteVolume(snap_name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/client.py"", line 216, in deleteVolume
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     response, body = self.http.delete('/volumes/%s' % name)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 327, in delete
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     return self._cs_request(url, 'DELETE', **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 231, in _cs_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 205, in _time_request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     resp, body = self.request(url, method, **kwargs)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/local/lib/python2.7/dist-packages/hp3parclient/http.py"", line 199, in request
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp     raise exceptions.from_response(resp, body)
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 32 - volume has a child
2013-11-11 15:18:29.135 TRACE cinder.openstack.common.rpc.amqp","3PAR: Raise Ex when del snapshot with depend vol

Deleting a 3PAR snapshot can result in an HTTP Conflict error message,
such as when a volume is dependent on it.
Previously, this exception was not caught, and the state was set
to ""Error_Deleting"" with no recovery path.
The code was updated to catch this exception and raise a
SnapShotIsBusy exception with an appropriate error message.
The state reverts back to 'Available' allowing the user to try again
after removing the dependency.

Change-Id: Ibe8c5d581af10c85397c37993c49f8fc3bce8620
Closes-Bug: #1250249"
1145,984f4192ede99951078e2f6091e4c3e26cdb0e01,1406815151,,1.0,12,5,2,2,1,0.997502546,False,,,,,True,,,,,,,,,,,,,,,,,1131,1586,1350846,nova,984f4192ede99951078e2f6091e4c3e26cdb0e01,1,1,,"Bug #1350846 in OpenStack Compute (nova): ""add/delete fixed ip fails with nova-network""","nova add-fixed-ip <server> <network> fails with following in compute log:
2014-07-31 06:01:48.697 ERROR oslo.messaging.rpc.dispatcher [req-6e04dd42-1ebe-4aa3-a37b-e84bb60b3413 admin demo] Exception during message handling: 'dict' object has no attribute 'get_meta'
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 414, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 327, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 315, in decorated_function
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 3737, in add_fixed_ip_to_instance
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self._inject_network_info(context, instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 4091, in _inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 5383, in inject_network_info
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self.firewall_driver.setup_basic_filtering(instance, nw_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 286, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     self.nwfilter.setup_basic_filtering(instance, network_info)
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/firewall.py"", line 123, in setup_basic_filtering
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher     if subnet.get_meta('dhcp_server'):
2014-07-31 06:01:48.697 TRACE oslo.messaging.rpc.dispatcher AttributeError: 'dict' object has no attribute 'get_meta'
same happens with remove-fixed-ip call","Return hydrated net info from novanet add/remove_fixed_ip calls

Compute manager expects hydrated network info from network_api

Change-Id: I36e48742144555004cbd1f71429a943c7e2952b0
Closes-bug: #1350846"
1146,9869cc1d100ced5f50cf03b557639835c364fbb5,1391840935,,1.0,38,5,2,2,1,0.583019417,True,6.0,3386918.0,108.0,11.0,False,13.0,91257.0,22.0,2.0,2.0,857.0,857.0,2.0,734.0,734.0,2.0,252.0,252.0,0.003921569,0.330718954,0.330718954,500,919,1286412,neutron,9869cc1d100ced5f50cf03b557639835c364fbb5,0,0,'Add support...’,"Bug #1286412 in neutron: ""Add support for router and network scheduling in Cisco N1kv Plugin.""",Added functionality to schedule routers and networks.,"Add support for router scheduling in Cisco N1kv Plugin

Added functionality for router and network scheduling
in Cisco N1kv Plugin.

Change-Id: I1a8d27670d10ca26fb62a8d02230a1aaf3e7bbca
Closes-Bug: #1286412"
1147,98b3f4a95104234e5106bc1ee58efd427da96c00,1395326598,,1.0,23,4,4,3,1,0.803189475,True,4.0,315657.0,58.0,14.0,False,11.0,6435933.5,23.0,4.0,250.0,1194.0,1294.0,245.0,867.0,965.0,189.0,486.0,548.0,0.18793274,0.481701286,0.543026706,658,1084,1295214,neutron,98b3f4a95104234e5106bc1ee58efd427da96c00,0,0,refactor the error message,"Bug #1295214 in neutron: ""LbaaS agent scheduling error exposes implementation""","Right now when there is no active lbaas agent for HAProxy driver (or other agent-based driver), the following error is returned to the client:
""No eligible loadbalancer agent found for pool <pool_id>""
We need to return some more generic error to the user, skipping the notion of agent.
Also, pool will remain in PENDING_CREATE state, while it probably should move to an ERROR state.","Return meaningful error message on pool creation error

Instead of returning exception specific to haproxy (agent-based),
return more generic BackendNotFound exception.
It also could be used later when binding to devices will be
introduced. That exception will indicate scheduling failure, e.g.
inability to find appropriate backend for the resource.
Resource (pool) is then marked with ERROR state with corresponding
error description.

Change-Id: Ic18ff20102b4bb2b97e7b186fcf797133bd3ba3d
Closes-Bug: #1295214"
1148,98e6891dfd4408c56644f55fe3cff88703beb4bf,1402518568,,1.0,151,42,2,2,1,0.683015256,False,,,,,True,,,,,,,,,,,,,,,,,802,1230,1308565,nova,98e6891dfd4408c56644f55fe3cff88703beb4bf,1,1,,"Bug #1308565 in OpenStack Compute (nova): ""Delete does not clean up pending resize files""","If an instance is deleted while a resize operation is in progress (i.e. before the VM state is RESIZED), the temporary files created during the resize operation (e.g. <instance_id>_resize with libvirt) are not cleaned up.  This would seem to be related to bug 1285000, except in that case the temporary files are on a different node.  Ideally a fix would address both.","Check for resize path on libvirt instance delete

If an instance is deleted after the instance's disk image path has
been renamed by adding the ""_resize"" suffix to it but before the
resize operation completes, the libvirt driver will not delete the
orphaned files and manual intervention is needed to get them deleted.

This fix addresses the issue by attempting to rename the instance path
by adding a ""_del"" suffix and if that fails, renaming the instance path
with the ""_resize"" suffix by replacing the ""_resize"" suffix with the
""_del"" suffix.  If both renaming operations fail, the sequence is
repeated, in case the the disk image path initially had the ""_resize""
suffix and another thread removed it before the second rename operation
was attempted. These rename operations are used in favor of checking
for the existence of paths and deleting if found, because rename
operations are atomic whereas another thread could rename the path
between the exist check and the deleting.

Regardless of the outcome of the renaming operations, the existence of
the instance path with the ""_del"" suffix is verified and if it exists,
it is deleted. This is done in case a prior delete operation that
managed to create the ""_del"" path was subsequently interrupted before
all instance files could be deleted.

Note that the LibvirtConnTestCase.test_delete_instance_files test case
was removed in order to eliminate redundancy.

Closes-Bug: #1308565

Change-Id: Ifcb2e18211347ccf3e5472779c5917a729a6eced"
1149,98f5496cc2b236f54848eb069ea1bc8c25f43b13,1400058609,,1.0,42,4,3,3,1,0.530291033,True,1.0,498304.0,12.0,4.0,False,30.0,964087.0,54.0,2.0,17.0,1616.0,1618.0,17.0,1514.0,1516.0,15.0,1585.0,1585.0,0.002005767,0.198821612,0.198821612,767,1194,1304790,nova,98f5496cc2b236f54848eb069ea1bc8c25f43b13,1,1,,"Bug #1304790 in OpenStack Compute (nova): ""flavor is not limit by osapi_max_limit when pagenation""","when flavor pagenation, the conf of osapi_max_limit does not work.so we must modify.","make flavors use common limit and marker

Flavors REST calls were not using the common limit and 
marker function, which means that the osapi_max_limit configuration
that was set by the operator would not be respected. 

Convert flavors to use the common infrastructure thereby providing
a more consistent experience for users and operators.

Change-Id: Iabbf74cf14ddab9a67092ba966270a658ce6be3f
Closes-Bug: #1304790"
1150,98fc828801f370c0561f22d97ffdd1055d4e0663,1395217967,,1.0,54,23,3,1,1,0.623794655,True,2.0,171592.0,29.0,7.0,False,6.0,155659.0,8.0,3.0,246.0,1438.0,1488.0,241.0,1204.0,1253.0,203.0,798.0,827.0,0.203389831,0.796610169,0.82552343,650,1076,1294537,neutron,98fc828801f370c0561f22d97ffdd1055d4e0663,1,1,,"Bug #1294537 in neutron: ""Sync excutils from oslo""",In order to fix undesired error logs in Neutron (bug 1288188) fixed save_and_reraise_exception() should be synced from oslo.,"Sync excutils from oslo

In order to fix undesired error logs in Neutron (bug 1288188)
fixed save_and_reraise_exception() should be synced from oslo.
Oslo commit: 33a2cee6a690ecfdb2cecfe8f01b0b1dacb5bd17

Also sync gettextutils module as excutils depends on it
Latest commit in oslo: fd33d1eaa039913d8c82b94c511a3eab0c3d5789

Closes-Bug: #1294537
Related-Bug: #1288188
Change-Id: I62ab3e4e22aa000f3a8d1be26ef9e1cfb1714959"
1151,9902400039018d77aa3034147cfb24ca4b2353f6,1413231936,,1.0,48,7,2,2,1,0.912115631,False,,,,,True,,,,,,,,,,,,,,,,,1403,1873,1381238,neutron,9902400039018d77aa3034147cfb24ca4b2353f6,1,1,,"Bug #1381238 in neutron: ""Race condition on processing DVR floating IPs""","A race condition can sometimes occur in l-3 agent when a dvr based floatingip is being deleted from one router and another dvr based floatingip is being configured on another router in the same node. Especially if the floatingip being deleted was the last floatingip on the node.  Although fix for Bug # 1373100 [1] eliminated frequent observation of this behavior in upstream tests, it still shows up. Couple of recent examples:
http://logs.openstack.org/88/128288/1/check/check-tempest-dsvm-neutron-dvr/8fdd1de/
http://logs.openstack.org/03/123403/7/check/check-tempest-dsvm-neutron-dvr/859534a/
Relevant log messages:
2014-10-14 16:06:15.803 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:15.838 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', 'addr', 'show', 'rfp-7ed86ca6-b']
Exit code: 0
Stdout: '2: rfp-7ed86ca6-b: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether c6:88:ee:71:a7:51 brd ff:ff:ff:ff:ff:ff\n    inet 169.254.30.212/31 scope global rfp-7ed86ca6-b\n       valid_lft forever preferred_lft forever\n    inet6 fe80::c488:eeff:fe71:a751/64 scope link \n       valid_lft forever preferred_lft forever\n'
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:15.839 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.221 22303 DEBUG neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'fip-82fb2751-30ba-4015-a5da-6c8563064db9', 'ip', 'link', 'del', 'fpr-7ed86ca6-b']
Exit code: 0
Stdout: ''
Stderr: '' execute /opt/stack/new/neutron/neutron/agent/linux/utils.py:81
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.l3_agent [-] DVR: unplug: fg-f04e25ef-e3 _destroy_fip_namespace /opt/stack/new/neutron/neutron/agent/l3_agent.py:679
2014-10-14 16:06:16.222 22303 DEBUG neutron.agent.linux.utils [-] Running command: ['ip', '-o', 'link', 'show', 'br-ex'] create_process /opt/stack/new/neutron/neutron/agent/linux/utils.py:46
2014-10-14 16:06:16.251 22303 ERROR neutron.agent.linux.utils [-]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'exec', 'qrouter-7ed86ca6-b42d-4ba9-8899-447ff0509174', 'ip', '-4', 'addr', 'add', '172.24.4.91/32', 'brd', '172.24.4.91', 'scope', 'global', 'dev', 'rfp-7ed86ca6-b']
Exit code: 1
Stdout: ''
Stderr: 'Cannot find device ""rfp-7ed86ca6-b""\n'
[1] https://bugs.launchpad.net/neutron/+bug/1373100","Fix race condition on processing DVR floating IPs

Fip namespace and agent gateway port can be shared by multiple dvr routers.
This change uses a set as the control variable for these shared resources
and ensures that Test and Set operation on the control variable are
performed atomically so that race conditions do not occur among
multiple threads processing floating IPs.
Limitation: The scope of this change is limited to addressing the race
condition described in the bug report. It may not address other issues
such as pre-existing issue with handling of DVR floatingips on agent
restart.

closes-bug: #1381238

Change-Id: I6dc2b7bad6e8ddbaa86c1f7a1e2028aeacc3afef"
1152,993facb3bf753d23332236dc05b397850bf99144,1397785596,,1.0,45,5,2,2,1,0.242292189,True,2.0,2476398.0,16.0,10.0,False,24.0,3214689.0,34.0,11.0,436.0,1130.0,1279.0,362.0,1016.0,1124.0,422.0,1064.0,1200.0,0.26721415,0.672773215,0.758686039,796,1224,1308058,cinder,993facb3bf753d23332236dc05b397850bf99144,1,1,"""https://github.com/openstack/cinder/commit/da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2 - this patch removes the default of None from the getattr call.”","Bug #1308058 in Cinder: ""Cannot create volume from glance image without checksum""","It is no longer possible to create a volume from an image that does not have a checksum set.
https://github.com/openstack/cinder/commit/da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2 - this patch removes the default of None from the getattr call.
If this is intended it would be nice to see something more informative in the logs.
2014-04-15 11:52:26.035 19000 ERROR cinder.api.middleware.fault [req-cf0f7b89-a9c1-4a10-b1ac-ddf415a28f24 c139cd16ac474d2184237ba837a04141 83d5198d5f5a461798c6b843f57540d
f - - -] Caught error: checksum
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault Traceback (most recent call last):
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/middleware/fault.py"", line 75, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return req.get_response(self.application)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     application, catch_exc_info=False)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     app_iter = application(self.environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 615, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.app(env, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     response = self.app(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return resp(environ, start_response)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     resp = self.call_func(req, *args, **self.kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.func(req, *args, **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 895, in __call__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     content_type, body, accept)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 943, in _process_stack
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     action_result = self.dispatch(meth, request, action_args)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/openstack/wsgi.py"", line 1019, in dispatch
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return method(req=request, **action_args)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/api/v2/volumes.py"", line 346, in create
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/api.py"", line 189, in create
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     flow_engine.run()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/lock_utils.py"", line 54, in wrapper
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return f(*args, **kwargs)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 96, in run
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     self._run()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 111, in _ru
n
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     misc.Failure.reraise_if_any(failures.values())
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 649, in reraise_if_any
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     failures[0].reraise()
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 656, in reraise
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     six.reraise(*self._exc_info)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 34, in _e
xecute_task
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     result = task.execute(**arguments)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/flows/api/create_volume.py"", line 341, in execute
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     self._check_image_metadata(context, image_id, size)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/volume/flows/api/create_volume.py"", line 180, in _check_image_metadata
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     image_meta = self.image_service.show(context, image_id)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 228, in show
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     base_image_meta = self._translate_from_glance(image)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 336, in _translate_from_glance
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     image_meta = _extract_attributes(image)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/cinder/cinder/image/glance.py"", line 434, in _extract_attributes
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     output[attr] = getattr(image, attr)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-glanceclient/glanceclient/openstack/common/apiclient/base.py"", line 462, in __getattr__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     return self.__getattr__(k)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault   File ""/opt/stack/python-glanceclient/glanceclient/openstack/common/apiclient/base.py"", line 464, in __getattr__
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault     raise AttributeError(k)
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault AttributeError: checksum
2014-04-15 11:52:26.035 19000 TRACE cinder.api.middleware.fault
2014-04-15 11:52:26.037 19000 INFO cinder.api.middleware.fault [req-cf0f7b89-a9c1-4a10-b1ac-ddf415a28f24 c139cd16ac474d2184237ba837a04141 83d5198d5f5a461798c6b843f57540df - - -] http://162.13.156.43:8776/v2/83d5198d5f5a461798c6b843f57540df/volumes returned with HTTP 500","Fallback to None on missing Glance image attrs

It's possible for glance images to be missing attributes like name and
checksum. We'll just set those to None by default to avoid missing key
exceptions being raised.

Closes-Bug: #1308058
Change-Id: I85c42f8351763da201021a22f5ff0ebd62c6b2db"
1153,996f7f949002d3d23048550b69579889f4e1c5a2,1379049233,,1.0,2,2,2,1,1,1.0,True,1.0,39102.0,7.0,3.0,False,12.0,222447.0,24.0,3.0,8.0,992.0,995.0,8.0,894.0,897.0,5.0,852.0,852.0,0.006048387,0.859879032,0.859879032,1551,1224790,1224790,cinder,996f7f949002d3d23048550b69579889f4e1c5a2,1,1," ""Fixes the use of exception.InvalidInput with the wrong arguments”","Bug #1224790 in Cinder: ""Argument of exception.InvalidInput are incorrect in iscsi.py and utils.py""","cinder/volume/drivers/netapp/iscsi.py and
cinder/volume/drivers/netapp/utils.py are used as follows.
  raise exception.InvalidInput(data=msg)
I think the following is correct.
  raise exception.InvalidInput(reason=msg)","Fixes the use of exception.InvalidInput with the wrong arguments

A mapping key of 'message' in exception.InvalidInput is 'reason'.
cinder/volume/drivers/netapp/iscsi.py and
cinder/volume/drivers/netapp/utils.py had used 'data'.

Change-Id: Ie40fc2f954984350827414edee9226adc80c8033
Closes-Bug: #1224790"
1154,99cbfb3492c80ded53fa018fe5a0881f977dc407,1381759261,,1.0,11,11,3,2,1,0.943188781,True,1.0,300893.0,15.0,5.0,False,18.0,263855.3333,85.0,4.0,860.0,3457.0,3741.0,843.0,2780.0,3056.0,841.0,3145.0,3415.0,0.133523628,0.498889946,0.541706311,1683,1239709,1239709,nova,99cbfb3492c80ded53fa018fe5a0881f977dc407,1,1,“A typo made early in the object prototyping process has meant that”,"Bug #1239709 in OpenStack Compute (nova): ""NovaObject does not properly honor VERSION""","The base object infrastructure has been comparing Object.version instead of the Object.VERSION that *all* the objects have been setting and incrementing when changes have been made. Since the base object defined a .version, and that was used to determine the actual version of an object, all objects defining a different VERSION were ignored.
All systems in the wild currently running broken code are sending version '1.0' for all of their objects. The fix is to change the base object infrastructure to properly examine, compare and send Object.VERSION.
Impact should be minimal at this point, but getting systems patched as soon as possible will be important going forward.","Fix NovaObject versioning attribute usage

A typo made early in the object prototyping process has meant that
all objects thus far have been exposing a version of '1.0' because
the base object has been using an attribute of 'version'. The
object implementations have been exposing a 'VERSION' attribute
with their desired version advertisement.

Since we are still relatively early in the process of deploying
objects, the impact from this should be relatively minor, although
it is important to get this fixed ASAP before any incompatible
object changes are made.

Change-Id: If9154dc663111f304f72ef7f6f37c465e84ff504
Closes-bug: #1239709"
1155,99f3ce38e9ad1e272a4a04858ff1acc709e7c9cb,1387283386,,1.0,2,2,1,1,1,0.0,True,1.0,16751.0,5.0,2.0,False,10.0,50414.0,25.0,2.0,741.0,498.0,1235.0,650.0,478.0,1124.0,17.0,466.0,480.0,0.014251781,0.369754553,0.380839272,198,601,1261731,cinder,99f3ce38e9ad1e272a4a04858ff1acc709e7c9cb,1,1,"One LOG is just a debug, no error or warning. Typo?","Bug #1261731 in Cinder: ""Unable to deactivate LVM snapshot""","One of the new error conditions that shows up in the gate now that we are failing on unknown errors is a failure by c-vol to deactivate the LVM snapshot.
The c-vol error condition is the following:
[req-798a0f63-4a42-4f68-9087-54a90ec50376 1b40a47efb8243579e1830d573ac30ed f6fcf023ac3c4fa0a670e28917331b3f] Error reported running lvremove: CMD: sudo cinder-rootwrap /etc/cinder/rootwrap.conf lvremove -f stack-volumes/_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22, RESPONSE:   Unable to deactivate open stack--volumes-_snapshot--8a0e5904--d394--4193--97fe--3658e0964c22 (252:4)
  Unable to deactivate logical volume ""_snapshot-8a0e5904-d394-4193-97fe-3658e0964c22""
http://logs.openstack.org/14/62514/1/check/check-tempest-dsvm-postgres-full/18e9cf9/logs/screen-c-vol.txt.gz#_2013-12-17_02_10_46_029
It looks like it actually succeeds on attempt 2 after the quiece, so the the fix is probably to reduce the level on these to make them not error/warn unless there is a real failure.","make delete recovery messages debug level

lvm.delete has recovery logic within it to do a second attempt
if it should fail the first time. It is currently however logging
those second attempts at error and warn levels. However, as this
is normal flow these should be debug level instead.

This is currently causing a few resets in the gate because this
is a non whitelisted error.

Change-Id: Ie684331d9d97c089ec23569d06981e90eb7f6a67
Closes-Bug: #1261731"
1156,9a01e62693a28a73120544b27ee2104558e0250e,1403876770,,1.0,10,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1010,1452,1334345,nova,9a01e62693a28a73120544b27ee2104558e0250e,1,1,,"Bug #1334345 in OpenStack Compute (nova): ""Unexpected task state: expecting [None] but the actual state is powering-off""","It appears that nova compute is expecting (in a number of cases) an instance to be in powering-off state, but the state is actually none.  This appears to have a wide range of failure modes.
I'm seeing this ~2800 times in the last 7 days
http://logstash.openstack.org/#eyJzZWFyY2giOiJ0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIEFORCBtZXNzYWdlOlwib3Nsby5tZXNzYWdpbmcucnBjLmRpc3BhdGNoZXIgVW5leHBlY3RlZFRhc2tTdGF0ZUVycm9yOiBVbmV4cGVjdGVkIHRhc2sgc3RhdGU6IGV4cGVjdGluZyAodSdwb3dlcmluZy1vZmYnLCkgYnV0IHRoZSBhY3R1YWwgc3RhdGUgaXMgTm9uZVwiIEFORCBidWlsZF9zdGF0dXM6XCJGQUlMVVJFXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwibW9kZSI6IiIsImFuYWx5emVfZmllbGQiOiIiLCJzdGFtcCI6MTQwMzcxMTkxODcxNH0=
Example traceback:
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None
 to caller
2014-06-25 05:01:31.058 ERROR oslo.messaging._drivers.common [req-ae166761-731e-4ead-b9da-eb8e8b1cf6af None None]
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 278, in decorated_function
    LOG.info(_(""Task possibly preempted: %s"") % e.format_message())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 336, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 314, in decorated_function
    kwargs['instance'], e, sys.exc_info())
  File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 302, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2346, in stop_instance
    instance.save(expected_task_state=task_states.POWERING_OFF)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 187, in wrapper
    ctxt, self, fn.__name__, args, kwargs)
  File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 354, in object_action
    objmethod=objmethod, args=args, kwargs=kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
    retry=self.retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
    timeout=timeout, retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
    retry=retry)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
    raise result
UnexpectedTaskStateError_Remote: Unexpected task state: expecting (u'powering-off',) but the actual state is None
Traceback (most recent call last):
  File ""/opt/stack/new/nova/nova/conductor/manager.py"", line 404, in _object_dispatch
    return getattr(target, method)(context, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/base.py"", line 196, in wrapper
    return fn(self, ctxt, *args, **kwargs)
  File ""/opt/stack/new/nova/nova/objects/instance.py"", line 473, in save
    columns_to_join=_expected_cols(expected_attrs))
  File ""/opt/stack/new/nova/nova/db/api.py"", line 780, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 164, in wrapper
    return f(*args, **kwargs)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2229, in instance_update_and_get_original
    columns_to_join=columns_to_join)
  File ""/opt/stack/new/nova/nova/db/sqlalchemy/api.py"", line 2280, in _instance_update
    actual=actual_state, expected=expected)
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None","Enforce task_state is None in ec2 create_image stop instance wait loop

We're hitting races in the gate where the instance.vm_state is STOPPED
but the task_state is POWERING_OFF so when the compute_api.start method
is called, we're in an invalid task state and fail.

The compute manager's stop_instance method is correctly setting the
vm_state to STOPPED and the task_state to None when the instance is
powered off via the virt driver, so we must be hitting this from races
in the ec2 API as noted in the TODO above the method definition.

This change simply checks the task_state in addition to the vm_state
in the wait loop before continuing. The error message is also updated
for context by including the instance uuid, vm_state and task_state,
and removes the timeout value in the message since it was in
milliseconds, not seconds, to begin with.

There is already a unit test that covers this change (which was racing,
hence the bug). There are no changes to that unit test since it's really
an integration test that's running through the compute API and compute
manager code, so the fix tests itself.

Closes-Bug: #1334345

Change-Id: I13f0c743cadda6439ae15607a9ef6e4e4985626d"
1157,9a64271bd9f741b13f6cf4a9074524d3854fdd14,1382425375,,1.0,3,3,1,1,1,0.0,True,3.0,3152898.0,23.0,14.0,False,1.0,15101440.0,1.0,8.0,22.0,725.0,744.0,22.0,688.0,707.0,4.0,289.0,290.0,0.010504202,0.609243697,0.611344538,7,319,1243073,neutron,9a64271bd9f741b13f6cf4a9074524d3854fdd14,1,1,,"Bug #1243073 in neutron: ""Mistake in usage drop_constraint parameters""","In miration 63afba73813_ovs_tunnelendpoints_id_unique mistake in usage drop_constraint parameters tablename and type instead of table_name and type_.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/63afba73813_ovs_tunnelendpoints_id_unique.py"", line 63, in downgrade
    type='unique'
TypeError: drop_constraint() takes at least 2 arguments (1 given)","Fix downgrade in migration

In 63afba73813_ovs_tunnelendpoints_id_unique migration mistake in
usage drop_constraint parameter type_ and positional agruments name
and table_name.

Closes-Bug: #1243073

Change-Id: I60cd6e7f18b193b71f8f20759251b9f59a79965a"
1158,9a830b370551019a4bd3a0c7504f48961e755bd4,1397057564,,1.0,3,3,1,1,1,0.0,True,5.0,2597082.0,39.0,24.0,False,8.0,1995256.0,8.0,20.0,2748.0,5483.0,5648.0,2207.0,4399.0,4564.0,32.0,1051.0,1051.0,0.029359431,0.93594306,0.93594306,754,1181,1303890,neutron,9a830b370551019a4bd3a0c7504f48961e755bd4,1,1,,"Bug #1303890 in neutron: ""Uncaught qpid error can break a consumer""","The following exception was originally observed against the old rpc code, but the same problem exists in oslo.messaging.
 Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 78, in inner_func
     return infunc(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 698, in _consumer_thread
     self.consume()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 689, in consume
     it.next()
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 606, in iterconsume
     yield self.ensure(_error_callback, _consume)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 540, in ensure
     return method(*args, **kwargs)
   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/impl_qpid.py"", line 597, in _consume
     nxt_receiver = self.session.next_receiver(timeout=timeout)
   File ""<string>"", line 6, in next_receiver
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 665, in next_receiver
     if self._ecwait(lambda: self.incoming, timeout):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 50, in _ecwait
     result = self._ewait(lambda: self.closed or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 571, in _ewait
     result = self.connection._ewait(lambda: self.error or predicate(), timeout)
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 214, in _ewait
     self.check_error()
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py"", line 207, in check_error
     raise self.error
 InternalError: Traceback (most recent call last):
   File ""/usr/lib/python2.6/site-packages/qpid/messaging/driver.py"", line 667, in write
     self._op_dec.write(*self._seg_dec.read())
   File ""/usr/lib/python2.6/site-packages/qpid/framing.py"", line 269, in write
     if self.op.headers is None:
 AttributeError: 'NoneType' object has no attribute 'headers'
It's possible for something to put the qpid client into a bad state.  In particular, I have observed a case that will cause session.next_receiver() to immediately raise an InternalError.  This exception makes it all the way out.  If the eventlet executor is used, the forever_retry_uncaught_exceptions() decorator will get hit.  It will go back into this code and get the same error, stuck in an infinite loop of retrying.
The connection needs to be reset in this case to recover.","Update ensure()/reconnect() to catch MessagingError

The error handling code that gets connections reset if necessary
caught ConnectionError. It really needs to catch MessagingError,
which ConnectionError inherits from. There are other types of
MessagingErrors that may occur, such as InternalError, and they need
to cause the connection to reset, as well.

This fix has already been merged into oslo.messaging.

--

Cherry-picked from oslo-incubator 234f64d608266f43d8856ff98c89ceba6699d752
See also https://bugzilla.redhat.com/show_bug.cgi?id=1086077

Closes-bug: #1303890
Change-Id: Ic5082b74a362ded8b35cbc75cf178fe6e0db62d0"
1159,9adba6727bbec051c1437ee95367ecb97b207f35,1382054009,,1.0,148,46,9,2,1,0.871944526,True,16.0,12006448.0,142.0,39.0,False,50.0,227523.3333,110.0,5.0,21.0,1730.0,1732.0,21.0,1566.0,1568.0,21.0,898.0,900.0,0.003450439,0.140997491,0.141311167,1506,1220459,1220459,nova,9adba6727bbec051c1437ee95367ecb97b207f35,1,1, ,"Bug #1220459 in OpenStack Compute (nova): ""VMware Driver reports incorrect disk usage""","VMware VCDriver currently reports the disk usage 'local_gb_used' metric as the capacity of a randomly chosen datastore in the vCenter cluster. The right value would be the aggregate capacity of all datastores in the cluster.
Eg.
With a cluster having 3 Datastores (3x 100GB LUN), only 100GB appears in ""free_disk_gb"" when you execute ""nova hypervisor-show"".","VMware Driver update correct disk usage stat

The nodes managed by VMware VCDriver driver can have multiple
datastores (disk abstraction) attached to it. The driver currently
reports the disk usage of a datastore that has the largest capacity.
But it doesn't report a consolidated disk usage. As a result, the
node's storage capability is under-reported in many occassions.

This patch fixes this issue by aggregating the disk freespace,
capacity from all attached datastores.

Closes-Bug: #1220459

Change-Id: I818a041f33a045773fc799de2c69628064ef8cbf"
1160,9af846caf7f4d00eddb2d839c032b909eb79d403,1392290365,,1.0,45,27,22,13,1,0.899263344,True,7.0,895942.0,160.0,20.0,False,45.0,592161.5455,102.0,5.0,31.0,653.0,669.0,21.0,580.0,588.0,10.0,359.0,360.0,0.013853904,0.453400504,0.45465995,403,817,1279769,neutron,9af846caf7f4d00eddb2d839c032b909eb79d403,0,0,Not a bug now: ‘It may cause ‘,"Bug #1279769 in neutron: ""duplicated config-option registering""","Some config options(interface_driver, use_namespaces, periodic_interval) are defined in multiple sources in ad-hoc way.
It may cause DuplicateOptError exception when using those module at the same time.
Right now the exception is avoided in ad-hoc way by each executables.
Those definition/registering should be consolidated.
This is the blocker for BP of l3 agent consolidation.
https://blueprints.launchpad.net/neutron/+spec/l3-agent-consolidation","options: consolidate options definitions

Some config options(interface_driver, use_namespaces) are defined
multiple times in ad-hoc way.  It causes DuplicateOptError exception
when using those module at the same time.  Right now the exception is
avoided in ad-hoc way by each executable.  Those duplicated
definitions should be consolidated and treated in uniformed way.

This is the blocker for blueprint: l3-agent-consolidation

neutron.services.loadbalancer.drivers.haproxy.agent periodic_interval
conflicts with neutron.service one. Since there is no way to fix it
without changing existing behavior/default value, it is untouched for now.

Closes-bug: #1279769
Change-Id: Ifed79b7ee0033644854499416f8a2b22a20416fe"
1161,9afbd3fbba201ee606cf6e3b57675d3fbf9514b2,1391028560,,1.0,52,16,2,2,1,0.26086291,True,1.0,342816.0,6.0,3.0,False,7.0,1877215.0,10.0,3.0,3.0,792.0,794.0,3.0,770.0,772.0,2.0,752.0,753.0,0.002189781,0.549635036,0.550364964,338,748,1274252,cinder,9afbd3fbba201ee606cf6e3b57675d3fbf9514b2,1,1,,"Bug #1274252 in Cinder: ""volume_type_encryption extension allows create when volumes exist""","Location: cinder.api.contrib.volume_type_encryption
Method: create
Bug:
When a volume type is made an encrypted volume type, through the volume_type_encryption API extension, the extension does not confirm no volumes exist with the volume type before making the volume type encrypted.  If volumes exist with the volume type before the volume type is made an encrypted volume type, these volumes will not be encrypted, although the user may think they are encrypted because they have an encrypted volume type.
Proposed Fix:
Add a check in the volume_type_encryption extension to stop the creation of an encrypted volume type if there are currently volumes with the volume type.  Also add a unit test confirming functionality.","Stop volume_type_encryption creation when in use

This bug fix addresses bug #1274252.  It requires a volume
type to have no volumes with the type before allowing it
to be made encrypted.  This prevents a situation where
non-encrypted volumes could have an encrypted volume type.
This fix also adds a unit test to confirm functionality.

Change-Id: Iab0954c2d0af73bce28be7fa319b8b34ca20e720
Closes-Bug: #1274252"
1162,9b083d7636508cbc5addfe3457c1aac706f0e267,1387357414,,1.0,19,3,2,2,1,0.845350937,True,7.0,4675635.0,85.0,33.0,False,5.0,9307471.5,7.0,6.0,1.0,748.0,749.0,1.0,650.0,651.0,0.0,326.0,326.0,0.0015625,0.5109375,0.5109375,36,348,1245310,neutron,9b083d7636508cbc5addfe3457c1aac706f0e267,0,0,Might potentially  cause consumers to not work properly,"Bug #1245310 in neutron: ""'binding:host_id' is set None when port update without portbinding changes""","When I try to update port for additional fixed IP or any other attributes other than portbinding, _process_portbindings_create_and_update is set 'None' in port update messages sent to the agents in spite of it has host_id.
It may not be a problem if the agents do not use portbinding information but if the agent uses 'binding:host_id' information for port update it will cause a problem, which is in my case.
When look at the codes, Neutron server sets 'binding:host_id' to 'None' as long as 'binding:host_id' is not in the requested update items. It should query DB to set correct port binding instead of 'None' in that case.","Fix binding:host_id is set to None when port update

when updating a port 'binding:host_id' is reset if not specified among
the parameter to be updated. As a result, a None value for
'binding:host_id' is sent from the notifier which might potentially
cause consumers to not work properly.

Closes-Bug: #1245310
Change-Id: Icfb5179940cca9f8a705eb36bdbfcbc8a421a272"
1163,9b41af32bbbfc063eabf570fb95cdf516a8bb4a7,1399285818,,1.0,23,3,1,1,1,0.0,True,1.0,207409.0,44.0,8.0,False,8.0,8495530.0,11.0,4.0,74.0,3257.0,3305.0,57.0,2549.0,2584.0,74.0,2890.0,2938.0,0.009457755,0.364564943,0.370617907,861,1294,1316074,nova,9b41af32bbbfc063eabf570fb95cdf516a8bb4a7,1,1,,"Bug #1316074 in OpenStack Compute (nova): ""nova network dhcpbridge has direct DB access""","nova-network is currently broken due to direct DB access in dhcpbridge.
This issue was found using a multihost devstack setup where the non-controller node has an empty sql connection string.","Remove and block DB access in dhcpbridge

This fixes an issue of direct DB access from happening in dhcpbridge
by using an object method instead. For this to work and to prohibit
future DB access from happening, the objects indirection API is set
and DB access is blocked in a similar way to how it is done in
nova-compute and nova-network.

Change-Id: I5e3152aad13343a6a1ecd9b5b511429159c9d4c2
Closes-Bug: #1316074"
1164,9b44d9d75d3767c6b7d1af4f237da8c1cf16266a,1384973478,,1.0,6,6,1,1,1,0.0,True,2.0,1417890.0,22.0,16.0,False,3.0,20997671.0,4.0,7.0,308.0,4192.0,4225.0,305.0,3632.0,3664.0,290.0,3155.0,3186.0,0.043818702,0.475229634,0.479897606,1770,1251920,1251920,nova,9b44d9d75d3767c6b7d1af4f237da8c1cf16266a,1,1,“local has a broken TLS symbol”,"Bug #1251920 in OpenStack Compute (nova): ""Tempest failures due to failure to return console logs from an instance""","Logstash search: http://logstash.openstack.org/#eyJzZWFyY2giOiJmaWxlbmFtZTpjb25zb2xlLmh0bWwgQU5EIG1lc3NhZ2U6XCJhc3NlcnRpb25lcnJvcjogY29uc29sZSBvdXRwdXQgd2FzIGVtcHR5XCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6IjYwNDgwMCIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJ1c2VyX2ludGVydmFsIjowfSwic3RhbXAiOjEzODQ2NDEwNzIxODl9
An example failure is http://logs.openstack.org/92/55492/8/check/check-tempest-devstack-vm-full/ef3a4a4/console.html
console.html
===========
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,775 Request: POST http://127.0.0.1:8774/v2/3f6934d9aabf467aa8bc51397ccfa782/servers/10aace14-23c1-4cec-9bfd-2c873df1fbee/action
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:20,776 Request Body: {""os-getConsoleOutput"": {""length"": 10}}
2013-11-16 21:54:27.998 | 2013-11-16 21:41:21,000 Response Status: 200
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Nova request id: req-7a2ee0ab-c977-4957-abb5-1d84191bf30c
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Headers: {'content-length': '14', 'date': 'Sat, 16 Nov 2013 21:41:20 GMT', 'content-type': 'application/json', 'connection': 'close'}
2013-11-16 21:54:27.999 | 2013-11-16 21:41:21,001 Response Body: {""output"": """"}
2013-11-16 21:54:27.999 | }}}
2013-11-16 21:54:27.999 |
2013-11-16 21:54:27.999 | Traceback (most recent call last):
2013-11-16 21:54:27.999 |   File ""tempest/api/compute/servers/test_server_actions.py"", line 281, in test_get_console_output
2013-11-16 21:54:28.000 |     self.wait_for(get_output)
2013-11-16 21:54:28.000 |   File ""tempest/api/compute/base.py"", line 133, in wait_for
2013-11-16 21:54:28.000 |     condition()
2013-11-16 21:54:28.000 |   File ""tempest/api/compute/servers/test_server_actions.py"", line 278, in get_output
2013-11-16 21:54:28.000 |     self.assertTrue(output, ""Console output was empty."")
2013-11-16 21:54:28.000 |   File ""/usr/lib/python2.7/unittest/case.py"", line 420, in assertTrue
2013-11-16 21:54:28.000 |     raise self.failureException(msg)
2013-11-16 21:54:28.001 | AssertionError: Console output was empty.
n-api
====
2013-11-16 21:41:20.782 DEBUG nova.api.openstack.wsgi [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Action: 'action', body: {""os-getConsoleOutput"": {""length"": 10}} _process_stack /opt/stack/new/nova/nova/api/openstack/wsgi.py:963
2013-11-16 21:41:20.782 DEBUG nova.api.openstack.wsgi [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Calling method <bound method ConsoleOutputController.get_console_output of <nova.api.openstack.compute.contrib.console_output.ConsoleOutputController object at 0x3c1f990>> _process_stack /opt/stack/new/nova/nova/api/openstack/wsgi.py:964
2013-11-16 21:41:20.865 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Making synchronous call on compute.devstack-precise-hpcloud-az2-663635 ... multicall /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:553
2013-11-16 21:41:20.866 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] MSG_ID is a93dceabf6a441eb850b5fbb012d661f multicall /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:556
2013-11-16 21:41:20.866 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is 706ab69dc066440fbe1bd7766b73d953. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
2013-11-16 21:41:20.869 22679 DEBUG amqp [-] Closed channel #1 _do_close /usr/local/lib/python2.7/dist-packages/amqp/channel.py:95
2013-11-16 21:41:20.869 22679 DEBUG amqp [-] using channel_id: 1 __init__ /usr/local/lib/python2.7/dist-packages/amqp/channel.py:71
2013-11-16 21:41:20.870 22679 DEBUG amqp [-] Channel open _open_ok /usr/local/lib/python2.7/dist-packages/amqp/channel.py:429
2013-11-16 21:41:20.999 INFO nova.osapi_compute.wsgi.server [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] 127.0.0.1 ""POST /v2/3f6934d9aabf467aa8bc51397ccfa782/servers/10aace14-23c1-4cec-9bfd-2c873df1fbee/action HTTP/1.1"" status: 200 len: 205 time: 0.2208662
n-cpu
=====
2013-11-16 21:41:20.878 23086 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_msg_id': u'a93dceabf6a441eb850b5fbb012d661f', u'_context_quota_class': None, u'_context_request_id': u'req-7a2ee0ab-c977-4957-abb5-1d84191bf30c', u'_context_service_catalog': [{u'endpoints_links': [], u'endpoints': [{u'adminURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'region': u'RegionOne', u'publicURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'internalURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'id': u'5725cc234a8346408dfe343aea62d3aa'}], u'type': u'volume', u'name': u'cinder'}], u'_context_auth_token': '<SANITIZED>', u'_context_user_id': u'eae9401ba3794296ae8248748ef8e26e', u'_reply_q': u'reply_546cb0ef381e4ff29c76ee96eba21c22', u'namespace': None, u'_context_is_admin': False, u'version': u'2.0', u'_context_timestamp': u'2013-11-16T21:41:20.781052', u'_context_user': u'eae9401ba3794296ae8248748ef8e26e', u'method': u'get_console_output', u'_context_remote_address': u'127.0.0.1', u'_context_roles': [u'_member_'], u'args': {u'instance': {u'vm_state': u'active', u'availability_zone': None, u'terminated_at': None, u'ephemeral_gb': 0, u'instance_type_id': 6, u'user_data': None, u'cleaned': False, u'vm_mode': None, u'deleted_at': None, u'reservation_id': u'r-dnps4t4l', u'id': 40, u'security_groups': [{u'deleted_at': None, u'user_id': u'eae9401ba3794296ae8248748ef8e26e', u'description': u'default', u'deleted': False, u'created_at': u'2013-11-16T21:37:03.000000', u'updated_at': None, u'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'id': 47, u'name': u'default'}], u'disable_terminate': False, u'root_device_name': u'/dev/vda', u'display_name': u'ServerActionsTestJSON-instance-tempest-308881308', u'uuid': u'10aace14-23c1-4cec-9bfd-2c873df1fbee', u'default_swap_device': None, u'info_cache': {u'instance_uuid': u'10aace14-23c1-4cec-9bfd-2c873df1fbee', u'deleted': False, u'created_at': u'2013-11-16T21:37:03.000000', u'updated_at': u'2013-11-16T21:37:08.000000', u'network_info': [{u'ovs_interfaceid': None, u'network': {u'bridge': u'br100', u'label': u'private', u'meta': {u'tenant_id': None, u'should_create_bridge': True, u'bridge_interface': u'eth0'}, u'id': u'44a93331-6869-4e8a-8742-f92bb9cea7b5', u'subnets': [{u'ips': [{u'meta': {}, u'type': u'fixed', u'floating_ips': [], u'version': 4, u'address': u'10.1.0.6'}], u'version': 4, u'meta': {u'dhcp_server': u'10.1.0.1'}, u'dns': [{u'meta': {}, u'type': u'dns', u'version': 4, u'address': u'8.8.4.4'}], u'routes': [], u'cidr': u'10.1.0.0/24', u'gateway': {u'meta': {}, u'type': u'gateway', u'version': 4, u'address': u'10.1.0.1'}}, {u'ips': [], u'version': None, u'meta': {u'dhcp_server': None}, u'dns': [], u'routes': [], u'cidr': None, u'gateway': {u'meta': {}, u'type': u'gateway', u'version': None, u'address': None}}]}, u'devname': None, u'qbh_params': None, u'meta': {}, u'address': u'fa:16:3e:38:6d:dd', u'type': u'bridge', u'id': u'2bd53919-68f4-4fdb-bd51-dfc3238c6cbb', u'qbg_params': None}], u'deleted_at': None}, u'hostname': u'serveractionstestjson-instance-tempest-308881308', u'launched_on': u'devstack-precise-hpcloud-az2-663635', u'display_description': u'ServerActionsTestJSON-instance-tempest-308881308', u'key_data': None, u'deleted': False, u'config_drive': u'', u'power_state': 1, u'default_ephemeral_device': None, u'progress': 0, u'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'launched_at': u'2013-11-16T21:37:18.000000', u'scheduled_at': u'2013-11-16T21:37:04.000000', u'node': u'devstack-precise-hpcloud-az2-663635', u'ramdisk_id': u'cd16eea0-986d-443f-a345-3b82c8f4fce6', u'access_ip_v6': None, u'access_ip_v4': None, u'kernel_id': u'b1803c3b-8f66-408b-9105-73568c7181ca', u'key_name': None, u'updated_at': u'2013-11-16T21:38:00.000000', u'host': u'devstack-precise-hpcloud-az2-663635', u'user_id': u'eae9401ba3794296ae8248748ef8e26e', u'system_metadata': {u'image_kernel_id': u'b1803c3b-8f66-408b-9105-73568c7181ca', u'instance_type_memory_mb': u'64', u'instance_type_swap': u'0', u'instance_type_vcpu_weight': None, u'instance_type_root_gb': u'0', u'instance_type_id': u'6', u'image_ramdisk_id': u'cd16eea0-986d-443f-a345-3b82c8f4fce6', u'instance_type_name': u'm1.nano', u'instance_type_ephemeral_gb': u'0', u'instance_type_rxtx_factor': u'1.0', u'image_disk_format': u'ami', u'instance_type_flavorid': u'42', u'image_container_format': u'ami', u'instance_type_vcpus': u'1', u'image_min_ram': u'0', u'image_min_disk': u'0', u'image_base_image_ref': u'11f4317b-d294-4d21-b2c4-2d7198aa32b8'}, u'task_state': None, u'shutdown_terminate': False, u'cell_name': None, u'root_gb': 0, u'locked': False, u'name': u'instance-00000028', u'created_at': u'2013-11-16T21:37:03.000000', u'locked_by': None, u'launch_index': 0, u'memory_mb': 64, u'vcpus': 1, u'image_ref': u'11f4317b-d294-4d21-b2c4-2d7198aa32b8', u'architecture': None, u'auto_disk_config': False, u'os_type': None, u'metadata': {}}, u'tail_length': 10}, u'_unique_id': u'706ab69dc066440fbe1bd7766b73d953', u'_context_project_name': u'ServerActionsTestJSON-tempest-2102529866-tenant', u'_context_read_deleted': u'no', u'_context_tenant': u'3f6934d9aabf467aa8bc51397ccfa782', u'_context_instance_lock_checked': False, u'_context_project_id': u'3f6934d9aabf467aa8bc51397ccfa782', u'_context_user_name': u'ServerActionsTestJSON-tempest-2102529866-user'} _safe_log /opt/stack/new/nova/nova/openstack/common/rpc/common.py:277
2013-11-16 21:41:20.879 23086 DEBUG nova.openstack.common.rpc.amqp [-] unpacked context: {'tenant': u'3f6934d9aabf467aa8bc51397ccfa782', 'project_name': u'ServerActionsTestJSON-tempest-2102529866-tenant', 'user_id': u'eae9401ba3794296ae8248748ef8e26e', 'roles': [u'_member_'], 'timestamp': u'2013-11-16T21:41:20.781052', 'auth_token': '<SANITIZED>', 'remote_address': u'127.0.0.1', 'quota_class': None, 'is_admin': False, 'user': u'eae9401ba3794296ae8248748ef8e26e', 'service_catalog': [{u'endpoints': [{u'adminURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'region': u'RegionOne', u'id': u'5725cc234a8346408dfe343aea62d3aa', u'internalURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782', u'publicURL': u'http://127.0.0.1:8776/v1/3f6934d9aabf467aa8bc51397ccfa782'}], u'endpoints_links': [], u'type': u'volume', u'name': u'cinder'}], 'request_id': u'req-7a2ee0ab-c977-4957-abb5-1d84191bf30c', 'instance_lock_checked': False, 'project_id': u'3f6934d9aabf467aa8bc51397ccfa782', 'user_name': u'ServerActionsTestJSON-tempest-2102529866-user', 'read_deleted': u'no'} _safe_log /opt/stack/new/nova/nova/openstack/common/rpc/common.py:277
2013-11-16 21:41:20.880 AUDIT nova.compute.manager [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] [instance: 10aace14-23c1-4cec-9bfd-2c873df1fbee] Get console output
2013-11-16 21:41:20.894 DEBUG nova.openstack.common.processutils [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] Running cmd (subprocess): sudo nova-rootwrap /etc/nova/rootwrap.conf chown 1006 /opt/stack/data/nova/instances/10aace14-23c1-4cec-9bfd-2c873df1fbee/console.log execute /opt/stack/new/nova/nova/openstack/common/processutils.py:147
2013-11-16 21:41:20.993 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is 7abfb6072ea44239ae71367bde0a4485. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
2013-11-16 21:41:20.995 DEBUG nova.openstack.common.rpc.amqp [req-7a2ee0ab-c977-4957-abb5-1d84191bf30c ServerActionsTestJSON-tempest-2102529866-user ServerActionsTestJSON-tempest-2102529866-tenant] UNIQUE_ID is b463e1d350f441efb159170784f9ba41. _add_unique_id /opt/stack/new/nova/nova/openstack/common/rpc/amqp.py:341
In other words, nova doesn't log much here, but I think the console log might be genuinely empty?","Sync local from oslo.

local has a broken TLS symbol - strong_store - in Nova today, fixed in oslo
some time ago in Ib544be1485823f6c619312fdee5a04031f48bbb4

Change-Id: If4dd973acc23921dbc2bc69bb76225deb2802dad
Closes-Bug: #1251920"
1165,9b559c31b781689fb66551f29a0cb8d10c7bac94,1403565138,1.0,1.0,14,3,2,1,1,0.977417818,False,,,,,True,,,,,,,,,,,,,,,,,996,1437,1332660,nova,9b559c31b781689fb66551f29a0cb8d10c7bac94,0,0,“Update statistics from computes if RBD ephemeral is used”,"Bug #1332660 in OpenStack Compute (nova): ""Update statistics from computes if RBD ephemeral is used""","If we use RBD as the backend for ephemeral drives, compute nodes still calculate their available disk size looking back to the local disks.
This is the path how they do it:
* nova/compute/manager.py
    def update_available_resource(self, context):
        """"""See driver.get_available_resource()
        Periodic process that keeps that the compute host's understanding of
        resource availability and usage in sync with the underlying hypervisor.
        :param context: security context
        """"""
        new_resource_tracker_dict = {}
        nodenames = set(self.driver.get_available_nodes())
        for nodename in nodenames:
            rt = self._get_resource_tracker(nodename)
            rt.update_available_resource(context)
            new_resource_tracker_dict[nodename] = rt
....................
    def _get_resource_tracker(self, nodename):
        rt = self._resource_tracker_dict.get(nodename)
        if not rt:
            if not self.driver.node_is_available(nodename):
                raise exception.NovaException(
                        _(""%s is not a valid node managed by this ""
                          ""compute host."") % nodename)
            rt = resource_tracker.ResourceTracker(self.host,
                                                  self.driver,
                                                  nodename)
            self._resource_tracker_dict[nodename] = rt
        return rt
* nova/compute/resource_tracker.py
    def update_available_resource(self, context):
        """"""Override in-memory calculations of compute node resource usage based
        on data audited from the hypervisor layer.
        Add in resource claims in progress to account for operations that have
        declared a need for resources, but not necessarily retrieved them from
        the hypervisor layer yet.
        """"""
        LOG.audit(_(""Auditing locally available compute resources""))
        resources = self.driver.get_available_resource(self.nodename)
* nova/virt/libvirt/driver.py
    def get_local_gb_info():
        """"""Get local storage info of the compute node in GB.
        :returns: A dict containing:
             :total: How big the overall usable filesystem is (in gigabytes)
             :free: How much space is free (in gigabytes)
             :used: How much space is used (in gigabytes)
        """"""
        if CONF.libvirt_images_type == 'lvm':
            info = libvirt_utils.get_volume_group_info(
                                 CONF.libvirt_images_volume_group)
        else:
            info = libvirt_utils.get_fs_info(CONF.instances_path)
        for (k, v) in info.iteritems():
            info[k] = v / (1024 ** 3)
        return info
It would be nice to have something like ""libvirt_utils.get_rbd_info"" which could be used in case CONF.libvirt_images_type == 'rbd'","Use Ceph cluster stats to report disk info on RBD

Local disk statistics on compute nodes are irrelevant when ephemeral
disks are stored in RBD. With RBD, local disk space is not consumed when
instances are started on a compute node, yet it is possible for
scheduler to refuse to schedule an instance when combined disk usage of
instances already running on the node exceeds total disk capacity
reported by the hypervisor driver.

Change-Id: I9718c727db205b6f2191f8435583391584e96e6e
Closes-bug: #1332660
Signed-off-by: Dmitry Borodaenko <dborodaenko@mirantis.com>"
1166,9b88e54998f0e197a29413a28dc101c5bbb7e89e,1379623788,,1.0,6,7,2,2,1,0.89049164,True,2.0,837352.0,29.0,8.0,False,176.0,10712.0,940.0,3.0,0.0,1209.0,1209.0,0.0,986.0,986.0,0.0,1172.0,1172.0,0.00016442,0.192864189,0.192864189,1561,1226226,1226226,Nova,9b88e54998f0e197a29413a28dc101c5bbb7e89e,1,1,"""Workloads should not require a Fixed IP Address for migration""","Bug #1226226 in OpenStack Compute (nova): ""pre_live_migration should not require a static IP Address""","The nova/compute/manager.py - pre_live_migration method is checking to ensure that there is at least a single fixed IP Address on the instance.  The comment above it indicates that this shouldn't be required (and it is not clear that it is even required).  Live migrations are blocked for systems that do not have a fixed IP Address.
If a system does not have a fixed IP, then the following error is thrown:
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     event_type, level, payload)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 76, in wrapped
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3850, in pre_live_migration
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp     instance_uuid=instance['uuid'])
2013-09-11 16:01:02.548 97997 TRACE nova.openstack.common.rpc.amqp FixedIpNotFoundForInstance: Instance eb586597-3565-4283-88d3-d0664f4d5747 has zero fixed ips.","Removes pre_live_migration need for Fixed IPs

This commit removes the validation that requires Fixed IP Addresses
for pre_live_migration.  The original comment regarding the
pre_live_migration requiring Fixed IP Addresses alluded to uncertainty
about it not being required.  Workloads should not require a Fixed IP
Address for migration, typically just the storage is required to be
similar.

The commit also updates the associated test case to ensure that no
exception is thrown in the pre_live_migration if a fixed IP Address
is not specified.

Change-Id: I2c4d4dae2cb0f00c67f0f731b5080196564b37da
Closes-Bug: #1226226"
1167,9bb9a77e41b009cb940e98feda666dd8d806c862,1392018710,,1.0,2,4,2,2,1,0.918295834,True,2.0,65918.0,24.0,10.0,False,52.0,61930.0,131.0,7.0,1734.0,3431.0,4459.0,1473.0,2925.0,3733.0,852.0,2872.0,3193.0,0.116593767,0.392700929,0.436577365,382,795,1278149,nova,9bb9a77e41b009cb940e98feda666dd8d806c862,1,1,typo in code,"Bug #1278149 in OpenStack Compute (nova): ""VMware: InstanceNotRescuable hit during rescue tempest tests""","Minesweeper CI is seeing the following Tempest suites fail due to errors during performing rescue operations.
tempest.api.compute.servers.test_server_rescue.ServerRescueTestJSON
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueTestXML
tempest.api.compute.v3.servers.test_server_rescue.ServerRescueV3Test
Error message seen in the nova cpu log is:
Traceback (most recent call last):
  File ""/opt/stack/oslo.messaging/oslo/messaging/_executors/base.py"", line 36, in _dispatch
    incoming.reply(self.callback(incoming.ctxt, incoming.message))
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 134, in __call__
    return self._dispatch(endpoint, method, ctxt, args)
  File ""/opt/stack/oslo.messaging/oslo/messaging/rpc/dispatcher.py"", line 104, in _dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 356, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 240, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 226, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 291, in decorated_function
    function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2731, in rescue_instance
    reason=_(""Driver Error: %s"") % unicode(e))
InstanceNotRescuable: Instance 3f317fe3-1777-4f0d-a60a-f2370d9b2fb0 cannot be rescued: Driver Error: can't set attribute
Full logs for a run that saw this error is available here: http://208.91.1.172//logs/nova/72125/1/","VMware: fix instance rescue bug

Commit I6448cb929e9afce814d0580eada91b8fd37befa3 added object support
to the rescue operations. This exposed a bug in the VMware driver
which was caught by the minesweeper. A rescue instance is named
'<instance-uuid>-rescue'. In the past this name was updated on the
instance, now it is passed as a parameter to the spawn method.

Change-Id: I633614c6c634d41a32a9b1b70d941e9bfdcadedd
Closes-bug: #1278149"
1168,9bc29208bda6071a34bcc0da36a396eb8bab4f30,1393174625,1.0,1.0,22,1,2,2,1,0.886540893,True,7.0,6479957.0,103.0,28.0,False,20.0,432331.0,24.0,8.0,35.0,1423.0,1432.0,31.0,1214.0,1220.0,28.0,736.0,738.0,0.033997655,0.864009379,0.866354045,457,872,1283765,neutron,9bc29208bda6071a34bcc0da36a396eb8bab4f30,1,1,Bug. plugin changes port it shouldn't,"Bug #1283765 in neutron: ""ovs plugin changes port it shouldn't change""","(Using ML2 with ovs)
While playing with https://github.com/stackforge/cookbook-openstack-network/blob/master/files/default/neutron-ha-tool.py to migrate routers between L3 agents, I had the issue that the connectivity got lost.
After investigating, it turns out that interface for the port of the router that is connected to the external network (attached to br-public -- which is usually named br-ext) gets a tag. Manually removing the tag makes things work.
I'm attaching a bit of the log where the port_update message is received (it's received for the two interfaces of the router, so this needs some care when reading). We can see the following:
 Running command: ['sudo', '/usr/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--timeout=2', 'set', 'Port', 'qg-923f9b0e-aa', 'tag=2']
However, reading the code, it seems that this kind of actions should only be done for ports on br-int (because of ""vif_port = self.int_br.get_vif_port_by_id(port['id'])""). So this shouldn't be run for other ports.","Fix get_vif_port_by_id to only return relevant ports

This is returning any port, even if it's not on the switch that we're
looking at. As a side-effect, this means that we can actually manipulate
these ports while we really shouldn't.

Co-Authored-By: Rossella Sblendido <rsblendido@suse.com>
Change-Id: Ia4f4e93237c1c2ea6cb4b6c2f5adf78d6b34c7bf
Closes-Bug: #1283765"
1169,9bf0e6654480f98f2315a43687b267263a82a823,1375715222,,1.0,107,308,73,38,1,0.930013338,True,23.0,22102853.0,369.0,89.0,False,27.0,93588.15068,48.0,2.0,391.0,159.0,550.0,353.0,89.0,442.0,42.0,1.0,43.0,0.296551724,0.013793103,0.303448276,1457,1207402,1207402,neutron,9bf0e6654480f98f2315a43687b267263a82a823,1,0,“According to neutron/db/migration/README this means that I have to stamp it to manually.”,"Bug #1207402 in neutron: ""neutron should automatically stamp the database version when it is deployed""","When neutron automatically deploys the database schema it does not ""stamp"" the version.  According to neutron/db/migration/README this means that I have to stamp it to manually.
Neutron should automatically stamp the version to head when deploys the schema the first time.  That way I don't have to determine what version I'm running so that I can do a schema migration.","Remove auto-generation of db schema from models at startup

This patch removes the Neutron capability of creating database tables
from sqlalchemy models for all those model classes for which
a table is not found in the database schema.
Migrations should be the official and only solution for creating and
managing the Neutron db schema.
This patch also adapts unit tests in order to ensure test schemas
are still correctly created.

DocImpact
Update deployment documentation accordingly.

Closes-Bug: #1207402

Change-Id: Ie4ee5507888ecad5f6dc32ce7a029c43014687a2
Co-Authored-By: Henry Gessau <gessau@cisco.com>"
1170,9c044d2c94812e18cf84927fbf719cd073fe6c4f,1378453669,0.0,1.0,0,1,1,1,1,0.0,True,4.0,2813952.0,28.0,12.0,False,27.0,3769467.0,38.0,2.0,5.0,2203.0,2207.0,2.0,1883.0,1885.0,2.0,2071.0,2072.0,0.000504117,0.348176777,0.348344816,1591,1229625,1229625,nova,9c044d2c94812e18cf84927fbf719cd073fe6c4f,1,1, ,"Bug #1229625 in OpenStack Compute (nova): "" hairpin mode on vnet bridge ports causes false positives on IPv6 duplicate address detection""","This is bug 1011134 again happening in a cloud that does not have the ipv6 flag set,
so the previous patch from https://review.openstack.org/14017
is not used.
Guest VMs will try to configure IPv6 link-local addrs even without the outer parts supporting it
and can throw errors when they see inbound packets with their own MAC address.
Note: I think, this bug can not be unit-tested as it requires a complex setup including running a VM in a cloud.","Always filter out multicast from reflection

Instances will try IPv6 neighbour discovery via multicast
even in an IPv4-only cloud and can throw errors
if they see inbound packets from their own MAC address

Closes-bug: #1229625

Change-Id: I3539e788fe07519d87ce7c3800c5d38b7bd99d3b"
1171,9c1ad54e7815d50d9561d88337724ad2042c92a2,1389909581,,1.0,27,23,2,2,1,0.995378439,True,2.0,446171.0,7.0,3.0,False,14.0,112480.0,33.0,1.0,98.0,1106.0,1181.0,98.0,934.0,1009.0,33.0,963.0,973.0,0.025411061,0.720478326,0.727952167,297,705,1269958,cinder,9c1ad54e7815d50d9561d88337724ad2042c92a2,0,0,"I dont see the bug, just a report that shouldnt use env","Bug #1269958 in Cinder: ""cinder allows 'env' as commandfilter in rootwrap""","cinder/image/image_utils.py uses
  def qemu_img_info(path):
      """"""Return a object containing the parsed output from qemu-img info.""""""
      out, err = utils.execute('env', 'LC_ALL=C', 'LANG=C',
                               'qemu-img', 'info', path,
                               run_as_root=True)
      return QemuImgInfo(out)
This was added as part of I849b04b8aae76da068abcd2a20c1fcecca8a5caa
There is nothing wrong with that per se, however the rootwrap filters were updated with:
+ env: CommandFilter, /usr/bin/env, root
env is a wrapper that allows to run any command in the $PATH, so this is more or less equivalent to allowing bash in commandfilter. As a hardening precaution, env should not be allowed in CommandFilter.
The code in question can be easily reworked and EnvFilter can be used instead to harden the check.","Remove env from rootwrap filter

Allowing 'env' as a CommandFilter is similar to
allowing '/bin/bash', which makes all of rootwrap pointless.
Use EnvFilter instead. Change corresponding commands
that use env for setting C locale and adjust rootwrap
filters accordingly.

Several commands that output information that is
parsed as input by cinder change their behavior
based on the environment locale, which is depending
on local system settings. The code is however only
able to parse in C locale, so enforce that one.

Closes-Bug: #1269958

Change-Id: Ie1463e608c80204c7a8906efb95899a66aa733da"
1172,9cc9cf111ef8a0265df5323fecaed12b0dabe312,1409930886,,1.0,3,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1275,1737,1366083,cinder,9cc9cf111ef8a0265df5323fecaed12b0dabe312,1,1,,"Bug #1366083 in Cinder: ""Race condition in remoteFS _ensure_shares_mounted()""","There is a race condition in the remoteFS _ensure_shares_mounted() method:
https://github.com/openstack/cinder/blob/master/cinder/volume/drivers/remotefs.py#L141
The list of known mounted shares is cleared before being rebuilt.  This can cause a race where an operation attempts to run and finds an empty list, simply because it is in the process of being refreshed.","Don't clear _mounted_shares list in remoteFS while updating

This fix makes the updating of the _mounted_shares list in
remoteFS more of an atomic operation.  Previously this list
would be cleared, then rebuilt.  That allowed a race
condition where operations that took place during this
update would have a list of 0 shares to work with.

Change-Id: I740d8f1b87db911242326a38ff398b81c5974cea
Closes-Bug: #1366083"
1173,9cd96fe116ac1d990410cb1135473bd15c1f7dce,1409734119,,1.0,35,43,4,4,1,0.712406325,False,,,,,True,,,,,,,,,,,,,,,,,1259,1719,1364839,neutron,9cd96fe116ac1d990410cb1135473bd15c1f7dce,1,1,,"Bug #1364839 in neutron: ""DVR namespaces not deleted on LBaaS VIP Port removal""",The removal of LBaaS VIP Port (and other DVR Serviced ports except compute port) does not delete the DVR namespace from the service nodes.,"DVR to delete router namespaces for service ports

Earlier merge that enabled LBaaS in DVR with review #114141
had not covered the removal of DVR router namespace on
VIP Port deletion in ml2 plugin.

Here we fix the ml2 plugin to attempt namespace removal for
all dvr serviced ports.

Change-Id: Ie7930ebedb12212886d45294132fefff7296e104
Closes-Bug: #1364839"
1174,9ceee09594cd35c264cab3e3b3d1d80aaa3fdbc4,1382623176,,1.0,36,8,3,2,1,0.852819625,True,4.0,9070373.0,38.0,15.0,False,47.0,127811.0,100.0,4.0,1362.0,2635.0,3768.0,1230.0,2104.0,3108.0,536.0,2379.0,2696.0,0.083488806,0.370024876,0.419309701,13,325,1243193,nova,9ceee09594cd35c264cab3e3b3d1d80aaa3fdbc4,1,1,,"Bug #1243193 in OpenStack Compute (nova): ""VMware: snapshot backs up wrong disk when instance is attached to volume""","When a volume is attached to an instance and we backup the instance or try to create an image from it, the volume's disk is being backed up and not the instance's primary disk.
More info: https://communities.vmware.com/community/vmtn/openstack/blog/2013/08/28/introducing-vova-an-easy-way-to-try-out-openstack-on-vsphere#comment-29775","VMware: fix image snapshot with attached volume

A snapshot of a instance with an attached volume will result in
the wrong volume being uploaded to glance. The reason for this
are as follows:
1. the last image in the list of images would be exported
2. the base image would not be exported but the first found

The same happens with a VM rescue if there is a cinder volume
attached.

The solution is to search for an image that contains the instance
UUID.

Change-Id: Ic85b0ffd4f1c34f10d07b1a198eaad2030803d6f
Closes-bug: #1243193"
1175,9d3a07ed04952c1fd8e65b5f46671208b793d659,1390592055,,1.0,44,12,4,3,1,0.985714375,True,5.0,3026226.0,80.0,32.0,False,6.0,4495852.0,8.0,5.0,15.0,1321.0,1324.0,15.0,1110.0,1113.0,9.0,575.0,575.0,0.013755158,0.792297111,0.792297111,288,694,1268711,neutron,9d3a07ed04952c1fd8e65b5f46671208b793d659,0,0,No bug. Just Remove psutil dependency ,"Bug #1268711 in neutron: ""Unlock the psutil version requirement or use an alternative to psutil""","psutil is currently version locked at psutil>=0.6.1,<1.0
These versions are not in PyPI, which is not liked by the new pip.
For further information see the comments in https://review.openstack.org/65209","Remove psutil dependency

The version of psutil that was being required is not hosted on
PyPi which caused some issues. This patch removes the psutil
dependency in favor of using the method that was proposed for
the havana backport of polling minimization.

Closes-bug: #1268711
Change-Id: I5a1672cfd195099d92578321153c42b8bfd09b7d"
1176,9d4b49c542e2076c8a572d1e8c6d50e255efe087,1404313411,,1.0,26,9,2,2,1,0.775512658,False,,,,,True,,,,,,,,,,,,,,,,,1043,1487,1338451,nova,9d4b49c542e2076c8a572d1e8c6d50e255efe087,1,1,,"Bug #1338451 in OpenStack Compute (nova): ""shelve api does not work in the nova-cell environment""","If you run nova shelve api in nova-cell environment It throws following error:
Nova cell (n-cell-child) Logs:
2014-07-06 23:57:13.445 ERROR nova.cells.messaging [req-a689a1a1-4634-4634-974a-7343b5554f46 admin admin] Error processing message locally: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging Traceback (most recent call last):
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 200, in _process_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 1287, in _process_message_locally
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return fn(message, **message.method_kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/cells/messaging.py"", line 700, in run_compute_api_method
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 192, in wrapped
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return func(self, context, target, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 182, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return function(self, context, instance, *args, **kwargs)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 163, in inner
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     return f(self, context, instance, *args, **kw)
2014-07-06 23:57:13.445 TRACE nova.cells.messaging   File ""/opt/stack/nova/nova/compute/api.py"", line 2458, in shelve
2014-07-06 23:57:13.445 TRACE nova.cells.messaging     instance.save(expected_task_state=[None])
2014-07-06 23:57:13.445 TRACE nova.cells.messaging TypeError: save() got an unexpected keyword argument 'expected_task_state'
2014-07-06 23:57:13.445 TRACE nova.cells.messaging
Nova compute log:
2014-07-07 00:05:19.084 ERROR oslo.messaging.rpc.dispatcher [req-9539189d-239b-4e74-8aea-8076740
31c2f admin admin] Exception during message handling: 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _
dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _
dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _
do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dis
t-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     pass
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 330, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 308, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 3847, in shelve_instance
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     self.conductor_api.notify_usage_exists(
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/api.py"", line 206, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 320, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     extra_usage_info=extra_usage_info_p)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     retry=self.retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout, retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 401, in send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     retry=retry)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 392, in _send
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     system_metadata, extra_usage_info)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     ignore_missing_network_data)
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher     macs = [vif['address'] for vif in nw_info]
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.084 TRACE oslo.messaging.rpc.dispatcher
2014-07-07 00:05:19.093 ERROR oslo.messaging._drivers.common [req-9539189d-239b-4e74-8aea-807674031c2f admin admin] Returning exception 'NoneType' object is not iterable
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/conductor/manager.py"", line 351, in notify_usage_exists
    system_metadata, extra_usage_info)
  File ""/opt/stack/nova/nova/compute/utils.py"", line 250, in notify_usage_exists
    ignore_missing_network_data)
  File ""/opt/stack/nova/nova/notifications.py"", line 285, in bandwidth_usage
    macs = [vif['address'] for vif in nw_info]
TypeError: 'NoneType' object is not iterable
 to caller
Shelve api is failing in nova-cell environment because the compute_api shelve/unshelve
methods expect an Instance object, but cell is still passing the sqlalchemy form.
Also 'info_cache' and 'metadata' are not present in instance-object and shelve requires these
properties to be present in Instance object.","shelve doesn't work on nova-cells environment

The compute_api shelve/unshelve methods expect an Instance object,
but cell is still passing the sqlalchemy form.
So adding shelve/unshelve to the list of methods that will receive an
Instance object when using nova-cell.

When shelving instance with nova-cell environment,
instance object needs to contain 'metadata' and 'info_cache'.
So fetching the same from databse and adding it to
instance object before calling shelve/unshelve methods.

Closes-Bug: #1338451
Change-Id: Ia31b33c9a1d347f15d3e0a67fa31ef6b25a7407a"
1177,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,1406150152,,2.0,272,14,9,8,1,0.761085799,False,,,,,True,,,,,,,,,,,,,,,,,1092,1543,1347028,nova,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,1,1,,"Bug #1347028 in OpenStack Compute (nova): ""block_device mapping identifies ephemeral disks incorrectly""","Ephemeral drives are destinaton == local, but the new bdm code bases it on source instead.  This leads to improper errors:
$ nova boot --flavor m1.tiny --block-device source=blank,dest=volume,bus=virtio,size=1,bootindex=0 test
ERROR (BadRequest): Ephemeral disks requested are larger than the instance type allows. (HTTP 400) (Request-ID: req-53247c8e-d14e-43e2-b01e-85b49f520e61)
The code is here:
https://github.com/openstack/nova/blob/106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71/nova/block_device.py#L411
This should be checking destination_type == 'local' instead of source type.","Allow empty volumes to be created

The following patch allows an empty volume to be created and
attached to an instance at boot time. Today, you can define a
bootable volume that is sourced from an image, volume, or
snapshot. However, you cannot define a volume that is sourced
from 'blank', even though it is one of the source options
available. For example, the following command will not work:
nova boot --flavor m1.tiny
--block-device source=blank,dest=volume,size=1,bootindex=0 test.
This is because the method used to identify ephemeral volumes
(new_format_is_ephemeral) believes any block device with
source_type=blank is an ephemeral, and there is no logic to
handle volumes where source_type=blank. The following patch
fixes these bugs by properly identifying ephemeral block
devices and creating blank/empty volumes when source_type=blank
and destination_type=volume.

Change-Id: I5aa9684bfad1749fadff3018b13a225ed8f16fe8
Closes-Bug: #1347028
Closes-Bug: #1347499"
1178,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,1406150152,,2.0,272,14,9,8,1,0.761085799,False,,,,,True,,,,,,,,,,,,,,,,,1096,1547,1347499,nova,9d64a827b9c2c5f332b3e57f6cb818d3f4735d23,0,0,"Feature ""Allow empty volumes to be created”","Bug #1347499 in OpenStack Compute (nova): ""block-device source=blank,dest=volume is allowed as a combination, but won't work""","This is a spin-off of https://bugs.launchpad.net/nova/+bug/1347028
As per the example given there -  currently source=blank, destination=volume will not work. We should either make it create an empty volume and attach it, or disallow it in the API.","Allow empty volumes to be created

The following patch allows an empty volume to be created and
attached to an instance at boot time. Today, you can define a
bootable volume that is sourced from an image, volume, or
snapshot. However, you cannot define a volume that is sourced
from 'blank', even though it is one of the source options
available. For example, the following command will not work:
nova boot --flavor m1.tiny
--block-device source=blank,dest=volume,size=1,bootindex=0 test.
This is because the method used to identify ephemeral volumes
(new_format_is_ephemeral) believes any block device with
source_type=blank is an ephemeral, and there is no logic to
handle volumes where source_type=blank. The following patch
fixes these bugs by properly identifying ephemeral block
devices and creating blank/empty volumes when source_type=blank
and destination_type=volume.

Change-Id: I5aa9684bfad1749fadff3018b13a225ed8f16fe8
Closes-Bug: #1347028
Closes-Bug: #1347499"
1179,9da60d0a417dd70c16ae34f5877c564e425e4cf8,1396389737,1.0,1.0,3,12,2,2,1,0.970950594,True,4.0,574900.0,59.0,7.0,False,31.0,55877.0,53.0,4.0,1194.0,2005.0,2031.0,1022.0,1494.0,1520.0,731.0,877.0,899.0,0.686035614,0.822867854,0.84348641,1483,1215387,1215387,neutron,9da60d0a417dd70c16ae34f5877c564e425e4cf8,1,1, ,"Bug #1215387 in neutron: ""can not stop l3-agent router forwarding packets by admin_state_up false""","We can not stop router forwarding packets by admin_state_up false.
Master branch has this problem (stable/grizzly branch don't have the problem).
I know the cause. When run router-update --admin_state_up false, transitions as follows:
sync_routers(l3_rpc_base)→list_active_sync_routers_on_active_l3_agent(agent schedulers_db)→get_sync_data(l3_db)→_get_sync_routers(l3_db)
list_active_sync_routers_on_active_l3_agent pass key 'active=True', and router that set admin_state_up false is ignored by filters['admin_state_up'] = [active] in _get_sync_routers.
I think that active=True is not identical  admin_state_up True and that is more similar status Active.
I will modify here.","Delete routers that are requested but not reported as active

There are two cases that I can think of that result in a router being
requested but not reported as active.  One is that admin_state_up has
been set to False.  In this case, the router is never removed and
continues to be operational.

The other case is if a router is changed and then deleted before the
change is processed.  In this case, it is prudent to be sure that the
router is queued for deletion.

Change-Id: I4738f599a18f0d130cc8ad4d4dafc488eec75ffd
Closes-Bug: #1215387"
1180,9df867c672bfb3f80511086c889b744113c56604,1396406041,,1.0,7,6,2,2,1,0.619382195,True,2.0,255210.0,33.0,4.0,False,7.0,1568187.0,10.0,4.0,1443.0,2032.0,2224.0,1197.0,1519.0,1654.0,802.0,883.0,938.0,0.749766573,0.825396825,0.8767507,719,1145,1301105,neutron,9df867c672bfb3f80511086c889b744113c56604,1,1,,"Bug #1301105 in neutron: ""Second firewall creation returns 500""","Second firewall creation returns 500.
It is an expected behavior of firewall reference implementation and an internal server error should not be returned.
It is some kind of quota error and 409 looks appropriate.","Return 409 for second firewall creation

Second firewall creation returns 500, but it is an expected behavior
of firewall reference implementation and an internal server error
should not be returned.

Change-Id: I9f537b238007d35172e2504591d9d3568ba3a41a
Closes-Bug: #1301105"
1181,9e1c61b93ab3523bc1b5510775c1ee3331097f21,1398369043,0.0,1.0,6,8,3,3,1,0.941734626,True,1.0,417868.0,34.0,10.0,False,35.0,65010.0,63.0,8.0,125.0,1584.0,1608.0,125.0,1332.0,1356.0,118.0,972.0,992.0,0.100932994,0.825275657,0.842239186,833,1262,1312402,neutron,9e1c61b93ab3523bc1b5510775c1ee3331097f21,1,1,,"Bug #1312402 in neutron: ""Setting gateway in L3 should use ""replace default via ..."" instead of add.""",Just came across this.  I noticed that the ip_lib.py code uses replace instead of add.  Using the common code would be good anyway.,"Use set_gateway from ip_lib

Change-Id: I7df0f2b09a0d312902fc4745e0a245b5756074d9
Closes-Bug: #1312402"
1182,9e247277e2d916fa705715e018ef5f3f221f0efa,1392688976,,1.0,27,32,15,6,1,0.955473313,True,4.0,6513046.0,70.0,29.0,False,40.0,981744.0667,70.0,13.0,50.0,1644.0,1661.0,40.0,1426.0,1436.0,29.0,743.0,745.0,0.036540804,0.906211937,0.90864799,331,741,1273259,neutron,9e247277e2d916fa705715e018ef5f3f221f0efa,0,0,tests,"Bug #1273259 in neutron: ""Unit test failure: delete_port() got an unexpected keyword argument 'l3_port_check'""","FAIL: neutron.tests.unit.test_extension_ext_gw_mode.TestL3GwModeMixin.test_update_router_gw_with_gw_info_none
tags: worker-3
----------------------------------------------------------------------
Empty attachments:
pythonlogging:''
stderr
stdout
Traceback (most recent call last):
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 251, in test_update_router_gw_with_gw_info_none
self._test_update_router_gw(None, True)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/tests/unit/test_extension_ext_gw_mode.py"", line 238, in _test_update_router_gw
self.context, self.router.id, gw_info)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_gwmode_db.py"", line 62, in _update_router_gw_info
context, router_id, info, router=router)
File ""/home/jenkins/workspace/gate-neutron-python27/neutron/db/l3_db.py"", line 205, in _update_router_gw_info
   l3_port_check=False)
TypeError: delete_port() got an unexpected keyword argument 'l3_port_check'","tests/unit: Initialize core plugin in TestL3GwModeMixin

TestL3GwModeMixin can fail randomly because it doesn't initialize
core_plugin and can be run random core plugin depending on execution
order of tests. It also fails with core plugin uninitialized when it
is run without other tests.

This patch refactors the setup code of core plugin and apply it to the
related tests.
This patch reveled the same bug of test_metaplugin.py which is also
fixed by this patch.

Closes-bug: #1273259
Change-Id: I3c1d4d8b3d69262b89c7747daa8267bf2c8e7f6b"
1183,9e7e8daa66f0c05ac7ce3927562d246baf70067f,1387280877,,1.0,17,2,2,2,1,0.981940787,True,10.0,2697742.0,32.0,13.0,False,38.0,698234.0,70.0,7.0,2.0,2250.0,2251.0,2.0,1978.0,1979.0,2.0,1203.0,1204.0,0.000436237,0.175076341,0.175221754,195,598,1261675,nova,9e7e8daa66f0c05ac7ce3927562d246baf70067f,1,1, Unable to snapshot an instance with ephemeral RBD ,"Bug #1261675 in OpenStack Compute (nova): ""Unable to snapshot an instance with ephemeral RBD""","When creating a snapshot from a VM instance, following error appears in Nova compute log:
 2013-12-17 10:39:47.943 28210 ERROR nova.openstack.common.rpc.amqp [req-3a82cad9-3213-47e8-8a42-0f6e2a75008a a2eecc1caf5f40c5ab50405b68730c20 e255029e4a614ed1a5412192db588e74] Exception during message handling
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     **args)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 353, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     payload)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     pass
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 319, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     % image_id, instance=instance)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 309, in decorated_function
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     *args, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2293, in snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     task_states.IMAGE_SNAPSHOT)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 2324, in _snapshot_instance
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     update_task_state)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py"", line 1399, in snapshot
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     snapshot_backend.snapshot_extract(out_path, image_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/libvirt/imagebackend.py"", line 531, in snapshot_extract
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     images.convert_image(snap, target, out_format)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/virt/images.py"", line 179, in convert_image
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     utils.execute(*cmd, run_as_root=run_as_root)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/utils.py"", line 177, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     return processutils.execute(*cmd, **kwargs)
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 178, in execute
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp     cmd=' '.join(cmd))
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Command: qemu-img convert -O qcow2 rbd:instances/instance-0000000f_disk /var/lib/nova/instances/snapshots/tmp63tXq0/b916d3ca7fbe46cba4ca7ce5e0138ee9
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Exit code: 1
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stdout: ''
2013-12-17 10:39:47.943 28210 TRACE nova.openstack.common.rpc.amqp Stderr: ""qemu-img: error connecting\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk': Operation not supported\nqemu-img: Could not open 'rbd:instances/instance-0000000f_disk'\n""","Pass rbd_user id and conf path  as part of RBD URI for qemu-img

Unlike rados.py, qemu-img does not use id from CEPH_ARGS and has to be
given a ceph user id and conf path explicitly in RBD URI

Closes-Bug: #1261675
Change-Id: I9226515f2a860ed45d12012fd5b6bc7e842f6d32"
1184,9e858bebb89de05b1c9ecc27f5bd9fbff95a728e,1394490341,,1.0,2,2,2,2,1,1.0,True,1.0,42663.0,6.0,2.0,False,5.0,7639588.5,7.0,2.0,2.0,1204.0,1206.0,2.0,963.0,965.0,2.0,1059.0,1061.0,0.001976285,0.69828722,0.699604743,414,829,1280409,cinder,9e858bebb89de05b1c9ecc27f5bd9fbff95a728e,1,1,,"Bug #1280409 in Cinder: ""Cinder Eqlx driver fails to SSHInjectionThreat""","Cinder's Equallogic driver fails to create any volumes because /usr/lib/python2.6/site-packages/cinder/utils.py check_ssh_injection treats spaces as bad things. Tested with latest RDO Havana release.
2014-02-14 14:16:35.386 12786 ERROR cinder.openstack.common.rpc.common [req-b7476613-4e55-4407-be61-738081c20040 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection\n    conn_info = self.driver.initialize_connection(volume, connector)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection\n    volume[\'name\'])\n', '  File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__\n    self.gen.next()\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection\n    self._eql_execute(*cmd)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute\n    args, attempts=self.configuration.eqlx_cli_max_retries)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh\n    utils.check_ssh_injection(cmd_list)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection\n    raise exception.SSHInjectionThreat(command=str(cmd_list))\n', ""SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')\n""]
2014-02-14 14:16:38.308 12786 INFO cinder.volume.manager [req-2d51da59-6aba-4212-a9a2-61864fe18cf3 None None] Updating volume status
2014-02-14 14:16:39.470 12786 ERROR cinder.volume.drivers.eqlx [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Failed to initialize connection to volume volume-3045438e-096a-4838-a769-ec39692fa41f
2014-02-14 14:16:39.470 12786 ERROR cinder.openstack.common.rpc.amqp [req-a73e9c7c-e8ed-4dd1-a9f4-f6897b52f996 d9ac62582e7f4d4ab19a8df75bc8c06d bdf89879d78f4d278e8aad9f88cfb92e] Exception during message handling
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 624, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 406, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     volume['name'])
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/contextlib.py"", line 23, in __exit__
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 397, in initialize_connection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     self._eql_execute(*cmd)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 219, in _eql_execute
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     args, attempts=self.configuration.eqlx_cli_max_retries)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py"", line 177, in _run_ssh
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     utils.check_ssh_injection(cmd_list)
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 166, in check_ssh_injection
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp     raise exception.SSHInjectionThreat(command=str(cmd_list))
2014-02-14 14:16:39.470 12786 TRACE cinder.openstack.common.rpc.amqp SSHInjectionThreat: SSH command injection detected: ('volume', 'select', u'volume-3045438e-096a-4838-a769-ec39692fa41f', 'access', 'create', 'initiator', u'iqn.1994-05.com.redhat:249bde2d589', 'authmethod chap', 'username', 'cinder')
Reason and fix for this is simple:
Line 395 @ /usr/lib/python2.6/site-packages/cinder/volume/drivers/eqlx.py:
                cmd.extend(['authmethod chap', 'username',
When it should be (to pass the current requirements at /usr/lib/python2.6/site-packages/cinder/utils.py's check_ssh_injection):
                cmd.extend(['authmethod', 'chap', 'username',","Fixes ssh-injection error while using chap authentication

A space in the command construction was being caught by the
ssh-injection check. The fix is to separate the command strings.

Change-Id: If1f719f9c2ceff31ed5386c53cf60bc7f522f4d7
Closes-Bug: #1280409"
1185,9e8657839844ecef3348dad51aca6411f05da99d,1401870284,,1.0,20,3,2,2,1,0.987692509,False,,,,,True,,,,,,,,,,,,,,,,,904,1340,1321653,nova,9e8657839844ecef3348dad51aca6411f05da99d,1,1,,"Bug #1321653 in OpenStack Compute (nova): ""Got 500  when adding list type host to an aggregate""","Steps to reproduce as admin:
1. create an aggregate  (ID 1)
$ nova aggregate-create foo
2. curl -i ""http://127.0.0.1:8774/v2/""`keystone token-get | awk '/ tenant_id /{print $4}'`""/os-aggregates/1/action"" -X POST -H ""Content-Type: application/json"" -H ""X-Auth-Token: ""`keystone token-get | awk '/ id /{print $4}'` -d '{""add_host"": {""host"": [""host-2"", ""host-1""]}}
HTTP/1.1 500 Internal Server Error
Content-Length: 128
Content-Type: application/json; charset=UTF-8
X-Compute-Request-Id: req-f6ea35a8-029a-444a-9741-7c6abd27f294
Date: Wed, 21 May 2014 08:34:57 GMT
{""computeFault"": {""message"": ""The server has either erred or is incapable of performing the requested operation."", ""code"": 500}
Expected solution:
A:  Response with  400(Bad Request) when the ""host"" value is not the expected type.
B:  Add multiple hosts by  a single request","Correctly reject request to add lists of hosts to an aggregate

Attempting to add or remove a list of hosts to an aggregate
results in an internal server error. This patch fixes the input
validation so such a request is correctly rejected and a 400
error returned.

Change-Id: I760db36d685cff20b01a72160b8b3fbb8a70b412
Closes-Bug: #1321653"
1186,9ea42f50d3136885ffb1f7b23a7d831165f25247,1406018752,,1.0,20,3,2,2,1,0.666578358,False,,,,,True,,,,,,,,,,,,,,,,,1089,1540,1346638,neutron,9ea42f50d3136885ffb1f7b23a7d831165f25247,1,0,"""Now that the DB is healed, neutron-db-manage revision --autogenerate needs to be updated.”","Bug #1346638 in neutron: ""neutron-db-manage --autogenerate needs update after DB healing""","Now that the DB is healed, neutron-db-manage revision --autogenerate needs to be updated.
The template should do unconditional upgrade/downgrade.
The env.py should include all models from head to compare against the schema.","Change autogenerate to be unconditional

Template for generating db migration scripts are now not run according
to used plugins. Target environment is set to use all available models
but also used plugins are kept for backward compatibility.

Part of this patch is script that will drop unused tables to avoid
generating the drop in future migration scripts.

Change-Id: I90a7d3416de24f2317ccdeb828bb8bd973ee5e61
Closes-bug: #1346638"
1187,9f138a1efdab9dd0f7b4bbfc9835bd53dfed762b,1384761332,,1.0,1,1,1,1,1,0.0,True,1.0,287088.0,19.0,6.0,False,40.0,1143390.0,91.0,4.0,7.0,703.0,708.0,7.0,679.0,684.0,2.0,639.0,639.0,0.002579536,0.550300946,0.550300946,1772,1252175,1252175,cinder,9f138a1efdab9dd0f7b4bbfc9835bd53dfed762b,1,1,Bug in comments,"Bug #1252175 in Cinder: ""Fixes error message when create snapshot""","Error message in create_snapshot not appropriate while volume is migrating,as follows:
    msg = _(""Volume cannot be deleted while migrating"")","Fixes inappropriate error message.

""Volume cannot be deleted while migrating"" as error message in
create_snapshot is inappropriate while volume is migrating.

Change-Id: I6cfaf312f9b1edbf4e54f737664290d6ab3b6332
Closes-Bug: #1252175"
1188,9f1f9a8b6fadbba2eb6733e0f13b727f14a52a3f,1408587497,,1.0,32,1,2,2,1,0.32984607,False,,,,,True,,,,,,,,,,,,,,,,,1221,1680,1359427,nova,9f1f9a8b6fadbba2eb6733e0f13b727f14a52a3f,1,1,,"Bug #1359427 in OpenStack Compute (nova): ""Potential codec errors during rescheduling operations""","In the compute manager's 'retry' logic (i.e., handling RescheduledException) in _build_and_run_instance, it has a catch-all case for all exceptions and sets reason=str(e).  This works well in most cases, but for cases in which lower level code may generate locale-specific messages, it's possible to see the ""UnicodeEncodeError: 'ascii' codec can't encode character..."" error, which ultimately masks the message in the compute logs, etc.
It also means the instance won't be rescheduled as it fails with an error similar to...
WARNING nova.compute.manager [req-7fe662b1-c947-4303-b43f-114fbdafe875 None] [instance: 333474eb-dd07-4e05-b920-787da0fd5b32] Unexpected build failure, not rescheduling build.
(i.e., as a result of the UnicodeEncodeError previously mentioned)
The simple solution is to use six.text_type(e) instead of str(e).","Handle non-ascii characters in spawn exception msg

If a compute driver's spawn function returns an exception whose text
has any non-ascii characters (e.g., for globalization), the compute
manager's ""reschedule logic"" doesn't handle it properly as it runs
the text through the str() function, which causes UnicodeEncodeError
exceptions.

This simply sends the exception text through six.text_type so as to
no longer blow up and proceed with rescheduling as intended.

Closes-Bug: #1359427

Change-Id: I3fef783feffbe99c6971543c881e3fce2b60693b"
1189,9f673c2482528bdde15776c52928dbaceecdf811,1397712824,,1.0,82,11,3,2,1,0.66925203,True,3.0,1833860.0,63.0,10.0,False,9.0,182334.0,17.0,1.0,24.0,223.0,234.0,24.0,223.0,234.0,24.0,223.0,234.0,0.021777003,0.195121951,0.204703833,809,1237,1308927,neutron,9f673c2482528bdde15776c52928dbaceecdf811,1,1,,"Bug #1308927 in neutron: ""ofagent: inbound flow for tunnels is spec-wise broken""","the flow _provision_local_vlan_inbound_for_tunnel installs needs push_vlan.
while the current code happens to work with older versions of OVS,
the latest OVS correctly rejects the flow.","ofagent: Add a missing push_vlan action

Fix the flow for _provision_local_vlan_inbound_for_tunnel
to push-vlan explicitly.  While the old code happened to
work with older versions of OVS, it was spec-wise incorrect because
it failed to meet the prerequisite of the following set-field.
The latest version of OVS correctly rejects such a flow.

Closes-Bug: #1308927
Change-Id: I66221eec0cb4083d178d7d5651360ee1874e3d1b"
1190,9f7c5bf5ad38237f2adac980afb69fcdb26678a3,1389325417,,1.0,2,2,1,1,1,0.0,True,2.0,2683934.0,21.0,9.0,False,98.0,139393.0,240.0,4.0,7.0,4337.0,4337.0,7.0,3500.0,3500.0,7.0,3348.0,3348.0,0.001142857,0.478428571,0.478428571,274,679,1267300,nova,9f7c5bf5ad38237f2adac980afb69fcdb26678a3,0,0,It’s better to use…,"Bug #1267300 in OpenStack Compute (nova): ""type() method should be replaced with isinstance() in nova/utils.py""","In nova/utils.py, a func use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.
The code is:
def convert_version_to_int(version):
    try:
        if type(version) == str:
            version = convert_version_to_tuple(version)
       if type(version) == tuple:
           return reduce(lambda x, y: (x * 1000) + y, version)
    except Exception:
           raise exception.NovaException(message=""Hypervisor version invalid."")
this bug is fixed in glance:
https://review.openstack.org/#/c/65611/4","replace type() to isinstance() in nova

In nova/utils.py, a func use the ""type"" method to
determine the type. It's bertter to use the ""isinstance""
method instead.this bug is fixed in glance:
https://review.openstack.org/#/c/65611/4

Change-Id: I10d25d2308436239c50cf3932c212e22c45f92a8
Closes-Bug: #1267300"
1191,9fc8b6c4b4d43df50b9bc15f4fdf274ad6dd711c,1408045569,,1.0,0,15,3,3,1,0.844353685,False,,,,,True,,,,,,,,,,,,,,,,,1195,1653,1357048,neutron,9fc8b6c4b4d43df50b9bc15f4fdf274ad6dd711c,0,0,"Refactoring ""does not have anymore any restriction on transformation of centralized routers in distributed.”","Bug #1357048 in neutron: ""NSX: restriction on distributed update must be lifted""","The NSX backend from version 4.1 does not have anymore any restriction on transformation of centralized routers in distributed.
Version 3.x instead could not transform distributed routers into centralized, which is anyway consistent with the current DVR extension.
The current restriction specific for the NSX plugin must therefore be lifted.","NSX: lift restriction on DVR update

The restriction in place in the code is not justified considered
the capabilities of the DVR extension and the NSX backend.

Transformations of centralized routers into distributed should be
allowed.

Change-Id: I099b90d39247bc9a7adfb87344d77ccd8acfad9e
Closes-Bug: #1357048"
1192,a00f41a02b90ca93e21e9a98236af21b454872b3,1407420053,,1.0,12,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1169,1626,1354801,cinder,a00f41a02b90ca93e21e9a98236af21b454872b3,1,0,"""The Swift backup driver for Cinder supports only Auth 1.0,”","Bug #1354801 in Cinder: ""Cinder backup for Swift does not work with Auth 2.0""","The Swift backup driver for Cinder supports only Auth 1.0, for Swift clusters that work with Auth 2.0, the backup driver cannot authenticate with Swift.","Enable Swift backup driver for auth 2.0

This patch adds support for Swift auth 2.0 to the cinder-backup
Swift driver.

DocImpact

Change-Id: I99585661792420e363b5259c9ed45a853a939a74
Closes-Bug: #1354801"
1193,a058e90c5ec33e35c3bc88fed644b598e490e9ad,1400919156,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,875,1309,1317573,cinder,a058e90c5ec33e35c3bc88fed644b598e490e9ad,1,1,,"Bug #1317573 in Cinder: ""logger: fail to mount gluster storage shows as warning and not error ""","failure to mount gluster storage appears in the cinder volume log as a warning and not an error
since we would basically not be able to create/delete/attach when we fail to mount I think that this should be logged as ERROR and if possible print the trace in the log.
2014-05-08 18:54:45.306 3121 WARNING cinder.volume.drivers.glusterfs [-] Exception during mounting Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf mount -t glusterfs 10.35.64.104:/gluster2-cinder-tshefi /var/lib/cinder/mnt/8f2d3277bf483ae9bae356bd866fed5e
Exit code: 1
Stdout: 'Mount failed. Please check the log file for more details.\n'
Stderr: ''","Use error instead of warning to log mount exc

Gluster's driver uses warning to log mount exceptions. This patch
replaces that warning with an error message.

Change-Id: I54a10ec8abf460ca9ff7bb009b8d702cfad0fed0
Closes-Bug: #1317573"
1194,a060b6c0b7d26b02ac2ca15ede49fa56a026efda,1392180285,,1.0,6,6,6,5,1,1.0,True,2.0,1015305.0,43.0,6.0,False,19.0,2879108.333,30.0,3.0,11.0,1083.0,1094.0,11.0,951.0,962.0,0.0,603.0,603.0,0.00127551,0.770408163,0.770408163,399,813,1279611,neutron,a060b6c0b7d26b02ac2ca15ede49fa56a026efda,1,0,ulrparse not compatible with python3,"Bug #1279611 in neutron: "" urlparse is incompatible for python 3""","import urlparse
should be changed to :
import six.moves.urllib.parse as urlparse
for python3 compatible.","Use six.moves.urllib.parse instead of urlparse

To keep Python 3.x compatibility, use six.moves.urllib.parse to
replace urlparse.

Closes-Bug: #1279611
Change-Id: I712035926f449e1bc5ab1fe85927369b826cb9cc"
1195,a0693a91f55b2b08a46f1396dd1a1102783df9c6,1385439588,,1.0,15,7,2,2,1,0.684038436,True,3.0,5288124.0,18.0,10.0,False,13.0,331109.0,24.0,2.0,8.0,669.0,671.0,8.0,635.0,637.0,6.0,630.0,630.0,0.005877414,0.529806885,0.529806885,79,476,1254963,cinder,a0693a91f55b2b08a46f1396dd1a1102783df9c6,0,0,removes redundant conde,"Bug #1254963 in Cinder: ""Remove the redundant check for 'os-migrate_volume_completion'""","Remove the redundant KeyError check for 'os-migrate_volume_completion'
The fact that this extension is registered as WSGI action implies that the body must contain a key with the same name as action.","Redundant check in os-migrate_volume_completion

Remove the redundant check for 'migrate_volume_completion', and add the UT
for it.

Change-Id: I6b643dbf9ae1302cafe6748a361123d7d76c437f
Closes-Bug: #1254963"
1196,a074fec5f5d27ba66602a5b6133e5bfd49808c57,1405697364,,1.0,3,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1080,1530,1344072,cinder,a074fec5f5d27ba66602a5b6133e5bfd49808c57,1,1,"""The following commit 4fdcbff96790753a4c1a508600e5d78b2c3b7172 introduced a”","Bug #1344072 in Cinder: ""Hard coded references from gettextutils.py should be removed""",The following commit 4fdcbff96790753a4c1a508600e5d78b2c3b7172 introduced a few hard coded references to '/home/jsbryant/cinder-dev/gettextutilsSync/' in cinder/openstack/common/gettextutils.py.  This should be removed.,"Remove hard coded reference from gettextutils.py

The following commit 4fdcbff96790753a4c1a508600e5d78b2c3b7172
introduced a few hard coded references to
'/home/jsbryant/cinder-dev/gettextutilsSync/' in
cinder/openstack/common/gettextutils.py. This patch will
removed those references.

Change-Id: Ib880863d275ac102c1eb723972d1ebd2eb3405a1
Closes-Bug: #1344072"
1197,a0a462f0303f68d885ff344898437e310c64188c,1383159647,,1.0,56,5,2,2,1,0.560576943,True,3.0,13464085.0,51.0,19.0,False,7.0,1634027.0,10.0,8.0,13.0,1128.0,1129.0,13.0,1041.0,1042.0,8.0,432.0,432.0,0.01754386,0.844054581,0.844054581,1734,1246080,1246080,neutron,a0a462f0303f68d885ff344898437e310c64188c,1,1,Wrong logic,"Bug #1246080 in neutron: ""Cisco nexus plugin fails to untrunk vlan if other hosts using vlan""","If two or more compute hosts have instances which are
sharing a given VLAN on a Nexus switch, and then
all instances on one of the hosts which are using that
VLAN are terminated, while instances which are using
that VLAN on other hosts remain active, then
the VLAN is not being untrunked from the
corresponding interface on the Nexus switch as
expected.
Note that the VLAN is correctly untrunked from
the Nexus interface when the instance being
terminated is the last instance which is using that
VLAN on the Nexus switch.
The correct logic should be:
If this the last instance using this VLAN on this switch interface:
____untrunk the vlan from the switch interface
____If this the last instance using this VLAN on this switch:
_________delete the VLAN from the switch
Note that this bug also exists in the Cisco ML2
mechanism driver, but the code which implements
this is being redesigned, so it will be addressed for
the ML2 separately.","Cisco nexus plugin fails to untrunk vlan if other hosts using vlan

Closes-Bug: #1246080

Without this fix, if two or more compute hosts have
instances which are
sharing a given VLAN on a Nexus switch, and then
all instances on one of the hosts which are using that
VLAN are terminated, while instances which are using
that VLAN on other hosts remain active, then
the VLAN is not being untrunked from the
corresponding interface on the Nexus switch as
expected.

This fix changes the VLAN removal logic from:
----If this the last instance using this VLAN on this switch:
--------untrunk the vlan from the switch interface
--------delete the VLAN from the switch
To:
----If this the last instance using this VLAN on this switch interface:
--------untrunk the vlan from the switch interface
--------If this the last instance using this VLAN on this switch:
------------delete the VLAN from the switch

Note that this bug also exists in the Cisco ML2
mechanism driver, but the code which implements
this is being redesigned, so it will be addressed for
the ML2 separately.

Change-Id: Icb1f95d1db4baa56c0f6fd68ce6342bbff27641d"
1198,a0a6017f9b58941a4f8e67300a5dc57e34aada35,1408012809,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1194,1652,1356815,nova,a0a6017f9b58941a4f8e67300a5dc57e34aada35,1,1,“commit 243879f5c51fc45f03491bcb78765945ddf76be8 was bad”,"Bug #1356815 in OpenStack Compute (nova): ""Nova hacking check for jsonutils used invalid number""","It should have been 324 and not 324
commit 243879f5c51fc45f03491bcb78765945ddf76be8 was bad","Hacking: a new hacking check was added that used an existing number

Commit 243879f5c51fc45f03491bcb78765945ddf76be8 added in a new hacking
check that used an existing number.

The new number is 324 (and not 323)

Change-Id: I7e604a408387438105c435ad16a1fa3d6491b642
Closes-bug: #1356815"
1199,a120ede9ae3f18756db07d1d6696b9ac773b84bf,1400821053,,1.0,112,14,4,3,1,0.737261698,False,,,,,True,,,,,,,,,,,,,,,,,1272,1734,1365884,cinder,a120ede9ae3f18756db07d1d6696b9ac773b84bf,1,1,,"Bug #1365884 in Cinder: ""E-Series driver creates incorrect host type""","Various performance related issues with block IO to the mapped devices on linux hosts it became clear that the choices that were made in how the driver does host creation are not in line with best practices for E-Series arrays. The driver creates a host that is of type 'linux' as it appears from querying the proxy and type ""MPP/RDAC"" when viewing the host within Santricity itself. At first glance ""linux"" would appear reasonable, but in fact the correct default choice should have been LnxALUA (from the proxy) or Linux (DM-MP).","NetApp fix for default host type in eseries

This fixes the issue where the default host type
provided in mapping should be high performing LnxALUA
type for eseries. It also makes it configurable in case
users want to configure a different host type.

Closes-Bug: #1365884

Change-Id: I30992ca69c25c3c02334470aae90c32731a5f3f4"
1200,a17d38b96c4b776de8938195f86953ff26f94c2f,1393293556,,1.0,47,10,2,2,1,0.949452015,True,2.0,24631.0,12.0,4.0,False,51.0,1261046.5,122.0,3.0,55.0,2891.0,2908.0,55.0,2382.0,2399.0,55.0,2825.0,2842.0,0.007526882,0.37983871,0.382123656,469,884,1284312,nova,a17d38b96c4b776de8938195f86953ff26f94c2f,1,1,race condition,"Bug #1284312 in OpenStack Compute (nova): ""vmware driver races to create instance images""","Change Ia0ebd674345734e7cfa31ccd400fdba93646c554 traded one race condition for another. By ignoring all mkdir() calls that would otherwise fail because an instance directory already exists, two nodes racing to create a single image will corrupt or lose data, or fail in a strange way. This call should fail in that case, but doesn't after the recent patch was merged:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L350","vmware: driver races to create instance images

Addresses race condition that occurs when multiple processes
arrive at a fresh Datastore with no vmware_base cache directory.

Change-Id: If1bb6804e3668a56872a653a8ea44d0c7fc70311
Co-Authored-by: Tracy Jones <tjones@vmware.com>
closes-bug: #1284312"
1201,a1dae85094fd1efc9c329885d212b684cd12ffa5,1381832556,,1.0,11,10,3,1,1,0.786583413,True,1.0,34328.0,8.0,4.0,False,137.0,45042.0,399.0,4.0,1319.0,1979.0,3060.0,1194.0,1717.0,2675.0,500.0,1876.0,2149.0,0.079146919,0.296524487,0.339652449,1687,1240022,1240022,nova,a1dae85094fd1efc9c329885d212b684cd12ffa5,1,1,“missing in following files:”,"Bug #1240022 in OpenStack Compute (nova): ""libvirt: i18n missing support""","missing in following files:
    nova/virt/libvirt/config.py
    nova/virt/libvirt/driver.py
    nova/virt/libvirt/volume.py","libvirt: add missing i18n support

There were some log messages that did not support the i18n
format.

Change-Id: Id75de9fb7be2924b431562b6bd527ede399c3d37
Closes-Bug: #1240022"
1202,a1de76d1407952572cfe081c4872d7a6127995b3,1382619235,,1.0,46,35,5,5,1,0.755102031,True,11.0,10885259.0,118.0,37.0,False,7.0,71992.0,24.0,7.0,140.0,1236.0,1258.0,140.0,1169.0,1191.0,99.0,443.0,451.0,0.203665988,0.904276986,0.920570265,11,323,1243129,neutron,a1de76d1407952572cfe081c4872d7a6127995b3,0,0,this should be prohibited,"Bug #1243129 in neutron: ""[LBaaS] Deletion of associated-to-pool(s) health monitor should fail""","Version
=======
Havana on rhel
Description
===========
It's possible to delete health monitor while they are associated to one or more pools, I think that this should be prohibited, and dissociation from pool should be required first.","LBaaS: check for associations before deleting health monitor

Need to prohibit health monitor deletion if it has associations with
pools. Given that pools may belong to different lbaas drivers the process
of monitor deletion becomes complex and unreliable since association
deletion may fail on any single driver.

DocImpact

Closes-Bug: #1243129
Change-Id: I27c20e7a5be8433f90569534ecf838e33027cb00"
1203,a1e1b0a6251c9b0838f871591316c41f37b4b3e0,1387855673,,1.0,36,19,3,2,1,0.451967484,True,8.0,3209760.0,118.0,13.0,False,8.0,437794.0,11.0,2.0,2.0,560.0,560.0,2.0,499.0,499.0,2.0,234.0,234.0,0.004608295,0.360983103,0.360983103,272,677,1267290,neutron,a1e1b0a6251c9b0838f871591316c41f37b4b3e0,1,1,performance bug,"Bug #1267290 in neutron: ""Net-list is very slow under metaplugin""","If there are many networks when using metaplugin, net-list (GET networks API)
takes very long time.
For example: (showing hardware spec etc. is omitted since it is relative comparison.)
--- 200 networks, openvswitch plugin used natively
$ time neutron net-list
...snip
real    0m2.007s
user    0m0.428s
sys     0m0.100s
---
--- 200 openvswitch networks, under metaplugin
$ time neutron net-list
...snip
real    0m7.700s
user    0m0.472s
sys     0m0.072s
---
Note that the quantum-server wastes a lot of cpu usage too.","Enhance GET networks performance of metaplugin

Change to call plugin.get_networks() per target plugin
instead of calling plugin.get_network() per network.
Hook routines are used to select networks which belong
the target plugin.

Change-Id: Ieff06ac7bc7a150501f91aecc3197b0bb664d5fa
Closes-Bug: #1267290"
1204,a1fac106479f9c3c5559f8b2cfbc01fe12d3a575,1413267179,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1387,1857,1378756,neutron,a1fac106479f9c3c5559f8b2cfbc01fe12d3a575,1,1,,"Bug #1378756 in neutron: ""set_context in L3NatTestCaseMixin.floatingip_with_assoc does not work""","We have following code in neutron.test.unit.L3NatTestCaseMixin.floatingip_with_assoc get ""set_context"" from external but not use it.
    @contextlib.contextmanager
    def floatingip_with_assoc(self, port_id=None, fmt=None, fixed_ip=None,
    ######################################
                              set_context=False):                                                 # <---- We get set_context here
    ######################################
        with self.subnet(cidr='11.0.0.0/24') as public_sub:
            self._set_net_external(public_sub['subnet']['network_id'])
            private_port = None
            if port_id:
                private_port = self._show('ports', port_id)
            with test_db_plugin.optional_ctx(private_port,
                                             self.port) as private_port:
                with self.router() as r:
                    sid = private_port['port']['fixed_ips'][0]['subnet_id']
                    private_sub = {'subnet': {'id': sid}}
                    floatingip = None
                    self._add_external_gateway_to_router(
                        r['router']['id'],
                        public_sub['subnet']['network_id'])
                    self._router_interface_action(
                        'add', r['router']['id'],
                        private_sub['subnet']['id'], None)
                    floatingip = self._make_floatingip(
                        fmt or self.fmt,
                        public_sub['subnet']['network_id'],
                        port_id=private_port['port']['id'],
                        fixed_ip=fixed_ip,
    ######################################
                        set_context=False)                                                    ### <---- But we don't really use it
    ######################################
                    yield floatingip","Handle unused set_context in L3NatTestCaseMixin.floatingip_with_assoc

set_context which is passed to floatingip_with_assoc method
is not passed further to self._make_floatingip.

Change-Id: Iecf2ad88e4bad5b1f8fd60668401863bdeecce8f
Closes-Bug: #1378756"
1205,a1fe496e1113737d0b133a64078bc45c485dd3b2,1377891396,2.0,1.0,122,34,3,1,1,0.441031724,True,2.0,5394139.0,11.0,10.0,False,11.0,365102.0,11.0,5.0,1580.0,2322.0,2323.0,1390.0,2093.0,2094.0,36.0,542.0,542.0,0.039827772,0.584499462,0.584499462,1426,1178375,1178375,cinder,a1fe496e1113737d0b133a64078bc45c485dd3b2,1,1,"“Sync the following fix from oslo-incubator:
    76972e2 Support a new qpid topology
    This includes one other commit, so that the above fix could be brought”","Bug #1178375 in Cinder: ""Orphan exchanges in Qpid and lack of option for making queues [un]durable""","Start qpid, nova-api, nova-scheduler, and nova-conductor, and nova-compute.
There are orphan direct exchanges in qpid. Checked using qpid-config exchanges. The exchanges continue to grow, presumably, whenever nova-compute does a periodic update over AMQP.
Moreover, the direct and topic exchanges are by default durable which is a problem. We want the ability to turn on/off the durable option just like Rabbit options.","Sync rpc fix from oslo-incubator

Sync the following fix from oslo-incubator:

76972e2 Support a new qpid topology

This includes one other commit, so that the above fix could be brought
over cleanly:

5ff534d Add config for amqp durable/auto_delete queues

Change-Id: I1fd5aaf87ec87836df3e44e83247bf82301475f5
Closes-bug: #1178375"
1206,a2059b67b207d908b1c65c78dd806b1f5e79582d,1397134745,,1.0,42,17,4,4,1,0.966133036,True,6.0,1108945.0,104.0,35.0,False,216.0,238166.25,1253.0,8.0,1911.0,5068.0,6036.0,1584.0,3880.0,4580.0,999.0,4099.0,4340.0,0.128650457,0.527466873,0.558471633,750,1177,1303360,nova,a2059b67b207d908b1c65c78dd806b1f5e79582d,1,1,,"Bug #1303360 in OpenStack Compute (nova): ""GroupAntiAffinityFilter scheduler hint still doesn't work""","After seeing that the following bug was fixed
https://bugs.launchpad.net/nova/+bug/1296913
executed the cmd:
nova boot --flavor m1.nano --image cirros-0.3.1-x86_64-uec --nic net-id=909e7fa9-b3af-4601-84c2-01145b1dea72 --hint group=foo server-foo
now the server-foo is stuck in scheduling state forever.
see attached logs.","Scheduler: enable scheduler hint to pass the group name

Commit ea6d8403bdc3c1d3ede87a2ed1c8740420c32080 partially
dealt with the problem. The actual scheduling would fail when
the group name was passed.

Change-Id: I668621770e59504164af9a2e861a59e900300bba
Closes-bug: #1303360"
1207,a221a30b32aa94077f352719b2637b51c1809069,1398135414,,1.0,37,17,2,2,1,0.556421567,True,3.0,1183652.0,16.0,6.0,False,25.0,276579.0,63.0,4.0,484.0,1131.0,1131.0,480.0,1016.0,1016.0,464.0,1083.0,1083.0,0.292821159,0.682619647,0.682619647,818,1246,1310559,cinder,a221a30b32aa94077f352719b2637b51c1809069,1,1,,"Bug #1310559 in Cinder: ""Storwize/SVC driver detach volume failed""","The case on our system where create two hosts, one for fc and the other is iscsi
larry@larry-ubuntu:~$ ssh superuser@9.115.247.251  'lshost'
superuser@9.115.247.251's password:
id name                   port_count iogrp_count status
...
7  OpenstackUbun-28670410 1          4           offline
...
9  OpenstackUbun-20533817 2          4           degraded
larry@larry-ubuntu:~/Desktop$ less -R screen-c-vol.2014-04-16-162932.log
larry@larry-ubuntu:~/Desktop$ ssh superuser@9.115.247.251  'lsvdiskhostmap volume-d86758fb-190a-44b9-9442-845c8af93d16'
superuser@9.115.247.251's password:
id  name                                        SCSI_id host_id host_name              vdisk_UID                        IO_group_id IO_group_name
162 volume-d86758fb-190a-44b9-9442-845c8af93d16 0       7       OpenstackUbun-28670410 60050760008F03000C00000000000104 0           io_grp0
2014-04-18 13:55:45.683 ERROR cinder.volume.drivers.ibm.storwize_svc.ssh [req-e31cb4b8-cd98-4415-a8cf-ca525e416524 cbf3ff0436ed4c13a9da2fb2eb4e9277 36e2e0918ba54edaa4ce561926870bca] CLI Exception output:
 command: ['svctask', 'rmvdiskhostmap', '-host', '""OpenstackUbun-20533817""', u'volume-d86758fb-190a-44b9-9442-845c8af93d16']
 stdout:
 stderr: CMMVC5842E The action failed because an object that was specified in the command does not exist.
when  detach the volume, find the host logic error.","Storwize/SVC driver detach volume failed

If config two backends on the same storage, but the protocol is
different, one is FC and the other is iSCSI,  that will create two
hosts on the storage.

When detaching the volume which is attached to the iSCSI host, the
driver will get FC host by default, that causes volume status keeps
'detaching'.

Change-Id: Idbdb3861574197549625b4a6b6141197e42857ff
Closes-Bug: #1310559"
1208,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1382101960,4.0,2.0,134,62,4,2,1,0.722558157,True,20.0,9321064.0,102.0,34.0,False,44.0,274247.0,118.0,7.0,1333.0,2498.0,3540.0,1208.0,2095.0,3019.0,512.0,2184.0,2454.0,0.08044535,0.342637604,0.384977262,1428,1180044,1180044,nova,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1,1,"I’m not sure, maybe when the commit was done the specifications were only one datacenter…","Bug #1180044 in OpenStack Compute (nova): ""nova failures when vCenter has multiple datacenters""","The method at vmops.py _get_datacenter_ref_and_name does not calculate datacenter properly.
    def _get_datacenter_ref_and_name(self):
        """"""Get the datacenter name and the reference.""""""
        dc_obj = self._session._call_method(vim_util, ""get_objects"",
                ""Datacenter"", [""name""])
        vm_util._cancel_retrieve_if_necessary(self._session, dc_obj)
        return dc_obj.objects[0].obj, dc_obj.objects[0].propSet[0].val
This will not be correct on systems with more than one datacenter.
Stack trace from logs:
ERROR nova.compute.manager [req-9395fe41-cf04-4434-bd77-663e93de1d4a foo bar] [instance: 484a42a2-642e-4594-93fe-4f72ddad361f] Error: ['Traceback (most recent call last):\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 942, in _build_instance\n    set_access_ip=set_access_ip)\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 1204, in _spawn\n    LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', '  File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__\n    self.gen.next()\n', '  File ""/opt/stack/nova/nova/compute/manager.py"", line 1200, in _spawn\n    block_device_info)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 176, in spawn\n    block_device_info)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 208, in spawn\n    _execute_create_vm()\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 204, in _execute_create_vm\n    self._session._wait_for_task(instance[\'uuid\'], vm_create_task)\n', '  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 559, in _wait_for_task\n    ret_val = done.wait()\n', '  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait\n    return hubs.get_hub().switch()\n', '  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch\n    return self.greenlet.switch()\n', 'NovaException: A specified parameter was not correct. \nspec.location.folder\n']
vCenter error is:
""A specified parameter was not correct. spec.location.folder""
Work around:
use only one datacenter, use only one cluster, turn on DRS
Additional failures:
2013-07-18 10:59:12.788 DEBUG nova.virt.vmwareapi.vmware_images [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Got image size of 687865856 for the image cde14862-60b8-4360-a145-06585b06577c get_vmdk_size_and_properties /usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmware_images.py:156
2013-07-18 10:59:12.963 WARNING nova.virt.vmwareapi.network_util [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [(ManagedObjectReference){
   value = ""network-1501""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-1458""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-2085""
   _type = ""Network""
 }, (ManagedObjectReference){
   value = ""network-1143""
   _type = ""Network""
 }]
2013-07-18 10:59:13.326 DEBUG nova.virt.vmwareapi.vmops [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Creating VM on the ESX host _execute_create_vm /usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py:207
2013-07-18 10:59:14.258 3145 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-07-18 10:59:14.259 3145 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is 8ef36d061a9341a09d3a5451df798673 multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-07-18 10:59:14.259 3145 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is 680b790574c64a9783fd2138c43f5f6d. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-07-18 10:59:18.757 3145 WARNING nova.virt.vmwareapi.driver [-] Task [CreateVM_Task] (returnval){
   value = ""task-33558""
   _type = ""Task""
 } status: error The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:18.758 ERROR nova.compute.manager [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Instance failed to spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Traceback (most recent call last):
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1103, in _spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] block_device_info)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 177, in spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] block_device_info)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 217, in spawn
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] _execute_create_vm()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 213, in _execute_create_vm
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] self._session._wait_for_task(instance['uuid'], vm_create_task)
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 554, in _wait_for_task
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] ret_val = done.wait()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] return hubs.get_hub().switch()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] return self.greenlet.switch()
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] NovaException: The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:18.758 3145 TRACE nova.compute.manager [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539]
2013-07-18 10:59:20.029 ERROR nova.compute.manager [req-e8306ffe-c6c7-4d0f-a466-fb532375cbd3 7799f10ca7da47f3b2660feb363b370b 0e1771f8db984a3599596fae62609d9a] [instance: 5b3961b6-38d9-409c-881e-fe50f67b1539] Error: ['Traceback (most recent call last):\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 848, in _run_instance\n set_access_ip=set_access_ip)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1107, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', ' File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__\n self.gen.next()\n', ' File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1103, in _spawn\n block_device_info)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 177, in spawn\n block_device_info)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 217, in spawn\n _execute_create_vm()\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/vmops.py"", line 213, in _execute_create_vm\n self._session._wait_for_task(instance[\'uuid\'], vm_create_task)\n', ' File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 554, in _wait_for_task\n ret_val = done.wait()\n', ' File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait\n return hubs.get_hub().switch()\n', ' File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch\n return self.greenlet.switch()\n', 'NovaException: The input arguments had entities that did not belong to the same datacenter.\n']
2013-07-18 10:59:23.831 3145 WARNING nova.virt.vmwareapi.driver [-] Task [CreateVM_Task] (returnval){
   value = ""task-33558""
   _type = ""Task""
 } status: error The input arguments had entities that did not belong to the same datacenter.
2013-07-18 10:59:23.832 3145 WARNING nova.virt.vmwareapi.driver [-] In vmwareapi:_poll_task, Got this error Trying to re-send() an already-triggered event.
2013-07-18 10:59:23.833 3145 ERROR nova.utils [-] in fixed duration looping call
2013-07-18 10:59:23.833 3145 TRACE nova.utils Traceback (most recent call last):
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/nova/utils.py"", line 594, in _inner
2013-07-18 10:59:23.833 3145 TRACE nova.utils self.f(*self.args, **self.kw)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/nova/virt/vmwareapi/driver.py"", line 580, in _poll_task
2013-07-18 10:59:23.833 3145 TRACE nova.utils done.send_exception(excep)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 208, in send_exception
2013-07-18 10:59:23.833 3145 TRACE nova.utils return self.send(None, args)
2013-07-18 10:59:23.833 3145 TRACE nova.utils File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 150, in send
2013-07-18 10:59:23.833 3145 TRACE nova.utils assert self._result is NOT_USED, 'Trying to re-send() an already-triggered event.'
2013-07-18 10:59:23.833 3145 TRACE nova.utils AssertionError: Trying to re-send() an already-triggered event.
2013-07-18 10:59:23.833 3145 TRACE nova.utils","VMware: fix bug when more than one datacenter exists

In the case that there was more than one datacenter defined on the VC,
then spawning an instance would result in an exception. The reason for this
was that the nova compute would not set the correct datacenter for the
selected datastore.

The fix also takes care of the correct folder selection. This too was a
result of not selecting the correct folder for the data center.

The 'fake' configuration was updated to contain an additional data
center with its on datastore.

Closes-Bug: #1180044
Closes-Bug: #1214850

Co-authored-by: Shawn Harsock <hartsocks@vmware.com>

Change-Id: Ib61811fffcbc80385efc3166c9e366fdaa6432bd"
1209,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1382101960,4.0,2.0,134,62,4,2,1,0.722558157,True,20.0,9321064.0,102.0,34.0,False,44.0,274247.0,118.0,7.0,1333.0,2498.0,3540.0,1208.0,2095.0,3019.0,512.0,2184.0,2454.0,0.08044535,0.342637604,0.384977262,1479,1214850,1214850,nova,a25b2ac5f440f7ace4678b21ada6ebf5ce5dff3c,1,0,Software evolution. After some point they need to use two DC and the software was not prepared,"Bug #1214850 in OpenStack Compute (nova): ""vmware driver does not work with more than one datacenter in vC""","""CreateVM_Task"", vm_folder_ref,
config=config_spec, pool=res_pool_ref)
specifies a vm_folder_ref that has no relationship to the datastore.
This may lead to VM construction and placement errors.
NOTE: code selects the 0th datacenter","VMware: fix bug when more than one datacenter exists

In the case that there was more than one datacenter defined on the VC,
then spawning an instance would result in an exception. The reason for this
was that the nova compute would not set the correct datacenter for the
selected datastore.

The fix also takes care of the correct folder selection. This too was a
result of not selecting the correct folder for the data center.

The 'fake' configuration was updated to contain an additional data
center with its on datastore.

Closes-Bug: #1180044
Closes-Bug: #1214850

Co-authored-by: Shawn Harsock <hartsocks@vmware.com>

Change-Id: Ib61811fffcbc80385efc3166c9e366fdaa6432bd"
1210,a269541c603f8923b35b7e722f1b8c0ebd42c95a,1386889949,,1.0,16,6,2,2,1,0.976020648,True,13.0,5206918.0,126.0,34.0,False,20.0,702683.0,26.0,3.0,49.0,1005.0,1019.0,49.0,911.0,925.0,42.0,495.0,505.0,0.068471338,0.789808917,0.805732484,1688,1240027,1240027,neutron,a269541c603f8923b35b7e722f1b8c0ebd42c95a,0,0,"feature, ""Allow multiple DNS forwarders for dnsmasq”","Bug #1240027 in neutron: ""dhcp_agent only allows one dnsmasq_dns_server""","As quantum.agent.linux.dhcp.Dnsmasq makes use of the --server command line when starting dnsmasq there can only be one server configured for the dnsmasq_dns_server option in dhcp_agent.ini
This is not ideal.
Ideally if a network is not created with its own dns server options, the options provided in dnsmasq_dns_server should be used and put into the opts file just as the network-configured options would be.
This would enable dns to work without dns options configured on the network using defaults provided by the deployer.","Allow multiple DNS forwarders for dnsmasq

This patch change the dnsmasq_server configuration option to a ListOpt
in order to enable user to specify multiple DNS forwarders for each
dnsmasq instance.

DocImpact

Change-Id: I21963b4a6c99e4edb11040d77a6aeaa35ff44641
Closes-bug: #1240027"
1211,a272a2838313b56edc1463f6d81a01414b37a78f,1381478759,,1.0,2,2,1,1,1,0.0,True,3.0,93969.0,16.0,8.0,False,1.0,2138223.0,1.0,5.0,797.0,1338.0,1357.0,755.0,1172.0,1191.0,328.0,394.0,395.0,0.744343891,0.893665158,0.895927602,1676,1238561,1238561,neutron,a272a2838313b56edc1463f6d81a01414b37a78f,1,1, ,"Bug #1238561 in neutron: ""Mistake in usage alter_column's parameter type_""","In alembic alter_column have a parameter type_ not _type.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/338d7508968c_vpnaas_peer_address_.py"", line 47, in upgrade
    _type=sa.String(255), existing_type=sa.String(64))
  File ""<string>"", line 7, in alter_column
  File ""<string>"", line 1, in <lambda>
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/util.py"", line 271, in go
    raise TypeError(""Unknown arguments: %s"" % "", "".join(names))
TypeError: Unknown arguments: _type","Fix migration

In migration 338d7508968c_vpnaas_peer_address_ was mistake in
usage alter_column's parameter type_.

Closes-Bug: #1238561

Change-Id: I6e0eb8260cf5e8d8f557b174c7985e2f7ca1c912"
1212,a2ef082cb0b25d14595a8810adb16df3f9661e43,1410545795,,1.0,5,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1309,1775,1368900,neutron,a2ef082cb0b25d14595a8810adb16df3f9661e43,1,1,,"Bug #1368900 in neutron: ""Exception masked due to log exception""","In neutron/plugins/cisco/cfg_agent/device_drivers/driver_mgr.py, if an exception occurs loading the Cfg Agent, a log message is created and it causes a second exception due to incorrect key used to access template name.","Access correct key for template name

When an exception occurs while loading the config agent driver, the
handler tries to log a message, but accesses the wrong key to get the
template name. This causes another exception, which masks the original
exception.

This change accesses the correct key and performs logging inside a
with block to (defensively) preserve the exception context.

Change-Id: I3991bc4c509bbd8d68f7a1c8c8314404f2e0eafa
Closes-Bug: #1368900"
1213,a369f9e39691c01a4e4f7f8668cb37fc17ba03b3,1375867186,4.0,1.0,156,63,5,4,1,0.671763641,True,7.0,1347851.0,42.0,19.0,False,9.0,52277.0,9.0,2.0,0.0,591.0,591.0,0.0,580.0,580.0,0.0,123.0,123.0,0.006451613,0.8,0.8,1441,1196963,1196963,Neutron,a369f9e39691c01a4e4f7f8668cb37fc17ba03b3,0,0,"""As part of the changes to support multiple tunnel_types [1] and deprecate enable_tunneling [2]""","Bug #1196963 in neutron: ""Update the OVS agent code to program tunnels using ports instead of tunnel IDs""","As part of the changes to support multiple tunnel_types [1] and deprecate enable_tunneling [2] in the agent configuration, it was decided during the review to program tunnels using port endpoints instead of using tunnel IDs. Once the OVS agent can concurrently support both GRE and VXLAN tunnel types in the context of ML2, this becomes relevant and will prevent tunnels sharing the same tunnel ID from polluting their BUM traffic between each other.
[1] https://review.openstack.org/#/c/33107/16
[2] https://review.openstack.org/#/c/34779/","Enable GRE and VXLAN with the same ID

Current packet processing in br-tun is based on tun-id,
as a consequence, two networks using different tunnel
types but sharing the same tun-id would not be properly isolated.

To ensure proper isolation within a single bridge, NORMAL action
can't be used any more as it floods unknown unicasts on all
bridges ports. It is replaced by a learn action that dynamically
sets-up flows when packets are recieved from tunnel ports. As mac
address are learnt in explicit flows (in table 20), we can use a
default action in that table to flood unknown unicasts to the
right set of ports, like broadcasts and multicasts packets.

See https://wiki.openstack.org/wiki/Ovs-flow-logic for a more
detailled explanation of the flow logic

Another alternative could have been to use distinct bridges for
each tunnel type (whithout modifying the current flow logic),
but previous alternative may be preferable as it paves the way
for new tunneling optimisations (like RPC based mac learning and
partial-mesh flooding proposed in bp/l2-population)

Change-Id: I1dfe74f96680c2c6fe4d8d4aac4821c6b020c005
Closes-Bug: #1196963"
1214,a38d73d9449675374f45c44b35978735e53cdbaf,1392710461,0.0,1.0,58,18,8,4,1,0.889405618,True,3.0,823483.0,44.0,4.0,False,16.0,2652104.625,33.0,2.0,471.0,1426.0,1642.0,395.0,1241.0,1414.0,214.0,628.0,692.0,0.261557178,0.765206813,0.843065693,425,840,1281574,neutron,a38d73d9449675374f45c44b35978735e53cdbaf,1,1,race condition,"Bug #1281574 in neutron: ""nec plugin should handle OFC port-deletion race condition""","There is a case where multiple OFC delete-port operations run in parallel. It is usually observed in tempest api tests:
ofc-delete-port triggered by delete-network API request and ofc-delete-port request from dhcp-agent (release_dhcp_port) triggered by delete-subnet.
There are several manifests I see,  however this kind of ""not found"" is a valid situation and should be ignored during deleting OFC port.
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/675/ (PortNotFound)
2014-01-28 08:12:37.449 32365 ERROR neutron.api.v2.resource [req-13a26437-c201-4fc6-8af4-dfbff6663f3b None] delete failed
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 356, in delete_network
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     port = self.deactivate_port(context, port)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 252, in deactivate_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     port_status)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 166, in _update_resource_status
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     obj_db = obj_getter(context, id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 266, in _get_port
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource     raise q_exc.PortNotFound(port_id=id)
2014-01-28 08:12:37.449 32365 TRACE neutron.api.v2.resource PortNotFound: Port b6d8480f-4c59-4095-bc72-bc7516fba1d7 could not be found
http://133.242.19.163:8000/neutron-ci-logs/Neutron_Gate/FAILURES/588/
2014-01-24 11:54:40.217 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [202:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 DEBUG neutron.plugins.nec.common.ofc_client [-] OFC returns [404:] do_request /opt/stack/neutron/neutron/plugins/nec/common/ofc_client.py:85
2014-01-24 11:54:40.410 31910 WARNING neutron.plugins.nec.common.ofc_client [-] Operation on OFC failed: status=404, detail=
2014-01-24 11:54:40.411 31910 ERROR neutron.plugins.nec.nec_plugin [-] delete_ofc_port() failed due to An OFC exception has occurred: Operation on OFC failed
2014-01-24 11:54:40.523 31910 ERROR neutron.api.v2.resource [-] delete failed
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nec/nec_plugin.py"", line 362, in delete_network
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource     raise nexc.OFCException(reason=reason)
2014-01-24 11:54:40.523 31910 TRACE neutron.api.v2.resource OFCException: An OFC exception has occurred: Failed to delete port(s)=f77286eb-8493-406c-8240-6e62e027c59d from OFC.","Handle racing condition in OFC port deletion

Multiple delete_port operations can run in parallel.
For such case later operation(s) will receive 404 (NotFound)
error from OFC or NotResultFound sqlalchemy exception.
These are valid exceptions and they should be ignored
in delete_port operations.

Closes-Bug: #1281574

OFCConsistencyBroken is renamed to OFCMappingNotFound
because when multiple delete_port operations run in parallel
OFCConsistencyBroken can occur and it is a valid case
so the excepion name looks inappropriate.

Change-Id: I1511d55994c88b8828f0ff62610c18ddc6dfac8f"
1215,a3a8a86d7bccb824725d91735bd032237786f5aa,1399067817,,1.0,14,5,2,2,1,0.485460761,True,2.0,1469117.0,25.0,7.0,False,7.0,325959.0,13.0,4.0,326.0,1615.0,1702.0,302.0,1298.0,1364.0,258.0,726.0,775.0,0.215295096,0.604322527,0.645054032,855,1288,1315538,neutron,a3a8a86d7bccb824725d91735bd032237786f5aa,1,1,,"Bug #1315538 in neutron: ""NSX: cluster is passed instead of context during metadata operation""","The following stacktrace has been observed using NSX DHCP:
2014-05-02 14:00:36.295 30957 DEBUG neutron.plugins.vmware.api_client.base [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Released connection https://192.168.1.13:443. 10 connection(s) available. release_connect
ion /opt/stack/neutron/neutron/plugins/vmware/api_client/base.py:176
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.eventlet_request [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] [0] Completed request 'POST /ws.v1/lservices-node/20e0dc1c-a1da-455f-8841-3c52d78
6696c/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/vmware/api_client/eventlet_request.py:152
2014-05-02 14:00:36.296 30957 DEBUG neutron.plugins.vmware.api_client.client [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] Request returns ""<httplib.HTTPResponse instance at 0x4c49368>"" request /opt/stack/neutron/n
eutron/plugins/vmware/api_client/client.py:93
2014-05-02 14:00:36.297 30957 ERROR neutron.api.v2.resource [req-2a7489ae-ec5c-4bf4-868d-0f929e3588c6 None] add_router_interface failed
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 193, in _handle_action
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     return getattr(self._plugin, name)(*arg_list, **kwargs)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/plugins/base.py"", line 1719, in add_router_interface
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, router_id, interface=router_iface_info)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcpmeta_modes.py"", line 157, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/combined.py"", line 89, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     plugin, context, router_id, interface)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/nsx.py"", line 312, in handle_router_metadata_access
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, subnet_id, is_enabled)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 294, in lsn_metadata_configure
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     self.lsn_port_metadata_setup(context, lsn_id, subnet)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 225, in lsn_port_metadata_setup
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     lsn_port_id = self.lsn_port_create(self.cluster, lsn_id, data)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 453, in lsn_port_create
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     subnet_info['mac_address'], lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dhcp_meta/lsnmanager.py"", line 442, in lsn_port_save
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     context, lsn_port_id, subnet_id, mac_addr, lsn_id)
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/vmware/dbexts/lsn_db.py"", line 96, in lsn_port_add_for_lsn
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource     with context.session.begin(subtransactions=True):
2014-05-02 14:00:36.297 30957 TRACE neutron.api.v2.resource AttributeError: 'NSXCluster' object has no attribute 'session'
This is because the wrong parameter is passed to during the DB operation.","NSX: pass the right argument during metadata setup

The NSX cluster was erroneously passed instead of context.
This patch addressed that and tweaks UT to verify the right
parameter is passed instead.

Change-Id: Ife660b3a23705342043d121c7dcecad43e22abc1
Closes-bug: #1315538"
1216,a3aeace9afd5c533f040452f33482fdb55d93927,1383125323,,1.0,10,14,3,3,1,0.980834038,True,1.0,2443740.0,7.0,5.0,False,212.0,60761.0,1049.0,5.0,42.0,2309.0,2314.0,41.0,2037.0,2042.0,38.0,2133.0,2136.0,0.006016662,0.329219377,0.329682197,1735,1246103,1246103,nova,a3aeace9afd5c533f040452f33482fdb55d93927,1,0,“because it depends on the Python cinderclient library”,"Bug #1246103 in OpenStack Compute (nova): ""encryptors module forces cert and scheduler services to depend on cinderclient""","When Nova Scheduler is installed via packstack as the only explicitly installed service on a particular node, it will fail to start.  This is because it depends on the Python cinderclient library, which is not marked as a dependency in 'nova::scheduler' class in Packstack.","Pass volume_api to get_encryption_metadata

The module encryptors was creating a volume API instance in the module
scope which caused all the modules importing it to depend on
cinderclient.

This was affecting scheduler and cert services which at some point import
the compute manager module only to access their config options. It makes
no sense to force scheduler and cert services to require cinderclient.

This patch makes the callers of get_encryption_metadata to pass the
volume api object to this method to prevent this dependency.

Closes-Bug: #1246103
Change-Id: I9eb4ae3754fa2a5ac646560a62477d6ed672e272"
1217,a3c944bb1860faeb4eb0e90534ace07c1e869ea1,1386042453,,1.0,25,48,2,2,1,0.977001239,True,2.0,193369.0,15.0,4.0,False,12.0,566676.0,12.0,3.0,320.0,950.0,1162.0,251.0,875.0,1031.0,45.0,465.0,480.0,0.076794658,0.777963272,0.803005008,119,518,1257014,neutron,a3c944bb1860faeb4eb0e90534ace07c1e869ea1,0,0, Atomically setup OVS ports  - Feature,"Bug #1257014 in neutron: ""Atomically setup OVS ports""","Normal operation of the openvswitch-agent results in various warning and error messages being logged to /var/log/openvswitch/ovs-vswitchd.log, which, although not representing actual failures, can cause concern.
For example, setting up the patch port connecting br-int and br-tun results in:
2013-12-02T16:35:45Z|00103|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00104|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00105|netdev_vport|ERR|patch-tun: patch type requires valid 'peer' argument
2013-12-02T16:35:45Z|00106|bridge|WARN|could not configure network device patch-tun (Invalid argument)
2013-12-02T16:35:45Z|00107|bridge|INFO|bridge br-int: added interface patch-tun on port 1
2013-12-02T16:35:46Z|00108|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device patch-int failed: No such device
2013-12-02T16:35:46Z|00109|dpif|WARN|system@ovs-system: failed to add patch-int as port: No such device
2013-12-02T16:35:46Z|00110|netdev_vport|ERR|patch-int: patch type requires valid 'peer' argument
2013-12-02T16:35:46Z|00111|bridge|WARN|could not configure network device patch-int (Invalid argument)
2013-12-02T16:35:46Z|00112|bridge|INFO|bridge br-tun: added interface patch-int on port 1
And setting up a tunnel port on br-tun results in:
2013-12-02T16:35:48Z|00113|netdev_linux|WARN|ethtool command ETHTOOL_GFLAGS on network device gre-1 failed: No such device
2013-12-02T16:35:48Z|00114|dpif|WARN|system@ovs-system: failed to add gre-1 as port: No such device
2013-12-02T16:35:49Z|00115|netdev_vport|ERR|gre-1: gre type requires valid 'remote_ip' argument
2013-12-02T16:35:49Z|00116|bridge|WARN|could not configure network device gre-1 (Invalid argument)
2013-12-02T16:35:49Z|00117|bridge|INFO|bridge br-tun: added interface gre-1 on port 2
These messages seem to be due to the neutron.agent.linux.ovs_lib.add_patch_port() and neutron.agent.linux.ovs_lib.add_tunnel_port() functions setting DB attributes one-by-one, which results in invalid intermediate states. In addition to the noise in the log files, attempts to use the ports while in these intermediate states could result in transient incorrect behavior.
This can be resolved by changing these functions to atomically create and configure the OVS ports using a single ovs-vsctl command, with multiple sub-commands separated by ""--"" as arguments. This would also have the benefit of reducing the overhead of executing multiple commands via rootwrap when a single command would do.","atomically setup ovs ports

Change-Id: I7c4d4cac8150439f26b81d5727a5b7efb0a115f4
Closes-Bug: #1257014"
1218,a41ef0ede2cdace1db4d932e0931509a84ba837e,1384986471,0.0,1.0,59,46,4,2,1,0.928177461,True,4.0,4383775.0,55.0,14.0,False,12.0,764014.25,27.0,6.0,4.0,3352.0,3353.0,4.0,2887.0,2888.0,2.0,2404.0,2405.0,0.000451603,0.362035225,0.362185759,576,997,1290403,nova,a41ef0ede2cdace1db4d932e0931509a84ba837e,1,1, erroneous values collected,"Bug #1290403 in OpenStack Compute (nova): ""Hyper-V agent does not enable disk metrics for individual disks""","The Hyper-V agent is currently enabling metrics collection per vm, instead of per disk.
This leads to erroneous values collected for disk metrics.","Fixes the Hyper-V agent individual disk metrics

Replaces aggregated metric values with separated values for each disk.

Change-Id: I3c264dfd52572e5c429ee606a47a5727a4a3af3c
Closes-Bug: #1290403"
1219,a4c750f3027977551c1fdab430040689a620edb0,1388476457,,1.0,0,2,1,1,1,0.0,True,1.0,263269.0,19.0,5.0,False,17.0,4792395.0,29.0,5.0,8.0,659.0,663.0,8.0,620.0,624.0,1.0,593.0,593.0,0.001557632,0.462616822,0.462616822,236,640,1265148,cinder,a4c750f3027977551c1fdab430040689a620edb0,0,0,redundant variable creation,"Bug #1265148 in Cinder: ""Redundant variable declarations  in create()""","The method create() in /cinder/api/v2/volumes.py has two redundant variable declarations:
 image_href = None
 image_uuid = None
these should be reduced.","Reduce the redundant variable declarations

The method create() in /cinder/api/v2/volumes.py has two redundant
variable declarations: image_href = None image_uuid = None
These should be reduced

Change-Id: If3798d4bd96998707d7a5f87ece95ed8c93cc042
Closes-bug: #1265148"
1220,a4fd236baa2fc544f174773e32a3f91ec52a4fe5,1406285164,,1.0,69,33,10,10,1,0.678736186,False,,,,,True,,,,,,,,,,,,,,,,,1104,1555,1348623,nova,a4fd236baa2fc544f174773e32a3f91ec52a4fe5,1,0,“Upgrade-impact: the hyervisor type will intentionally no longer distinguish libvirt Xen from XenAPI within a compute cloud. The driver name should be used instead”,"Bug #1348623 in OpenStack Compute (nova): ""XenAPI and Baremetal drivers use bogus hypervisor type for supported instances""","The XenAPI driver reports a hypervisor type of 'xapi' for supported instances. This is confusing the hypervisor type, which should be 'xen', with the management API type which is 'xapi'.
The Baremetal driver reports a hypervisor type of 'baremetal' for supported instances. This is confusing the hypervisor type with the nova driver type. There is no hypervisor concept with the bare metal driver, things just run natively, so the type should be 'native'.","virt: use compute.virttype constants and validate virt type

Where we have hardcoded hypervisor types, use compute.hvtype
constants. Where we get hypervisor types from external systems,
validate them against the list of acceptable names.

The Xen hypervisor is reporting a virt type of 'xapi' for
supported instances which is confusing the virtualization
type with the hypervisor API access method. It should be
reporting 'xen' as the virtualization type.

On the other side, the ImagePropertiesFilter will canonicalize
the hvtype it fetches from image metadata, so that 'xapi' gets
remapped to 'xen' during comparison.

Upgrade-impact: the hyervisor type will intentionally no longer
  distinguish libvirt Xen from XenAPI within a compute cloud.
  The driver name should be used instead
Closes-bug: #1348623
Change-Id: I2c740e8eb068a5d93ba5f72292fb30d899931ea5"
1221,a507d42cf5d9912c2b3622e84afb8b7d3278595b,1406903788,,1.0,0,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1139,1594,1351350,nova,a507d42cf5d9912c2b3622e84afb8b7d3278595b,0,0,Refactorings,"Bug #1351350 in OpenStack Compute (nova): ""Warnings and Errors in Document generation""","Just pick any recent docs build and you will see a ton of issues:
Example from:
http://logs.openstack.org/46/111146/1/check/gate-nova-docs/4f3e8c4/console.html
2014-08-01 03:40:18.805 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/contrib/hosts.py:docstring of nova.api.openstack.compute.contrib.hosts.HostController.index:5: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/api/openstack/compute/plugins/v3/hosts.py:docstring of nova.api.openstack.compute.plugins.v3.hosts.HostController.index:6: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.806 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:7: ERROR: Unexpected indentation.
2014-08-01 03:40:18.807 | /home/jenkins/workspace/gate-nova-docs/nova/compute/resource_tracker.py:docstring of nova.compute.resource_tracker.ResourceTracker.resize_claim:8: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:23: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:24: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.847 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/api.py:docstring of nova.db.sqlalchemy.api.instance_get_all_by_filters:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/db/sqlalchemy/utils.py:docstring of nova.db.sqlalchemy.utils.create_shadow_table:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.848 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:15: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.849 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:18: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.850 | /home/jenkins/workspace/gate-nova-docs/nova/hooks.py:docstring of nova.hooks:23: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.850 | <autodoc>:0: WARNING: Inline emphasis start-string without end-string.
2014-08-01 03:40:18.851 | <autodoc>:0: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.851 | /home/jenkins/workspace/gate-nova-docs/nova/image/api.py:docstring of nova.image.api.API.get_all:8: WARNING: Inline strong start-string without end-string.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/keymgr/key_mgr.py:docstring of nova.keymgr.key_mgr.KeyManager.copy_key:9: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/notifications.py:docstring of nova.notifications.info_from_instance:6: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.852 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/base.py:docstring of nova.objects.base.NovaObject.obj_make_compatible:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.853 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.Instance.save:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/instance.py:docstring of nova.objects.instance.InstanceList.get_active_by_window_joined:9: WARNING: Field list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.854 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:31: ERROR: Unexpected indentation.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/objects/pci_device.py:docstring of nova.objects.pci_device.PciDevice:33: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.887 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/network_utils.py:docstring of nova.openstack.common.network_utils.set_tcp_keepalive:6: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/openstack/common/report/report.py:docstring of nova.openstack.common.report.report.ReportSection:16: ERROR: Unexpected indentation.
2014-08-01 03:40:18.888 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_manager.py:docstring of nova.pci.pci_manager.PciDevTracker.get_free_devices_for_requests:5: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.889 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request:16: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.890 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:17: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:25: ERROR: Unexpected indentation.
2014-08-01 03:40:18.891 | /home/jenkins/workspace/gate-nova-docs/nova/pci/pci_request.py:docstring of nova.pci.pci_request.get_pci_requests_from_flavor:27: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.892 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:10: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/isolated_hosts_filter.py:docstring of nova.scheduler.filters.isolated_hosts_filter.IsolatedHostsFilter.host_passes:11: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:9: ERROR: Unexpected indentation.
2014-08-01 03:40:18.893 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/pci_passthrough_filter.py:docstring of nova.scheduler.filters.pci_passthrough_filter.PciPassthroughFilter:10: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.894 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.895 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.927 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:3: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.928 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.930 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:10: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/scheduler/filters/trusted_filter.py:docstring of nova.scheduler.filters.trusted_filter:20: WARNING: Inline interpreted text or phrase reference start-string without end-string.
2014-08-01 03:40:18.933 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/plugins/v3/test_servers.py:docstring of nova.tests.api.openstack.compute.plugins.v3.test_servers.ServersAllExtensionsTestCase:13: ERROR: Unexpected indentation.
2014-08-01 03:40:18.934 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:11: ERROR: Unexpected indentation.
2014-08-01 03:40:18.935 | /home/jenkins/workspace/gate-nova-docs/nova/tests/api/openstack/compute/test_servers.py:docstring of nova.tests.api.openstack.compute.test_servers.ServersAllExtensionsTestCase:13: WARNING: Definition list ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.936 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:3: ERROR: Unexpected indentation.
2014-08-01 03:40:18.937 | /home/jenkins/workspace/gate-nova-docs/nova/tests/compute/test_resource_tracker.py:docstring of nova.tests.compute.test_resource_tracker.NoInstanceTypesInSysMetadata:4: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.938 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:21: ERROR: Unexpected indentation.
2014-08-01 03:40:18.940 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:22: WARNING: Block quote ends without a blank line; unexpected unindent.
2014-08-01 03:40:18.941 | /home/jenkins/workspace/gate-nova-docs/nova/tests/db/test_migrations.py:docstring of nova.tests.db.test_migrations:24: ERROR: Unexpected indentation.
2014-08-01 03:40:18.942 | /home/jenkins/workspace/gate-nova-docs/nova/tests/image_fixtures.py:docstring of nova.tests.image_fixtures.get_image_fixtures:10: SEVERE: Unexpected section title.
2014-08-01 03:40:18.942 |","docs - Set pbr 'warnerrors' option for doc build

By setting this pbr option in setup.cfg, the doc build will fail in case
of any warnings or errors occur during the build process.

Closes-Bug: #1351350

Change-Id: Id4858062d2aaa4c2fe5b597e40e4e8947f544a4d"
1222,a5147669f4d584fdbee10b4a2c77b93b31ef3108,1410485179,,1.0,68,34,4,3,1,0.844405158,False,,,,,True,,,,,,,,,,,,,,,,,1306,1772,1368495,nova,a5147669f4d584fdbee10b4a2c77b93b31ef3108,1,0,“Reverting those attribute same as V2 to work with V2.1”,"Bug #1368495 in OpenStack Compute (nova): ""'type'/'mac_adrr' attribute of server's address field not converted to V2.1""","For 'extended_ips'/'extended_ips_mac' extension, there are difference between V2 and V3 server show/index & server address index API response listed below-
'address' field of V2->V3 server API response-
""OS-EXT-IPS:type"" -> ""type""
""OS-EXT-IPS-MAC:mac_addr"" -> ""mac_addr""
Above attribute needs to be fixed in V2.1 to make it backward compatible with V2.","Port extended_ips/extended_ips_mac extension to V2.1

This patch port extended_ips/extended_ips_mac extension to V2.1

There are difference between V2 and V3 server show/index &
server address index API response listed below-

'address' field of V2->V3 API response-
""OS-EXT-IPS:type"" -> ""type""
""OS-EXT-IPS-MAC:mac_addr"" -> ""mac_addr""

Reverting those attribute same as V2 to work with V2.1

Closes-Bug: #1368495
Partially implements blueprint v2-on-v3-api

Change-Id: I44902b0402115d1b6e833975e6c2f020ac5ab7c3"
1223,a516ae71d57eda013f7cb9428f945dfea08ead3e,1401957332,,1.0,55,2,4,3,1,0.668098551,False,,,,,True,,,,,,,,,,,,,,,,,566,987,1289397,nova,a516ae71d57eda013f7cb9428f945dfea08ead3e,1,1,,"Bug #1289397 in OpenStack Compute (nova): ""nova  instance delete fails if dhcp_release fails""","ssatya@devstack:~$ nova boot --image 1e95fe6b-cec6-4420-97d1-1e7bc8c81c49 --flavor 1  testdummay
+--------------------------------------+-----------------------------------------------------------+
| Property                             | Value                                                     |
+--------------------------------------+-----------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                    |
| OS-EXT-AZ:availability_zone          | nova                                                      |
| OS-EXT-STS:power_state               | 0                                                         |
| OS-EXT-STS:task_state                | networking                                                |
| OS-EXT-STS:vm_state                  | building                                                  |
| OS-SRV-USG:launched_at               | -                                                         |
| OS-SRV-USG:terminated_at             | -                                                         |
| accessIPv4                           |                                                           |
| accessIPv6                           |                                                           |
| adminPass                            | fK8SPGtHLUds                                              |
| config_drive                         |                                                           |
| created                              | 2014-03-07T14:33:49Z                                      |
| flavor                               | m1.tiny (1)                                               |
| hostId                               | 2c1ae30aa2a235d9c0c8b04aae3f4199cd98356e44a03b5c8f878adb  |
| id                                   | eae503d9-c6f7-4e3e-9adc-0b8b6803c90e                      |
| image                                | debian-2.6.32-i686 (1e95fe6b-cec6-4420-97d1-1e7bc8c81c49) |
| key_name                             | -                                                         |
| metadata                             | {}                                                        |
| name                                 | testdummay                                                |
| os-extended-volumes:volumes_attached | []                                                        |
| progress                             | 0                                                         |
| security_groups                      | default                                                   |
| status                               | BUILD                                                     |
| tenant_id                            | 209ab7e4f3744675924212805db3ad74                          |
| updated                              | 2014-03-07T14:33:50Z                                      |
| user_id                              | f3756a4910054883b84ee15acc15fbd1                          |
+--------------------------------------+-----------------------------------------------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | BUILD  | spawning   | NOSTATE     |                  |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ACTIVE | -          | Running     | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+
ssatya@devstack:~$ nova stop testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+---------+------------+-------------+------------------+
| ID                                   | Name       | Status  | Task State | Power State | Networks         |
+--------------------------------------+------------+---------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | SHUTOFF | -          | Shutdown    | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR   | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+---------+------------+-------------+------------------+
ssatya@devstack:~$ nova delete testdummay
ssatya@devstack:~$ nova list
+--------------------------------------+------------+--------+------------+-------------+------------------+
| ID                                   | Name       | Status | Task State | Power State | Networks         |
+--------------------------------------+------------+--------+------------+-------------+------------------+
| eae503d9-c6f7-4e3e-9adc-0b8b6803c90e | testdummay | ERROR  | deleting   | Shutdown    | private=10.0.0.3 |
| d1e982c4-85c2-422d-b046-1643bd81e674 | testvm1    | ERROR  | deleting   | Shutdown    | private=10.0.0.2 |
+--------------------------------------+------------+--------+------------+-------------+------------------+","Network: enable instance deletion when dhcp release fails

In the event that the 'dhcp_release' fails we should continue
with the deletion of the instance.

This will no longer the leave the instance in ERROR state.

Change-Id: Ie8259180a9df12865940907c728a8d9da3c9fda0
Closes-bug: #1289397"
1224,a528302a248f4128c3440665f38b964a3593c3f5,1382614261,,1.0,22,17,2,2,1,0.291818257,True,4.0,10309.0,19.0,8.0,False,24.0,822610.0,42.0,2.0,39.0,2143.0,2162.0,39.0,1903.0,1922.0,35.0,2038.0,2053.0,0.005598756,0.317107309,0.319440124,27,339,1244220,nova,a528302a248f4128c3440665f38b964a3593c3f5,1,0,Evolution,"Bug #1244220 in OpenStack Compute (nova): ""Consoleauth check_token is broken with rpcapi v2""","After adding the 2.0 rpc API in consoleauth manager (See [1]), the method check_token in the new proxy class for v2 lacks the return statement and causes all calls to verify a token from other components to fail.
[1] https://review.openstack.org/#/c/51731/","Fix consoleauth check_token for rpcapi v2

In 48dd52095869d36970de232f5dfa60656fcd578b support for the 2.0
consoleauth rpc API was added which missed the return statement in the
check_token method. Because of this all the components that had to
verify a token against consoleauth component would fail to do so.

This patch fixes the check_token method and extends the tests to cover
the new v2.0 class.

Closes-Bug: #1244220
Change-Id: I4e5186943d28609083f723f4b43ab6f0ecbffd27"
1225,a5371712ce67b8e769f5fc12a3048dc5d0eef3eb,1381187963,,1.0,12,2,1,1,1,0.0,True,1.0,19544.0,6.0,2.0,False,15.0,2855792.0,29.0,2.0,29.0,495.0,516.0,24.0,467.0,484.0,18.0,457.0,467.0,0.017707363,0.426840634,0.436160298,1645,1236626,1236626,cinder,a5371712ce67b8e769f5fc12a3048dc5d0eef3eb,1,1, ,"Bug #1236626 in Cinder: ""Nexenta iSCSI driver fix lu exists""","Catch ""does not exist"" exception of NMS scsidisk lu_exists method.","Nexenta iSCSI driver fix _lu_exists

Catch ""does not exist"" exception of NMS scsidisk lu_exists method.

Change-Id: I082b588dfbf54efec68a9affc76466097765b6db
Closes-Bug: #1236626"
1226,a5405fa3532d9dd3d33e848f36cc6530e74e6bb7,1395623464,,1.0,30,4,4,2,1,0.985667643,True,1.0,2250646.0,14.0,4.0,False,17.0,987452.0,32.0,1.0,90.0,899.0,977.0,69.0,898.0,956.0,72.0,894.0,954.0,0.009522567,0.116749283,0.12457605,672,1098,1296478,nova,a5405fa3532d9dd3d33e848f36cc6530e74e6bb7,1,1,,"Bug #1296478 in OpenStack Compute (nova): ""The Hyper-V driver's list_instances() returns an empty result set on certain localized versions of the OS""",This issue is related to different values that MSVM_ComputerSystem's Caption property can have on different locales.,"Fixes a Hyper-V list_instances localization issue

The Hyper-V WMI MSVM_ComputerSystem class Caption property can
have different values on various locales. This patch uses a
different query to avoid any localization issue.

Co-Authored-By: Robert Tingirica <rtingirica@cloudbasesolutions.com>
Change-Id: I15103b8edb3ff8934c835826ed00e08593cdeacc
Closes-Bug: #1296478"
1227,a55f41492e5ce9bbc2f2ef3435a7e7e65bf6cb3e,1400774505,,1.0,2,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,912,1348,1322195,nova,a55f41492e5ce9bbc2f2ef3435a7e7e65bf6cb3e,1,1,,"Bug #1322195 in OpenStack Compute (nova): ""Admin creates volume backed instance snapshot in image tenant""","For instance booted from volume with legacy bdm and image (this method is documented as workaround in http://docs.openstack.org/grizzly/openstack-ops/content/attach_block_storage.html) admin user creates instance snapshot in the image tenant rather than current tenant.
Created snapshot cannot be used.
Environment: DevStack
Steps to reproduce:
1 Create bootable volume from public image from not current tenant.
For example, use demo tenant in DevStack.
$ cinder create --image-id xxx 1
Note: I used cirros-0.3.2-x86_64-uec ami image.
2 Boot an instance from the volume passing the original image.
$ nova boot --flavor m1.nano --image xxx --block-device-mapping /dev/vda=yyy inst
3 Create instance snapshot under admin user
$ nova image-create inst snap
4 List images and make sure there is no the created snapshot.
$ glance image-list
5 List images from the original image tenant and found the snapshot.
$ glance --os-tenant-name nnn image-list
snapshot_volume_backed in nova/compute/api.py receives image in image_meta parameter, cleans some attributes, but forgets to deal something with owner attribute.","Store volume backed snapshot in current tenant.

Fix owner of a creating volume backed snapshot.

Snapshot of an instance booted on a volume based on another tenant's
public image is created in the wrong tenant when invoked by admin.

Snapshot metadata (including owner) is based on image metadata. But
when the snapshot is being created by admin, Glance doesn't change
it's owner if it's set. So we forcibly remove owner (tenant)
attribute from image metadata.

Change-Id: I662dfa4f81e24cb2553ffa2578f4c8530eee9fd3
Closes-Bug: #1322195"
1228,a5765179201ee03ed26bb8f3bcda9f75bbdb191c,1406646929,1.0,1.0,49,7,2,2,1,0.966618633,False,,,,,True,,,,,,,,,,,,,,,,,1116,1571,1349895,neutron,a5765179201ee03ed26bb8f3bcda9f75bbdb191c,0,0,Feature. “If the Port is not found it should create a new Port.”,"Bug #1349895 in neutron: ""Radware LBaaS driver create extra Neutron Ports""","The method ' _create_port_for_pip' should try and check if a Neutron Port exists and reuse it.
If the Port is not found it should create a new Port.
See: https://github.com/openstack/neutron/blob/master/neutron/services/loadbalancer/drivers/radware/driver.py#L616","Radware: When a pip is needed, reuse the Port

When a pip (Proxy IP) is needed by the driver, do not create
a new Neutron Port every time a pip is needed. Reuse the
existing Port.

Change-Id: I769a9d85e217b30a1ea4d09449ff39bf1ab23c5a
Closes-bug: #1349895"
1229,a5a33a6833b29457d2ea7fc5b8c31f74a7aa4140,1398782783,,1.0,16,1,2,2,1,0.977417818,True,2.0,1883964.0,14.0,6.0,False,8.0,10836559.0,11.0,2.0,21.0,540.0,556.0,21.0,524.0,540.0,13.0,513.0,521.0,0.008744535,0.321049344,0.326046221,825,1254,1311243,cinder,a5a33a6833b29457d2ea7fc5b8c31f74a7aa4140,1,1,,"Bug #1311243 in Cinder: ""cinder: List API versions does not accept XML""","The cinder list versions API call works fine for JSON:
curl -i http://23.253.228.211:8776/ -H ""Accept: application/json""   -H ""X-Auth-Token: $token""
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 298
Date: Tue, 22 Apr 2014 17:35:12 GMT
{""versions"": [{""status"": ""CURRENT"", ""updated"": ""2012-01-04T11:33:21Z"", ""id"": ""v1.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v1/"", ""rel"": ""self""}]}, {""status"": ""CURRENT"", ""updated"": ""2012-11-21T11:33:21Z"", ""id"": ""v2.0"", ""links"": [{""href"": ""http://23.253.228.211:8776/v2/"", ""rel"": ""self""}]}]}
However, the same call with XML fails:
curl -i http://23.253.228.211:8776/.xml -H ""Accept: application/xml""   -H ""X-Auth-Token: $token""
HTTP/1.1 500 Internal Server Error
Content-Length: 189
Content-Type: application/xml; charset=UTF-8
Date: Tue, 22 Apr 2014 17:37:09 GMT
<computeFault code=""500"" xmlns=""http://docs.openstack.org/volume/api/v1""><message>The server has either erred or is incapable of performing the requested operation.</message></computeFault>
Additionally, the XML call to show v1 details fails:
curl -i http://23.253.228.211:8776/v1/.xml -H ""Accept: application/xml"" -H ""X-Auth-Token:$token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9ff05fb1-0b20-4cb7-a60f-8d956ec81dd9
Date: Tue, 22 Apr 2014 17:49:59 GMT
404 Not Found
The resource could not be found.
Call for XML to show v2 details fails:
curl -i http://23.253.228.211:8776/v2/.xml -H ""Accept: application/xml""   -H ""X-Auth-Token: $token""
HTTP/1.1 404 Not Found
Content-Length: 52
Content-Type: text/plain; charset=UTF-8
X-Openstack-Request-Id: req-9f96a04b-dc35-4ceb-bb53-ab6ade803075
Date: Tue, 22 Apr 2014 17:58:52 GMT
404 Not Found
The resource could not be found.
Also, the JSON call for show cinder v2 details seems to show cinder v1:
curl -i http://23.253.228.211:8776/v2/ -H ""Accept: application/json""   -H ""X-Auth-Token: $token""
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}","fix atom link in XML Version API

This patch fixes tag names where the tag name has ':'
for links templates.
Some examples are
'{http://www.w3.org/2005/Atom}link'
should be
['{http://www.w3.org/2005/Atom}link']

and

'test1:test2'
should be
['test1', 'test2']

Closes-Bug: #1311243
Change-Id: I6e7c068e41eb6b069c24ef9d6c46b726ea722074"
1230,a5b78ae0b502ad0ee95037f5b1d9619dba3dab81,1389123926,1.0,1.0,130,11,5,5,1,0.849933796,True,5.0,7973086.0,20.0,6.0,False,63.0,350667.4,245.0,2.0,11.0,694.0,695.0,11.0,531.0,532.0,11.0,539.0,540.0,0.010919017,0.491355778,0.492265696,248,652,1265711,glance,a5b78ae0b502ad0ee95037f5b1d9619dba3dab81,1,1,Negative numbers raise a bug,"Bug #1265711 in Glance: ""image is creating with option min_disk and min_ram as negative number""","I try to create a image with the min_ram and min_disk as negative number, and create image operation is successful.
I use the glance api V2 to create  image.
curl -i -X POST  -H ""X-Auth-Token: $token"" -H ""content-type: application/json"" -d '{""name"": ""image-7"", ""type"": ""kernel"", ""foo"": ""bar"", ""disk_format"": ""aki"", ""container_format"": ""aki"", ""protected"": false, ""tags"": [""test"",""image""], ""visibility"": ""public"", ""min_ram"":-1, ""min_disk"":-1' \
http://192.168.0.100:9292/v2/images
glance image-show image-7
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| Property 'foo'   | bar                                  |
| Property 'type'  | kernel                               |
| container_format | aki                                  |
| created_at       | 2014-01-03T06:01:14                  |
| deleted          | False                                |
| disk_format      | aki                                  |
| id               | aaa8e463-10aa-4518-a590-52feebaabcb5 |
| is_public        | True                                 |
| min_disk         | -1                                   |
| min_ram          | -1                                   |
| name             | image-7                              |
| owner            | 25adaa8f93ee4199b6a362c45745231d     |
| protected        | False                                |
| status           | queued                               |
| updated_at       | 2014-01-03T06:01:14                  |
+------------------+--------------------------------------+
I think glance need to check the value of min_ram and  min_disk. They need to be greater than or equal 0.","Prevent min_disk and min_ram from being negative

At the moment, in both api versions, min_disk and min_ram can be set to
negative value - which doesn't make sense. This commit fixes this behavior
in the two versions of the glance api.

Change-Id: Ib7855c1d928ab8262dd2a109619a4e3d2299ff9e
Closes-bug: #1265711"
1231,a5c9340130f86d1f2a54a808add32d9d7d7355cd,1410333863,,1.0,40,10,3,3,1,0.782471067,False,,,,,True,,,,,,,,,,,,,,,,,1290,1755,1367575,nova,a5c9340130f86d1f2a54a808add32d9d7d7355cd,1,1,server actions were missed for V2.1 API.,"Bug #1367575 in OpenStack Compute (nova): """"os-start/os-stop"" server actions does not work for v2.1 API""","""os-start/os-stop"" server action does not work for V2.1 API.
Those needs to be converted to V2.1 from V3 base code.
This needs to be fixed to make V2.1 backward compatible with V2 APIs","Fix 'os-start/os-stop' server actions for V2.1 API

'os-start/os-stop' server actions were missed for V2.1 API.
This patch converts 'os-start/os-stop' server action for V2.1 API

The differences between v2 and v3 are described on the wiki page
https://wiki.openstack.org/wiki/NovaAPIv2tov3.

Closes-Bug: #1367575

Change-Id: I8b0f27fb639034d368bcc1fc51d20daf9c2cdeb5"
1232,a5cf28a68ab400fc59ae77bb11542a84b97b0d56,1409722644,,1.0,67,6,2,2,1,0.24729444,False,,,,,True,,,,,,,,,,,,,,,,,1257,1717,1364692,nova,a5cf28a68ab400fc59ae77bb11542a84b97b0d56,1,1,,"Bug #1364692 in OpenStack Compute (nova): ""Error msg says ""No valid host found for cold migrate"" when resizing VM""","Because resize and cold migrate share common code, some of the migrate code shows through in the error handling code in that when a valid host can't be found when performing a resize operation, the user sees:
""No valid host found for cold migrate""
This can be confusing, especially when a number of different actions can be going in parallel.","Specify correct operation type when NVH is raised

When nova-conductor received a NoValidHost (NVH) exception for a
cold migrate or resize operation, its error message back through
the API layer always displayed the string ""cold migrate"" in the
error text, even when the operation was a resize. This leads to
potentially confusing situations when triaging problems in the
cloud environment.

This patch corrects the error message based on the operation; we use
the flavor IDs as a way to determine what operation is really being
performed. In the case of a resize, different flavor IDs will be
used and they will be indentical in the case of a cold migration.

Note there was a lot of IRC discussion on this as well in terms of
the ""algorithm"" to check the operation type; the flavor ID check
is also used in the API layer as well since it's the only way you
can determine this given the underlying operation.  We also tried
to make the error message more general, but the consensus was that
was pretty unfriendly from the user's perspective - so we've landed
with this approach again with the end users close to heart.  :-)

Change-Id: I762425b2602c2798b94d3c770f2401dd0aedc942
Closes-Bug: #1364692"
1233,a5d996e381bd342f0204fcd99e8548cf28b20e92,1384788685,,1.0,2,6,2,2,1,0.811278124,True,2.0,1452308.0,18.0,12.0,False,17.0,2102223.0,21.0,10.0,34.0,2218.0,2235.0,28.0,1987.0,1998.0,13.0,520.0,520.0,0.025408348,0.945553539,0.945553539,1774,1252284,1252284,neutron,a5d996e381bd342f0204fcd99e8548cf28b20e92,1,1, ,"Bug #1252284 in neutron: ""OVS agent doesn't reclaim local VLAN""","Locally to an OVS agent, when the last port of a network disappears the local VLAN isn't reclaim.","Fix OVS agent reclaims local VLAN

If a port disappears on an OVS agent, the port is removed of the lvm
vif_ports list. And if it's the last port of the network on this agent,
the network local VLAN is reclaim.

Change-Id: I07e03107eb86a84eeb8e4d06f27a1d2fbd7cea57
Closes-Bug: #1252284"
1234,a630b3bae84091f0ff1029a87e3fc27eb9d7f5e3,1403725798,,1.0,36,10,3,3,2,0.835962545,False,,,,,True,,,,,,,,,,,,,,,,,1011,1453,1334412,neutron,a630b3bae84091f0ff1029a87e3fc27eb9d7f5e3,0,0,"Feature. “Since nova notifications introduce a run-time dependency on novaclient, there should be an external sanity check added that tests the running systems ability to import neutron.notifiers.nova.”","Bug #1334412 in neutron: ""Add an external sanity check for nova notification availability""","Since nova notifications introduce a run-time dependency on novaclient, there should be an external sanity check added that tests the running systems ability to import neutron.notifiers.nova.","Add sanity check for nova notification support

Since nova notification support adds a runtime dependency on
novaclient, add an external sanity check testing the system's
ability to import neutron.notifiers.nova

Change-Id: I8a72c8763bb69a01178fc4ec91071a64a4648aab
Closes-bug: #1334412"
1235,a6d28e811ad916d686e350ce2dd8f6b8b0e128d6,1390847051,,1.0,38,2,2,2,1,0.286396957,True,4.0,2651525.0,43.0,16.0,False,9.0,56763.0,11.0,3.0,29.0,1391.0,1402.0,29.0,1215.0,1226.0,25.0,1323.0,1330.0,0.003625715,0.184632548,0.185608702,330,740,1273139,nova,a6d28e811ad916d686e350ce2dd8f6b8b0e128d6,1,1,typo in code,"Bug #1273139 in OpenStack Compute (nova): ""HostBinaryNotFound exception isn't caught in service_update""","When I update a service with not existing hostname or binary , I got the 500 error in nova api log.
2014-01-28 03:15:29.829 ERROR nova.api.openstack.extensions [req-5b1f3fc5-349a-4415-a4f5-63eab1c259a0 admin demo] Unexpected exception in API method
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 470, in wrapped
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/services.py"", line 172, in update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     self.host_api.service_update(context, host, binary, status_detail)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 3122, in service_update
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/objects/base.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     result = fn(cls, context, *args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/objects/service.py"", line 105, in get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     db_service = db.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/api.py"", line 131, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return IMPL.service_get_by_args(context, host, binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 112, in wrapper
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 469, in service_get_by_args
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions     raise exception.HostBinaryNotFound(host=host, binary=binary)
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions HostBinaryNotFound: Could not find binary nova-cert on host xu-de.
2014-01-28 03:15:29.829 TRACE nova.api.openstack.extensions","Catch HostBinaryNotFound exception when updating a service

When updating a service with a wrong host or binary name,
HostBinaryNotFound exception is raised. However this exception
isn't caught and the ServiceNotFound exception which won't happen
here is caught instead.
This patch fixes this bug.

Change-Id: I64b138bad74c39d087addcd766b890c4076a2ce8
Closes-Bug: #1273139"
1236,a70c5eb83d14541c2297b6d834dff1c0ac00962e,1391187215,,1.0,48,22,4,2,1,0.815209196,True,5.0,3604541.0,71.0,20.0,False,195.0,9168.0,988.0,4.0,197.0,1175.0,1312.0,185.0,1148.0,1273.0,197.0,1164.0,1301.0,0.02739347,0.161178749,0.180132817,346,757,1274627,nova,a70c5eb83d14541c2297b6d834dff1c0ac00962e,0,0,"I dont see the bug, feature?. Volume attach /detach should be blocked ","Bug #1274627 in OpenStack Compute (nova): ""Volume attach /detach should be blocked during some opertaions  ""","Currently volume attach, detach, and swap check on vm_state but not task_state.  This means that, for example, volume attach is allowed during a reboot, rebuild, or migration.
As with other operations the check should be against a task state of ""None""","Volume operations should be blocked for non-null task state

Currently volume attach, detach, and swap check on vm_state but
not task_state.  This means that, for example, volume attach is
allowed even when other operations such as migration are in
progress.  As with other operations the check should be against
a task state of ""None""

This change also refactors the volume_attach() method a little
so that the cells api can include the state decorator checks.

Change-Id: I0c4d697cf9d9e431a050c3c1b03e46b8629d182a
Closes-Bug: #1274627"
1237,a72a3f4c956338b05db2c6fa958112cf5df8dcad,1393765559,,1.0,6,2,1,1,1,0.0,True,4.0,201621.0,45.0,14.0,False,1.0,3161272.0,1.0,3.0,86.0,1895.0,1939.0,70.0,1727.0,1762.0,70.0,1854.0,1882.0,0.009484371,0.247795886,0.251536201,501,920,1286528,nova,a72a3f4c956338b05db2c6fa958112cf5df8dcad,1,0,Windows dependency?,"Bug #1286528 in OpenStack Compute (nova): ""guru-meditation fails on Windows due to non portable signal handling""","The guru-meditation report fails on Hyper-V due to missing signal handling.
This is a blocking issue on Windows.
http://64.119.130.115/74060/3/Hyper-V_logs/hv-compute1/nova-console.log.gz
Traceback (most recent call last):
  File ""c:\OpenStack\virtualenv\Scripts\nova-compute-script.py"", line 9, in <module>
    load_entry_point('nova==2014.1.dev954.g3a611cc', 'console_scripts', 'nova-compute')()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 353, in load_entry_point
    return get_distribution(dist).load_entry_point(group, name)
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2321, in load_entry_point
    return ep.load()
  File ""c:\OpenStack\virtualenv\lib\site-packages\pkg_resources.py"", line 2048, in load
    entry = __import__(self.module_name, globals(),globals(), ['__name__'])
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\cmd\compute.py"", line 32, in <module>
    from nova.openstack.common.report import guru_meditation_report as gmr
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 63, in <module>
    class GuruMeditation(object):
  File ""c:\OpenStack\virtualenv\lib\site-packages\nova\openstack\common\report\guru_meditation_report.py"", line 100, in GuruMeditation
    def setup_autorun(cls, version, signum=signal.SIGUSR1):
AttributeError: 'module' object has no attribute 'SIGUSR1'
Nova patch that introduced the issue:
https://github.com/openstack/nova/commit/cec532848f569afb4832029bce4969578472a57a
Review link:
https://review.openstack.org/#/c/69058/","Sync latest Guru Meditation Reports from Oslo

Nova fails to start on Windows due to non portable signal handling
(SIGUSR1) employed in this patch.

This patch enables the reporting feature only if signal is available
on the current platform.

Oslo commit: d3a285e33fa936cec5d14dfefa20dc03cc90ac18

Closes-Bug: #1286528

Change-Id: I070ab57b18b2bc43df5ec84c51ccb86be1ff0d2a"
1238,a7885385fb34f7040684343deed040d302dbbfea,1386343791,0.0,1.0,18,2,6,5,1,0.975614963,True,1.0,385672.0,5.0,2.0,False,107.0,87135.0,318.0,2.0,25.0,1362.0,1383.0,25.0,1288.0,1309.0,25.0,1338.0,1359.0,0.003821282,0.196796002,0.199882422,1639,1235389,1235389,nova,a7885385fb34f7040684343deed040d302dbbfea,1,1, ,"Bug #1235389 in OpenStack Compute (nova): ""Quota violations should not cause a stacktrace in the logs""","Right now, when we overrun a quota, we get an ugly stack trace in (at least) nova-api's log:
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack Traceback (most recent call last):
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/__init__.py"", line 119, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return req.get_response(self.application)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/python-keystoneclient/keystoneclient/middleware/auth_token.py"", line 545, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self.app(env, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return resp(environ, start_response)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/usr/local/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 917, in __call__
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     content_type, body, accept)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 976, in _process_stack
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/wsgi.py"", line 1057, in dispatch
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1248, in _action_resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return self._resize(req, id, flavor_ref, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/api/openstack/compute/servers.py"", line 1113, in _resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     self.compute_api.resize(context, instance, flavor_id, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 198, in wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return func(self, context, target, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 188, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return function(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 215, in _wrapped
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return fn(self, context, instance, *args, **kwargs)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 169, in inner
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     return f(self, context, instance, *args, **kw)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack   File ""/opt/stack/new/nova/nova/compute/api.py"", line 2304, in resize
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack     resource=resource)
2013-10-04 16:23:43.711 24241 TRACE nova.api.openstack TooManyInstances: Quota exceeded for ram: Requested 51137, but already used 128 of 51200 ram
See this example for more:
http://logs.openstack.org/23/49623/3/check/check-tempest-devstack-vm-full/b0d348a/logs/screen-n-api.txt.gz?level=TRACE
This happens a lot, clearly multiple times per every tempest run:
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIlRvb01hbnlJbnN0YW5jZXM6IFF1b3RhIGV4Y2VlZGVkIGZvciByYW1cIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzgwOTA5MzQ3MjQ3fQ==","Quota violations should not cause a stacktrace in the logs

Don't show exception logs in api log for resize operations
if the resize operations lead to quota violations

Change-Id: Ia101c599908c7652cc07fa4c7e7f1b057943e031
Closes-Bug: #1235389"
1239,a79aa79b2ae1222bd8776cde10bebeefc6bc8791,1395216517,,1.0,83,1,4,2,1,0.747980019,True,1.0,105749.0,17.0,3.0,False,6.0,1726988.25,9.0,2.0,509.0,1481.0,1704.0,413.0,1206.0,1375.0,252.0,772.0,854.0,0.252747253,0.772227772,0.854145854,649,1075,1294527,neutron,a79aa79b2ae1222bd8776cde10bebeefc6bc8791,0,0, To avoid unnecessary user-visible errors ,"Bug #1294527 in neutron: ""nec plugin: should honor retry-after from openflow controller""",OpenFlow controller which nec plugin talks to sometimes returns retry-after when it is busy. It is better to honor retry-after header to avoid unnecessary user-visible errors due to temporary busy condition.,"NEC plugin: Honor Retry-After response from OFC

A backend OpenFlow controller nec plugin talks to can return
503 response with retry-after header when it is busy.
It is better to honor retry-after header to avoid unnecessary
user-visible errors due to temporary busy condition.

Change-Id: I2ff1c3ac8402a2207bd955e9a9bb61e147950c5c
Closes-Bug: #1294527"
1240,a7b6d82fd170bc594ceff791eb10b900a61c9175,1390241161,,1.0,1,1,1,1,1,0.0,True,1.0,1210791.0,15.0,4.0,False,13.0,181216.0,19.0,2.0,0.0,1256.0,1256.0,0.0,1079.0,1079.0,0.0,1105.0,1105.0,0.00074129,0.819866568,0.819866568,266,670,1266906,cinder,a7b6d82fd170bc594ceff791eb10b900a61c9175,1,1,,"Bug #1266906 in Cinder: ""Volume type defaults remain in Defaults tab even after volume type is removed""","After creating a volume type 'abc' a number of default quotas are added to the Default tab's quota table:
- Volumes Abc
- Snapshots Abc
- Gigabytes Abc
I expected these defaults to be removed from the table once I remove the volume type. However, after removing the volume type they are still there. I verified that volume type was removed from the Volumes tab, and also marked deleted in Cinder database.","Do not show quota of removed volume types in Default Quotas panel

Fixes the issue where quota for removed volume types still show up
in Admin -> Defaults -> Default Quotas view.

Change-Id: I2a822ac08b8347a8d76e8de70deca0e34fafe6e3
Closes-Bug: #1266906"
1241,a7e977bfd46ffc932f32f7e4e5a60ea3db4ed574,1386875534,1.0,1.0,36,4,3,3,1,0.933966066,True,10.0,3874834.0,53.0,19.0,False,61.0,75233.33333,212.0,2.0,6.0,207.0,207.0,6.0,207.0,207.0,6.0,175.0,175.0,0.006566604,0.165103189,0.165103189,179,582,1260314,glance,a7e977bfd46ffc932f32f7e4e5a60ea3db4ed574,1,1,,"Bug #1260314 in Glance: ""glance image-create with invalid store fails but still creates image""","glance checks whether or not a specified store is valid, but if it is invalid the image has already been created.
I pulled the latest  devstack code and then ran these commands after sourcing openrc:
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID                                   Name                           Disk Format          Container Format     Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec        ami                  ami                        25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari                  ari                         3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki                  aki                         4955792
ubuntu@devstack-glance:/mnt/devstack$
ubuntu@devstack-glance:/mnt/devstack$ glance -d image-create --store s3e --disk-format raw --container-format bare --name complete_gibberish </etc/hosts
curl -i -X POST -H 'x-image-meta-container_format: bare' -H 'Transfer-Encoding: chunked' -H 'x-image-meta-store: s3e' -H 'User-Agent: python-glanceclient' -H 'x-image-meta-size: 221' -H 'x-image-meta-is_public: False' -H 'X-Auth-Token: <redacted_token>' -H 'Content-Type: application/octet-stream' -H 'x-image-meta-disk_format: raw' -H 'x-image-meta-name: complete_gibberish' -d '<open file '<stdin>', mode 'r' at 0x7f16181b6150>' http://10.4.36.1:9292/v1/images
HTTP/1.1 400 Bad Request
date: Thu, 12 Dec 2013 12:47:37 GMT
content-length: 52
content-type: text/plain; charset=UTF-8
x-openstack-request-id: req-c9bad6ee-d79c-41f3-bd96-d3929afd742c
400 Bad Request
Store for scheme s3e not found
Request returned failure status.
400 Bad Request
Store for scheme s3e not found
    (HTTP 400)
ubuntu@devstack-glance:/mnt/devstack$ glance index
ID                                   Name                           Disk Format          Container Format     Size
------------------------------------ ------------------------------ -------------------- -------------------- --------------
b26c03e4-7cdf-44fe-9187-7de315c9b38b complete_gibberish             raw                  bare                            221
6792e9a7-f4f8-48cb-b407-80e360b8a773 cirros-0.3.1-x86_64-uec        ami                  ami                        25165824
7808c034-3fdd-4975-af26-e7d5a15d2113 cirros-0.3.1-x86_64-uec-ramdis ari                  ari                         3714968
4efcddb2-9f20-413f-86a3-3bf69455e09b cirros-0.3.1-x86_64-uec-kernel aki                  aki                         4955792
This problem occurs using the v1 API. If using the V2 API the '--store' option does not seem to be present.","Check --store parameter validity before _reserve

Currently the value of the --store parameter is checked after the call
to _reserve is made. Thus the image is actually created on the
database even though the store is invalid, and then returns a E400.

Change-Id: Ic27ed30e605794342161530e8a8eacd6e6deb5d9
Closes-bug: #1260314"
1242,a7f24f0a2395594ead7eb8a5cef894b257cd7e4f,1399326284,,1.0,1,2,1,1,1,0.0,True,2.0,1034480.0,25.0,7.0,False,6.0,12305480.0,8.0,6.0,0.0,1723.0,1723.0,0.0,1422.0,1422.0,0.0,1511.0,1511.0,0.000622278,0.940883634,0.940883634,881,1316,1318108,cinder,a7f24f0a2395594ead7eb8a5cef894b257cd7e4f,1,0,"Change in requirements “However, SolarisISCSIDriver
    still called the _execute function, which no longer exists in it's
    parent classes.”","Bug #1318108 in Cinder: ""SolarisISCSIDriver fails on _execute""","A previous refactor [1] of SolarisISCSIDriver and SanDriver renamed
the `_execute` function to `san_execute`. However, SolarisISCSIDriver
still called the _execute function, which no longer exists in it's
parent classes.
[1] https://review.openstack.org/#/c/38194/","Fix solaris_execute in SolarisISCSIDriver.

A previous refactor [1] of SolarisISCSIDriver and SanDriver renamed
the `_execute` function to `san_execute`. However, SolarisISCSIDriver
still called the _execute function, which no longer exists in it's
parent classes.

This change calls san_execute instead of _execute in
super(SolarisISCSIDriver, san).

[1] https://review.openstack.org/#/c/38194/

Closes-Bug: #1318108
Change-Id: I4cafefdb3eef8f3c2c345907d7eabd4e8f88ef65"
1243,a82bae3f7b56fb84377fa6571b9f21ca6809da0e,1405679882,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,379,792,1278028,cinder,a82bae3f7b56fb84377fa6571b9f21ca6809da0e,0,0,Update the default settings to a new value,"Bug #1278028 in Cinder: ""    VMware: update the default 'task_poll_interval' time""","https://review.openstack.org/70079
Dear documentation bug triager. This bug was created here because we did not know how to map the project name ""openstack/nova"" to a launchpad project name. This indicates that the notify_impact config needs tweaks. You can ask the OpenStack infra team (#openstack-infra on freenode) for help if you need to.
commit 73c87a280e77e03d228d34ab4781ca2e3b02e40e
Author: Gary Kotton <email address hidden>
Date:   Thu Jan 30 01:44:10 2014 -0800
    VMware: update the default 'task_poll_interval' time
    The original means that each operation against the backend takes at
    least 5 seconds. The default is updated to 0.5 seconds.
    DocImpact
        Updated default value for task_poll_interval from 5 seconds to
        0.5 seconds
    Change-Id: I867b913f52b67fa9d655f58a2e316b8fd1624426
    Closes-bug: #1274439","VMware: Update default task_poll_interval value

The task_poll_interval option is used as the polling interval of vCenter/ESX
task status. This patch updates the default value to 0.5s in order to reduce
the wait time for task completion. Similar change has been done in nova and
this change is needed to keep the driver config values consistent.

DocImpact
   Updated default value of task_poll_interval to 0.5 seconds.

Change-Id: I66b80c629e255d61047592455ca42632f93051b2
Closes-bug: #1278028"
1244,a83506d170e90b111b7e63fab9f585883971f316,1386619120,,1.0,11,2,1,1,1,0.0,True,6.0,253307.0,32.0,11.0,False,55.0,558667.0,121.0,2.0,174.0,602.0,698.0,173.0,566.0,662.0,165.0,560.0,649.0,0.134630981,0.454987835,0.527169505,166,568,1259279,cinder,a83506d170e90b111b7e63fab9f585883971f316,0,0,Improve the logs,"Bug #1259279 in Cinder: ""Failed stats update doesn't include driver name""","When a driver fails to initialize at startup, the get_volume_stats() will always fail.  The problem is that the log file doesn't include the driver name in the update.
""WARNING cinder.volume.manager [-] Unable to update stats, driver is uninitialized""","Add the driver name to get stats log output

When the manager call tries to update the driver
stats, it checks to ensure the driver is initialized.
When the driver hasn't been initialized, it logs
a warning without the driver name.  This is confusing
for multiple backends.   This patch adds the driver
name, version and the config group name, to the log output,
so the admin can see which driver is failing.

Change-Id: I82e8aa969e7baa55db9c8dddc1c08db2a1a41091
Closes-Bug: #1259279"
1245,a84a8a5d83f545d0c00b7a83ad91b52dc31c1194,1404630023,,1.0,22,1,2,2,1,0.426228657,False,,,,,True,,,,,,,,,,,,,,,,,1070,1519,1342880,neutron,a84a8a5d83f545d0c00b7a83ad91b52dc31c1194,1,1,,"Bug #1342880 in neutron: ""Exception during message handling: 'NoneType' object is not iterable""","q-svc frequently tries to iterate on None Type.
The job can succeed even if this issue happens.
message: ""Exception during message handling"" AND message:""NoneType"" AND message:""object is not iterable"" AND filename:""logs/screen-q-svc.txt""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOiBcIkV4Y2VwdGlvbiBkdXJpbmcgbWVzc2FnZSBoYW5kbGluZ1wiIEFORCBtZXNzYWdlOlwiTm9uZVR5cGVcIiBBTkQgbWVzc2FnZTpcIm9iamVjdCBpcyBub3QgaXRlcmFibGVcIiBBTkQgZmlsZW5hbWU6XCJsb2dzL3NjcmVlbi1xLXN2Yy50eHRcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiODY0MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA1NTMzMDE3NzE4LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
 [req-ef892503-3f93-4c68-adeb-17394b66406c ] Exception during message handling: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 63, in sync_routers
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     self._ensure_host_set_on_ports(context, plugin, host, routers)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 76, in _ensure_host_set_on_ports
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     interface)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 84, in _ensure_host_set_on_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     {'port': {portbindings.HOST_ID: host}})
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 870, in update_port
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     need_notify=need_port_update_notify)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/plugins/ml2/plugin.py"", line 302, in _bind_port_if_needed
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher     plugin_context, port_id, binding, bind_context)
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-07-16 17:28:47.782 26480 TRACE oslo.messaging.rpc.dispatcher","Return a tuple of None's instead of one None

The expected return value of _commit_port_binding is something that
can be unpacked into two values. This changes the condition where a
concurrent delete was detected to return a 2-tuple of None's
instead of a bare None to prevent an iterable error.

Closes-Bug: #1342880
Change-Id: I9d3a51eab33819ffda6a65a00c3b5d1006a3577e"
1246,a868fcedf8e46070cae6aa8e59e61934fa23db1c,1396078547,,1.0,74,3,2,2,1,0.294615206,True,2.0,2411709.0,19.0,8.0,False,155.0,41362.0,461.0,2.0,291.0,1053.0,1284.0,252.0,1050.0,1242.0,203.0,1039.0,1182.0,0.026472878,0.134959772,0.15351674,704,1130,1299331,nova,a868fcedf8e46070cae6aa8e59e61934fa23db1c,1,1,,"Bug #1299331 in OpenStack Compute (nova): ""There isn't effect when attach/detach interface for paused instance""","$ nova boot --flavor 1 --image 76ae1239-0973-44cf-9051-0e1bc8f41cdd --nic net-id=a15cfbed-86d8-4660-9593-46447cb9464e vm1
$ nova list
+--------------------------------------+------+--------+------------+-------------+-------------------+
| ID                                   | Name | Status | Task State | Power State | Networks          |
+--------------------------------------+------+--------+------------+-------------+-------------------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1  | ACTIVE | -          | Running     | private=10.0.0.22 |
+--------------------------------------+------+--------+------------+-------------+-------------------+
$ brctl show
bridge name	bridge id		STP enabled	interfaces
br-eth0		0000.fe989d8bd148	no
br-ex		0000.8a1d06d8854e	no
br-ex2		0000.4a98bdebe544	no
br-int		0000.229ad5053a41	no
br-tun		0000.2e58a2f0e047	no
docker0		8000.000000000000	no
lxcbr0		8000.000000000000	no
qbr0ad6a86e-d9		8000.9e5491dd719a	no		qvb0ad6a86e-d9
       tap0ad6a86e-d9
$ neutron port-list
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                          |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
| 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d |      | fa:16:3e:3a:3e:5a | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.22""}   |
| 1e6bed8d-aece-4d3e-abcc-3ad7957d6d72 |      | fa:16:3e:9e:dc:83 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.12""} |
| 5f522a9a-2856-4a95-8bd8-c354c00abf0f |      | fa:16:3e:01:47:43 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.1""}    |
| 6226f6d3-3814-469c-bf50-8c99dfec481e |      | fa:16:3e:46:0e:35 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.2""}    |
| a3f2ab1c-a634-446d-8885-d7d8e5978fa1 |      | fa:16:3e:cf:02:d6 | {""subnet_id"": ""94575a05-796f-4ff5-b892-3c3b8231b303"", ""ip_address"": ""10.0.0.20""}   |
| c10390a9-6f84-44f5-8a17-91cb330a9e12 |      | fa:16:3e:41:7c:34 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.15""} |
| c814425c-be1a-4c06-a54b-1788c7c6fb31 |      | fa:16:3e:f5:fc:d3 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.2""}  |
| ebd874b7-43e6-4d18-b0ed-f86bb349d8b9 |      | fa:16:3e:e6:b5:09 | {""subnet_id"": ""e5dbc790-c26f-45b7-b2c7-574f12ad8b41"", ""ip_address"": ""172.24.4.19""} |
+--------------------------------------+------+-------------------+------------------------------------------------------------------------------------+
$ nova pause vm1
$ nova interface-detach vm1 0ad6a86e-d967-424e-9bf5-e6821cc0cd0d
$ nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID                                   | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| f7e2877d-c7f5-4493-89d4-c68e9839a7ff | vm1  | PAUSED | -          | Paused      |          |
+--------------------------------------+------+--------+------------+-------------+----------+
$ brctl show
bridge name	bridge id		STP enabled	interfaces
br-eth0		0000.fe989d8bd148	no
br-ex		0000.8a1d06d8854e	no
br-ex2		0000.4a98bdebe544	no
br-int		0000.229ad5053a41	no
br-tun		0000.2e58a2f0e047	no
docker0		8000.000000000000	no
lxcbr0		8000.000000000000	no
But tap still alive
$ ifconfig|grep tap0ad6a86e-d9
tap0ad6a86e-d9 Link encap:Ethernet  HWaddr fe:16:3e:3a:3e:5a
And login into instance, exec 'ifconfig', it will found the interface still attach to the instance","Attach/detach interface to paused instance with affect live flag

Currently attach/detach interface to paused instance is only with
VIR_DOMAIN_AFFECT_CONFIG flags. After instance unpause, there isn't
any effect on the instance, unless restart the instance. So add
flag VIR_DOMAIN_AFFECT_LIVE when attach/detach interface.

Change-Id: Iedfc7f6fc06e72d8e3eecebede1274582c8043fd
Closes-Bug: #1299331"
1247,a88d9d5936aabc52046ed3ae566741422bfaed78,1394017832,,1.0,401,321,17,7,1,0.649169232,True,7.0,6039988.0,72.0,15.0,False,148.0,287030.8235,543.0,1.0,57.0,752.0,776.0,57.0,752.0,776.0,55.0,747.0,769.0,0.00744978,0.099507782,0.102434482,525,946,1288178,nova,a88d9d5936aabc52046ed3ae566741422bfaed78,0,0,Rewrite nova policy,"Bug #1288178 in OpenStack Compute (nova): ""Sync new policy from oslo""","The oslo has changed the common policy for a long time, using a Enforer class to replace the old check function .In order to sync the common policy to nova, we have to rewrite the nova policy and the related unittests.","Rewrite nova policy to use the new changes of common policy

The oslo has rewritten the common policy for a long time, using a
Enforer class to replace the old check function. In order to sync the
common policy to nova, We have to also changed the nova policy and
related unittests.
Sync policy and relate module from Oslo.
   Some related change-id:Ife909bdf3277ef33c2fb1eae16ae261fa6374c63
                change-id:Ife84189be4b86a3ee90da4539ff2dbed125be23d

Closes-Bug: #1288178

Change-Id: Ic106851a24cfdc9d847f7d0e0d8a6ecbff888e05"
1248,a8d664d95f22a713d54c8ea30471dd3a4e976924,1402245043,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,958,1396,1327975,neutron,a8d664d95f22a713d54c8ea30471dd3a4e976924,1,0,“The name of the synchronized queue class is queue instead of Queue in Python3.”,"Bug #1327975 in neutron: ""Use import from six.moves to import the queue module""",The name of the synchronized queue class is queue instead of Queue in Python3.,"Use import from six.moves to import the queue module

The name of the synchronized queue class is queue instead of
Queue in Python3.

Change-Id: I5c5aba89b30b311bbfba2b64c61ea566d339b7c9
Closes-Bug: #1327975"
1249,a8d67485ab498a647b1f50184755c47b18e97e2c,1403721580,,1.0,3,2,3,2,1,0.960229718,False,,,,,True,,,,,,,,,,,,,,,,,991,1431,1332290,neutron,a8d67485ab498a647b1f50184755c47b18e97e2c,1,0,“It should be a configurable parameter.”,"Bug #1332290 in neutron: ""Cisco: Http timeout for connection to controller is not configurable""",The http timeout parameter used in the Cisco n1kv client module is a constant defined in cisco_constants module. It should be a configurable parameter.,"Add configurable http_timeout parameter for Cisco N1K

http_timeout is a constant defined in the code. This
change makes it configurable by providing it as an
option in the config file.

DocImpact

Closes-Bug: #1332290
Change-Id: I429dd9efa43f6a596933359f4627a7761411e621"
1250,a8ef64c4eccbc0639e0e7a81ff584287a4f9ddd0,1388718076,0.0,1.0,30,6,6,5,1,0.899204299,True,5.0,773692.0,29.0,8.0,False,36.0,409338.0,153.0,2.0,137.0,691.0,727.0,137.0,529.0,565.0,127.0,536.0,562.0,0.116788321,0.489963504,0.513686131,220,623,1263729,glance,a8ef64c4eccbc0639e0e7a81ff584287a4f9ddd0,1,1, Image size won't be updated if locations are updated to empty ,"Bug #1263729 in Glance: ""Image size won't be updated if locations are updated to empty""","See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L225
Based on current design, the image status will be updated to 'queued' if all locations are removed from the target image, but for now, the image size won't be updated. It doesn't make sense and will confuse the end user.","Set image size to None after removing all locations

Based on current design, the image size should be updated to
None if there is no location assocaited with the image. This
patch fixes this issue and introduces some related changes.

Closes-Bug: #1263729

Change-Id: I893468f1dc320ea9434f07c3a32f978cd5941b33"
1251,a8fcfdbe8b2f46a100f986eeef5c7d43aef69bb7,1379401735,,1.0,5,6,1,1,1,0.0,True,3.0,138255.0,28.0,17.0,False,14.0,418728.0,36.0,6.0,9.0,583.0,584.0,9.0,578.0,579.0,6.0,500.0,501.0,0.006972112,0.499003984,0.5,1567,1226442,1226442,cinder,a8fcfdbe8b2f46a100f986eeef5c7d43aef69bb7,1,1,“information in 'src_id' is lost from the exception message.”,"Bug #1226442 in Cinder: ""Argument of exception.VolumeNotFound is incorrect in storwize_svc.py""","cinder/volume/drivers/storwize_svc.py is used as follows.
  raise exception.VolumeNotFound(exception_msg,
                                 volume_id=src_id)
If 'exception_msg' is used, 'volume_id=src_id' is ignored.
Likewise,
  raise exception.SnapshotNotFound(exception_msg,
                                   snapshot_id=src_id)
If 'exception_msg' is used, 'snapshot_id=src_id' is ignored.
Therefore, information in 'src_id' is lost from the exception message.","Fixes call VolumeNotFound in the invalid argument

A mapping key of 'volume_id' is ignored when we use the non keyword
argument. Thus information in 'volume_id' is lost.  Similarly, in the
case of exception.SnapshotNotFound, information in 'snapshot_id' is
lost.

Change-Id: I1a704cc1866ad9b0d9578e16e77c376a77ea5b3e
Closes-Bug: #1226442"
1252,a9267644ee09591e2d642d6c1204d94a9fdd8c82,1388698139,1.0,1.0,28,6,3,3,1,0.770395378,True,7.0,4474754.0,45.0,24.0,False,30.0,751915.0,97.0,3.0,8.0,1207.0,1208.0,8.0,1040.0,1041.0,8.0,1061.0,1062.0,0.006965944,0.821981424,0.822755418,63,391,1248415,cinder,a9267644ee09591e2d642d6c1204d94a9fdd8c82,0,0,Add functionality,"Bug #1248415 in Cinder: ""cinder extend Bug : tgt update not implemented""","When using 'cinder extend' command, tgt update process doesn't implemented.
So, If you want to see extended size of volume inside your vm, you must run 'tgt-admin --update iqn.2010-10.org.openstack:volume-xxxxx' in hostmachine that has the extended volume .
See my test case as follow.
1. Create cinder volume
# cinder create --volume-type LVM01-type --display-name test-extend-vol01 3
2. List cinder volume & confirm the size 3 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Display Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 |  3   |  LVM01-type |  false   |             |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
3. run lvs command to confirm the size 3 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 3.00g
4. run 'tgt-admin --show' command to confirm the size 3 GB
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
5. Extend cinder volume
# cinder extend 885f850f-8e03-4576-a60d-5d3d1362a9da 5
6. List cinder volume and confirm the extended size to 5 GB
# cinder list
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
|                  ID                  |   Status  |    Display Name   | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
| 885f850f-8e03-4576-a60d-5d3d1362a9da | available | test-extend-vol01 |  5   |  LVM01-type |  false   |             |
+--------------------------------------+-----------+-------------------+------+-------------+----------+-------------+
7. Run 'lvs' command to confirm the extended size to 5 GB
# lvs | grep 885f850f-8e03-4576-a60d-5d3d1362a9da
  volume-885f850f-8e03-4576-a60d-5d3d1362a9da cinder-volumes -wi-ao 5.00g
8. Run 'tgt-admin --show' command to confirm the extended size to 5 GB , but it still 3 GB because didn't implemented 'tgt update'
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 3221 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
9. Run 'tgt-admin --update' command to update
# tgt-admin --update iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
10. Run 'tgt-admin --show' command to confirm 5 GB updated
# tgt-admin --show
Target 1: iqn.2010-10.org.openstack:volume-885f850f-8e03-4576-a60d-5d3d1362a9da
    System information:
        Driver: iscsi
        State: ready
    I_T nexus information:
    LUN information:
        LUN: 0
            Type: controller
            SCSI ID: IET     00010000
            SCSI SN: beaf10
            Size: 0 MB, Block size: 1
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: null
            Backing store path: None
            Backing store flags:
        LUN: 1
            Type: disk
            SCSI ID: IET     00010001
            SCSI SN: beaf11
            Size: 5369 MB, Block size: 512
            Online: Yes
            Removable media: No
            Readonly: No
            Backing store type: rdwr
            Backing store path: /dev/cinder-volumes/volume-885f850f-8e03-4576-a60d-5d3d1362a9da
            Backing store flags:
    Account information:
    ACL information:
        ALL
Please check this bug and answer to me.
Thank you.","LVM: update iscsi target on volume attach

This patch updates the existing iSCSI target for a LVM volume during
attach.  This is necessary in the event that a user extended the volume
after creation so that the correct size is visible to the host.

This adds support only for tgtadm.  Other changes may be needed for
ietadm/lioadm to provide the same functionality.

Change-Id: I185a90ffc4d50dd9f91381df07289476fa792043
Closes-Bug: #1248415"
1253,a957488450e6d31dfc34eade38e03b5535c1d017,1410435319,,1.0,9,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1301,1767,1368202,nova,a957488450e6d31dfc34eade38e03b5535c1d017,0,0,Refactoring “was deprecated”,"Bug #1368202 in OpenStack Compute (nova): ""Nova console VMRCConsole will break with ESX driver""",VMware ESX driver was deprecated in J.This console class will break.,"Console: warn that the Nova VMRC console driver will be deprecated in K

This code is not used and should be deprecated. In addition to this the
ESX driver was deprecated in commit 1deb31f85a8f5d1e261b2cf1eddc537a5da7f60b

Closes-bug: #1368202

Change-Id: Ic71cf4d01ccd7eee19f6c44e1101e542900235d6"
1254,a957beb94720ca3f171149155f9dcabd20e6141b,1378115348,,1.0,15,2,2,2,1,0.787126586,True,2.0,237344.0,10.0,3.0,False,3.0,2057667.0,4.0,2.0,156.0,1952.0,2046.0,156.0,1688.0,1782.0,154.0,1812.0,1904.0,0.026266734,0.307236062,0.32282664,1499,1218861,1218861,nova,a957beb94720ca3f171149155f9dcabd20e6141b,1,1, ,"Bug #1218861 in OpenStack Compute (nova): ""Nova compute throws an exception if ephemeral is passed wiithout size.""","Alex Xu	reports on a related code review https://review.openstack.org/#/c/41647/
""""""
Xavier, When create vm as below, i got some error from nova-compute side:
'{""server"": {""name"": ""vm3"", ""image_ref"": ""b8cd5faa-a65f-4e47-bfc8-68061574b428"", ""flavor_ref"": ""1"", ""max_count"": 1, ""min_count"": 1, ""os-block-device-mapping:block_device_mapping"": [{""device_name"": ""/dev/vdc"", ""source_type"": ""blank"", ""destination_type"": ""local"", ""boot_index"": 0}], ""networks"": [{""uuid"": ""b6ba34f1-5504-4aca-825b-04511c104802""}]}}'
2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] Traceback (most recent call last): 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1018, in _build_instance 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] set_access_ip=set_access_ip) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1392, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] LOG.exception(_('Instance failed to spawn'), instance=instance) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/compute/manager.py"", line 1388, in _spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] block_device_info) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1689, in spawn 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] admin_pass=admin_password) 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2002, in _create_image 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] size = eph['size'] * 1024 * 1024 * 1024 2013-08-30 11:01:32.500 TRACE nova.compute.manager [instance: bc919048-049b-445a-a676-e037d7b1fe31] TypeError: unsupported operand type(s) for *: 'NoneType' and 'int'
That because I miss 'volume_size' in the request.
""""""
I was also able to reproduce on devstack with latest master.","Interpret BDM None size field as 0 on compute side

This patch makes sure that even if we receive an ephemeral or swap BDM
without size information - it is defaulted to 0 by the compute service
prior to booting the VM, as the size is expected to be present and an
integer number by the driver code that uses it.

Closes-bug: #1218861

Change-Id: Ic6fc15d1cf876aa4fd99e17c582c1ab55e3a18c1"
1255,a98dc7680d4689f5ce5f602f9c5cb3bcc77de215,1393382216,,2.0,54,107,23,9,1,0.929412088,True,6.0,5641877.0,114.0,19.0,False,58.0,176661.6522,144.0,6.0,34.0,2073.0,2085.0,23.0,1788.0,1794.0,13.0,396.0,398.0,0.016241299,0.460556845,0.46287703,424,839,1281481,neutron,a98dc7680d4689f5ce5f602f9c5cb3bcc77de215,0,0,tests,"Bug #1281481 in neutron: ""test_openvswitch_plugin sometimes fail""","depending on the sequence of tests, test_openvswitch_plugin sometimes ends up to kick
unexpected code via impl_fake rpc backend.
an example of the failure:
http://logs.openstack.org/91/71791/5/check/gate-neutron-python27/2317e6a/","tests/unit: refactor reading neutron.conf.test

neutron.conf.test includes rpc_backend whose value is stashed.
Thus it is required to reset when tearing down, otherwise the stale status
will be used by succeeding tests causing random error.

This patch refactors reading neutron.conf.test and resets the status of
rpc_backend properly.

Closes-Bug: #1281481
Closes-Bug: #1284549
Change-Id: I0fa5945b6adbb9945d353028ec88d00ccbf4e31a"
1256,a9a36082528a6d46d950b5e83cb46c2af389317a,1380738265,,1.0,5,5,2,2,1,0.721928095,True,1.0,446167.0,10.0,2.0,False,26.0,1048696.0,72.0,1.0,164.0,969.0,1038.0,164.0,855.0,924.0,155.0,833.0,896.0,0.146478873,0.783098592,0.842253521,1626,1233907,1233907,cinder,a9a36082528a6d46d950b5e83cb46c2af389317a,0,0,"Feature, “cinder should use cinder/utils.get_root_helper()”","Bug #1233907 in Cinder: ""cinder should use cinder/utils.get_root_helper()""","Cinder has many places where code manually constructs the same root_helper instead of using the
cinder.utils.get_root_helper to do the same thing.","use cinder utils.get_root_helper

Replaced all of the manual entries of
the root_helper with the cinder
utils.get_root_helper()

Closes-Bug: #1233907

Change-Id: I4c9b4c27ed4b779595a272436d79ec068e7ee537"
1257,a9c6bb4647afd3df985cf40531d48957ad5d9580,1389215009,,1.0,22,2,1,1,1,0.0,True,2.0,207683.0,14.0,6.0,False,5.0,6423008.0,6.0,4.0,611.0,959.0,1351.0,560.0,767.0,1129.0,156.0,338.0,402.0,0.233979136,0.505216095,0.600596125,271,676,1267246,neutron,a9c6bb4647afd3df985cf40531d48957ad5d9580,0,0,tests,"Bug #1267246 in neutron: ""missing test coverage in port_security tests""",missing code coverage in port_security tests,"Add test to port_security to test with security_groups

This patch adds a missing testcase to the port_security tests to test
for creating a port with port_security_enabled=False and passing in
a security group.

Closes-bug: #1267246
Change-Id: Ifb5a5571f016a5d7c5b5075c97dc27279cd79bb7"
1258,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1409685115,,2.0,78,5,2,2,1,0.922259647,False,,,,,True,,,,,,,,,,,,,,,,,259,663,1266611,nova,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1,1,a fix commit add a bug,"Bug #1266611 in OpenStack Compute (nova): ""test_create_image_with_reboot fails with InstanceInvalidState in gate-nova-python*""","Looks like an intermittent failure:
http://logs.openstack.org/25/64725/4/check/gate-nova-python27/e603e9e/testr_results.html.gz
2014-01-06 21:49:45.870 | Traceback (most recent call last):
2014-01-06 21:49:45.870 |   File ""nova/tests/api/ec2/test_cloud.py"", line 2343, in test_create_image_with_reboot
2014-01-06 21:49:45.870 |     self._do_test_create_image(False)
2014-01-06 21:49:45.871 |   File ""nova/tests/api/ec2/test_cloud.py"", line 2316, in _do_test_create_image
2014-01-06 21:49:45.871 |     no_reboot=no_reboot)
2014-01-06 21:49:45.871 |   File ""nova/api/ec2/cloud.py"", line 1709, in create_image
2014-01-06 21:49:45.872 |     name)
2014-01-06 21:49:45.872 |   File ""nova/compute/api.py"", line 161, in inner
2014-01-06 21:49:45.872 |     method=f.__name__)
2014-01-06 21:49:45.873 | InstanceInvalidState: Instance b1d4d924-069c-409c-bbdb-4f0478526057 in task_state powering-off. Cannot snapshot_volume_backed while the instance is in this state.","Stop stack tracing when trying to auto-stop a stopped instance

Commit cc5388bbe81aba635fb757e202d860aeed98f3e8 added locks to
stop_instance and the _sync_power_states periodic task to try and fix a
race between stopping the instance via the API where the task_state is
set to powering-off, and the periodic task seeing the instance
power_state as shutdown in _sync_instance_power_state and calling the
stop API again, at which point the task_state is already None from the
first stop API call and we get an UnexpectedTaskStateError.

The handle_lifecycle_event method is getting callbacks from the libvirt
driver on state changes on the VM and calling the
_sync_instance_power_state method which may try to stop the instance
asynchronously, and lead to UnexpectedTaskStateError if the instance is
already stopped by the time it gets the lock and the task_state has
changed.

Attempting to lock in handle_lifecycle_event just moves the race around
so this change adds logic to stop_instance such that if the instance
says it's active but the virt driver says it's not running, then we add
None to the expected_task_state so we don't stacktrace on
instance.save().

An alternative and/or additional change to this would be doing a call
rather than a cast when _sync_instance_power_state calls the stop API
but in some previous testing it doesn't appear to make a significant
difference in the race found when we hit the stop_instance method.

Adds a bunch of debug logging since this code is inherently racey and
is needed when looking at failures around these operations.

Closes-Bug: #1339235
Closes-Bug: #1266611
Related-Bug: #1320628

Change-Id: Ib495a5ab15de88051c5fa7abfb58a5445691dcad"
1259,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1409685115,,2.0,78,5,2,2,1,0.922259647,False,,,,,True,,,,,,,,,,,,,,,,,1047,1491,1339235,nova,aa1792eb4c1d10e9a192142ce7e20d37871d916a,1,1,"“Commit cc5388bbe81aba635fb757e202d860aeed98f3e8 added locks to
    stop_instance and the _sync_power_states periodic task to try and fix a”","Bug #1339235 in OpenStack Compute (nova): ""UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None""","This is showing up all over the n-cpu logs on teardown of tempest tests:
UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None
For example:
http://logs.openstack.org/06/103206/4/check/check-tempest-dsvm-postgres-full/b5e8f3c/logs/screen-n-cpu.txt.gz?level=TRACE
We have nearly 40K hits on this in logstash in 7 days:
message:""UnexpectedTaskStateError: Unexpected task state: expecting (u'powering-off',) but the actual state is None"" AND tags:""screen-n-cpu.txt""
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5leHBlY3RlZFRhc2tTdGF0ZUVycm9yOiBVbmV4cGVjdGVkIHRhc2sgc3RhdGU6IGV4cGVjdGluZyAodSdwb3dlcmluZy1vZmYnLCkgYnV0IHRoZSBhY3R1YWwgc3RhdGUgaXMgTm9uZVwiIEFORCB0YWdzOlwic2NyZWVuLW4tY3B1LnR4dFwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDA0ODQxMjQ3MDk4fQ==
This is the interesting traceback from the compute manager:
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/exception.py"", line 88, in wrapped
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/exception.py"", line 71, in wrapped
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 272, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     LOG.info(_(""Task possibly preempted: %s"") % e.format_message())
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 266, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 330, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 308, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     kwargs['instance'], e, sys.exc_info())
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 296, in decorated_function
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2356, in stop_instance
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     instance.save(expected_task_state=task_states.POWERING_OFF)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/objects/base.py"", line 187, in wrapper
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     ctxt, self, fn.__name__, args, kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/nova/nova/conductor/rpcapi.py"", line 349, in object_action
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     objmethod=objmethod, args=args, kwargs=kwargs)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     retry=self.retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout, retry=retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 404, in send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     retry=retry)
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 395, in _send
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-07-08 00:46:47.922 18853 TRACE oslo.messaging.rpc.dispatcher UnexpectedTaskStateError_Remote: Unexpected task state: expecting (u'powering-off',) but the actual state is None","Stop stack tracing when trying to auto-stop a stopped instance

Commit cc5388bbe81aba635fb757e202d860aeed98f3e8 added locks to
stop_instance and the _sync_power_states periodic task to try and fix a
race between stopping the instance via the API where the task_state is
set to powering-off, and the periodic task seeing the instance
power_state as shutdown in _sync_instance_power_state and calling the
stop API again, at which point the task_state is already None from the
first stop API call and we get an UnexpectedTaskStateError.

The handle_lifecycle_event method is getting callbacks from the libvirt
driver on state changes on the VM and calling the
_sync_instance_power_state method which may try to stop the instance
asynchronously, and lead to UnexpectedTaskStateError if the instance is
already stopped by the time it gets the lock and the task_state has
changed.

Attempting to lock in handle_lifecycle_event just moves the race around
so this change adds logic to stop_instance such that if the instance
says it's active but the virt driver says it's not running, then we add
None to the expected_task_state so we don't stacktrace on
instance.save().

An alternative and/or additional change to this would be doing a call
rather than a cast when _sync_instance_power_state calls the stop API
but in some previous testing it doesn't appear to make a significant
difference in the race found when we hit the stop_instance method.

Adds a bunch of debug logging since this code is inherently racey and
is needed when looking at failures around these operations.

Closes-Bug: #1339235
Closes-Bug: #1266611
Related-Bug: #1320628

Change-Id: Ib495a5ab15de88051c5fa7abfb58a5445691dcad"
1260,aa453576ff13c97c63dcb3cd5941675ffdc93360,1386056675,,3.0,28,28,8,2,1,0.786626343,True,2.0,175694.0,10.0,3.0,False,48.0,847335.5,125.0,2.0,487.0,1065.0,1283.0,481.0,916.0,1133.0,471.0,922.0,1129.0,0.38911789,0.760923331,0.931574608,112,511,1256737,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,0,0,Tests and new version,"Bug #1256737 in Cinder: ""Use assertNotEqual instead of assertNotEquals in unit tests""","The method assertNotEquals has been deprecated.
In Python 3, a deprecated warning is raised when using assertNotEquals
therefore we should use assertNotEqual instead.","Update hacking to hacking>=0.8.0,<0.9

Update hacking version to match version specified in requirements repo.

Fixed the following issues, which the newer version checks for:
print """" -> print("""")
self.assertEquals -> self.assertEqual
self.assertNotEquals -> self.assertNotEqual

Change-Id: Ic4b70fd8f565cda28e23fe6b1da0e278f949373c
Closes-Bug: #1256738
Closes-Bug: #1256737
Closes-Bug: #1257274"
1261,aa453576ff13c97c63dcb3cd5941675ffdc93360,1386056675,,3.0,28,28,8,2,1,0.786626343,True,2.0,175694.0,10.0,3.0,False,48.0,847335.5,125.0,2.0,487.0,1065.0,1283.0,481.0,916.0,1133.0,471.0,922.0,1129.0,0.38911789,0.760923331,0.931574608,113,512,1256738,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,0,0,tests,"Bug #1256738 in Cinder: ""Use assertEqual instead of assertEquals in unitttest""","The method assertEquals has been deprecated since python 2.7.
http://docs.python.org/2/library/unittest.html#deprecated-aliases
Also in Python 3, a deprecated warning is raised when using assertEquals
therefore we should use assertEqual instead.","Update hacking to hacking>=0.8.0,<0.9

Update hacking version to match version specified in requirements repo.

Fixed the following issues, which the newer version checks for:
print """" -> print("""")
self.assertEquals -> self.assertEqual
self.assertNotEquals -> self.assertNotEqual

Change-Id: Ic4b70fd8f565cda28e23fe6b1da0e278f949373c
Closes-Bug: #1256738
Closes-Bug: #1256737
Closes-Bug: #1257274"
1262,aa453576ff13c97c63dcb3cd5941675ffdc93360,1386056675,,3.0,28,28,8,2,1,0.786626343,True,2.0,175694.0,10.0,3.0,False,48.0,847335.5,125.0,2.0,487.0,1065.0,1283.0,481.0,916.0,1133.0,471.0,922.0,1129.0,0.38911789,0.760923331,0.931574608,131,531,1257274,cinder,aa453576ff13c97c63dcb3cd5941675ffdc93360,1,0,Issue of version,"Bug #1257274 in Cinder: ""Bump hacking to 0.8""","Due to Bump hacking dependency is not the 0.8, some compatibility checks with python 3.x are not being done on gate and it is bringing code issues.","Update hacking to hacking>=0.8.0,<0.9

Update hacking version to match version specified in requirements repo.

Fixed the following issues, which the newer version checks for:
print """" -> print("""")
self.assertEquals -> self.assertEqual
self.assertNotEquals -> self.assertNotEqual

Change-Id: Ic4b70fd8f565cda28e23fe6b1da0e278f949373c
Closes-Bug: #1256738
Closes-Bug: #1256737
Closes-Bug: #1257274"
1263,aa4a89eda8941bce22ead9db2dd5f7bc6ca90e04,1394548135,,1.0,132,8,5,4,1,0.363278728,True,6.0,841907.0,39.0,10.0,False,18.0,238136.2,37.0,3.0,12.0,1061.0,1067.0,12.0,970.0,976.0,12.0,1007.0,1013.0,0.008547009,0.662721893,0.666666667,548,969,1288962,cinder,aa4a89eda8941bce22ead9db2dd5f7bc6ca90e04,0,0,No bug. doesn't enforce 32 block-ranges limit,"Bug #1288962 in Cinder: ""NetApp iSCSI driver doesn't enforce 32 block-ranges limit""","The NetApp zapi for clone create has an undocumented limit of 32 block ranges (of a max 2^24 blocks each).
The driver should check to make sure the number of block range segments does not exceed 32.  In the case of an excessively large request, the xml may be rejected by the filer silently and the volume will become stuck in an extending state, with 2 flexVols (the original and the new sized one) present on the filer.","Allow NetApp iSCSI driver to sub-clone large volumes

The NetApp zapi used during certain extend operations has several limits
imposed on it.  Each block-range provided can only be 2^24 in size, and
there can only be 32 block-ranges per zapi call.  This fix allows the
NetApp iSCSI driver to send multiple zapi calls if necessary, to allow
for extend operations on volumes of an arbitrary size.

Closes-Bug: #1288962
Change-Id: I981d22f32cb2182112fbea3ea9880d1e8c8c91ab"
1264,aa5ace6b7555dd9ed59dd07ea022585f44767f27,1393809089,,1.0,2,2,2,2,1,1.0,True,3.0,96274.0,53.0,11.0,False,9.0,357146.0,11.0,4.0,15.0,1162.0,1170.0,15.0,1013.0,1021.0,7.0,681.0,682.0,0.009049774,0.771493213,0.772624434,508,928,1287031,neutron,aa5ace6b7555dd9ed59dd07ea022585f44767f27,0,0,unused code,"Bug #1287031 in neutron: ""Unused code 'as e' in exception blocks""","unused code  'as e' in exception blocks with flowing files:
neutron/api/v2/resource.py
neutron/plugins/vmware/nsxlib/router.py","Remove unused 'as e' in exception blocks

Change-Id: I0bd26e1657a4e0ec40405d03d43aee9e10a13636
Closes-bug: #1287031"
1265,aa622eb799556fd8a7c96d9c431b7037eaf439d0,1311781267,2.0,,180,0,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1739,1249181,1249181,Swift,aa622eb799556fd8a7c96d9c431b7037eaf439d0,1,1, ,"Bug #1249181 in OpenStack Object Storage (swift): ""500 Error when getting /recon/diskusage""","I yanked a disk from my system by mistake .Then I execute the command ""curl -v http://proxy-ip:6000/recon/diskusage"" and 500 reported.
It is unreasonable that the recon daemon crash when the only one disk removed while the others disks works fine.
Thanks.",recon middlewear for the object server and utils for cluster monitoring
1266,aa85a97ca2dcb06996ed133d864705f1dca722b1,1392013494,,1.0,47,25,2,2,1,0.986040032,True,2.0,226289.0,39.0,8.0,False,19.0,458422.0,21.0,3.0,455.0,2366.0,2468.0,385.0,2060.0,2152.0,198.0,595.0,649.0,0.257772021,0.772020725,0.841968912,1822,1277029,1277029,Neutron,aa85a97ca2dcb06996ed133d864705f1dca722b1,1,0,"Changed requirements ""Previously list operations in ovs_lib returns an empty list if RuntimeError occurs and a caller cannot distinguish an error from normal results.”","Bug #1277029 in neutron: ""cannot distinguish error from no port in ovs_lib list operation""","list operations in ovs_lib returns an empty list even if Runtime Error occurs.
As a result, when Runtime Error occurs, a caller thinks all ovs ports have disappeared.
ovs-vsctl sometimes fails (mostly raises alarm error?)
ovs_lib should provide a way to distinguish these two situations.
list operations in ovs_lib
- get_vif_port_set (used by OVS agent and ryu agent)
- get_vif_ports (used by NEC agent, OVS cleanup)
- get_bridge (used by OVS agent, OVS cleanup)
It affects all agent using the above list operation.
It affects OVS agent and NEC agent. It triggers unexpected port deletions.
In OVS cleanup, there is no negative effect. It just nothing for this case.","Raise an error from ovs_lib list operations

Previously list operations in ovs_lib returns an empty list
if RuntimeError occurs and a caller cannot distinguish an error
from normal results. This commit changes ovs_lib list operations
(get_vif_port_set, get_vif_ports, get_bridges) to raise an
exception when RuntimeError occurs.

Note: callers of these commands are ovs/nec/ryu-agent and ovs_cleanup.
- plugin agents: these commands are inside in try/except clause
  in daemon loop and there is no need to change.
- ovs_cleanup: there is no error catch logic in main() at now
  and it calls commands other than ovs_lib, so it can be cleanup
  later if required.

It also fixes the code to use excutils.save_and_reraise_exception
when reraising an exception.

Change-Id: I2aa3b51b8661c75846cb588c08c8f8ee00c37004
Closes-Bug: #1277029"
1267,aa8938d696e6e2ecf23709f748de825dc2d07fd0,1380528401,0.0,1.0,34,4,4,4,1,0.869977324,True,5.0,2625874.0,38.0,20.0,False,43.0,429337.0,92.0,6.0,398.0,2706.0,2960.0,315.0,2400.0,2588.0,339.0,1821.0,2023.0,0.055284553,0.296260163,0.329105691,1617,1233026,1233026,nova,aa8938d696e6e2ecf23709f748de825dc2d07fd0,1,1, ,"Bug #1233026 in OpenStack Compute (nova): ""exception.InstanceIsLocked is not caught in start and stop server api""","when port nova-v3-test: test_server_actions.ServerActionsV3TestXML.test_lock_unlock_server. We found the exception.InstanceIsLocked is not caught in start and stop server API.
the following is the nova log:
2013-09-30 15:03:29.306 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mAction: 'action', body: <?xml version=""1.0"" encoding=""UTF-8""?>
<stop xmlns=""http://docs.openstack.org/compute/api/v1.1""/>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:935^[[00m
2013-09-30 15:03:29.307 ^[[00;32mDEBUG nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m^[[00;32mCalling method <bound method ServersController._stop_server of <nova.api.openstack.compute.plugins.v3.servers.ServersController object at 0x577c250>>^[[00m ^[[00;33mfrom (pid=23798) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:936^[[00m
2013-09-30 15:03:29.339 ^[[00;32mDEBUG nova.api.openstack.compute.plugins.v3.servers [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;32m] ^[[01;35m[instance: cd4fec81-d2e8-43cd-ab5d-47da72dd90fa] ^[[00;32mstop instance^[[00m ^[[00;33mfrom (pid=23798) _stop_server /opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py:1372^[[00m
2013-09-30 15:03:29.340 ^[[01;31mERROR nova.api.openstack.extensions [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[01;31m] ^[[01;35m^[[01;31mUnexpected exception in API method^[[00m
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 1374, in _stop_server
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    self.compute_api.stop(context, instance)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/compute/api.py"", line 198, in wrapped
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    return func(self, context, target, *args, **kwargs)
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m  File ""/opt/stack/nova/nova/compute/api.py"", line 187, in inner
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m    raise exception.InstanceIsLocked(instance_uuid=instance['uuid'])
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00mInstanceIsLocked: Instance cd4fec81-d2e8-43cd-ab5d-47da72dd90fa is locked
^[[01;31m2013-09-30 15:03:29.340 TRACE nova.api.openstack.extensions ^[[01;35m^[[00m
2013-09-30 15:03:29.341 ^[[00;36mINFO nova.api.openstack.wsgi [^[[01;36mreq-d791baac-2015-4e65-8d02-720b0944e824 ^[[00;36mdemo demo^[[00;36m] ^[[01;35m^[[00;36mHTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.InstanceIsLocked'>^[[00m","catch exception in start and stop server api

this catches exception.InstanceIsLocked in start and stop
server api.

Closes-Bug: #1233026

Change-Id: Ibb882178ab0770837702d503ef88c6510072e33f"
1268,aa9383081230b92ecc7c1b176cb3eb62a237949c,1397713495,,1.0,41,1,2,2,1,0.276195428,True,12.0,1935898.0,113.0,29.0,False,56.0,4126643.0,89.0,4.0,61.0,2179.0,2218.0,47.0,1920.0,1948.0,61.0,2087.0,2126.0,0.007927375,0.266973533,0.271960107,808,1236,1308839,nova,aa9383081230b92ecc7c1b176cb3eb62a237949c,1,1,,"Bug #1308839 in OpenStack Compute (nova): ""ProcessExecutionError exception is not defined in exception.py now""","ubuntu@devstack-master:/opt/stack/nova$ grep -r exception.ProcessExecutionError *
nova/virt/libvirt/volume.py:        except exception.ProcessExecutionError as exc:
commit 5e016846708ef62c92dcf607f03c67c36ce5c23f has been fixed all other wrong used places, but this one is added after this change.
[Impact]
Error doesn't exist, if error encountered then wrong error reported.
[Test Case]
grep -r exception.ProcessExecutionError /usr/lib/python2.7/dist-packages/nova/*
[Regression Potential]
Minimal. This change is in an error path already. When error path is encountered, the exception doesn't exist and causes a different error to occur but calling code treats both the same way. This ensures the right error is thrown.","Fix wrong used ProcessExecutionError exception

This class has been moved to nova.openstack.common.processutils,
but a wrong usage is exists in nova.virt.libvirt.volume,
correct here.

Closes-bug: #1308839
Change-Id: I76f99b63dc5097b462dcff6ff63cbbb13d7580fb"
1269,aaa9d6a8a28335b8dac4e1d4045571aa565e3196,1404136546,,1.0,5,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1023,1466,1335859,nova,aaa9d6a8a28335b8dac4e1d4045571aa565e3196,0,0,Bug in test,"Bug #1335859 in OpenStack Compute (nova): ""Wrong assert in nova.tests.virt.vmwareapi.test_vmops.py""","bad assertion in nova.tests.virt.vmwareapi.vmwareapi.test_vmops.py:640
self.assertTrue(3, len(mock_mkdir.mock_calls)) should be replaced with assertEqual","Fixed wrong assertion in test_vmops.py

There is wrong assert method in
nova.tests.virt.vmwareapi.vmwareapi.test_vmops.py:640
while checking a number of calls mocked method 'mkdir'.

We should use assertEqual(3, len(mock_mkdir.mock_calls))
instead of self.assertTrue(3, len(mock_mkdir.mock_calls))

But number of calling method 'mkdir' not always is 3.
If dict block_device_info contains key 'block_device_mapping'
with any information, method 'mkdir' wouldn't be called in method
self._vmops.spawn(). That is why corresponding check been added.

Change-Id: Ifc26e112b9c2974e0fb7a91ba39d5b2d887bd549
Closes-Bug: #1335859"
1270,aab3c6178571456ea3f2523f401d0a18c04e573c,1382431939,,1.0,70,6,2,2,1,0.596510992,True,9.0,4988597.0,54.0,13.0,False,39.0,1194.0,83.0,2.0,8.0,1911.0,1916.0,8.0,1648.0,1653.0,8.0,1810.0,1815.0,0.001405372,0.282792005,0.283572767,10,322,1243108,nova,aab3c6178571456ea3f2523f401d0a18c04e573c,1,1,Bug in a fixing bug,"Bug #1243108 in OpenStack Compute (nova): ""Image device should be reset after mounting and teardown""","When fixing bug#1208387, Image device info is returned in action setup_container.
But in that patch(https://review.openstack.org/#/c/41891/), I missed to reset the image device value in mount() and teardown().
I'm sorry to make such easy mistake, a new patch will be commited soon.","LXC: Image device should be reset in mount() and teardown()

When fixing bug#1208387, image device name is returned in function
setup_container. But it's missed to reset in function mount() and
teardown().

This patch is to fix it. We now just return the device name when
mounting disk image during setting up container and remove the
member variable 'device' from Class _DiskImage added in last patch.

Closes-bug: #1243108
Related-bug: #1208387

Change-Id: I2aaed6f3367edce4fb23b78f0e979440954609a8"
1271,aabb0fa1f68974e2fa4828cde1462dc47c429655,1397682198,,1.0,125,63,2,2,1,0.736573979,True,3.0,2927995.0,21.0,4.0,True,,,,,,,,,,,,,,,,,797,1225,1308253,cinder,aabb0fa1f68974e2fa4828cde1462dc47c429655,1,1,,"Bug #1308253 in Cinder: ""Reset state doesn't update migration status""","In the case of a failed migration the volume's status and the migration status are both set to error state.  We have the ability to use reset-state to get the volume back to a usable state, however the migration-status isn't updated which makes it impossible to do things like delete the volume or perhaps retry the migration.
We should have the reset-state command clean up outliers like this as well.","Allow reset-state on attach and migration fields

The reset-state API call only sets that main status
column on the volume object.  There's also a method
to set attach status, however it turns out that due
to the implementation of the status validation method
in the parent class, this was impossible to use because
it only checked and accepted ""status"" NOT ""attach_status"".

This patch fixes that attach_status problem, it also implements
the ability to update the migration status.  A cinderclient change
will be needed to expose these calls as well.

DocImpact

Change-Id: I59e1bb2522f033c944fa07acf4876ca71c8c3d3f
Closes-Bug: #1308253"
1272,aad4e2437736270af7164df4fc34174bf3fb49f9,1406321576,,1.0,33,34,9,7,1,0.792101389,False,,,,,True,,,,,,,,,,,,,,,,,1107,1559,1348787,cinder,aad4e2437736270af7164df4fc34174bf3fb49f9,0,0,Feature or bug “Split this into a separate file to reduce overhead (and things that can break)”,"Bug #1348787 in Cinder: ""cinder-manage loads more modules than necessary""","cinder-manage loads up more stuff than it needs to, should probably look at reducing this since it's an admin command.
For one, it loads up the paramiko library since it's used in utils.py.  This seems rather unnecessary.
(Noticed because the python crypto module loaded by paramiko was throwing warnings to my terminal about linking against a newer libgmp.)","Move generate_password into volume utils

This is only used in places where volume/utils is
relevant, and moving it there reduces the amount of
code that has to load the python Crypto library.

Closes-Bug: #1348787

Change-Id: Id74fdd2d6f12f0196055deb1b0dd610677caff0f"
1273,aadfffaaa69c6a8939f6c88cfbaeb026c7604163,1397638791,0.0,1.0,87,2,2,2,1,0.264517368,True,3.0,2936862.0,76.0,19.0,False,56.0,1188798.5,161.0,7.0,0.0,4238.0,4238.0,0.0,3403.0,3403.0,0.0,2994.0,2994.0,0.000127943,0.383188332,0.383188332,790,1217,1307408,nova,aadfffaaa69c6a8939f6c88cfbaeb026c7604163,1,1,,"Bug #1307408 in OpenStack Compute (nova): ""VMWare - Destroy fails when Claim is not successful""","If Claim is not successful, compute manager triggers a call to destroy instance.
Destroy fails since the compute node (cluster) is set only after claim is successful.
This issue occurs when multiple parallel nova boot operations are triggered simultaneously.
Snippet from nova-compute.log
2014-04-06 22:48:52.454 [00;32mDEBUG nova.compute.utils [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mInsufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    with rt.instance_claim(context, instance, limits):
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    return f(*args, **kwargs)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    overhead=overhead, limits=limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    self._claim_test(resources, limits)
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m    ""; "".join(reasons))
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[00;32m2014-04-06 22:48:52.454 TRACE nova.compute.utils [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00m
2014-04-06 22:48:52.455 [00;32mDEBUG nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [00;32mClean up resource before rescheduling.[00m [00;33mfrom (pid=9041) _reschedule_or_error /opt/stack/nova/nova/compute/manager.py:1401[00m
2014-04-06 22:48:52.455 [01;36mAUDIT nova.compute.manager [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[01;36m] [01;35m[instance: b22186ec-9f05-4f7d-a0d6-2276baeb6572] [01;36mTerminating instance[00m
2014-04-06 22:48:52.544 [00;32mDEBUG nova.network.api [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[00;32mUpdating cache with info: [][00m [00;33mfrom (pid=9041) update_instance_cache_with_nw_info /opt/stack/nova/nova/network/api.py:74[00m
2014-04-06 22:48:52.555 [00;32mDEBUG nova.objects.instance [[01;36mreq-0663cdf1-9969-446a-af08-299f18366394 [00;36mdemo demo[00;32m] [01;35m[00;32mLazy-loading `system_metadata' on Instance uuid b22186ec-9f05-4f7d-a0d6-2276baeb6572[00m [00;33mfrom (pid=9041) obj_load_attr /opt/stack/nova/nova/objects/instance.py:519[00m
2014-04-06 22:48:52.563 [00;32mDEBUG nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mDeallocating network for instance[00m [00;33mfrom (pid=9041) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1784[00m
2014-04-06 22:48:52.593 [01;31mERROR nova.compute.manager [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[01;31m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [01;31mError: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.[00m
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1289, in _build_instance
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    with rt.instance_claim(context, instance, limits):
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 249, in inner
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    return f(*args, **kwargs)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 122, in instance_claim
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    overhead=overhead, limits=limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 95, in __init__
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    self._claim_test(resources, limits)
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/claims.py"", line 148, in _claim_test
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    ""; "".join(reasons))
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mComputeResourcesUnavailable: Insufficient compute resources: Free memory 975.00 MB < requested 2000 MB.
[01;31m2014-04-06 22:48:52.593 TRACE nova.compute.manager [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m
2014-04-06 22:48:52.664 [00;32mDEBUG nova.compute.utils [[01;36mreq-8cf2f302-42af-46e2-b745-fa30902c3319 [00;36mdemo demo[00;32m] [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00;32mThe resource None does not exist[00m [00;33mfrom (pid=9041) notify_about_instance_usage /opt/stack/nova/nova/compute/utils.py:336[00m
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mTraceback (most recent call last):
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1202, in _run_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    instance, image_meta, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1366, in _build_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    filter_properties, bdms, legacy_bdm_in_spec)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1412, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    self._log_original_error(exc_info, instance_uuid)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 1407, in _reschedule_or_error
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    bdms, requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 2136, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    requested_networks)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 68, in __exit__
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    six.reraise(self.type_, self.value, self.tb)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/compute/manager.py"", line 2126, in _shutdown_instance
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    block_device_info)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 656, in destroy
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    _vmops = self._get_vmops_for_compute_node(instance['node'])
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 544, in _get_vmops_for_compute_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    resource = self._get_resource_for_node(nodename)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 536, in _get_resource_for_node
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00m    raise exception.NotFound(msg)
[00;32m2014-04-06 22:48:52.664 TRACE nova.compute.utils [01;35m[instance: 1deeb6c0-ed7f-4f5a-bdcc-97765803d18b] [00mNotFound: The resource None does not exist","VMWare - Check for compute node before triggering destroy

While booting an instance, if Claim on that compute node is not successful
compute manager triggers a call to destroy instance. Destroy fails since the
compute node (Cluster/ESX host) is set only after claim is successful.
Hence the check for node is made before destroy, to ensure that exception
does not get thrown. This ensures that the instance gets rescheduled
Closes-Bug: #1307408

Change-Id: Iaaf931e9c1e6cf046497e2c64952f92e802ad4be"
1274,aaf5762be5d37cac022dc321b6400b9743a25303,1393589300,,1.0,4,8,2,2,1,0.41381685,True,2.0,1597524.0,37.0,13.0,False,49.0,2321825.0,98.0,4.0,33.0,2738.0,2742.0,33.0,2299.0,2303.0,33.0,2630.0,2634.0,0.004549712,0.352067443,0.352602703,476,893,1284733,nova,aaf5762be5d37cac022dc321b6400b9743a25303,1,1,Bug. But they only deletes 1 line,"Bug #1284733 in OpenStack Compute (nova): ""metadata injected at nova boot does not arrive in /meta.js""","The command:
   nova boot --flavor $FLAV --key_name $KEY --image $IMG --meta foo=bar meta1
should inject a file into `/meta.js` with content `{""foo"":""bar""}`. Currently in devstack this doesn't work.
It looks as if the data is arriving to n-cpu as:
    metadata={u'foo': u'bar'}
But n-cpu is expecting:
    metadata = [{""key"": ""foo"", ""value"": ""bar""}]","Libvirt: Repair metadata injection into guests

The change to use Instance objects when spawning instances
changed the format of the 'metadata' property.  But this wasn't
taken into account in the code which actually injects metadata
into /meta.js in the guest's fs.

This patch ensures that the meta.js file injection expects the
new format.

Closes-bug: #1284733

Change-Id: I37049ac01b65d30d588578e351fcddac457f95a7"
1275,aafe39cd94c7311ae6d02a6b98cfd09d70ae0b56,1411425408,,1.0,4,4,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1347,1815,1372672,nova,aafe39cd94c7311ae6d02a6b98cfd09d70ae0b56,1,1,,"Bug #1372672 in OpenStack Compute (nova): ""VMware: 'NoneType' object has no attribute 'keys' in the driver""","There are a couple of places in the driver where we use the keys() method without checking for None.
I have seen several times the following exception:
2014-09-22 11:45:07.312 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object has no attribute 'keys'
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 198, in run_periodic_tasks
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     task(self, context)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 5909, in update_available_resource
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     nodenames = set(self.driver.get_available_nodes())
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 426, in get_available_nodes
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     self._update_resources()
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 306, in _update_resources
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task     added_nodes = set(self.dict_mors.keys()) - set(self._resource_keys)
2014-09-22 11:45:07.312 TRACE nova.openstack.common.periodic_task AttributeError: 'NoneType' object has no attribute 'keys'","VMware: get_all_cluster_refs_by_name default to {}

Currently get_all_cluster_refs_by_name returns None when no cluster
is found.
driver.py is using the keys() method without checking whether the
object returned is None. This can potentially end up throwing an
AttributeError.

This patch fixes the issue by returning an empty dictionary when no
cluster is found.

Change-Id: I3293c916d79aaa4de8913e8ab9c609c9c0bb023e
Closes-Bug: #1372672"
1276,ab55af8ed5afd0765a23a85d608d4b6d35bdd166,1378957030,,1.0,4,3,2,2,1,0.591672779,True,1.0,1098746.0,6.0,2.0,False,81.0,1414670.0,269.0,2.0,0.0,2138.0,2138.0,0.0,1915.0,1915.0,0.0,2007.0,2007.0,0.000166306,0.333943123,0.333943123,1546,1224251,1224251,nova,ab55af8ed5afd0765a23a85d608d4b6d35bdd166,1,0,“Modify this behavior to match EC2” software evolution,"Bug #1224251 in OpenStack Compute (nova): ""disassociate_address on unassociated address does not return proper EC2 response""","disassociate_address does not return the appropriate EC2 response when disassociating an unassociated address. Despite this seeming failure, EC2 will respond success.
EC2 will return as such:
<DisassociateAddressResponse xmlns=""http://EC2.amazonaws.com/doc/2012-08-15/"">
    <requestId>aabbccdd-0146-4952-bdbe-710e4fee8387</requestId>
    <return>true</return>
</DisassociateAddressResponse>
yet, when the EC2 api encounters this scenario, it responds with a 400:
ERROR:boto:400 Bad Request
ERROR:boto:<?xml version=""1.0""?>
<Response><Errors><Error><Code>InvalidInstanceID.NotFound</Code><Message>Instance None could not be found.</Message></Error></Errors><RequestID>req-7a9b4d03-b9f5-4243-95d8-f1ccab180631</RequestID></Response>
The EC2 api should evaluate whether it receives an instance_id from:
instance_id = self.network_api.get_instance_id_by_floating_address(context, public_ip)
and, if so, then continue attempting to disassociate. Else, bail out early and return successfully.","disassociate_address response should match ec2

The current behavior of disassociate_address does not match that of
what EC2 would return when calling it upon an unassociated address.
In EC2 when disassociate_address is called on an unassociated
address, it returns successfully as such:

<DisassociateAddressResponse
    xmlns=""http://EC2.amazonaws.com/doc/2012-08-15/"">
    <requestId>aabbccdd-0146-4952-bdbe-710e4fee8387</requestId>
    <return>true</return>
</DisassociateAddressResponse>

However, when this same called is made against the OpenStack EC2
implementation, this would return a 400:

<Response><Errors><Error><Code>InvalidInstanceID.NotFound</Code>
<Message>Instance None could not be found.</Message></Error></Errors>
<RequestID>req-7a9b4d03-b9f5-4243-95d8-f1ccab180631</RequestID>
</Response>

This is due to the fact that we continue to attempt disassociation
despite instance_id == None.

Modify this behavior to match EC2 by simply checking that instance_id
is set before continuing disassociation. Else, bail early, and in
either case report success.

This change also modifies unit tests to account for this change.

Change-Id: I17c822d70918fefa55da2ab096af5b98b87e23e7
Closes-Bug: #1224251"
1277,ab7ea6baf7e67ee6353257f112e315369dd8d36e,1385481309,0.0,1.0,89,3,2,2,1,0.258018669,True,10.0,745964.0,49.0,12.0,False,53.0,531299.5,170.0,2.0,0.0,347.0,347.0,0.0,259.0,259.0,0.0,223.0,223.0,0.00097371,0.218111003,0.218111003,1804,1254521,1254521,glance,ab7ea6baf7e67ee6353257f112e315369dd8d36e,0,0,Feature3 “Set upload_image policy to control data upload”,"Bug #1254521 in Glance: ""Add upload_image policy for glance v1 api""","Currently there exists no policy to control data uploads.
https://bugs.launchpad.net/glance/+bug/1250918 is for adding upload_image policy in glance v2 api, this bug is for adding it to the glance v1 api.","Set upload_image policy to control data upload

There was no policy to control data upload.
Up until today, the add_image policy was a all or nothing, from metadata
to the actual data. Now, with the combination of add_image and upload_image
an administrator will have finer control over the whole chain.

Change-Id: I1a7966ffb5c63dd8239a54fe2963b031d9fe1f9a
Closes-bug: #1254521"
1278,ab97446132da9b0aff4d13ac01591f86aa066d48,1388584747,,1.0,0,1,1,1,1,0.0,True,1.0,49426.0,13.0,2.0,False,30.0,638523.0,67.0,2.0,10.0,1054.0,1064.0,9.0,927.0,937.0,1.0,20.0,21.0,0.007434944,0.078066914,0.081784387,238,642,1265267,neutron,ab97446132da9b0aff4d13ac01591f86aa066d48,0,0,unused import,"Bug #1265267 in neutron: ""Remove unused imports""","************* Module neutron.plugins.plumgrid.plumgrid_plugin.plumgrid_plugin
W: 33, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.metaplugin.meta_neutron_plugin
W: 30, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.db.api
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nec.nec_plugin
W: 36, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nec.common.config
W: 21, 0: Unused import rpc (unused-import)
--
************* Module neutron.plugins.ryu.ryu_neutron_plugin
W: 43, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ryu.agent.ryu_neutron_agent
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.plugin
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.midonet.agent.midonet_driver
W: 24, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.db.mlnx_db_v2
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.agent.eswitch_neutron_agent
W: 39, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.common.comm_utils
W: 23, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.mlnx.mlnx_plugin
W: 37, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.hyperv.hyperv_neutron_plugin
W: 29, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.nicira.check_nvp_config
W: 28, 0: Unused import nvp_cfg (unused-import)
************* Module neutron.plugins.nicira.NeutronServicePlugin
W: 32, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.nicira.vshield.vcns_driver
W: 22, 0: Unused import nicira_cfg (unused-import)
--
************* Module neutron.plugins.nicira.NeutronPlugin
W: 48, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.plugin
W: 31, 0: Unused import quota_db (unused-import)
W: 46, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.mech_arista.mechanism_arista
W: 26, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.l2pop.mech_driver
W: 27, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.ml2.drivers.cisco.network_db_v2
W: 24, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.plugins.linuxbridge.db.l2network_db_v2
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.linuxbridge.lb_neutron_plugin
W: 40, 0: Unused import quota_db (unused-import)
--
************* Module neutron.plugins.linuxbridge.agent.linuxbridge_neutron_agent
W: 50, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.agent.ovs_neutron_agent
W: 51, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.openvswitch.ovs_neutron_plugin
W: 49, 0: Unused import quota_db (unused-import)
W: 62, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.embrane.base_plugin
W: 33, 0: Unused import config (unused-import)
--
************* Module neutron.plugins.cisco.db.network_db_v2
W: 29, 0: Unused import nexus_models_v2 (unused-import)
--
************* Module neutron.tests.unit.nec.test_db
W: 25, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.nec.test_security_group
W: 25, 0: Unused import ndb (unused-import)
--
************* Module neutron.tests.unit.nec.test_ofc_manager
W: 24, 0: Unused import nmodels (unused-import)
--
************* Module neutron.tests.unit.ryu.test_defaults
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_plugin
W: 19, 0: Unused import ryu_models_v2 (unused-import)
--
************* Module neutron.tests.unit.ryu.test_ryu_db
W: 24, 0: Unused import ryu_models_v2 (unused-import)
W: 22, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_defaults
W: 19, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.mlnx.test_mlnx_comm_utils
W: 20, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.nicira.test_nvplib
W: 25, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.openvswitch.test_ovs_defaults
W: 18, 0: Unused import config (unused-import)
--
************* Module neutron.tests.unit.embrane.test_embrane_l3_plugin
W: 26, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_defaults
W: 25, 0: Unused import config (unused-import)
************* Module neutron.tests.unit.embrane.test_embrane_neutron_plugin
W: 26, 0: Unused import config (unused-import)
--
************* Module install_venv
W: 29, 0: Unused import subprocess (unused-import)","Remove unused imports

Remove unused imports in tools/install_venv.py

Change-Id: Ia2fbb4ece5c8bcdee96503f0f853f826ae4f3dfa
Closes-Bug: #1265267"
1279,abca726e405fec960b546319ea81295b0c6adb0c,1392640509,,1.0,10,1,3,3,1,0.782775953,True,4.0,285608.0,92.0,10.0,False,10.0,1230667.0,13.0,6.0,230.0,865.0,953.0,229.0,736.0,824.0,169.0,428.0,482.0,0.208333333,0.525735294,0.591911765,422,837,1281083,neutron,abca726e405fec960b546319ea81295b0c6adb0c,1,1,bad validation code,"Bug #1281083 in neutron: ""Firewall policy update should validate rules as list of uuids""","Firewall policy update should validate rules as list of uuids, otherwise malformed request will result in ""500 Internal server error""  returned to the client.","Validate rule uuids provided for update_policy

Add corresponding validation method to fwaas extension

Change-Id: I643c10a996813d251684d3b5de04c8826729129f
Closes-Bug: #1281083"
1280,abe30e973442271b0093eaa61c59d171a68a2028,1412711141,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1386,1856,1378525,neutron,abe30e973442271b0093eaa61c59d171a68a2028,0,0,"No bug, it does not have a BFC. “This is deceiving and confusing     and should be blocked until the migration itself is fixed     in a future patch.”","Bug #1378525 in neutron: ""Broken L3 HA migration should be blocked""","While the HA property is update-able, and resulting router-get
invocations suggest that the router is HA, the migration
itself fails on the agent. This is deceiving and confusing
and should be blocked until the migration itself is fixed
in a future patch.","Forbid update of HA property of routers

While the HA property is update-able, and resulting router-get
invocations suggest that the router is HA, the migration
itself fails on the agent. This is deceiving and confusing
and should be blocked until the migration itself is fixed
in a future patch.

Change-Id: I4171ab481e3943e0110bd9a300d965bbebe44871
Related-Bug: #1365426
Closes-Bug: #1378525"
1281,abf2f328e87195e87a03ccdef11d3bd787aec891,1378314119,2.0,1.0,54,41,5,4,1,0.808769579,True,3.0,1223929.0,19.0,6.0,False,4.0,1690764.2,4.0,2.0,1.0,726.0,726.0,1.0,687.0,687.0,1.0,228.0,228.0,0.007194245,0.823741007,0.823741007,1519,1221336,1221336,neutron,abf2f328e87195e87a03ccdef11d3bd787aec891,1,1,“Last minute modification introduced typo”,"Bug #1221336 in neutron: ""Midonet plugin, FIP and MD server connectivity problems""","While doing more thorough integration tests, we discovered two problems:
1. Floating IP is not working correctly. When the packet for a floating ip reaches the router, the router drops it due to a security rule wrongly set.
2. Last minute modification introduced typo in the code that made MD server unreachable","Do not apply security groups to logical ports

Security groups rules were applied to logical ports and this caused
Floating IP not to work correctly.
Correct some typo.

Change-Id: I174e60f8eb8a4d00b71fffc127e2d2d36836835d
Closes-Bug: #1221336"
1282,aca0fa8354670d2d7b5ae0a65a539db4cf455995,1394141780,,1.0,19,16,1,1,1,0.0,True,1.0,56110.0,7.0,3.0,False,2.0,431982.0,3.0,3.0,0.0,1377.0,1377.0,0.0,1141.0,1141.0,0.0,1222.0,1222.0,0.000664894,0.813164894,0.813164894,565,986,1289382,cinder,aca0fa8354670d2d7b5ae0a65a539db4cf455995,1,1,Bad log raise exception,"Bug #1289382 in Cinder: ""GPFS driver Log statement creates exception if hit""","String formatting template Issues
Name collision on the _ variable","gpfs driver: fix logging problems

Fixed the string formatting template issues
Fixed the name collision on the _ variable by changing the variable name to err

Closes-Bug: #1289382

Change-Id: Id690c2b6dcd5e9732330e6b98be34819586b78fc"
1283,acae91475775a8c85598b1bfdc4910e5fe81ced9,1396735852,,1.0,1,1,1,1,1,0.0,True,1.0,116483.0,19.0,5.0,False,2.0,4566922.0,2.0,6.0,1278.0,1726.0,1773.0,1101.0,1454.0,1500.0,792.0,999.0,1024.0,0.722222222,0.910746812,0.933515483,748,1175,1303179,neutron,acae91475775a8c85598b1bfdc4910e5fe81ced9,1,1,,"Bug #1303179 in neutron: ""Missing comma in nsx router mappings migration""","Found during review https://review.openstack.org/40296
There is a comma missing in the migration_for_plugins list
https://github.com/openstack/neutron/blob/master/neutron/db/migration/alembic_migrations/versions/4ca36cfc898c_nsx_router_mappings.py#L30","Add missing comma in nsx router mappings migration

Change-Id: I85bcc9b7fe636f34dbdf6f8c3172352c8e586e2a
Closes-bug: #1303179
Related-bug: #1207402"
1284,accd56fea86a0cd5f1706cbdd5b1bbc31c4cf479,1401259792,1.0,1.0,230,16,4,3,1,0.593535475,False,,,,,True,,,,,,,,,,,,,,,,,928,1364,1323867,neutron,accd56fea86a0cd5f1706cbdd5b1bbc31c4cf479,0,0,Feature “Add new behavior to control update and delete operations “,"Bug #1323867 in neutron: ""Control update and delete operations for cisco-network-profile""","Add new behavior to control update and delete operations for the cisco-network-profile resource extension.
The new behavior is to allow update and delete operations only if there are no neutron networks associated with a particular cisco-network-profile instance.","Control update, delete for cisco-network-profile

Add new behavior to control update and delete operations for the
cisco-network-profile resource extension.
The new behavior is to allow update and delete operations only if
there are no neutron networks associated with a particular
cisco-network-profile instance

Change-Id: I000a03c1ffd5c02e7b827ec757287633e5a5dd64
Closes-Bug: #1323867"
1285,ace3a01ebf980c166b4dfe6d1216b17bf0aec2ed,1409292326,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1246,1705,1362480,neutron,ace3a01ebf980c166b4dfe6d1216b17bf0aec2ed,1,1,,"Bug #1362480 in neutron: ""Datacenter moid should be a value not a tuple""","In edge_appliance_driver.py, there is a comma added when setting the datacenter moid, so the result is the value datacenter moid is changed to the tuple type, that is wrong.
 if datacenter_moid:
edge['datacenterMoid'] = datacenter_moid,  ===> Should remove the ','
return edge","Datacenter moid should not be tuple

Remove the comma, because it will convert the string to tuple
Closes-bug: #1362480

Change-Id: I2debb76bae26fe3d437a431f2946f66a1cbe6851"
1286,acf0e109f3b01e4918d49305a6d95b3732a10b4e,1395145915,,1.0,2,2,1,1,1,0.0,True,1.0,232594.0,34.0,7.0,False,1.0,8457677.0,1.0,6.0,243.0,1608.0,1638.0,238.0,1332.0,1362.0,200.0,854.0,870.0,0.202416918,0.86102719,0.87713998,642,1068,1294096,neutron,acf0e109f3b01e4918d49305a6d95b3732a10b4e,0,0,some cleanup,"Bug #1294096 in neutron: ""LBaaS: small cleanup in agent device driver interface""","Need to remove unnecessary 'context' parameter from a couple of methods in
neutron.services.loadbalancer.agent.agent_device_driver.AgentDeviceDriver","LBaaS: small cleanup in agent device driver interface

remove unnecessary 'context' parameter

Change-Id: Iaa3896f89760817af9d72e653252830f8a33e390
Closes-Bug: #1294096"
1287,acf881a69b672936fec83900fc803c65c669f2a9,1408668410,1.0,1.0,53,17,3,2,1,0.994712282,False,,,,,True,,,,,,,,,,,,,,,,,1222,1681,1360022,nova,acf881a69b672936fec83900fc803c65c669f2a9,1,1,,"Bug #1360022 in OpenStack Compute (nova): ""min_ram and min_disk is ignored when boot from volume""","When boot from volume and the volume is created from a image,  the original image's min_ram, min_disk attributes are ignored, this is not good.
The reason of this failure is because the _check_requested_image() in compute/api.py ignore if the source if a volume.","Check min_ram and min_disk when boot from volume

Currently when boot from volume and the volume is created
from a image, the original image's min_ram, min_disk attributes
are ignored, this is not good.

We should extract these properties from the volume and check
it when boot from volume.

Change-Id: I861a78b5c7efa71e4bf7206d388b8d0d8048c78e
Closes-Bug: #1360022"
1288,acf9fddcf3129ffab9e01aeb1e9991830013bfbd,1391616018,,1.0,15,1,2,2,1,0.69621226,True,5.0,1738517.0,41.0,9.0,False,52.0,394380.0,128.0,3.0,1713.0,1849.0,3205.0,1459.0,1603.0,2708.0,834.0,1787.0,2278.0,0.114902986,0.246043759,0.313609467,369,781,1276644,nova,acf9fddcf3129ffab9e01aeb1e9991830013bfbd,1,1,Bug not defining a variable in python,"Bug #1276644 in OpenStack Compute (nova): ""VMware: exception when deleting a VM that has no datastore""","The following appears in the log files when this happens:
[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi:vmops:destroy, got this exception while deleting the VM contents from the disk: local variable 'datastore_name' referenced before assignment","VMware: ensure that datastore name exists prior to deleting disk

Treats cases when there is no datastore defined on the VM. This
can result from a exception in the spawn method prior to setting
the datastore for the VM.

Closes-bug: #1276644

Change-Id: I808e1cdaace344da021853d0ef224cadee5a8e05"
1289,acfcb523b15fbd9ccc509e4366e4a141a66d4783,1411926960,,1.0,4,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1357,1826,1374044,neutron,acfcb523b15fbd9ccc509e4366e4a141a66d4783,1,1,,"Bug #1374044 in neutron: ""_make_subnet_dict lazy loads not required attibutes causing high number of not required sql query""","The get_active_networks_info rpc call causes high number of sql query.
For example the following query
SELECT subnetroutes.destination AS subnetroutes_destination, subnetroutes.nexthop AS subnetroutes_nexthop, subnetroutes.subnet_id AS subnetroutes_subnet_id
FROM subnetroutes
WHERE %s = subnetroutes.subnet_id
was used on this trace:
  File ""/usr/lib/python2.7/site-packages/eventlet/greenpool.py"", line 82, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
    neutron_ctxt, version, method, namespace, **kwargs)
  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 92, in get_active_networks_info
    networks = self._get_active_networks(context, **kwargs)
  File ""/opt/stack/new/neutron/neutron/db/dhcp_rpc_base.py"", line 42, in _get_active_networks
    plugin.auto_schedule_networks(context, host)
  File ""/opt/stack/new/neutron/neutron/db/agentschedulers_db.py"", line 211, in auto_schedule_networks
    self.network_scheduler.auto_schedule_networks(self, context, host)
  File ""/opt/stack/new/neutron/neutron/scheduler/dhcp_agent_scheduler.py"", line 114, in auto_schedule_networks
    subnets = plugin.get_subnets(context, fields=fields)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 1333, in get_subnets
    page_reverse=page_reverse)
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 209, in _get_collection
    items = [dict_func(c, fields) for c in query]
  File ""/opt/stack/new/neutron/neutron/db/db_base_plugin_v2.py"", line 931, in _make_subnet_dict
    for route in subnet['routes']],
  File ""/opt/stack/new/neutron/neutron/openstack/common/db/sqlalchemy/models.py"", line 57, in __getitem__
    return getattr(self, key)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 237, in __get__
    return self.impl.get(instance_state(instance), dict_)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/attributes.py"", line 590, in get
    value = self.callable_(state, passive)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 529, in _load_for_state
    return self._emit_lazyload(session, state, ident_key, passive)
  File ""<string>"", line 1, in <lambda>
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/strategies.py"", line 598, in _emit_lazyload
    result = q.all()
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2363, in all
    return list(self)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2480, in __iter__
    return self._execute_and_instances(context)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/orm/query.py"", line 2495, in _execute_and_instances
    result = conn.execute(querycontext.statement, self._params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 730, in execute
    return meth(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/sql/elements.py"", line 322, in _execute_on_connection
    return connection._execute_clauseelement(self, multiparams, params)
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 827, in _execute_clauseelement
    compiled_sql, distilled_params
  File ""build/bdist.linux-x86_64/egg/sqlalchemy/engine/base.py"", line 913, in _execute_context
    tb = str(traceback.format_stack())
1. dhcp_agent_scheduler.py explicitly specifies he is interested only in two fields ['network_id', 'enable_dhcp']
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/scheduler/dhcp_agent_scheduler.py#L103
2. _get_collection makes lazy query (not fetching every related data)
3.  make_subnet_dict  forces the ORM to post load not required filed
https://github.com/openstack/neutron/blob/ee4f94e32c8422eb5785f0bb1eb39b94b4e9b064/neutron/db/db_base_plugin_v2.py#L830
4. The self._fields  drops the lazy fetched fields.
So the Db api provides the interface for efficient selective DB load, but at the and it does one of the most inefficient thing is doable on listing. Issues new SQL SELECt statements  per row/object.
Looks like all function does similar thing which uses the self._field method.
The make dict methods MUST NOT try to fetch the not requested fields from the data object at all.
In this case the post _filter method also a not needed.","Refactor _make_subnet_dict to avoid issuing unnecessary queries

Use joined loads for attributes dns_nameservers and host_routes.

As a result, particular scenarios like restarting DHCP agent
could benefit from improved server-side performance.

Change-Id: I6470356b601e2fcf74c7e0a6df438cef7099e9fe
Closes-Bug: #1374044"
1290,adeca091242eed0b691b88f6bcaac8c8044120f6,1403644056,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1068,1517,1342274,glance,adeca091242eed0b691b88f6bcaac8c8044120f6,0,0,Deprecated,"Bug #1342274 in Glance: ""auth_token middleware in keystoneclient is deprecated""",The auth_token middleware in keystoneclient is deprecated and will only get security updates. Projects should use the auth_token middleware in keystonemiddleware.,"Use auth_token from keystonemiddleware

The auth_token middleware in python-keystoneclient is now
deprecated and has been moved to keystonemiddleware.

Closes-Bug: #1342274

Change-Id: Ic8ba2d2f470ec052ba13c3b63b26d5e2270f7412"
1291,adeeabf313a15162c45d4e6ba6cf9596f318f3ed,1394420809,,1.0,44,10,5,5,1,0.854029522,True,13.0,2794881.0,153.0,48.0,False,183.0,914338.8,942.0,6.0,0.0,3768.0,3768.0,0.0,3073.0,3073.0,0.0,2802.0,2802.0,0.000132521,0.371455076,0.371455076,553,974,1289079,nova,adeeabf313a15162c45d4e6ba6cf9596f318f3ed,1,1,There is a bug. Add exception,"Bug #1289079 in OpenStack Compute (nova): ""using ""nova diagnostics"" with a not running instance, return a incorrect message""","Reproduce:
1. nova stop cirros
2.nova diagnostics cirros
[root@control-compute00 ~(keystone_admin)]# nova diagnostics cirros
ERROR: 'NoneType' object has no attribute 'iteritems'","Add exception handling in ""nova diagnostics""

when using ""nova diagnostics"" with a not running
instance, user will get an incorrect message. This
change handles that exception.

Change-Id: Ia2aa76a5c2c2aeed446411700d74d547dd73cf90
Closes-bug: #1289079"
1292,ae744b4b60169f653d6bafba237593388266cb90,1405212763,,1.0,44,33,2,2,1,0.520334616,False,,,,,True,,,,,,,,,,,,,,,,,1060,1506,1340885,nova,ae744b4b60169f653d6bafba237593388266cb90,1,1,,"Bug #1340885 in OpenStack Compute (nova): ""Can't unset a flavor-key""","I am able to set a flavor-key but not unset it.  devstack sha1=fdf1cffbd5d2a7b47d5bdadbc0755fcb2ff6d52f
ubuntu@d8:~/devstack$ nova help flavor-key
usage: nova flavor-key <flavor> <action> <key=value> [<key=value> ...]
Set or unset extra_spec for a flavor.
Positional arguments:
  <flavor>     Name or ID of flavor
  <action>     Actions: 'set' or 'unset'
  <key=value>  Extra_specs to set/unset (only key is necessary on unset)
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny set foo=bar
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property                   | Value          |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled   | False          |
| OS-FLV-EXT-DATA:ephemeral  | 0              |
| disk                       | 1              |
| extra_specs                | {""foo"": ""bar""} |
| id                         | 1              |
| name                       | m1.tiny        |
| os-flavor-access:is_public | True           |
| ram                        | 512            |
| rxtx_factor                | 1.0            |
| swap                       |                |
| vcpus                      | 1              |
+----------------------------+----------------+
ubuntu@d8:~/devstack$ nova flavor-key m1.tiny unset foo
ubuntu@d8:~/devstack$ nova flavor-show m1.tiny
+----------------------------+----------------+
| Property                   | Value          |
+----------------------------+----------------+
| OS-FLV-DISABLED:disabled   | False          |
| OS-FLV-EXT-DATA:ephemeral  | 0              |
| disk                       | 1              |
| extra_specs                | {""foo"": ""bar""} |
| id                         | 1              |
| name                       | m1.tiny        |
| os-flavor-access:is_public | True           |
| ram                        | 512            |
| rxtx_factor                | 1.0            |
| swap                       |                |
| vcpus                      | 1              |
+----------------------------+----------------+","Fix unset extra_spec for a flavor

A flavor extra_specs key cannot be deleted using 'nova flavor-key'.
Updates to a flavor's extra_specs are not being registered by
NovaObject, such that when NovaObject.obj_what_changed is called,
no changes in the extra_specs field are picked up. In order for
changes in extra_specs to be picked up, the updated extra_specs
has to be compared against the _orig_extra_specs. The same approach
also applies to the flavor's projects, where the updated projects
has to be compared against the _orig_projects.

Also, if you delete the last remaining extra_specs key, nova would
throw an sqlalchemy warning, i.e. 'SAWarning: The IN-predicate on
""instance_type_extra_specs.key"" was invoked with an empty sequence'.
This was caused by calling db.flavor_extra_specs_update_or_create
with an empty dict of keys to add or update. This patch fixes it
calling db.flavor_extra_specs_update_or_create only on a non-empty
dict of keys.

Change-Id: Icea492134239296f1570b2a4208eb0868315aa5f
Closes-Bug: #1340885"
1293,aee5344db7972e2e12ed056c2e6467f7952aec3f,1407859910,,1.0,25,3,6,3,1,0.890686758,False,,,,,True,,,,,,,,,,,,,,,,,1175,1632,1355409,neutron,aee5344db7972e2e12ed056c2e6467f7952aec3f,1,1,,"Bug #1355409 in neutron: ""Key error in l3_dvr_db""","The following stack trace observed in the gate:
ERROR oslo.messaging.rpc.dispatcher [req-110db567-3322-4922-95c8-a54d166c8ead ] Exception during message handling: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'
TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
TRACE oslo.messaging.rpc.dispatcher     incoming.message))
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_rpc_base.py"", line 57, in sync_routers
TRACE oslo.messaging.rpc.dispatcher     context, host, router_ids)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_agentschedulers_db.py"", line 191, in list_active_sync_routers_on_active_l3_agent
TRACE oslo.messaging.rpc.dispatcher     active=True)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 306, in get_sync_data
TRACE oslo.messaging.rpc.dispatcher     DEVICE_OWNER_DVR_INTERFACE])
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 1016, in _get_router_info_list
TRACE oslo.messaging.rpc.dispatcher     active=active)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_db.py"", line 919, in _get_sync_routers
TRACE oslo.messaging.rpc.dispatcher     return self._build_routers_list(context, router_dicts, gw_ports)
TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/new/neutron/neutron/db/l3_dvr_db.py"", line 241, in _build_routers_list
TRACE oslo.messaging.rpc.dispatcher     rtr['gw_port'] = gw_ports[gw_port_id]
TRACE oslo.messaging.rpc.dispatcher KeyError: u'b132e9da-3ee2-47ac-9ed5-f04ca73c01d1'a
Link: http://logs.openstack.org/48/112948/3/check/check-tempest-dsvm-neutron-pg/503d619/logs/screen-q-svc.txt.gz?#_2014-08-11_07_19_33_893","Fix KeyError during sync_routers

Method sync_routers is used by the L3 agent to query
routers it knows about. Routers and GW ports lists
are populated in two different times, which means that
they can be interleaved by a delete request which
results in gateway ports being missing in one of the
two data structures.

This patch takes care of the race condition.

Closes-bug: #1355409

Change-Id: Id3a6fe145058f690e107bfe7023980ede61cff90"
1294,aee892e7af2dbe5a600945f98be8f1915e8ac654,1397239651,,1.0,20,2,2,2,1,0.684038436,True,2.0,2189257.0,27.0,12.0,False,17.0,4015555.0,22.0,6.0,676.0,3342.0,3863.0,388.0,2677.0,2952.0,481.0,2089.0,2467.0,0.061945765,0.268603007,0.317182881,783,1210,1306727,nova,aee892e7af2dbe5a600945f98be8f1915e8ac654,1,1,,"Bug #1306727 in OpenStack Compute (nova): ""versions controller requests with a body log ERRORs""","Using Nova trunk (Juno). I'm seeing the following nova-api.log errors when unauthenticated /versions controller POST requests are made with a request body:
-----
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 ERROR nova.api.openstack.wsgi [-] Exception handling resource: index() got an unexpected keyword argument 'body'
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi Traceback (most recent call last):
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 983, in _process_stack
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi     action_result = self.dispatch(meth, request, action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi   File ""/opt/stack/venvs/nova/lib/python2.7/site-packages/nova/api/openstack/wsgi.py"", line 1070, in dispatch
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi     return method(req=request, **action_args)
Apr 11 07:04:06 overcloud-controller0-n2g3h54d6w6u nova-api[27022]: 2014-04-11 07:04:06.235 27044 TRACE nova.api.openstack.wsgi TypeError: index() got an unexpected keyword argument 'body'
-----
Both the index() and multi() actions in the versions controller are susceptible to this behavior. Ideally we wouldn't be logging stack traces when this happens.","versions API: ignore request with a body

Update the OS API versions controller so it ignores requests
with a body. Previously an incoming (unauthenticated)
request to the versions API would log a TypeError trace
in the nova-api.log file with the following message:

TypeError: index() got an unexpected keyword argument 'body'

Change-Id: Icc3ccfc537b13627b8d5ba43ae3ba91e34e08c9e
Closes-bug: #1306727"
1295,aeea31b21245864888f4744c19a0fed2a9c6567e,1396044627,,1.0,2,2,1,1,1,0.0,True,2.0,2702425.0,38.0,8.0,False,25.0,412737.0,34.0,6.0,3.0,1477.0,1479.0,3.0,1239.0,1241.0,1.0,838.0,838.0,0.001908397,0.800572519,0.800572519,465,880,1284265,neutron,aeea31b21245864888f4744c19a0fed2a9c6567e,0,0,tests,"Bug #1284265 in neutron: ""ovs-agent test_fdb_add_flows test invalid""","https://github.com/openstack/neutron/blob/14cb886809e5cccbf799a0dc2e5b99f31b1ab3be/neutron/tests/unit/openvswitch/test_ovs_neutron_agent.py#L577
has:
mock.patch.object(self.agent.tun_br, 'setup_tunnel_port')
but setup_tunnel_port is an attribute of self.agent and not self.agent.tun_br.","Invalid ovs-agent test case - test_fdb_add_flows

setup_tunnel_port is an attribute of self.agent and not self.agent.tun_br.
This patch fixes the testcase accordingly.

Closes-Bug: #1284265
Change-Id: I70582e477bb53de6482e32d5913c0864c5d910cc"
1296,af0c9ea3b8e8f5cc4fd523ff336887ae94bb6ef0,1395375228,,1.0,4,1,1,1,1,0.0,True,6.0,78237.0,90.0,9.0,False,2.0,1477888.0,2.0,3.0,751.0,1801.0,2226.0,641.0,1342.0,1713.0,262.0,763.0,856.0,0.259368836,0.753451677,0.845167653,663,1089,1295491,neutron,af0c9ea3b8e8f5cc4fd523ff336887ae94bb6ef0,0,0,add more info to the error msg,"Bug #1295491 in neutron: ""expected active pool error raised in lbaas agent""","2014-03-20 14:51:23.246 29247 ERROR neutron.openstack.common.rpc.amqp [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Exception during message handling
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 462, in _process_data
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     **args)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/common/agent_driver_base.py"", line 99, in get_logical_device
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp     raise n_exc.Invalid(_('Expected active pool'))
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp Invalid: Expected active pool
2014-03-20 14:51:23.246 29247 TRACE neutron.openstack.common.rpc.amqp
2014-03-20 14:51:23.247 29247 ERROR neutron.openstack.common.rpc.common [req-7805af6e-ec03-4f79-92c0-396144d6fc58 None] Returning exception Expected active pool to caller","Log received pool.status

This patch adds a log statement so we can figure out what status the
lbaas-agent is passing in. This way we can figure out what to prevent.

Change-Id: I1a0f809140127ccb9596d83c97add03a853991a3
Closes-bug: #1295491"
1297,af2607c457171975fe4f512a92a87b827d5b6528,1367358346,2.0,,262,176,6,4,2,0.56297055,True,5.0,1131698.0,21.0,7.0,False,28.0,528451.0,84.0,2.0,122.0,267.0,314.0,94.0,221.0,262.0,120.0,257.0,304.0,0.267108168,0.569536424,0.673289183,1788,1253478,1253478,Swift,af2607c457171975fe4f512a92a87b827d5b6528,1,1,"With the new early majority code, Este escenario no lo tenian en cuenta Asia es BIC o no3 “that's the client problem- also it already existed,”","Bug #1253478 in OpenStack Object Storage (swift): ""bulk delete 409s""","With the new early majority code, doing a bulk delete where you delete all the objects in a container and then try to delete the container at the end will very frequently fail. This is because it is very likely that all 3 objects have not been deleted by the time the middleware got a successful response.  A possible solution is to just sleep for a second and retry if you get a 409. idk
btw- this behavior would also happen for people emptying / deleting containers but I guess that's the client problem- also it already existed, its just now more frequent.
another possible solution is just to not do anything :)","Refactor Bulk middleware to handle long running requests

Change-Id: I8ea0ff86518d453597faae44ec3918298e2d5147"
1298,af44b50b6b8187c559c56b9d3f7dc047fc5be407,1396294011,,1.0,12,12,6,4,1,0.935524532,False,,,,,True,,,,,,,,,,,,,,,,,688,1114,1298131,nova,af44b50b6b8187c559c56b9d3f7dc047fc5be407,1,1,Wrong hhtp code,"Bug #1298131 in OpenStack Compute (nova): ""improper usage of HTTP 413 status code""","HTTP 413 is supposed to mean (per RFC2616) that the request entity was too large. E.g., if you send an enormous body with the request. That is not at all how it is being used in the server resize request example below. The nova/api/openstack/compute/servers.py is coded to return 413 for QuotaError and PortLimitExceeded on create as well as for QuotaError on resize, and there may be other places 413 is being returned inappropriately.
POST /v2/6ce8fae0510349dcbf9b3965f7a20061/servers/8ebaabfc-9018-4ac1-afc6-630aee8a8ae3/action
Request body: {  ""resize"": {
            ""flavor"": {
              ""vcpus"": 1,
              ""ram"": 9999999999,
              ""disk"": 20
          }}}
Response: HTTP 413 (Request Entity Too Large)
Response body:
{
overLimit: {
message: ""Quota exceeded for ram: Requested 1410063359, but already used 6144 of 8000000 ram""
code: 413
retryAfter: ""0""
}
-
}","Correct returned HTTP status code (Use 403 instead of 413)

The exception HTTPRequestEntityTooLarge should not be used as an
exception response based on RFC2616. Because of that, change
the returned response to HTTPForbidden.

Restore commit for Iab090c40c632a76b0528df8145ad0897c8b649bf
And add compute V3 changes.

Related Tempest commit to match this change:
If376eda0a7929ba2baa4ac4acbb457883bcfc96d

DocImpact: corrects HTTP return code from 413 to 403 for quota-related
limit faults

Closes-Bug: #1298131
Change-Id: I2bb8a60ef254afbfed514cfeebe75355d0de4475"
1299,af911f12fe726ae17601e2381455742882ca71e8,1378649383,,1.0,56,6,3,2,1,0.559363613,True,7.0,2780936.0,35.0,11.0,False,28.0,2963931.333,70.0,4.0,45.0,3444.0,3468.0,44.0,2928.0,2951.0,44.0,2416.0,2439.0,0.007545272,0.405264923,0.409121395,1517,1221244,1221244,nova,af911f12fe726ae17601e2381455742882ca71e8,1,1,"“Fix misuse of ""instance"" parameter”","Bug #1221244 in OpenStack Compute (nova): ""baremetal driver misuses ""instance"" parameter of attach/detach_volume()""","These methods that handle block device mapping pass instance['name'] to attach/detach_volume(), but it should be instance itself.
    def _attach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.attach_volume(
                    connection_info, instance['name'], mountpoint)
    def _detach_block_devices(self, instance, block_device_info):
        block_device_mapping = driver.\
                block_device_info_get_mapping(block_device_info)
        for vol in block_device_mapping:
            connection_info = vol['connection_info']
            mountpoint = vol['mount_device']
            self.detach_volume(
                    connection_info, instance['name'], mountpoint)","baremetal: Fix misuse of ""instance"" parameter of attach/detach_volume

Some parts of baremetal driver treat ""instance"" parameter as if it is
the name of an instance, but actually it is the instance itself (dict).

Closes-Bug: #1221244
Change-Id: I01a296a6ac0bbf7c5fec3434ddf1197828b849b6"
1300,afc517e093e171c2b143e91972dea19351f62592,1385402597,,1.0,2,1,2,2,1,0.918295834,True,3.0,164022.0,20.0,5.0,False,47.0,1567354.0,76.0,2.0,171.0,2055.0,2215.0,168.0,1730.0,1888.0,9.0,1953.0,1957.0,0.001495439,0.292208763,0.292806939,76,473,1254820,nova,afc517e093e171c2b143e91972dea19351f62592,1,0, fails on older versions of libvirt ,"Bug #1254820 in OpenStack Compute (nova): ""GlusterFS disk attach fails on older versions of libvirt""","Some versions of libvirt (such as RHEL6's 0.10.2) require a port specification with a GlusterFS network disk, or they reject the disk with an ""XML error: missing port for host"" error.
We can provide a blank value, rather than omitting the field, which allows qemu to default to a reasonable port.","libvirt: Provide a port field for GlusterFS network disks

Some versions of libvirt (RHEL6's 0.10.2) require a port
specification with a GlusterFS network disk, or they reject
the disk with an ""XML error: missing port for host"" error.

Provide port 24007 as part of the disk specification.

Closes-Bug: #1254820

Change-Id: Iedcce32618813ed093b8e9aa8a69d1219b2118a5"
1301,afcbfe053c9165ab84af30c8e663dfaee2a79e81,1412927196,,1.0,97,5,2,2,1,0.139232999,False,,,,,True,,,,,,,,,,,,,,,,,1394,1864,1379654,cinder,afcbfe053c9165ab84af30c8e663dfaee2a79e81,1,1,,"Bug #1379654 in Cinder: ""Storwize_SVC Create volume with replication type associate to qos-spec will always be failed.""","Test step :
1. Create a qos spec :
cinder qos-create qos-spec qos:IOThrottling=30000
2. Create type-1 with Pool-1 without replication.
3. Create type-2 with Primary Pool-1 and Secondary Pool-2, replication =TRUE.
4. qos-spec associate to type-1 and type-2.
cinder qos-associate qos-spec type-1
cinder qos-associate qos-spec type-2
5. cinder create --volume-type type-1 1 ,
The volume creation will be successed.
6. cinder create --volume-type type-2 1.
The volume creation will be failed.
Check the cinder log , it was found:
screen-c-sch.2014-09-11-151843.log:2014-10-10 14:01:41.497 ERROR cinder.scheduler.filter_scheduler [[[01;36mreq-9d20b04b-94fc-4936-a9c8-12a41690e313 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e] ^[[01;35mError scheduling None from last vol-service: ubuntu247@driver4#driver4 : [u'Traceback (most recent call last):\n', u' File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 35, in execute_task\n result = task.execute(**arguments)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 624, in execute\n **volume_spec)\n', u' File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 598, in _create_raw_volume\n return self.driver.create_volume(volume_ref)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init.py"", line 571, in create_volume\n model_update = self.replication.create_replica(ctxt, volume)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 78, in create_replica\n self.driver.add_vdisk_copy(volume[\'name\'], dest_pool, vol_type)\n', u' File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper\n return f(*args, **kwargs)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/init_.py"", line 661, in add_vdisk_copy\n self.configuration)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 858, in add_vdisk_copy\n volume_type=volume_type)\n', u' File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 561, in get_vdisk_params\n kvs = qos_specs.get_qos_specs(ctxt, qos_specs_id)[\'specs\']\n', u""UnboundLocalError: local variable 'ctxt' referenced before assignment\n""]","IBM Storwize driver: Add local variable assignment to ""ctxt""

* The method get_vdisk_params in helpers.py is missing a local variable
assignment for ""ctxt"", causing ""UnboundLocalError: local variable
'ctxt' referenced before assignment. Adding the assignment should
resolve this issue.

* Add the unit tests coverage for get_vdisk_params.

Change-Id: I564b1ef8cd1b6504d5ac8c9af0bb11bf29767d9a
closes-bug: #1379654"
1302,b025ccff2c7320caecfc005cfcbc3f4dfadfa505,1407955147,1.0,1.0,27,13,2,2,1,0.468995594,False,,,,,True,,,,,,,,,,,,,,,,,1185,1642,1356120,neutron,b025ccff2c7320caecfc005cfcbc3f4dfadfa505,1,1,,"Bug #1356120 in neutron: ""PortNotFound in update_device_up for DVR""","An example of a failure has been observed here:
http://logs.openstack.org/80/113580/3/experimental/check-tempest-dsvm-neutron-dvr/a0e0c32/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-13_00_13_00_674
More triaging needed but I suspect this is caused by interleaved create/delete requests of router resources.","Fix PortNotFound error during update_device_up for DVR

An agent's request to update the ARP entry for a VM port
may come after a deletion request has been processed,
resulting in a PortNotFound exception being raised.

This patch takes care of this condition. A test has
been added, which required a minor refactoring of the
test case class, in order to accommodate the use of
side effects for the objects being mocked.

Closes-bug: #1356120

Change-Id: I40d635bcf47c683663cb4dedf20323902dff2c7f"
1303,b030300b1687b853a95b6bd2e706045f549d1e50,1410426561,,1.0,3,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1303,1769,1368251,neutron,b030300b1687b853a95b6bd2e706045f549d1e50,1,0,"Environment. “was defined to be a boolean type and in postgresql this type is enforced,
while in mysql this just maps to tinyin”","Bug #1368251 in neutron: ""migrate_to_ml2 accessing boolean as int fails on postgresql""","The ""allocated"" variable used in migrate_to_ml2 was defined to be a boolean type and in postgresql this type is enforced,
while in mysql this just maps to tinyint and accepts both numbers and bools.
Thus the migrate_to_ml2 script breaks on postgresql","use TRUE in SQL for boolean var

The ""allocated"" variable was defined to be a boolean type
and in postgresql this type is enforced,
while in mysql this just maps to tinyint and accepts both numbers and bools

Closes-Bug: #1368251
Change-Id: If324c8b83e490e150085d044ac61360b647522ac"
1304,b04a1dd063d9678747194e416ed157109a17fdc7,1410880142,,1.0,11,5,4,2,1,0.968139062,False,,,,,True,,,,,,,,,,,,,,,,,1311,1777,1369136,cinder,b04a1dd063d9678747194e416ed157109a17fdc7,1,1,,"Bug #1369136 in Cinder: ""Timeout triggers failures running tempest for ZFSSA iSCSI Cinder driver.""","Running tempest  in a long distance link between the cinder controller and ZFS Storage Appliance, ZFSSA iSCSI Cinder driver timeouts, resulting in tests failures.
e.g
...
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTest.test_volume_create_get_update_delete_from_image [355.316953s] ... FAILED
{1} tempest.api.volume.test_volumes_get.VolumesV2GetTestXML.test_volume_create_get_update_delete_as_clone [389.165116s] ... FAILED
...
Or
{1} tempest.api.volume.test_volumes_get.VolumesV1GetTest.test_volume_create_get_update_delete [507.216176s] ... FAILED
Some of these errors are the results of a volume not been reported as created, or an error uploading an image to a volume.
e.g (from cinder volume service)
2014-09-12 11:15:43.940 ERROR oslo.messaging.rpc.dispatcher [req-1879fa63-44c7-4f22-92fd-b139cce4bae7 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out
2014-09-12 11:15:44.643 ERROR cinder.volume.flows.manager.create_volume [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d26
15e2e47dc 43ab44ae2c7d4606ae30bc3b551cfe25] Volume c74b1a80-0528-41dd-b3d5-275410fa557a: create failed
2014-09-12 11:15:44.648 ERROR oslo.messaging.rpc.dispatcher [req-da06124c-366b-48b7-95ca-96720967438c 85f8e9058af848aa9e10d2615e2e47dc 43
ab44ae2c7d4606ae30bc3b551cfe25] Exception during message handling: timed out","Timeout triggers failures running tempest for ZFSSA driver.

Adding a property to setup the RESTAPI connection timeout with
ZFS Storage Appliance, allow tempest to pass with slower network links.

DocImpact: New config parameter is added to allow configuring timeout value.
Closes-Bug: #1369136

Change-Id: I8de19f56a18106324ed414cf41ee2323de639359"
1305,b07c1a759c8a0cdaaaaaf3e6f1ad7402bf631a44,1401965487,,1.0,42,17,2,2,1,0.95285881,False,,,,,True,,,,,,,,,,,,,,,,,1850,1332482,1332482,Cinder,b07c1a759c8a0cdaaaaaf3e6f1ad7402bf631a44,1,1,"""This change fix those missing parameters""","Bug #1332482 in Cinder: ""VMware: Storage profile is ignored for volume creation from stream-optimized image""","The storage profile in the volume type is ignored while creating a volume from stream-optimized image. Even though the backing is placed in a compliant datastore, the profile is not associated with the backing.","VMware:Fix params for copy-image-to-volume

While creating a volume from stream-optimized image, the adapter type
in the image meta-data and the profile in the volume type extra spec
are ignored. This change fix those missing parameters.

Partial-Bug: #1284284
Closes-Bug: #1332482
Change-Id: I46c4755989fe61e0e654ca6daa3c3af0655071f0"
1306,b09c9950649c4c9fffa0b096eb21152cd84fe56f,1407927710,,1.0,4,20,2,2,1,0.41381685,False,,,,,True,,,,,,,,,,,,,,,,,1197,1655,1357102,neutron,b09c9950649c4c9fffa0b096eb21152cd84fe56f,1,0,"“However,
there is no longer a point to this protection because the DB lock is gone and”","Bug #1357102 in neutron: ""Big Switch: Multiple read calls to consistency DB fails""","The Big Switch consistency DB throws an exception if read_for_update() is called multiple times without closing the transaction in between. This was originally because there was a DB lock in place and a single thread could deadlock if it tried twice. However,
there is no longer a point to this protection because the DB lock is gone and certain response failures result in the DB being read twice (the second time for a retry).
2014-08-14 21:56:41.496 12939 ERROR neutron.plugins.ml2.managers [req-ee311173-b38a-481e-8900-d963c676b05f None] Mechanism driver 'bigswitch' failed in update_port_postcommit
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers Traceback (most recent call last):
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/managers.py"", line 168, in _call_on_drivers
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     getattr(driver.obj, method_name)(context)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/ml2/drivers/mech_bigswitch/driver.py"", line 91, in update_port_postcommit
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     port[""network""][""id""], port)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 555, in rest_update_port
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     self.rest_create_port(tenant_id, net_id, port)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 545, in rest_create_port
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     self.rest_action('PUT', resource, data, errstr)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 476, in rest_action
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     timeout)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     return f(*args, **kwargs)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 423, in rest_call
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     hash_handler=hash_handler)
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/servermanager.py"", line 139, in rest_call
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     headers[HASH_MATCH_HEADER] = hash_handler.read_for_update()
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers   File ""/usr/lib/python2.7/dist-packages/neutron/plugins/bigswitch/db/consistency_db.py"", line 56, in read_for_update
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers     raise MultipleReadForUpdateCalls()
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers MultipleReadForUpdateCalls: Only one read_for_update call may be made at a time.
2014-08-14 21:56:41.496 12939 TRACE neutron.plugins.ml2.managers","BSN: Allow concurrent reads to consistency DB

Allow concurrent reads to the consistency DB since the
database lock the block was originally protecting against
is gone.

Closes-Bug: #1357102
Change-Id: I1618b4046e995c796757e723f99288e486683339"
1307,b104d1c0c70898b59b6668970e05cebadecd064a,1382438270,,1.0,1,1,1,1,1,0.0,True,2.0,13535307.0,57.0,27.0,False,2.0,12330646.0,2.0,11.0,2.0,1428.0,1429.0,2.0,1294.0,1295.0,0.0,1064.0,1064.0,0.000898473,0.956873315,0.956873315,4,316,1242980,cinder,b104d1c0c70898b59b6668970e05cebadecd064a,1,1,,"Bug #1242980 in Cinder: ""volume transfer url is different from its href in detail body""","After creating a volume transfer, I got a detail information of volume transfer like this:
{
    ""transfer"": {
        ""auth_key"": ""cfcb4552f7c1f2df"",
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://192.168.83.241:8776/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""bookmark""
            }
        ],
        ""created_at"": ""2013-10-22T01:18:35.627545"",
        ""volume_id"": ""f636037c-1aa0-4533-aef2-bcbd7ab3896c"",
        ""id"": ""dfedf224-1aba-420a-a387-3b971447a28f"",
        ""name"": null
    }
}
But I cannot describe its detail information with this href of this retrun.
        ""links"": [
            {
                ""href"": ""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/transfers/dfedf224-1aba-420a-a387-3b971447a28f"",
                ""rel"": ""self""
            },
The correct url is:
""http://192.168.83.241:8776/v2/c7d8da28afff47189ff1d651992b593b/os-volume-transfer/dfedf224-1aba-420a-a387-3b971447a28f""
Then I found that the define of collection_name are defferent between /cinder/api/views/transfers.py and /cinder/api/contrib/volume_transfer.py.
So I think this is a bug.","Fix volume transfer href issue

The volume transfer collection in url defined as ""os-volume-tranfer"",
but I got another define ""tranfers"" in href url. They are different.
So it should be modify.

Change-Id: I69a43012f2bb70ce5fc7b0331d2759527a261170
closes-bug: #1242980"
1308,b12da559b6a40fba4e4431d74372b5e350047525,1383132113,0.0,1.0,62,22,2,2,1,0.918295834,True,11.0,7959704.0,103.0,53.0,False,28.0,87895.0,34.0,14.0,1401.0,2523.0,2523.0,1263.0,2256.0,2256.0,566.0,1456.0,1456.0,0.087459509,0.224741632,0.224741632,37,365,1246258,nova,b12da559b6a40fba4e4431d74372b5e350047525,1,1,,"Bug #1246258 in OpenStack Compute (nova): ""UnboundLocalError: local variable 'network_name' referenced before assignment""","The exception occurs when trying to create/delete an instance that is using a network that is not owned by the admin tenant. This prevents the deletion of the instance.
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 90, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/exception.py"", line 73, in wrapped
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 294, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1616, in run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     do_run_instance()
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1615, in do_run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 965, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     notify(""error"", msg=unicode(e))  # notify that build failed
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 949, in _run_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     instance, image_meta, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1078, in _build_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     filter_properties, bdms, legacy_bdm_in_spec)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1122, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     self._log_original_error(exc_info, instance_uuid)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1117, in _reschedule_or_error
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     bdms, requested_networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 1642, in _shutdown_instance
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     network_info = self._get_instance_nw_info(context, instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 879, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     instance)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 455, in get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     result = self._get_instance_nw_info(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 463, in _get_instance_nw_info
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     nw_info = self._build_network_info_model(context, instance, networks)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 1009, in _build_network_info_model
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     subnets)
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/dist-packages/nova/network/neutronv2/api.py"", line 962, in _nw_info_build_network
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp     label=network_name,
2013-10-29 00:51:36.173 18588 TRACE nova.openstack.common.rpc.amqp UnboundLocalError: local variable 'network_name' referenced before assignment","Fix bug for neutron network-name

There may be cases when the network-name is not updated correctly
due to the requested networks not being owned by the tenant.

In the case when there is no matched network we use the details
from the used port.

A failed deletion of a port will raise an exception. If the
port is not found no exception will be raised.

Co-authored-by: Evgeny Fedoruk <EvgenyF@Radware.com>

Change-Id: Ie28e88ec9a9c7a180410ae5e57f84b1f653bbffc
Closes-Bug: #1246258"
1309,b12e38327af83f44cc8e38a23d16bb51a2346f57,1396927523,1.0,1.0,4,1,2,1,1,0.721928095,True,1.0,8204.0,17.0,3.0,False,8.0,2759652.0,15.0,2.0,68.0,1469.0,1491.0,68.0,1218.0,1240.0,59.0,827.0,843.0,0.054054054,0.745945946,0.76036036,753,1180,1303605,neutron,b12e38327af83f44cc8e38a23d16bb51a2346f57,0,0,Bug in test,"Bug #1303605 in neutron: ""test_rollback_on_router_delete fails ""","gate-neutron-python26 failis for test_rollback_on_router_delete with following error:
2014-04-07 03:53:51,643    ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: POST failure for servers: ('localhost', 9000) Response: {'status': 'This server is broken, please try another'}
2014-04-07 03:53:51,643    ERROR [neutron.plugins.bigswitch.servermanager] ServerProxy: Error details: status=500, reason='Internal Server Error', ret={'status': 'This server is broken, please try another'}, data=""{'status': 'This server is broken, please try another'}""
}}}
Traceback (most recent call last):
  File ""neutron/tests/unit/bigswitch/test_router_db.py"", line 536, in test_rollback_on_router_delete
    expected_code=exc.HTTPInternalServerError.code)
  File ""neutron/tests/unit/test_db_plugin.py"", line 450, in _delete
    self.assertEqual(res.status_int, expected_code)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/jenkins/workspace/gate-neutron-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: 204 != 500
full log is here:
http://logs.openstack.org/29/82729/3/check/gate-neutron-python26/a1065eb/testr_results.html.gz","BigSwitch: Stop HTTP patch before overriding

Stops the default HTTPConnection patch used by the
Big Switch tests before patching HTTPConnection with
another substitution. This prevents mock from losing
track of the default patch which was resulting in it
not consistently being stopped by the stopall call in
cleanup.

This also corrects an incorrectly targeted mock for the
HTTP patch in one of the test files.

Closes-Bug: #1303605
Change-Id: Ia5b374c5f8d3a7905d915de4f1f8d4f3a6f0e58d"
1310,b18b2333a98889f6a4a2fd0e859b466116ce4aac,1386195365,,1.0,2,2,1,1,1,0.0,True,4.0,748068.0,43.0,15.0,False,162.0,31184.0,817.0,4.0,582.0,1469.0,1927.0,542.0,1339.0,1758.0,175.0,1227.0,1349.0,0.025943396,0.181014151,0.198997642,142,544,1257940,nova,b18b2333a98889f6a4a2fd0e859b466116ce4aac,0,0,Add debugs,"Bug #1257940 in OpenStack Compute (nova): ""Exceptions in _heal_instance_info_cache should be shown""",If an exception occurs in _heal_instance_info_cache the actual stack trace is not shown and is hard to track down something is failing as it's log level is debug.,"Log exception in _heal_instance_info_cache

Previously, if errors occurred in _heal_instance_info_cache() they would
go unnoticed as their log level was debug and the stace trace was not
displayed. This patch changes that so the exception is logged so periodic
failures here can be more easily detected and fixed.

Change-Id: I15bb3e7e29462e930f6159be9f4bbf4980b1270c
Closes-bug: #1257940"
1311,b1b83e42ae20f510eea452b9c7fb7673374f2a79,1402370924,,1.0,0,6,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,964,1402,1328362,neutron,b1b83e42ae20f510eea452b9c7fb7673374f2a79,0,0,"Refactoring “ extensions: remove 'check_env' method
    The method is not documented or used.”","Bug #1328362 in neutron: ""ext.check_env is not used""",check_env method for extension descriptor is not documented or used.,"extensions: remove 'check_env' method

The method is not documented or used.
It was introduced for quota but unused since commit 603e3b78.

Closes-Bug: #1328362
Change-Id: Id0a4d8606f34e54468ed0de26adbf25ff273079e"
1312,b1b97dd721980bd06a97b9a9ad0fb15d95370164,1407463474,,1.0,64,3,4,3,1,0.734184258,False,,,,,True,,,,,,,,,,,,,,,,,1152,1609,1353112,neutron,b1b97dd721980bd06a97b9a9ad0fb15d95370164,1,1,“extension is missed out and needs a fix.”,"Bug #1353112 in neutron: ""template attribute missing a support for subnet and router in Nuage plugin""",One can instantiate subnet and router based on the template provided to inherit most if not all template properties. This is a very useful feature for all Nuage's current and future customers. extension is missed out and needs a fix.,"Add template attr. for subnet, router create in Nuage plugin

subnet create and router create commands now gives the ability
to the user to create subnets and routers via neutron cli with
additional attributes nuage_subnet_template and nuage_router_template
respectively. The instantiated subnet and router inherits the properties
of the template that was mentioned in the additional parameters.

Change-Id: Ib73313e840cb35ede89df858baa536aab883efa2
Closes-Bug: #1353112"
1313,b1e7d0ad220d869323754683dbb447f6821ef705,1392895357,,1.0,10,9,1,1,1,0.0,True,1.0,46255.0,7.0,3.0,False,28.0,3105498.0,49.0,3.0,602.0,591.0,915.0,575.0,577.0,881.0,585.0,559.0,869.0,0.404416839,0.38647343,0.600414079,322,731,1272365,cinder,b1e7d0ad220d869323754683dbb447f6821ef705,0,0,Just because of pylinter: Ensure return,"Bug #1272365 in Cinder: ""QoS specs update return value""","In db/api.py, qos_specs_update calls ""IMPL.qos_specs_update()"" instead of ""return IMPL.qos_specs_update()"".
This generates a pylint lintstack error:
[""Assigning to function call which doesn't return"", ""res = db.qos_specs_update(context, qos_specs_id, specs)""]
This doesn't look right -- I assume the result should be returned from db.qos_specs_update.","Ensure return for db api functions

Functions in db/api.py should return whatever the implementation of
those functions return. This is important for qos_specs_update which
returns something for the sqlalchemy implementation, but for others as
well for future implementations.

Added 'return' for functions that were missing it.

Change-Id: I384b1cf915b2d2238fd75d23b17558cae2cf9f50
Closes-Bug: #1272365"
1314,b21d3b6402fe10616fa07df3c0fa6154f46f4b21,1407865751,,1.0,17,1,2,2,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,1182,1639,1355922,nova,b21d3b6402fe10616fa07df3c0fa6154f46f4b21,1,1,,"Bug #1355922 in OpenStack Compute (nova): ""instance fault not created when boot process fails""","If the build process makes it to build_and_run_instance in the compute manager no instance faults are recorded for failures after that point.  The instance will be set to an ERROR state appropriately, but no information is stored to return to the user.","Record instance faults during boot process

Once a build makes it to the compute manager build_and_run_instance
method no instance faults were being recorded.  This stores the instance
faults appropriately when an instance is placed into the ERROR state.

Change-Id: I348b539d0b5874bac3f7443e6dd4524264ce9766
Closes-Bug: #1355922"
1315,b22515e443449a83f423620cb1df5b92770bef32,1385483924,0.0,1.0,41,6,2,2,1,0.488908591,True,3.0,7858540.0,54.0,14.0,False,8.0,3258824.0,12.0,7.0,2.0,1104.0,1105.0,1.0,1000.0,1000.0,1.0,498.0,498.0,0.003490401,0.870855148,0.870855148,89,487,1255153,neutron,b22515e443449a83f423620cb1df5b92770bef32,1,1,There is a bug: it takes the whole interface down.,"Bug #1255153 in neutron: ""Linuxbridge plugin doesn't return IP to VLAN interface when deleting a bridge""","Given a network setup like this on the network node (before starting any of the neutron services):
eth0: inet 192.168.100.230/24
eth0.1001@eth0: inet 172.24.4.224/28
Now when the linuxbridge-agent needs to create a VLAN-bridge for the 1001 VLAN it will correctly enslave eth0.1001 into the bridge and move the IP Address from eth0.1001 to the bridge.
But when linuxbridge removes the bridge again (e.g. because the last port on it was deleted) it will not reassign the IP back to the eth0.1001 device, but instead just take the whole interface down.","Reassign IP to vlan interface when deleting a VLAN bridge

When deleting a VLAN bridge that has an IP address assigned to it, don't delete
the VLAN interface, but reassigned the IP address back to the underlying VLAN
interface.

Closes-Bug: #1255153

Change-Id: I5e39877c0786b43eddba9b5e1394d4c2ec023c0a"
1316,b2447503b0ba98e644439d31435d47435c46cd69,1395344684,,1.0,61,11,3,2,1,0.868429791,True,4.0,486581.0,24.0,7.0,False,22.0,927696.0,47.0,2.0,298.0,1401.0,1695.0,190.0,1147.0,1336.0,5.0,1247.0,1250.0,0.003893576,0.809863725,0.811810513,528,949,1288283,cinder,b2447503b0ba98e644439d31435d47435c46cd69,1,1,Fix the incorrect behavior,"Bug #1288283 in Cinder: ""NetApp QOS extra spec is not implemented properly""","The NetApp NFS and iSCSI QOS extra spec for volume types is not implemented correctly.  It currently requires a QOS policy to be applied at the flexVol level.  The scheduler then assigns a new cinder volume to the flexVol which has the QOS policy applied to it.  This results in a situation where multiple cinder volumes are all fighting for the same QOS limits, rather than each getting the implied limit.  For example:
QOS policy of 100 MB/s is applied to a flexVol by the NetApp admin.
5 cinder volumes are created with the 100 MB/s QOS policy applied via volume-type. They are all placed into the flexVol created in the first step.
These 5 cinder volumes are now fighting each other for the 100 MB/s that the flexVol has allocated to it.
The expected behavior is that each cinder volume would independently have their own 100 MB/s limit, not a combined limit.
In order to do this, the QOS policy should be applied at the LUN level for the iSCSI driver.  The NFS driver is another can of worms, as I'm not aware of a way to apply a QOS policy to a file.","NetApp cmode nfs: Fix QOS extra spec

This patch fixes the incorrect behavior where the NetApp cmode
nfs driver will choose to create a volume on a flexVol that has
a QOS policy group that matches the specified QOS policy group
intended for the cinder volume. The correct behavior is to ignore
the QOS policy group of the flexVol and instead assign the QOS policy
group to the newly created cinder volume.

Change-Id: I45d905da2a9a07b3ae8c00a225ab3b7f7ceb12d8
Closes-Bug: #1288283"
1317,b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c,1395631890,2.0,1.0,86,20,6,4,1,0.886502746,True,5.0,152320.0,93.0,18.0,False,38.0,560211.5,59.0,4.0,201.0,1166.0,1290.0,186.0,936.0,1054.0,159.0,609.0,710.0,0.155793574,0.593962999,0.692307692,599,1023,1291535,neutron,b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c,1,1,  Correct OVS VXLAN version check,"Bug #1291535 in neutron: ""'Unable to retrieve OVS kernel module version' when _not_ using DKMS openvswitch module ""","If we are using openvswitch in a system with a newer kernel (3.13/trusty) it should have the features required for neutron and not require an openvswitch dkms package. Therefore we should be able to use the native module.
In neutron/agent/linux/ovs_lib.py:
def get_installed_ovs_klm_version():
    args = [""modinfo"", ""openvswitch""]
    try:
        cmd = utils.execute(args)
        for line in cmd.split('\n'):
            if 'version: ' in line and not 'srcversion' in line:
                ver = re.findall(""\d+\.\d+"", line)
                return ver[0]
    except Exception:
        LOG.exception(_(""Unable to retrieve OVS kernel module version.""))
So if we run modinfo on a system without a DKMS package we get:
$ modinfo openvswitch
filename:       /lib/modules/3.13.0-16-generic/kernel/net/openvswitch/openvswitch.ko
license:        GPL
description:    Open vSwitch switching datapath
srcversion:     1CEE031973F0E4024ACC848
depends:        libcrc32c,vxlan,gre
intree:         Y
vermagic:       3.13.0-16-generic SMP mod_unload modversions
signer:         Magrathea: Glacier signing key
sig_key:        1A:EE:D8:17:C4:D5:29:55:C4:FA:C3:3A:02:37:FE:0A:93:44:6D:69
sig_hashalgo:   sha512
Because 'version' isn't provided we need an alternative way of checking if the openvswitch module has the required features.","Correct OVS VXLAN version check

Update the version checking logic used to determine if the combination of
Linux kernel, OVS userspace, and OVS kernel module can properly support
VXLAN.

Tested on Ubuntu 14.04 without the OVS DKMS module.

Closes-Bug: #1291535

Change-Id: If034164b775989d52c3c449caba6baadb970afd9"
1318,b30402eeeddefb855126a32fee15db164684ba8c,1381726064,,1.0,254,84,9,2,1,0.720236321,True,13.0,1991863.0,78.0,22.0,False,4.0,374928.8889,14.0,5.0,27.0,702.0,704.0,27.0,679.0,681.0,27.0,645.0,647.0,0.025735294,0.59375,0.595588235,1594,1229759,1229759,cinder,b30402eeeddefb855126a32fee15db164684ba8c,1,0,"“It's necessary to support XenServer, Windows, VMware ESX, etc.”","Bug #1229759 in Cinder: ""Huawei driver can not attach volume to Windows OS""","Huawei drivers create Linux hosts by default when attaching volumes. It's necessary to support XenServer, Windows, VMware ESX, etc.","Fix Huawei drivers to support other host OSs

Huawei drivers create Linux hosts by default when attaching volumes.
This patch makes them also support Windows, XenServer, AIX, etc.
The default OS is still Linux if it is not specified.

Users need to configure the host OS types in Huawei XML configuration
file. They need to set the items like this:
<Host OSType=""Windows"" HostIP=""10.10.0.1, 10.10.0.2, ..."" />
<Host .../>

When attaching a volume, the driver will get the host IP from nova. We
compare that IP with the IP in ""HostIP"" to get the corresponding OS type.

Closes-bug: #1229759
Change-Id: I36fd52b97f790f1c68eaf24b6c12e7ef5d16145d"
1319,b31d28415a6e5b7224d53eb674e4f76e8de738ac,1397052770,1.0,1.0,6,14,3,2,1,0.858672711,True,2.0,75326.0,45.0,12.0,False,34.0,931957.3333,52.0,7.0,56.0,2979.0,2981.0,56.0,2577.0,2579.0,56.0,1013.0,1015.0,0.050756901,0.902938557,0.904719501,770,1197,1305083,neutron,b31d28415a6e5b7224d53eb674e4f76e8de738ac,0,0,"Refactoring “The dhcp agent never uses the reuse_existing flag, which is set to true by default.”","Bug #1305083 in neutron: ""reuse_existing is never used by the dhcp agent""","The dhcp agent never uses the reuse_existing flag, which is set to true by default. It can be removed.","Remove ""reuse_existing"" from setup method in dhcp.py

This flag is never used, it's always set to True by
DhcpLocalProcess.enable()

Change-Id: Ic30e0f2c97679d5919cc4e4afeb38666a6d41392
Closes-bug: #1305083"
1320,b3225581befe14f21b909e176d0a4583b297e031,1397234667,,1.0,38,5,2,2,1,0.693127415,True,3.0,1073313.0,52.0,20.0,False,165.0,202903.0,515.0,5.0,1510.0,4232.0,4498.0,1153.0,3479.0,3614.0,1431.0,3545.0,3759.0,0.184132699,0.455959882,0.483476919,782,1209,1306718,nova,b3225581befe14f21b909e176d0a4583b297e031,1,1,,"Bug #1306718 in OpenStack Compute (nova): ""Instances become undelete-able if vif unplugging fails""","If an instance's vifs cannot be deleted (because, for example, they were never plugged in the first place), then compute manager will fail trying to delete the instance:
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/compute/manager.py"", line 2154, in _shutdown_instance
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     block_device_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 956, in destroy
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     destroy_disks)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 992, in cleanup
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_vifs(instance, network_info)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 864, in unplug_vifs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.vif_driver.unplug(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 783, in unplug
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_ovs(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 667, in unplug_ovs
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     self.unplug_ovs_hybrid(instance, vif)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/virt/libvirt/vif.py"", line 661, in unplug_ovs_hybrid
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     v2_name)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/network/linux_net.py"", line 1317, in delete_ovs_vif_port
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     _ovs_vsctl(['del-port', bridge, dev])
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/nova/nova/network/linux_net.py"", line 1302, in _ovs_vsctl
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher     raise exception.AgentError(method=full_args)
2014-04-11 09:25:27.754 TRACE oslo.messaging.rpc.dispatcher AgentError: Error during following call to agent: ['ovs-vsctl', '--timeout=120', 'del-port', 'br-int', u'qvo81ce661d-1a']","Don't explode if we fail to unplug VIFs after a failed boot

Libvirt currently attempts to unplug VIFs after destroying an
instance, which can fail if the instance was never built
properly in the first place. This patch makes it ignore errors
when trying to tear down VIFs during that cleanup. Failure
in this case would block the deletion of the instance (which
should never happen) and any real failures aren't something
we could do anything about.

Closes-bug: #1306718

Change-Id: I84a4433c3959b8f14e0259be0aa51da1654006c8"
1321,b32912f8b47f50f72a59d97387fc7f28e3e096a1,1408818895,,1.0,4,1,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,1227,1686,1360656,nova,b32912f8b47f50f72a59d97387fc7f28e3e096a1,1,0,"“Since this change https://review.openstack.org/#/c/98607/, if the conductor sends back “","Bug #1360656 in OpenStack Compute (nova): ""Objects remotable decorator fails to properly handle ListOfObjects field if it is in the updates dict""","Since this change https://review.openstack.org/#/c/98607/, if the conductor sends back  a field of type ListOfObjects field in the updates dictionary after a remotable decorator has called the object_action RPC method, restoring them into objects will fail since they will already be 'hydrated' but the field's from_primitive logic won't know hot to deal with that.","Make Object FieldType from_primitive pass objects

In case we have a ListOfObjects field on an object with a remotable
method, due to NovaObjectsSerializer now looking into dicts recursively,
and thus also into the updates dict that object_action returns,
it is possible that objects inside a list have already been
deserialized, so we make the Fields from_primitive just pass them on,
since the job was already done by the NovaObjectsSerializer.

Change-Id: Ib2a34a115cb2d0a2a0765c81d5dd7ef331077eb5
Closes-bug: #1360656"
1322,b32d01d44ca5711c96d192df51bf7acd34f52556,1372775577,,1.0,582,250,25,13,1,0.868547197,True,33.0,14422340.0,223.0,83.0,False,229.0,9356.44,1469.0,2.0,57.0,382.0,434.0,51.0,360.0,406.0,57.0,372.0,424.0,0.011308247,0.072723728,0.082862156,1440,1196924,1196924,nova,b32d01d44ca5711c96d192df51bf7acd34f52556,0,0,Feature should give guest a chance to shutdown,"Bug #1196924 in OpenStack Compute (nova): ""Stop and Delete operations should give the Guest a chance to shutdown""","This feature will cause an ACPI event to be sent to the system while shutting down, and the acpid running inside the system can catch the event, thus giving the system a chance to shutdown cleanly.
[Impact]
 * VMs being shutdown with any signal/notification from the The hypervisor level, services running inside VMs have no chance to perform a clean shutoff
[Test Case]
 * 1. stop a VM
   2. the VM is shutdown without any notification
The can be easily seen by ssh into the system before shutting down. With the patch in place, the ssh session will be close during shutdown, because the sshd has the chance to close the connection before being brought down. Without the patch, the ssh session will just hang there for a while until timeout, because the connection is not promptly closed.
To leverage the clean shutdown feature, one can create a file named /etc/acpi/events/power that contains the following:
              event=button/power
              action=/etc/acpi/power.sh ""%e""
Then   create   a  file  named  /etc/acpi/power.sh  that  contains  whatever required to gracefully shutdown a particular server (VM).
With the apicd running, shutdown of the VM will cause  the rule in /etc/acpi/events/power to trigger the script in /etc/acpi/power.sh, thus cleanly shutdown the system.
[Regression Potential]
 * none
Currently in libvirt stop and delete operations simply destroy the underlying VM.     Some GuestOS's do not react well to this type of power failure, and it would be better if these operations followed the same approach a a soft_reboot and give the guest a chance to shutdown gracefully.   Even where VM is being deleted, it may be booted from a volume which will be reused on another server.","Stop, Rescue, and Delete should give guest a chance to shutdown

Currently in libvirt stop, shelve, rescue, and delete simply
destroy the underlying VM. Some GuestOS's do not react well to this
type of power failure, and so it would be better if these operations
followed the same approach as soft_reboot and give the guest as
chance to shutdown gracefully.  Even where VM is being deleted,
it may be booted from a volume which will be reused on another
server.

The change is implemented by adding a clean_shutdown parameter
to the relevant methods from the compute/api layer downwards
and into the virt drivers.  The implementation in the libvirt
driver is also provided.  Other drivers are modified just to
expect the additional parameter.

The timer configuration value previous used by soft_reboot in
libvirt is moved up to virt/driver so it can be used by other drivers.

The timer logic itself is changed from simple loop counting with one
second sleeps to a more precise approach, since testing showed that
other calls in the loop could introduce a difference of up to +50% on
the expected duration. This can extent the default two minute to
three minutes, which would not be acceptable in some environments
and breaks some of the tempest tests.

A separate config value defines what the default shutdown
behaviour for delete should be (default False to keep compatibility
with current behaviour).

This code is structured to enable a subsequent API change to add
clean/forced options to the stop and delete methods

Also as a minor tidy-up moved the repeated definition of
FakeLoopingCall in test_libvirt be common across tests

Partially-Implements: blueprint user-defined-shutdown
Closes-Bug: #1196924

DocImpact

Change-Id: Ie69aa2621cb52d6fefdc19f664247b069d6782ee"
1323,b35728019e0eb89c213eed7bc35a1f062c99dcca,1387376297,1.0,1.0,39,3,4,3,1,0.9035691,True,11.0,4691356.0,51.0,31.0,False,34.0,763347.0,114.0,5.0,198.0,552.0,700.0,197.0,441.0,589.0,145.0,322.0,428.0,0.136067102,0.301025163,0.399813607,199,602,1261738,glance,b35728019e0eb89c213eed7bc35a1f062c99dcca,1,1, user_total_quota calculated incorrectly ,"Bug #1261738 in Glance: ""Openstack Glance: user_total_quota calculated incorrectly ""","Description of problem: Bug in quota calculation, if an image upload fails due to quota limit, the failed image size is still added to total storage sum figure! Thus future images may fail to upload even if it looks as quota hasn’t been reached yet.
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch
How reproducible:
Steps to Reproduce:
1. vim /etc/glance/glance-api.conf  user_storage_quota = 250mb (in byets)
2. service glance-api restart
3. Upload test small image - would be ok
4. Upload large image say 4Giga, - should fail with ""Error unable to create new image""
5. Try to upload another small file say 49MB.
Actual results:
If the large i,age file or sum of failed uploaded images are more than the quota, any image size will fail to upload.
Expected results:
I should be able to upload as long as the sum of all my images is less than configured qouta.
Additional info:
Mysql show databases;
connect glance;
SELECT * FROM images;
Noticed all the images i tired, initial successful uploaded image status=”active”, images that i deleted status=”deleted”, images that failed to upload due to quota status=”killed”
I than calculated the sum of all the “killed” images.
Set a new quota of the above calculated value + 100MB, restarted glance-api service.
Only than i was able to upload another image of 49MB.
When i set a lower quota value (below the calculated sum of all the killed images) wasn’t able to upload any image.
Images of status killed, which fail upload for any reason, should not be added to total storage sum calcualtion or quota.","Filter out deleted images from storage usage

All database API's currently include deleted images in the calc of
storage usage. This is not an issue when deleted images don't have
locations. However, there are cases where a deleted image has deleted
locations as well and that causes the current algorithm to count those
locations as if they were allocating space.

Besides this bug, it makes sense to not load deleted / killed /
pending_delete images from the database if we're actually not
considering them as valid images.

The patch also filters out deleted locations.

NOTE: In the case of locations, it was not possible to add a test for
the deleted locations because it requires some changes that are not
worth in this patch. In order to mark a location as deleted, it's
necessary to go through the API and use a PATCH operation. Since this is
a database test, it doesn't make much sense to add API calls to it.
Calling the image_destroy function with an empty location list will
remove all the locations which won't help testing that specific case.

I'll work on a better solution for that in a follow-up patch.

DocImpact

Change-Id: I82f08a8f522c81541e4f77597c2ba0aeb68556ce
Closes-Bug: #1261738"
1324,b36826ef3ddeafcf5f16034bccd971de800f677a,1377360167,0.0,1.0,60,15,3,2,1,0.929318542,True,25.0,6074615.0,137.0,39.0,False,127.0,478581.0,478.0,3.0,13.0,2843.0,2852.0,13.0,2342.0,2351.0,12.0,2598.0,2606.0,0.002237907,0.447409193,0.448786366,1478,1214720,1214720,nova,b36826ef3ddeafcf5f16034bccd971de800f677a,1,1, ,"Bug #1214720 in OpenStack Compute (nova): ""nova-manage db archive_deleted_rows fails if max_rows is a large number ""","The nova-manage db archive_deleted_rows fails if max_rows is a large number (I tried 1 million but a smaller value may also cause issues) because it receives an exception from the sqlalchemy and db layer regarding the number of parameters on the sql statement.
Database has a limite of maximum total length of host and indicator variables in SQL statement.  When I ran the archive, the table had 165822 rows in it and 18489 of those were not deleted. Therefore, 147333 rows had deleted=1. So get error from Database.","Add DeleteFromSelect to avoid database's limit

nova-manage db archive_deleted_rows fails if max_rows is a large
number. Database has a limit of maximum sql variables in one SQL
statement. It is more efficient to insert(select) directly and then
delete(same select) without ever returning the selected rows back to
Python. This also can avoid database's limit.

Closes-Bug: #1214720

Change-Id: I29e3a5ce14c59dd2979e45e8d31fc3df04c70266"
1325,b3e52b6b2462cf80dbdb5fa4785e51b0d147316e,1399547942,,1.0,1,1,1,1,1,0.0,True,1.0,166662.0,27.0,4.0,False,6.0,1245160.0,6.0,4.0,288.0,806.0,966.0,274.0,717.0,865.0,227.0,582.0,687.0,0.186885246,0.477868852,0.563934426,788,1215,1307295,neutron,b3e52b6b2462cf80dbdb5fa4785e51b0d147316e,1,1,,"Bug #1307295 in neutron: ""duplicate entry exception for vxlan-allocation""","I run multiple neutron-servers using haproxy. Here's the exception thrown by all the neutron-servers when services restart:
2014-04-14 11:42:18.315 6457 ERROR neutron.service [-] Unrecoverable error: please check log for details.
2014-04-14 11:42:18.315 6457 TRACE neutron.service Traceback (most recent call last):
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 103, in serve_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service     service.start()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 72, in start
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.wsgi_app = _run_wsgi(self.app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/service.py"", line 117, in _run_wsgi
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = config.load_paste_app(app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/common/config.py"", line 145, in load_paste_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = deploy.loadapp(""config:%s"" % config_path, name=app_name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 247, in loadapp
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return loadobj(APP, uri, name=name, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 272, in loadobj
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return context.create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/paste/urlmap.py"", line 25, in urlmap_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = loader.get_app(app_name, global_conf=global_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 144, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/auth.py"", line 69, in pipeline_factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     app = loader.get_app(pipeline[-1])
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 350, in get_app
2014-04-14 11:42:18.315 6457 TRACE neutron.service     name=name, global_conf=global_conf).create()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 710, in create
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return self.object_type.invoke(self)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/loadwsgi.py"", line 146, in invoke
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return fix_call(context.object, context.global_conf, **context.local_conf)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/PasteDeploy-1.5.0-py2.6.egg/paste/deploy/util.py"", line 56, in fix_call
2014-04-14 11:42:18.315 6457 TRACE neutron.service     val = callable(*args, **kw)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 71, in factory
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return cls(**local_config)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/api/v2/router.py"", line 75, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     plugin = manager.NeutronManager.get_plugin()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 211, in get_plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return cls.get_instance().plugin
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 206, in get_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     cls._create_instance()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/lockutils.py"", line 249, in inner
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return f(*args, **kwargs)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 200, in _create_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     cls._instance = cls()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 112, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     plugin_provider)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/manager.py"", line 140, in _get_plugin_instance
2014-04-14 11:42:18.315 6457 TRACE neutron.service     return plugin_class()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/plugin.py"", line 106, in __init__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.type_manager.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/managers.py"", line 74, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service     driver.obj.initialize()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 81, in initialize
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self._sync_vxlan_allocations()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/plugins/ml2/drivers/type_vxlan.py"", line 172, in _sync_vxlan_allocations
2014-04-14 11:42:18.315 6457 TRACE neutron.service     session.add(alloc)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 402, in __exit__
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.commit()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 314, in commit
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self._prepare_impl()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib64/python2.6/site-packages/SQLAlchemy-0.7.8-py2.6-linux-x86_64.egg/sqlalchemy/orm/session.py"", line 298, in _prepare_impl
2014-04-14 11:42:18.315 6457 TRACE neutron.service     self.session.flush()
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 615, in _wrap
2014-04-14 11:42:18.315 6457 TRACE neutron.service     _raise_if_duplicate_entry_error(e, get_engine().name)
2014-04-14 11:42:18.315 6457 TRACE neutron.service   File ""/usr/lib/python2.6/site-packages/neutron/openstack/common/db/sqlalchemy/session.py"", line 559, in _raise_if_duplicate_entry_error
2014-04-14 11:42:18.315 6457 TRACE neutron.service     raise exception.DBDuplicateEntry(columns, integrity_error)
2014-04-14 11:42:18.315 6457 TRACE neutron.service DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0)  ... displaying 10 of 100000 total bound parameter sets ...  (99999, 0), (100000, 0))
2014-04-14 11:42:18.315 6457 TRACE neutron.service
2014-04-14 11:42:18.331 6457 CRITICAL neutron [-] (IntegrityError) (1062, ""Duplicate entry '1' for key 'PRIMARY'"") 'INSERT INTO ml2_vxlan_allocations (vxlan_vni, allocated) VALUES (%s, %s)' ((1, 0), (2, 0), (3, 0), (4, 0), (5, 0), (6, 0), (7, 0), (8, 0)  ... displaying 10 of 100000 total bound parameter sets ...  (99999, 0), (100000, 0))","ML2 VxlanTypeDriver: Synchronize of VxlanAllocation table

At neutron startup VxlanTypeDriver syncs tunnel range from conf
to DB. In case multiple servers deployment restarting several
servers at the same time could lead to DB exceptions being thrown.
Need to synchronize between neutron servers by locking VxlanAllocation
table.

Change-Id: Idf9908f039070b9194612484603f592f1a8d74b8
Closes-Bug: #1307295"
1326,b48bd3c0cf1f4ccfada24a6aebc7ced308d44927,1392048453,,1.0,33,8,3,2,1,0.891559837,True,3.0,1954044.0,51.0,17.0,False,31.0,1055281.0,74.0,6.0,12.0,3566.0,3566.0,12.0,2892.0,2892.0,12.0,2630.0,2630.0,0.001775229,0.359278984,0.359278984,140,541,1257726,nova,b48bd3c0cf1f4ccfada24a6aebc7ced308d44927,0,0,Refactoring code,"Bug #1257726 in OpenStack Compute (nova): ""VMware: refactor volumeops._get_volume_uuid()""","Recently I have been doing some queries for extraConfig VM options and found that the most efficient way to retrieve a given property is to do:
session._call_method(vim_util, 'get_dynamic_property', vm_ref, 'VirtualMachine', 'config.extraConfig[""some_prop_here""]')
Right now we ask for all extraConfig options and then we iterate over the result set to find a particular one.","VMware: refactor _get_volume_uuid

It is more efficient to ask vCenter for specific extraConfig property instead
getting all properties and then manually searching the one which is
needed.

Closes-Bug: #1257726
Change-Id: I4fc8399c9a8b6a141ac489e8fb92117ec03ca42f"
1327,b493dcce93418ed09f0e2b7e1dccc20270deb75c,1393614139,,1.0,17,11,1,1,1,0.0,True,3.0,139526.0,16.0,5.0,False,17.0,166901.0,62.0,4.0,213.0,1565.0,1584.0,205.0,1334.0,1353.0,203.0,1400.0,1415.0,0.137281292,0.942799462,0.952893674,497,916,1286285,cinder,b493dcce93418ed09f0e2b7e1dccc20270deb75c,0,0,'It should be calling’ (Just in case),"Bug #1286285 in Cinder: ""3PAR driver common uses time.sleep""","hp_3par_common calls time.sleep()
It should be calling eventlet.greenthread.sleep() to make sure we don't block the volume manager.","change time.sleep to use loopingcall

We don't want to block the volume manager from servicing
requests.  So this patch changes our use of time.sleep
to use loopingcall wait, which uses eventlet greenthread
sleep.

Change-Id: I13a1e4932e24ff5f09e35b8baa7c0fd5410388b6
Closes-Bug: #1286285"
1328,b4964eb6a570e290545f95d45411dc8441985cd5,1394446780,,1.0,9,3,2,2,1,1.0,True,4.0,4408658.0,55.0,15.0,False,152.0,18852.0,452.0,1.0,57.0,786.0,839.0,47.0,786.0,829.0,57.0,781.0,834.0,0.007681102,0.103562442,0.11058138,573,994,1290294,nova,b4964eb6a570e290545f95d45411dc8441985cd5,1,1,terrible bug,"Bug #1290294 in OpenStack Compute (nova): ""Instance's XXX_resize dir never be deleted if we resize a pre-grizzly instance in havana""","reproduce steps:
1. create an instance under Folsom
2. update nova to Havana
3. resize the instance to another host
4. confirm the resize
5. examine the instance dir on source host
you will find the instance-0000xxxx_resize dir exists there which was not deleted while confirming resize.
the reason is that:
in the _cleanup_resize in libvirt driver:
def _cleanup_resize(self, instance, network_info):
        target = libvirt_utils.get_instance_path(instance) + ""_resize""
we get the instance path by using get_instance_path method in libvirt utils,
but we check the original instance dir of pre-grizzly instances' before we return it,
if this instance is a resized one which original instance dir exists on another host(the dest host),
the wrong instance path with uuid will be returned, and then the `target` existing check will be failed,
then the instance-xxxx_resize dir will never be deleted.
def get_instance_path(instance, forceold=False, relative=False):
    """"""Determine the correct path for instance storage.
    This method determines the directory name for instance storage, while
    handling the fact that we changed the naming style to something more
    unique in the grizzly release.
    :param instance: the instance we want a path for
    :param forceold: force the use of the pre-grizzly format
    :param relative: if True, just the relative path is returned
    :returns: a path to store information about that instance
    """"""
    pre_grizzly_name = os.path.join(CONF.instances_path, instance['name'])
    if forceold or os.path.exists(pre_grizzly_name):                  ############### here we check the original instance dir, but if we have resized the instance to another host, this check will be failed, and a wrong dir with instance uuid will be returned.
        if relative:
            return instance['name']
        return pre_grizzly_name
    if relative:
        return instance['uuid']
    return os.path.join(CONF.instances_path, instance['uuid'])","libvirt: return the correct instance path while cleanup_resize

If we resized a pre-grizzly instance with grizzly or later nova
to another host, while the resize confirmation process,
_cleanup_resize will find the instance resize backup dir and
delete it, but a wrong xxx_resize dir like ${uuid}_resize,
instead of the correct ${name}_resize will be found.
This is because the instance is a resized one which original
instance dir exists on another host(the dest host),
get_instance_path method could not find the original instance
dir on the source host, so the path with uuid will be returned,
and the `target` existing check in _cleanup_resize is failed,
then the ${name}_resize dir will never be deleted.

Closes-bug: #1290294
Change-Id: I904b6751dec740e001f5ec29f637ef456528746f"
1329,b4bf668ec40ee6a329740b587fa4a651a6e0dca2,1409630737,,1.0,11,10,4,3,1,0.850222402,False,,,,,True,,,,,,,,,,,,,,,,,1140,1595,1351466,neutron,b4bf668ec40ee6a329740b587fa4a651a6e0dca2,1,1,“Remove the reference for now.”,"Bug #1351466 in neutron: ""can't copy '.../cisco_cfg_agent.ini': doesn't exist ""","Started roughly 1800 UTC this evening
2014-08-01 19:36:06.878 | error: can't copy 'etc/neutron/plugins/cisco/cisco_cfg_agent.ini': doesn't exist or not a regular file
http://logs.openstack.org/70/111370/1/check-tripleo/check-tripleo-ironic-undercloud-precise-nonha/3bc75ae/console.html","Supply missing cisco_cfg_agent.ini file

cisco_cfg_agent.ini file was missed in the initial commit and
caused neutron startup issues. This patch supplies the proper
ini file and adds it back to neutron setup.cfg.
Also the introduced config options are put in a specific group
instead of default as was in the initial commit.

Change-Id: I74b3b77fe6e196524809580f522f91f3b62f5ba7
Closes-bug: #1351466"
1330,b4eaa0520990f30519aaa073f352541f17b0577b,1407456441,,1.0,0,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1162,1619,1354072,neutron,b4eaa0520990f30519aaa073f352541f17b0577b,1,1,"“Since change:
https://review.openstack.org/#/c/73234/
“","Bug #1354072 in neutron: ""Transaction is not rollbacked properly in bind_router""","This stracktrace has been observed:
http://logs.openstack.org/29/112229/5/gate/gate-tempest-dsvm-neutron-pg/db8ad58/logs/screen-q-svc.txt.gz?level=TRACE#_2014-08-07_15_49_00_061
Since change:
https://review.openstack.org/#/c/73234/
A unique constraint has been introduced. This prevents the race condition to cause multiple entries in the table, however concurrent changes can still occur, hence the DBDuplicateEntry can still occur. That  said, it looks like the transaction is not cleaned up properly.","Fix session's InvalidRequestError because of nested rollback

This patch addresses the issue by removing the extra nesting
(which is effectively redundant).

The longer story about this patch is the following:

Race conditions may cause DBDuplicateEntry exceptions that
require a transaction to be rollbacked back, and yet make
the whole operation succeed. A classic example is what has
been solved in commit fbc6b99. If the rollback is done in a
nested transaction, the above mentioned exception is raised.

To address the problem, we could use savepoints by means of
sqlalchemy's begin_nested(); Even though this approach is
preferable, it causes quite a bit of changes in the unit
tests (because of sqlite); it may also require that certain
DBMS, or certain DB backend configurations, support savepoints.

In the end, the simpler approach was chosen.

Closes-bug: #1354072

Change-Id: Ic9393319e55f71d681124bd3052e939724ebab6b"
1331,b4ef81d6a018a3e6781bbfd5c9a2c73727645cc5,1393747464,,1.0,56,10,8,8,1,0.765875804,True,18.0,6984707.0,202.0,59.0,False,250.0,317948.0,1322.0,1.0,1796.0,720.0,2280.0,1510.0,720.0,1994.0,906.0,715.0,1386.0,0.121175685,0.095657983,0.185303941,572,993,1290261,nova,b4ef81d6a018a3e6781bbfd5c9a2c73727645cc5,1,1, Missing translation support,"Bug #1290261 in OpenStack Compute (nova): ""Missing translation support""",There are a number of places that do not contain translation support in the virt drivers,"Add missing translation support

Update a number of files to add missing translation support.

The patch adds a new hacking check - N321. This ensures that
all log messages, except debug ones, have translations.

A '# noqa' indicates that the validation will not be done on
the specific log message. This should be used in cases where
the translations do not need to be done, for example, the log
message is logging raw data.

Closes-bug: #1290261
Change-Id: Ib2ca0bfaaf432e15448c96619682c2cfd073fbbc"
1332,b523699ce039d77e0bc02eb2c00e436c2e28a274,1385604136,,1.0,5,2,2,2,1,0.863120569,True,1.0,15998.0,7.0,3.0,False,13.0,156178.0,20.0,3.0,11.0,1297.0,1300.0,11.0,1151.0,1154.0,8.0,1127.0,1127.0,0.007493755,0.939217319,0.939217319,97,496,1255802,cinder,b523699ce039d77e0bc02eb2c00e436c2e28a274,1,1,,"Bug #1255802 in Cinder: ""A mistake verification for 'readonly' ""","A mistake verification for 'readonly' .
Because the param of  'readonly'  is bool , can not be checked in that way:
 readonly_flag = body['os-update_readonly_flag'].get('readonly')
        if not readonly_flag:
            msg = _(""Must specify readonly in request."")
            raise webob.exc.HTTPBadRequest(explanation=msg)
when the readonly == false , it will be throw  HTTPBadRequest","Fix the wrong verification for 'readonly'

A mistake verification for 'readonly', fix it, and add some tests.

Change-Id: Iea20245eef2e884cf4c0d8e62c2da4f1cd967106
Closes-Bug: #1255802"
1333,b5280671768bdf861d9f72f014a83bb07af09ab6,1396625881,,1.0,15,14,2,2,1,0.929363626,True,3.0,76329.0,25.0,13.0,False,211.0,180398.0,1179.0,5.0,1176.0,1664.0,2533.0,978.0,1562.0,2241.0,1157.0,1455.0,2305.0,0.149631735,0.188138002,0.297971314,740,1167,1302334,nova,b5280671768bdf861d9f72f014a83bb07af09ab6,1,1,“Change I140bfec2a52bf659a725a7dbe78ba5c527ed26de converted the_post_live_migration() call to assume it was passed an object”,"Bug #1302334 in OpenStack Compute (nova): ""live migration failed ""","When I try to live migration a VM, I got the following exception from source host.
2014-04-04 12:50:20.330 8862 INFO nova.compute.manager [-] [instance: 160fb719-7f84-466a-a19d-9284dd6d56fa] NV-FA2EA85 _post_live_migration() is started..
2014-04-04 12:50:20.371 8862 ERROR nova.openstack.common.loopingcall [-] in fixed duration looping call
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall Traceback (most recent call last):
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/loopingcall.py"", line 78, in _inner
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     self.f(*self.args, **self.kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 4495, in wait_for_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     migrate_data)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     payload)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     return f(self, context, *args, **kw)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 315, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     e, sys.exc_info())
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     six.reraise(self.type_, self.value, self.tb)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 302, in decorated_function
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     return function(self, context, *args, **kwargs)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 4600, in _post_live_migration
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall     ctxt, instance.uuid)
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall AttributeError: 'dict' object has no attribute 'uuid'
2014-04-04 12:50:20.371 8862 TRACE nova.openstack.common.loopingcall","Revert object-assuming changes to _post_live_migration()

Change I140bfec2a52bf659a725a7dbe78ba5c527ed26de converted the
_post_live_migration() call to assume it was passed an object
because the post_live_migration_at_destination() method uses them.
However, it is also called from live_migration() on the source,
which clearly is still using instance dicts.

This patch reverts the changes that assume an instance object,
as well as the tests that were changed as a result. It also
changes the clear_events_for_instance() call to use dict syntax,
since that is called from the live migration path currently.

(cherry picked from commit 887d214186631d3d1f49451fa72feb11251d2176)
Change-Id: I6e915035bc2c1b890e606d17a4195b88c3f26b13
Closes-bug: #1302334"
1334,b57d52aeb951bcfb9a2966ba7dc1b3f8b640bd8f,1405082066,,1.0,5,5,2,2,1,0.881290899,False,,,,,True,,,,,,,,,,,,,,,,,1133,1588,1350942,neutron,b57d52aeb951bcfb9a2966ba7dc1b3f8b640bd8f,0,0,"feature “Use oslo.db create_engine instead of SQLAlchemy
    oslo.db may set additional options to engines that we may be interested in.”","Bug #1350942 in neutron: ""Use oslo.db create_engine instead of SQLAlchemy""","oslo.db may set additional options to engines that we may be interested in.
This will also ease the switch to mysql-connector if that gets approved.","Use oslo.db create_engine instead of SQLAlchemy

oslo.db may set additional options to engines that we may be interested in.

blueprint enable-mysql-connector

Closes-Bug: #1350942
Change-Id: I6f67bb430c50ddacb2d53398de75fb5f494964a0"
1335,b5917e35acb6189079f33ebb5562b8d2288dcd4f,1395064978,,1.0,37,5,2,2,1,0.863120569,True,4.0,2891419.0,72.0,17.0,False,15.0,474568.0,17.0,11.0,28.0,1290.0,1291.0,28.0,1113.0,1114.0,28.0,783.0,784.0,0.029561672,0.799184506,0.800203874,634,1060,1293587,neutron,b5917e35acb6189079f33ebb5562b8d2288dcd4f,0,0,add support for https,"Bug #1293587 in neutron: ""https support for nova metadata requests""",Current metadata agent supports only http connection to nova metadata service. Implement https support.,"Add support for https requests on nova metadata

Adds new config value for accessing nova metadata api with SSL. In case
nova api requires client certificate other config values were added
providing client certificate and client private key.

DocImpact

Closes-bug: #1293587
Change-Id: I782a12eb77553f4369b782071b4ad19efb82e5e2"
1336,b5c4c5767fbfe9580e68343e722794326e6bb254,1392170574,,1.0,5,3,1,1,1,0.0,True,2.0,60563.0,9.0,2.0,False,3.0,13343788.0,3.0,2.0,55.0,793.0,810.0,55.0,768.0,785.0,43.0,760.0,765.0,0.031117397,0.538189533,0.541725601,391,804,1279146,cinder,b5c4c5767fbfe9580e68343e722794326e6bb254,0,0,remove unused code,"Bug #1279146 in Cinder: ""Removed unused context in _extend_snapshot method""","Input parameter 'context' of  _extend_snapshot method in cinder/api/extended_snapshot_attributes.py is not being used, so remove this 'context' parameter.","Removed unused context in _extend_snapshot method

Input parameter 'context' of _extend_snapshot method in
cinder/api/extended_snapshot_attributes.py is not being used, so remove
this 'context' parameter.

Change-Id: I8577e3938d5ed5bfb143a605320063a6b17c3be0
Closes-bug: #1279146"
1337,b5d09ffe604a1f6d8272b773bb183e5f2bc3d0f3,1392976410,,1.0,9,6,2,2,1,0.996791632,True,4.0,899835.0,52.0,10.0,False,22.0,529186.0,32.0,4.0,10.0,1172.0,1180.0,10.0,1009.0,1017.0,1.0,643.0,643.0,0.002352941,0.757647059,0.757647059,430,845,1281916,neutron,b5d09ffe604a1f6d8272b773bb183e5f2bc3d0f3,1,1,Cannot distinguish IPv6,"Bug #1281916 in neutron: ""OpenStack cannot assign IPv6 address to instance via dnsmasq dhcpserver""","Use dnsmasq as dhcp server, OpenStack deploy one instance, we found the deployed instance cannot get the targeted IPv6 address, but it can get the targeted IPv4 address.
At the earlier time, I found this issue when use Vmware vcenter driver, today I also found this issue is Linux env.
I use dnsmasq as dhcpv6 server, use 'tcpdump -i tapXXXX' to monitor the network data,  and in deployed instance, and run ""dhclient -6"",  the result of tcpdump as below:
  22:02:54.354287 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:54.354689 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:55.434954 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:55.435283 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:57.587164 IP6 fe80::f816:3eff:fe34:a80e.dhcpv6-client > ff02::1:2.dhcpv6-server: dhcp6 solicit
  22:02:57.587456 IP6 fe80::184d:82ff:fec4:d268.dhcpv6-server > fe80::f816:3eff:fe34:a80e.dhcpv6-client: dhcp6 advertise
  22:02:59.354082 IP6 fe80::184d:82ff:fec4:d268 > fe80::f816:3eff:fe34:a80e: ICMP6, neighbor solicitation, who has fe80::f816:3eff:fe34:a80e, length 32
  22:02:59.354922 IP6 fe80::f816:3eff:fe34:a80e > fe80::184d:82ff:fec4:d268: ICMP6, neighbor advertisement, tgt is fe80::f816:3eff:fe34:a80e, length 24
from dnsmasq log, I got ""no address available"" error.
The root cause is dnsmasq need to read host file, and distinguish MAC addresses from IPv6 addresses.
the current host file as below:
fa:16:3e:25:f4:31,host-2001-2011-0-f104--3.openstacklocal, 2001:2011:0:f104::3
We need to wrap the ipv6 address with '[]' to let dnsmasq can distinguish MAC addresses from IPv6 addresses.","Make sure dnsmasq can distinguish IPv6 address from MAC address

Currrently, due to in dnsmasq host file, the IPv6 address does
not be wrapped with '[]', dnsmasq cannot distinguish IPv6 address
from MAC address, it will cause the deployed instances cannot get
IPv6 address via dnsmasq dhcp service.

Change-Id: I023e44667a238664d11f2ac2cc484432cd301bcc
Closes-Bug: #1281916"
1338,b5f7cf989c6f233fb2b0dd759cc33afb94761ec2,1395863296,,1.0,8,5,1,1,1,0.0,True,2.0,181340.0,14.0,4.0,False,1.0,3431913.0,1.0,2.0,7.0,634.0,639.0,7.0,592.0,597.0,2.0,600.0,600.0,0.001936733,0.387992253,0.387992253,568,989,1289696,cinder,b5f7cf989c6f233fb2b0dd759cc33afb94761ec2,0,0,can cause problems when several requests …,"Bug #1289696 in Cinder: ""request_id middleware uses wrong request ID value""","The request_id middleware is designed to generate a request ID during process_request() and attach this value to the as an HTTP header during process_response(). Unfortunately, it stores the request ID value in a variable within the RequestIdMiddleware class. This violates the ""shared nothing"" rule, and can cause problems when several requests are run concurrently. For example, if requests A and B come in back-to-back, and A completes first, A will have B's request ID value in the HTTP response.
This problem was discovered when running nova's api.compute.servers.test_instance_actions test in parallel while working on  https://review.openstack.org/#/c/66903/","Import request_id middleware bug fix from oslo

There is a bug in request-id middleware that a subsequent API request
will overwrite a request-id of a previous request when multiple API
calls are processed in parallel in request_id middleware. The fix is
a drop-in replacement; cinder code does not need to be modified to
use the updated request_id middleware.

oslo change to fix the middleware:
d7bd9dc37ac3d6bc171cd2e290c772633ad20a32

Closes-Bug: #1289696
Change-Id: Ib211318c47681f841bcde04e490ccc2070bdf8de"
1339,b61fce6cbabf2181fed3f0c4bb83a2d3c40db100,1401687132,,1.0,117,7,2,2,2,0.313195047,False,,,,,True,,,,,,,,,,,,,,,,,873,1307,1317257,swift,b61fce6cbabf2181fed3f0c4bb83a2d3c40db100,1,1,,"Bug #1317257 in OpenStack Object Storage (swift): ""container auditor blows up if there's a file in devices""","On your saio if you `touch /srv/node1/asdf` and run the container-auditor you get an uncaught exception:
container-6011: UNCAUGHT EXCEPTION#012Traceback (most recent call last):#012  File ""/usr/local/bin/swift-container-auditor"", line 7, in <module>#012    execfile(__file__)#012  File ""/vagrant/swift/bin/swift-container-auditor"", line 23, in <module>#012    run_daemon(ContainerAuditor, conf_file, **options)#012  File ""/vagrant/swift/swift/common/daemon.py"", line 110, in run_daemon#012    klass(conf).run(once=once, **kwargs)#012  File ""/vagrant/swift/swift/common/daemon.py"", line 55, in run#012    self.run_once(**kwargs)#012  File ""/vagrant/swift/swift/container/auditor.py"", line 99, in run_once#012    self._one_audit_pass(reported)#012  File ""/vagrant/swift/swift/container/auditor.py"", line 54, in _one_audit_pass#012    for path, device, partition in all_locs:#012  File ""/vagrant/swift/swift/common/utils.py"", line 1756, in audit_location_generator#012    partitions = listdir(datadir_path)#012  File ""/vagrant/swift/swift/common/utils.py"", line 2195, in listdir#012    return os.listdir(path)#012OSError: [Errno 20] Not a directory: '/srv/node1/asdf/containers'
This may only happen if mount_check is false, otherwise it's probably skipped earlier.  Still, we should probably just skip over files in the devices root.","Container Auditor should log a warning if the devices path contains a non-directory.

If the devices path configured in container-server.conf contains a file
then an uncaught exception is seen in the logs. For example if file foo exists as such
/srv/1/node/foo then when the container-auditor runs, the exception that foo/containers is
not a directory is seen in the logs

This patch was essentially clayg and can be found in the bug

I tested it and wanted to get a feel of the openstack workflow so going through the
commit process

I have added a unit test as well as cleaned up and improved the unit test coverage
for this module.
- unit test for above fix is added
- unit test to verify exceptions that are raised in the module
- unit test to verify the logger's behavior
- unit test to verify mount_check behavior

Change-Id: I903b2b1e11646404cfb0551ee582a514d008c844
Closes-Bug: #1317257"
1340,b625f558862465184dd28da7215f34c77ec1ece6,1385715395,0.0,1.0,161,14,6,6,1,0.595071216,True,7.0,473135.0,21.0,7.0,False,57.0,1666139.833,175.0,3.0,20.0,675.0,685.0,20.0,635.0,645.0,11.0,635.0,636.0,0.009958506,0.52780083,0.528630705,101,500,1255917,cinder,b625f558862465184dd28da7215f34c77ec1ece6,1,1,Change to case insensitive,"Bug #1255917 in Cinder: ""The response is incorrect while creating snapshot metadata""","1.Create metadata for a snapshot with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the snapshot, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the snapshot
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}","Fixes case insensitive for resp body

Create metadata for a snapshot with key-value set, which key in uppercase
and lowercase(e.g.{""key"": ""v1"", ""KEY"": ""V1""), the server accept the
request and return the key-value set {""key"": ""v1"", ""KEY"": ""V1""}. But the
server just add one metadata because the server is not case sensitive.

The patch will modify the resp body with the one which the server added.

update_all has the same ploblem.

DocImpact
Change-Id: I684049412a4aa84f593e970c87157c74fffdfffe
Closes-Bug: #1255917"
1341,b65eecf2d8b4df9330c09dd31b818bbd5c0da3cb,1378999199,,1.0,29,2,3,2,1,0.431436341,True,2.0,979110.0,17.0,4.0,False,49.0,1240101.667,80.0,3.0,167.0,3223.0,3306.0,167.0,2793.0,2876.0,165.0,2315.0,2396.0,0.027561016,0.384525984,0.397974431,1542,1223890,1223890,nova,b65eecf2d8b4df9330c09dd31b818bbd5c0da3cb,1,1,"""Due to a typo”","Bug #1223890 in OpenStack Compute (nova): ""device_type not respected""","When launching an instance with block_device_mapping_v2, if source_type=""image"", destination_type=""volume"" and device_type=""cdrom"", the block device is attached as ""disk"" running on ""ide"" bus.  Expected to be ""cdrom"" on ""ide"" bus.
example with python-novaclient: https://dpaste.de/uojBC/
libvirt.xml generated from above command: https://dpaste.de/bSw71/","Libvirt: volume driver set correct device type

Due to a typo, libvirt volume driver was assigning the device type
passed to the connect_volume method to the wrong attribute of the
LibvirtConfigGuestDisk class (non existing device_type which is
completely disregarded, instead of the expected source_device), which
caused the rendered XML to always have the device set to 'disk'. This
patch fixes that issue.

Also adds the 'lun' type to the list of supported device_types since it
is supported by libvirt but was missed in the previous refactoring.

Tests are added to prevent future regressions.

Closes-bug: #1223890

Change-Id: Iee160afcc23f762a752df1ca20372944d5100291"
1342,b67b20832a5bfccd1bbf8d1e63ebcd7061856881,1386357153,,1.0,0,2,1,1,1,0.0,True,1.0,235612.0,11.0,8.0,False,6.0,4235950.0,5.0,8.0,589.0,1132.0,1485.0,549.0,1001.0,1329.0,143.0,466.0,512.0,0.234910277,0.76182708,0.836867863,161,563,1258629,neutron,b67b20832a5bfccd1bbf8d1e63ebcd7061856881,0,0,Obsolete code,"Bug #1258629 in neutron: ""remove dead code  _arp_spoofing_rule()""",remove dead code  _arp_spoofing_rule(),"Remove dead code _arp_spoofing_rule()

This code should have been removed when the allowed_address_pair
extension was added here (0efce6195fa7be80e110bd841dc9b3537a94c376).
The arp spoofing rules are handled in the method _setup_spoof_filter_chain().

Reported by: Amir Sadoughi that this was crud I left behind :)

Change-Id: Ib0e2e2a5c13fb8fa7af1f988510143f40ac335e2
Closes-bug: #1258629"
1343,b69584a523f41f8171805e22ac6a0b29172e832e,1393998206,,1.0,5,3,1,1,1,0.0,True,3.0,7674955.0,78.0,24.0,False,4.0,1303678.0,5.0,10.0,4.0,2778.0,2780.0,4.0,2332.0,2334.0,2.0,2701.0,2701.0,0.000399361,0.359691161,0.359691161,527,948,1288281,nova,b69584a523f41f8171805e22ac6a0b29172e832e,0,0,This can mislead…,"Bug #1288281 in OpenStack Compute (nova): ""Change parameters of add_timestamp in ComputeDriverCPUMonitor class""",'cls' may mislead developpers that this Decorator return a classmethod.  Change parameter 'cls' to 'self' in wrapper.,"Change parameters of add_timestamp in ComputeDriverCPUMonitor class

Change parameter 'cls' to 'self' in wrapper. 'cls' may mislead developpers
that this decorator returns a classmethod. Use functools.wraps to follow
the common pattern of decorator in nova.

closes-bug: #1288281

Change-Id: I6acbcba7297607dc6d07e8668ad07e6ca81d4456"
1344,b699c703e00eda1c8368b2470815b8cfc2fae2e4,1392042098,,1.0,5,4,1,1,1,0.0,True,3.0,1458538.0,50.0,14.0,False,9.0,1094001.0,13.0,5.0,183.0,2761.0,2793.0,181.0,2411.0,2442.0,174.0,1864.0,1887.0,0.023907104,0.254781421,0.257923497,384,797,1278279,nova,b699c703e00eda1c8368b2470815b8cfc2fae2e4,1,1,bad attributes,"Bug #1278279 in OpenStack Compute (nova): ""hypervisor and near attributes do not exist in scheduler_hints parameter""","The API samples of scheduler-hints contains ""hypervisor"" and ""near"" attributes like the following:
{
    ""server"" : {
        ""name"" : ""new-server-test"",
        ""image_ref"" : ""http://glance.openstack.example.com/openstack/images/70a599e0-31e7-49b7-b260-868f441e862b"",
        ""flavor_ref"" : ""http://openstack.example.com/openstack/flavors/1"",
        ""os-scheduler-hints:scheduler_hints"": {
            ""hypervisor"": ""xen"",
            ""near"": ""48e6a9f6-30af-47e0-bc04-acaed113bb4e""
        }
    }
}
However the attributes do not exist in the scheduler-hints parameter of ""create a server"" API.","Fix the sample and unittest params of v3 scheduler-hints

The API samples of scheduler-hints contains ""hypervisor"" and ""near""
attributes, but the attributes do not exist in the scheduler-hints
parameter of ""create a server"" API. This patch changes them to the
existing ""same_host"" attribute.
This patch also changes the attributes of some unit tests of the v3
scheduler-hints by the same attribute.

This nonexistent attributes are found during bp/nova-api-validation-fw
works. The implementation needs this change because it will deny the
unexpected attributes, which are not defined with API schema, in API
parameters.

DocImpact
Closes-Bug: #1278279

Change-Id: I8542f44b325ba909fdff8b569146c0015b150291"
1345,b6a8aea4d1fe8be6073af57fad2ab6863d8f359c,1385055297,2.0,1.0,202,99,4,3,1,0.839469573,True,6.0,7933389.0,76.0,18.0,False,11.0,2091376.5,15.0,4.0,15.0,974.0,975.0,15.0,904.0,905.0,10.0,462.0,462.0,0.019538188,0.822380107,0.822380107,1434,1190620,1190620,neutron,b6a8aea4d1fe8be6073af57fad2ab6863d8f359c,0,0,Test files,"Bug #1190620 in neutron: ""Improve unit test coverage for Cisco plugin model code""","Improve unit test coverage for ...
quantum/plugins/cisco/models/virt_phy_sw_v2	193	34	0	31	5	78%","Improve unit test coverage for Cisco plugin model code

Closes-Bug: #1190620

This fix improves unit test coverage for:
quantum/plugins/cisco/models/virt_phy_sw_v2.py
Test coverage is improved from about 78% to 99%.

One change included in this fix is removal of some code in
the _invoke_plugin() method in virt_phy_sw_v2.py which looks
like it's attempting to handle the case where the number of
arguments being passed to _invoke_plugin() exceeds the number
of arguments expected for the target plugin method. This
section of code does not get executed for any existing
calls to _invoke_plugin(), and it doesn't appear that
this logic would work (except when the target plugin method
includes a **kwargs expansion).

Change-Id: Ibceb7a462a213f3ba693bcbe94e77d97b2e1440a"
1346,b6b9df2940ac07f715f759f4c315b2cf088c2320,1382274036,,1.0,15,4,4,4,1,0.728712441,True,3.0,846252.0,17.0,9.0,False,55.0,0.0,152.0,4.0,29.0,1242.0,1242.0,28.0,1112.0,1112.0,29.0,1074.0,1074.0,0.027100271,0.971093044,0.971093044,1670,1238085,1238085,cinder,b6b9df2940ac07f715f759f4c315b2cf088c2320,1,1, ,"Bug #1238085 in Cinder: ""Cinder Migrate from NFS fails due to nfs_mount_point_base not being provided""","""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp **args)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 808, in wrapper
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp return func(self, *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 779, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.db.volume_update(ctxt, volume_ref['id'], updates)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 772, in migrate_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self._migrate_volume_generic(ctxt, volume_ref, host)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 710, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp new_volume['migration_status'] = None
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/manager.py"", line 690, in _migrate_volume_generic
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote='dest')
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 293, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp {'status': dest_orig_status})
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/usr/lib/python2.7/contextlib.py"", line 24, in __exit__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp self.gen.next()
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 287, in copy_volume_data
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp remote=dest_remote)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/volume/driver.py"", line 378, in _attach_volume
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/utils.py"", line 798, in brick_get_connector
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp device_scan_attempts)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 114, in factory
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/initiator/connector.py"", line 802, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp *args, **kwargs)
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp File ""/opt/stack/cinder/cinder/brick/remotefs/remotefs.py"", line 41, in __init__
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp err=_('nfs_mount_point_base required'))
2013-10-09 11:39:43.657 TRACE cinder.openstack.common.rpc.amqp InvalidParameterValue: An unknown exception occurred.","Brick connector fix for NFS drivers

This change fixes the error that comes
while getting brick connector and attaching volumes
in case of NFS drivers in cinder. The attribute for
mount point base was not passed to attach_volume method
as the connector initialization logic is common for all
types of protocols. It is fixed by populating
the required parameter in the RemoteFsConnector
for NFS drivers.

Change-Id: I8601326b318f6f8c53a03610f1b4f2bfd14070ff
Closes-Bug: #1238085"
1347,b6e23caddf05457b2337ac03d33b066a36aad154,1386029414,,1.0,1,1,1,1,1,0.0,True,2.0,8696457.0,16.0,8.0,False,2.0,8467690.0,4.0,5.0,222.0,1059.0,1060.0,179.0,948.0,949.0,64.0,348.0,349.0,0.108695652,0.58361204,0.585284281,122,521,1257119,neutron,b6e23caddf05457b2337ac03d33b066a36aad154,1,1,The id parameter is missing in… Typo?,"Bug #1257119 in neutron: ""PLUMgrid plugin is missing id in update_floatingip""","In PLUMgrid plugin, the update_floatingip is missing id as shown above:
def update_floatingip(self, net_db, floating_ip):
        self.plumlib.update_floatingip(net_db, floating_ip, id)","Adds id in update_floatingip API in PLUMgrid plugin driver

The id parameter is missing in the update_floatingip API, this patch
adds it.

Change-Id: I7407fe34628f8ef03a946a11fea9dad8b0f6c256
Closes-Bug: #1257119"
1348,b6e9922364fca4d8d141fbb2f27024f7db79ca9e,1407400507,,1.0,199,36,9,6,1,0.472965789,False,,,,,True,,,,,,,,,,,,,,,,,1866,1352893,1352893,Neutron,b6e9922364fca4d8d141fbb2f27024f7db79ca9e,1,0,"""On systems where ipv6 module is not loaded in kernel we need ""","Bug #1352893 in neutron: ""ipv6 cannot be disabled for ovs agent""","If ipv6 module is not loaded in kernel ip6tables command doesn't work and fails  in openvswitch-agent when processing ports:
2014-08-05 15:20:57.089 3944 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Error while processing VIF ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1262, in rpc_loop
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     ovs_restarted)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1090, in process_network_ports
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     port_info.get('updated', set()))
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 247, in setup_port_filters
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.prepare_devices_filter(new_devices)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/securitygroups_rpc.py"", line 164, in prepare_devices_filter
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.firewall.prepare_port_filter(device)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.gen.next()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/firewall.py"", line 108, in defer_apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.filter_defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_firewall.py"", line 370, in filter_defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self.iptables.defer_apply_off()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 353, in defer_apply_off
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     self._apply()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 369, in _apply
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     return self._apply_synchronized()
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/iptables_manager.py"", line 400, in _apply_synchronized
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     root_helper=self.root_helper)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/utils.py"", line 76, in execute
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     raise RuntimeError(m)
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent RuntimeError:
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip6tables-restore', '-c']
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Exit code: 2
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stdout: ''
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Stderr: ""ip6tables-restore v1.4.21: ip6tables-restore: unable to initialize table 'filter'\n\nError occurred at line: 2\nTry `ip6tables-restore -h' or 'ip6tables-restore --help' for more information.\n""
2014-08-05 15:20:57.089 3944 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
2014-08-05 15:20:58.261 3944 INFO neutron.plugins.openvswitch.agent.ovs_neutron_agent [-] Agent out of sync with plugin!
2014-08-05 15:20:58.749 3944 INFO neutron.agent.securitygroups_rpc [-] Preparing filters for devices set([u'5e646c57-0ce4-4705-9281-2cf991cd4135', u'1e0ea538-74a4-429d-97fb-08fbae37ad47'])","Ensure ip6tables are used only if ipv6 is enabled in kernel

On systems where ipv6 module is not loaded in kernel we need to avoid
usage of ip6tables. This patch reads
/proc/sys/net/ipv6/conf/default/disable_ipv6 file and if ipv6 is
disabled then ip6tables are not used in IptablesManager

Closes-Bug: #1352893

Change-Id: I07e5851aa25eb98b7a97dff86b9850475df85f64"
1349,b715701ff4387d55e7eb4c301a0edea92fa4e0e8,1380306177,,1.0,1,1,1,1,1,0.0,True,2.0,310704.0,24.0,11.0,False,52.0,167965.0,113.0,2.0,8.0,504.0,505.0,8.0,486.0,487.0,7.0,478.0,479.0,0.007626311,0.456625357,0.457578646,1612,1232177,1232177,cinder,b715701ff4387d55e7eb4c301a0edea92fa4e0e8,1,1, ,"Bug #1232177 in Cinder: ""volume manager fails in init_host on delete_volume""","cinder.volume.manager.init_host attempts to delete volumes that have status of ""deleting"".   But set_initialized is not called until after this step.
And manager.delete_volume has new decorator @utils.require_driver_initialized.
So the call to delete_volume from init_host will always fail because the driver is not yet marked as initialized.
Proposed solution:  move set_initailized up after the call to check_for_setup_error.","Set vol driver initialized before deleting volumes

Move the call to set_initialized before calling delete_volume in
init_host.  The delete_volume method has a precondition that the
driver be initialized.

Closes-Bug: #1232177

Change-Id: Ie73bd30e0ab80d489ab61ba8a60fac51ccb505b5"
1350,b78eea6146145793a7c61705a1602cf5e9ac3d3a,1389650224,,1.0,5,3,2,2,1,0.811278124,True,3.0,2452351.0,48.0,25.0,False,9.0,4334368.0,9.0,11.0,619.0,2498.0,2651.0,565.0,2201.0,2341.0,164.0,668.0,673.0,0.239825581,0.972383721,0.979651163,289,695,1268762,neutron,b78eea6146145793a7c61705a1602cf5e9ac3d3a,1,1,"Unfortunately, ovs does not reinitialize the interfaces","Bug #1268762 in neutron: ""Remove and recreate interfacein ovs  if already exists""","If the dhcp-agent machine restarts and openvswitch logs the following
warning message for all tap interfaces that have not been recreated yet:
bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)
Once the dhcp-agent starts he recreates the interfaces and readds them to the
ovs-bridge. Unfortinately, ovs does not reinitalize the interface as its
already in ovsdb and does not assign it an ofport number.
In order to correct this we should first remove interfaces that exist and
then readd them.
root@arosen-desktop:~# ovs-vsctl  -- --may-exist add-port br-int fake1
# ofport still -1
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []
root@arosen-desktop:~# ip link add fake1 type veth peer name fake11
root@arosen-desktop:~# ifconfig fake1
fake1     Link encap:Ethernet  HWaddr 56:c3:a1:2b:1f:f4
          BROADCAST MULTICAST  MTU:1500  Metric:1
          RX packets:0 errors:0 dropped:0 overruns:0 frame:0
          TX packets:0 errors:0 dropped:0 overruns:0 carrier:0
          collisions:0 txqueuelen:1000
          RX bytes:0 (0.0 B)  TX bytes:0 (0.0 B)
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []
root@arosen-desktop:~# ovs-vsctl  -- --may-exist add-port br-int fake1
root@arosen-desktop:~# ovs-vsctl  list inter | grep -A 2 fake1
name                : ""fake1""
ofport              : -1
ofport_request      : []","Remove and recreate interface if already exists

If the dhcp-agent machine restarts when openvswitch comes up it logs the
following warning messages for all tap interfaces that do not exist:

bridge|WARN|could not open network device tap2cf7dbad-9d (No such device)

Once the dhcp-agent starts it recreates the interfaces and re-adds them to the
ovs-bridge. Unfortunately, ovs does not reinitialize the interfaces as they
are already in ovsdb and does not assign them a ofport number.

This situation corrects itself though the next time a port is added to the
ovs-bridge which is why no one has probably noticed this issue till now.

In order to correct this we should first remove interface that exist and
then readd them.

Closes-bug: #1268762

Change-Id: I4bb0019135ab7fa7cdfa6d5db3bff6eafe22fc85"
1351,b7bf623d79608df8ff5bf8040dc3590da7eeb236,1383061312,,1.0,72,33,3,2,1,0.963421615,True,13.0,10609748.0,123.0,52.0,False,18.0,517059.0,39.0,6.0,1.0,1807.0,1807.0,1.0,1630.0,1630.0,1.0,965.0,965.0,0.000308928,0.149212234,0.149212234,1725,1245564,1245564,nova,b7bf623d79608df8ff5bf8040dc3590da7eeb236,1,1, ,"Bug #1245564 in OpenStack Compute (nova): ""datastore selection is incorrect if token is being used""","If the number of datastores is greater than the maximum, the PropertyCollector returns a token value that should be used to iterate over the entire result set. The get_datastore_ref_and_name function in vm_util.py is aware of tokens but it stops on the first found datastore (line 942) instead searching for the best match over the entire set.
We should improve get_datastore_ref_and_name to:
1) clearly specify how the datastore is selected (the current pydoc is wrong)
2) select the best match by iterating the whole result set","VMware: fix datastore selection when token is returned

Use the token returned by the PropertyCollector to iterate
over the complete set of datastores and select the best
match based on the regular expression and the free space on
the datastore.

Closes-Bug: #1245564
Change-Id: If24c37a41bfa994a990aaa4a3e21d16b33aad42f"
1352,b856bc5e348cc69923e3ee815303804323730026,1406132169,,1.0,3,1,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1097,1548,1347777,nova,b856bc5e348cc69923e3ee815303804323730026,1,1,,"Bug #1347777 in OpenStack Compute (nova): ""The compute_driver option description does not include the Hyper-V driver""","The description of the option ""compute_driver"" should include hyperv.HyperVDriver along with the other supported drivers
https://github.com/openstack/nova/blob/aa018a718654b5f868c1226a6db7630751613d92/nova/virt/driver.py#L35-L38","Add Hyper-V driver in the ""compute_driver"" option description

The description of the option ""compute_driver"" should include
hyperv.HyperVDriver along with the other supported driver.

Change-Id: Ic7e16fd5154609987b2ef0b6f8ee52619ccf489c
Closes-bug: #1347777"
1353,b85cc49588c9f40e091801af33da8c7575058ec0,1384355019,2.0,1.0,110,46,3,2,1,0.990865143,True,8.0,4923407.0,65.0,24.0,False,6.0,2072889.0,15.0,5.0,1.0,669.0,669.0,1.0,645.0,645.0,1.0,606.0,606.0,0.001737619,0.527367507,0.527367507,1757,1250816,1250816,cinder,b85cc49588c9f40e091801af33da8c7575058ec0,1,1, ,"Bug #1250816 in Cinder: ""VMware VMDK driver should use host mount count as the main criterion for datastore selection""",Currently the VMDK driver picks a datastore for volume creation based on space utilization. The shell VM corresponding to the volume is migrated to a different host and datastore if the volume is attached to an instance created in a host which cannot access the volume's current datastore. These migrations could be minimized if a datastore connected to multiple hosts is picked at the time of volume creation.,"VMDK:Using host mount info for datastore selection

Currently the VMDK driver picks a datastore for volume creation based
on space utilization. The shadow VM corresponding to the volume is
migrated to a different host and datastore if the volume is attached
to an instance created in a host which cannot access the volume's
current datastore. To reduce these migrations, this fix selects a
datastore which is connected to maximum number of hosts, provided
there is enough space to accommodate the volume. Ties are broken
based on space utilization; datastore with low space utilization
is preferred.

Closes-Bug: #1250816
Change-Id: I5ecf22d782857fca0889799229465c49afe188a0"
1354,b868ae707f9ecbe254101e21d9d7ffa0b05b17d1,1393406826,2.0,1.0,85,52,6,3,1,0.611105291,True,7.0,1508534.0,47.0,12.0,False,101.0,435452.5,314.0,1.0,616.0,1170.0,1488.0,588.0,944.0,1260.0,599.0,1025.0,1331.0,0.406504065,0.695121951,0.902439024,483,901,1285060,cinder,b868ae707f9ecbe254101e21d9d7ffa0b05b17d1,1,1,,"Bug #1285060 in Cinder: ""create_export and remove_export broken in driver.py""","There are some issues with create_export and remove_export in driver.py:
1) There is a call to a create_export RPC, but the volume manager does not implement create_export
2) remove_export is not called in _detach_volume in driver.py, which means we have exports left over from several calls","Fix create_export/remove_export in driver.py

1. There was a call to rpcapi.create_export which does not have a
matching volume manager function, causing volume migration to crash.
This call was not necessary because there is already an
initialize_connection RPC call, which calls the driver's create_export
function. Removed the create_export RPC call and function. Added better
error handling to that code in _attach_volume in driver.py as well.

2. The manager called remove_export from detach_volume, which was not
being called by these functions. I believe it makes more sense to call
it from terminate_connection. Moved it there, and fixed up the
corresponding code in _detach_volume in driver.py.

3. Remove leftovers of export code from create_volume manager flow.

Change-Id: I2b192630ebed54368f151db47b49cbc72601a8d7
Closes-Bug: #1285060"
1355,b8cf45218714937681d7df2e5b9d7d440cb2edd9,1406316339,1.0,1.0,27,3,3,3,1,0.918990046,False,,,,,True,,,,,,,,,,,,,,,,,1106,1558,1348766,neutron,b8cf45218714937681d7df2e5b9d7d440cb2edd9,1,1,,"Bug #1348766 in neutron: ""Big Switch: hash shouldn't be updated on unsuccessful calls""","The configuration hash db is updated on every response from the backend including errors that contain an empty hash. This is causing the hash to be wiped out if a standby controller is contacted first, which opens a narrow time window where the backend could become out of sync. It should only update the hash on successful REST calls.","Big Switch: Only update hash header on success

This patch moves the hash update call into the success block
of the server manager so the database isn't updated with
a hash header from an error response. Additionally, it prevents
the hash from being updated to an empty value if the hash header
is not present in the response.

Closes-Bug: #1348766
Change-Id: I512d01f9bb91b208dd58883d2464951ecc6748e1"
1356,b9031fb0ad890ad45530ccfdb4ea9df8b715f111,1382008136,,1.0,66,37,6,4,1,0.888714485,True,11.0,11234050.0,96.0,32.0,False,164.0,57693.0,877.0,3.0,61.0,2251.0,2290.0,61.0,1786.0,1825.0,61.0,2019.0,2058.0,0.0097423,0.317410434,0.323538655,1703,1241017,1241017,nova,b9031fb0ad890ad45530ccfdb4ea9df8b715f111,1,1, ,"Bug #1241017 in OpenStack Compute (nova): ""Unshelve bypasses resource tracker""","When unshelving an offloaded instance, a new host is scheduled for the instance. However, the call to unshelve the instance on the new host does not go through the resource tracker, which may lead to over-allocation of the hosts resources.","Add resource tracking to unshelve_instance()

When unshelving an offloaded instance, a new host is scheduled for
the instance. However, the call to unshelve the instance on the new
host does not go through the resource tracker, which may lead to
over-allocation of the hosts resources.

This change adds resource tracking to unshelve_instance().

Change-Id: I682713c8ebc6276ac49822db5c8efc373d5b8f4d
Closes-Bug: #1241017"
1357,b91e6e31ece12d5b1204680aa0a912e3b9569f3b,1386213311,,1.0,160,16,8,6,1,0.553668975,True,3.0,357217.0,15.0,4.0,False,64.0,229218.125,198.0,2.0,27.0,580.0,597.0,27.0,549.0,566.0,13.0,549.0,552.0,0.011447261,0.449713818,0.452166803,144,546,1258004,cinder,b91e6e31ece12d5b1204680aa0a912e3b9569f3b,1,1,Incorrect response,"Bug #1258004 in Cinder: ""The response is incorrect while creating volume metadata""","1.Create metadata for a volume with the request body:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
2.The server accept it, and return the response body, as bellow:
{""metadata"":{
             ""key1"": ""value1"",
             ""KEY1"": ""value2""
}}
3.Get the metadata of the volume, the server returned:
{""metadata"":{
             ""key1"": ""value1""
}}
4.I find that case ignore in cinder, so the server just add one metadata for the volume
5.So i think the response body should return the one which the server added when create action:
  {""metadata"":{
             ""key1"": ""value1""
}}","Fixes case insensitive for resp body

Create metadata for a volume with key-value set, which key in uppercase
and lowercase(e.g.{""key"": ""v1"", ""KEY"": ""V1""), the server accept the
request and return the key-value set {""key"": ""v1"", ""KEY"": ""V1""}. But the
server just add one metadata because the server is not case sensitive.

The patch will modify the resp body with the one which the server added.

update_all has the same ploblem.

Fixes errors on v2 unittest without difficulty.

DocImpact
Closes-Bug: #1258004

Change-Id: Ic337c0a351ac234493e1d73b86ba87520f32289a"
1358,b986d3d0b1ea91631df8c7b51e389a8150d497fb,1411163760,,1.0,2,11,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1108,1561,1349147,nova,b986d3d0b1ea91631df8c7b51e389a8150d497fb,0,0,Bug in test,"Bug #1349147 in OpenStack Compute (nova): ""test_db_api unit tests fail with: UnexpectedMethodCallError: Unexpected method call get_session.__call__(use_slave=False) -> None""","http://logs.openstack.org/62/104262/7/gate/gate-nova-python27/3adf0e2/console.html
2014-07-25 16:27:18.188 | Traceback (most recent call last):
2014-07-25 16:27:18.188 |   File ""nova/tests/db/test_db_api.py"", line 1236, in test_security_group_get_no_instances
2014-07-25 16:27:18.188 |     security_group = db.security_group_get(self.ctxt, sid)
2014-07-25 16:27:18.188 |   File ""nova/db/api.py"", line 1269, in security_group_get
2014-07-25 16:27:18.188 |     columns_to_join)
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 167, in wrapper
2014-07-25 16:27:18.188 |     return f(*args, **kwargs)
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 3668, in security_group_get
2014-07-25 16:27:18.188 |     query = _security_group_get_query(context, project_only=True).\
2014-07-25 16:27:18.188 |   File ""nova/db/sqlalchemy/api.py"", line 3635, in _security_group_get_query
2014-07-25 16:27:18.188 |     read_deleted=read_deleted, project_only=project_only)
2014-07-25 16:27:18.189 |   File ""nova/db/sqlalchemy/api.py"", line 237, in model_query
2014-07-25 16:27:18.189 |     session = kwargs.get('session') or get_session(use_slave=use_slave)
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 765, in __call__
2014-07-25 16:27:18.189 |     return mock_method(*params, **named_params)
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1002, in __call__
2014-07-25 16:27:18.189 |     expected_method = self._VerifyMethodCall()
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1049, in _VerifyMethodCall
2014-07-25 16:27:18.189 |     expected = self._PopNextMethod()
2014-07-25 16:27:18.189 |   File ""/home/jenkins/workspace/gate-nova-python27/.tox/py27/local/lib/python2.7/site-packages/mox.py"", line 1035, in _PopNextMethod
2014-07-25 16:27:18.189 |     raise UnexpectedMethodCallError(self, None)
2014-07-25 16:27:18.189 | UnexpectedMethodCallError: Unexpected method call get_session.__call__(use_slave=False) -> None
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiVW5leHBlY3RlZE1ldGhvZENhbGxFcnJvcjogVW5leHBlY3RlZCBtZXRob2QgY2FsbCBnZXRfc2Vzc2lvbi5fX2NhbGxfXyh1c2Vfc2xhdmU9RmFsc2UpIC0+IE5vbmVcIiBBTkQgcHJvamVjdDpcIm9wZW5zdGFjay9ub3ZhXCIgQU5EIHRhZ3M6XCJjb25zb2xlXCIiLCJmaWVsZHMiOltdLCJvZmZzZXQiOjAsInRpbWVmcmFtZSI6ImN1c3RvbSIsImdyYXBobW9kZSI6ImNvdW50IiwidGltZSI6eyJmcm9tIjoiMjAxNC0wNy0xM1QxNjo0MDo1NiswMDowMCIsInRvIjoiMjAxNC0wNy0yN1QxNjo0MDo1NiswMDowMCIsInVzZXJfaW50ZXJ2YWwiOiIwIn0sInN0YW1wIjoxNDA2NDc5MzkzMjc0LCJtb2RlIjoiIiwiYW5hbHl6ZV9maWVsZCI6IiJ9
8 hits in 2 weeks, check and gate, all failures, looks like it started around 7/21.","Change ""is lazy loaded"" detection method in db_api test

This changes the test_security_group_get_no_instances() method to use a simpler
method of determining if instances is loaded on a security_group after a get
operation.

Change-Id: Ifea6c20bcaa201167ef5679f3ddb9ae8141bbbf5
Closes-Bug: #1349147"
1359,b98dda29c9bf2587c2eddad0231e337a1cb4ce02,1404746545,,1.0,2,2,2,1,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1031,1475,1336556,neutron,b98dda29c9bf2587c2eddad0231e337a1cb4ce02,1,1,“Fix missing migration default value”,"Bug #1336556 in neutron: ""migration default value not being applied to dbase""","I have a private repo that is a few weeks behind the upstream, and was having problems stacking. Initially it was failing in one of the migrations, so I changed the SQLAlchemy version to 0.8.4 and alembic to 0.6.5 and retried (clean database).
What I saw was that the migration worked, but later a router create failed.  It was complaining that the enable_snat field was not specified and did not have a default value. Checking in the migrations, I saw that 128e042a2b68_ext_gw_mode.py adds this field and sets the default value to True. However, it was not doing that.
I changed the migration file to use server_default=sa.text(""true"") and now the migration works. BTW, it was the only migration file using 'default=True'.
Not sure if we should just apply this change to the existing migration file or if some other action is needed.","Fix missing migration default value

Fix 128e042a2b68_ext_gw_mode and 40b0aff0302e_mlnx_initial
migrations by properly using server_default parameter in column
definitions.

Change-Id: I199e206dccc36fcfc2457a17167c76611d69784a
Closes-bug: #1336556"
1360,b9a2ad044c68212665c15aa49f1257cd4168c738,1389062526,,1.0,0,5,1,1,1,0.0,True,1.0,141084.0,11.0,3.0,False,39.0,653863.0,68.0,3.0,46.0,1645.0,1687.0,46.0,1246.0,1288.0,11.0,1432.0,1439.0,0.00172117,0.205536431,0.206540448,255,659,1266450,nova,b9a2ad044c68212665c15aa49f1257cd4168c738,0,0,unused variables,"Bug #1266450 in OpenStack Compute (nova): ""Remove unused variables in imagebackend.py""","The method create() in /nova/virt/libvirt/ imagebackend.py has two unused
variable: old_format and features.These should be removed.","Remove unused variables in imagebackend.py

The method create() in /nova/virt/libvirt/ imagebackend.py has two unused
variable: old_format and features.These should be removed.

Change-Id: Id7b8661a162fbdf33de21b29892b5244ef301da7
Closes-Bug: #1266450"
1361,b9b55935cb61cc1726525b1c4963e352a3a81ea9,1382581310,,1.0,21,10,5,5,1,0.911368089,True,14.0,8479457.0,57.0,21.0,False,197.0,20100.0,1037.0,10.0,2.0,4000.0,4001.0,2.0,3424.0,3425.0,2.0,3012.0,3013.0,0.000466853,0.468876439,0.469032057,24,336,1244018,nova,b9b55935cb61cc1726525b1c4963e352a3a81ea9,1,1,,"Bug #1244018 in OpenStack Compute (nova): ""update security group raise HttpError500 exception ""","1.Set the item ""security_group_api=nova"" in nova.conf
2.Restart nova
3.Create a security group
4.Update the security group
   PUT http://192.168.83.241:8774/v2/99a7b3d4bd6540aaaceae89ac74bfab6/os-security-groups/7
   {
    ""security_group"": {
        ""name"": ""huangtianhua"",
        ""description"":""for test""
        }
   }
5.The server raises exception as bellow:
   {
    ""computeFault"": {
        ""message"": ""The server has either erred or is incapable of performing the requested operation."",
        ""code"": 500
       }
   }
6.I think it's a bug.When traversal the rules of the group before returning throws error:
   ""DetachedInstanceError: Parent instance &lt;SecurityGroup at 0x789eed0&gt; is not bound to a Session; lazy load operation of        attribute 'rules' cannot proceed.""","Fix a lazy-load exception in security_group_update()

When security_group_api is nova, the update_security_group api raises http
500 error: group rules fails with lazy load exception. Because the rule
infos of the security group will be return in response body, but the rule
attribute hasn't been preloaded.

Closes-Bug: #1244018
Change-Id: Ie07879ac22e4add90f75968778bf334915e8349c"
1362,b9ddfa566c6caad2384c791d1ae1873ce70c261e,1382529515,,2.0,6,2,1,1,1,0.0,True,2.0,510013.0,21.0,7.0,False,1.0,9131321.0,1.0,4.0,25.0,1000.0,1020.0,25.0,923.0,943.0,7.0,402.0,404.0,0.016460905,0.829218107,0.833333333,19,331,1243652,neutron,b9ddfa566c6caad2384c791d1ae1873ce70c261e,1,0,Bug in postgre,"Bug #1243652 in neutron: ""Enums do not have names in some migration""","Enums must contain name parameter for PostgreSQL
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/c88b6b5fea3_cisco_n1kv_tables.py"", line 90, in upgrade
    sa.PrimaryKeyConstraint('tenant_id', 'profile_id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 148, in create_table
    _ddl_runner=self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/event.py"", line 389, in __call__
    fn(*args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/util/langhelpers.py"", line 293, in __call__
    return getattr(self.target, self.name)(*arg, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/types.py"", line 1835, in _on_table_create
    t._on_table_create(target, bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 576, in _on_table_create
    self.create(bind=bind, checkfirst=checkfirst)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 527, in create
    bind.execute(CreateEnumType(self))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1536, in _execute_ddl
    compiled = ddl.compile(dialect=dialect)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/expression.py"", line 1778, in compile
    return self._compiler(dialect, bind=bind, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2927, in _compiler
    return dialect.ddl_compiler(dialect, self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 705, in __init__
    self.string = self.process(self.statement)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 724, in process
    return obj._compiler_dispatch(self, **kwargs)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/sql/visitors.py"", line 72, in _compiler_dispatch
    return getter(visitor)(self, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 757, in visit_create_enum_type
    self.preparer.format_type(type_),
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/dialects/postgresql/base.py"", line 898, in format_type
    raise exc.CompileError(""Postgresql ENUM type requires a name."")
CompileError: Postgresql ENUM type requires a name.","Fix required enum's name in migration

Enums must contain name parameter for PostgreSQL and need to be
deleted after dropping table.

Closes-Bug: #1243652
Closes-Bug: #1243213

Change-Id: I01d18ff25572bbe9e6a8b8c6ae27f250ab154734"
1363,b9ff8cd7ad8c402787324d2baca9b32f61eafb4a,1384800157,1.0,1.0,123,6,3,3,1,0.658398865,True,5.0,1609229.0,46.0,19.0,False,26.0,346375.0,80.0,7.0,164.0,1383.0,1394.0,162.0,1223.0,1234.0,146.0,1132.0,1136.0,0.126180258,0.972532189,0.975965665,1776,1252423,1252423,cinder,b9ff8cd7ad8c402787324d2baca9b32f61eafb4a,1,1,“This causes the volume clone from snapshot dd operation to fail”,"Bug #1252423 in Cinder: ""Ensure ThinLVM LV is active when cloning or creating from snapshot""","Thin-provisioned LVs may not be activated automatically in all cases.  (RHEL 6.5 does not activate them automatically by default.)
This causes the volume clone from snapshot dd operation to fail, as the source device does not exist in /dev/mapper/.  We should ensure the LV is active before attempting to clone to a new volume.","LVM: Activate source LV before cloning from it

LVM may be configured to not automatically activate
thin-provisioned LVs.

Ensure they are activated when performing a clone, otherwise
dd will fail as the device does not exist in /dev/mapper/.

Closes-Bug: #1252423

Change-Id: Ibcb946ffe7804b1976bf1b1863c48340c8cc7fd5"
1364,ba1f41d89d38286f769cfdf40b337fe5c7dad578,1397204862,,1.0,180,28,3,3,1,0.873083616,True,10.0,12647141.0,87.0,20.0,False,41.0,1000596.333,74.0,3.0,9.0,345.0,354.0,7.0,336.0,344.0,0.0,305.0,305.0,0.000827815,0.253311258,0.253311258,1486,1216247,1216247,glance,ba1f41d89d38286f769cfdf40b337fe5c7dad578,1,1, “This is due to an incorrect regular expression used int he code”,"Bug #1216247 in Glance: ""glance-replicator commands fail due to incorrect regex check""","When I pass correct parameters to the glance-replicator tool I get the following output:
ERROR: Bad format of the given arguments.
rohit@precise-dev-102:~/devstack$ glance-replicator compare 10.2.3.1:9292  10.2.3.2:9292
replication_compare compare <fromserver:port> <toserver:port>
    Compare the contents of fromserver with those of toserver.
    fromserver:port: the location of the master glance instance.
    toserver:port:   the location of the slave glance instance.
ERROR: Bad format of the given arguments.
This is due to an incorrect regular expression used int he code to match the input parameters.
The same regex is used by all the replicator functions to verify the parameters:
​https://github.com/openstack/glance/blob/master/glance/cmd/replicator.py#L58
SERVER_PORT_REGEX = '\w+:\w+'
The character class \w = [a-zA-Z0-9_]. So if we pass an IP Address or an FQDN having characters outside the set, this check will fail.
The regex should be corrected for glance-replicator to work correctly.","Fixes ""bad format"" in replicator for valid hosts

Due to a bad regex, the glance-replicator tool rejects any source or
target host:port combination that contains a period. As a result, only
hostnames without periods can be used.

This fix expands the host/port checks to:
  - allow periods in hostnames
  - allow and verify IPv6 addr/port pairs (i.e., [fe80::f00d:face]:1234)
  - allow and verify IPv4 addr/port pairs (i.e., 172.17.17.2:1234)
  - sanity-check port numbers

This also includes extensive tests for each component of the parsing,
and adds network_utils to openstack-common.conf.

Change-Id: I94fdd7a57a4cb0aa5d79f66d68be159d1f1266d1
Closes-Bug: #1216247"
1365,ba49c5eb387d841d40f81f57d5cefbb00c835bff,1410372040,,1.0,70,2,2,2,1,0.972552317,False,,,,,True,,,,,,,,,,,,,,,,,1295,1761,1367918,nova,ba49c5eb387d841d40f81f57d5cefbb00c835bff,1,1,,"Bug #1367918 in OpenStack Compute (nova): ""Xenapi attached volume with no VM leaves instance in undeletable state""","As shown by the stack trace below, when a volume is attached but the VM is not present the volume can't be cleaned up by Cinder and will raise an Exception which puts the instance into an error state.  The volume attachment isn't removed because an if statement is hit in the xenapi destroy method which logs ""VM is not present, skipping destroy..."" and then moves on to trying to cleanup the volume in Cinder.  This is because most operations in xen rely on finding the vm_ref and then cleaning up resources that are attached there.  But if the volume is attached to an SR but not associated with an instance it ends up being orphaned.
014-08-29 15:54:02.836 8766 DEBUG nova.volume.cinder [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] Cinderclient connection created using URL: https://localhost/v1/<tenant>
cinderclient /opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py:108
2014-08-29 15:54:03.251 8766 ERROR nova.compute.manager [req-341cd17d-0f2f-4d64-929f-a94f8c0fa295 None] [instance: <uuid>] Setting instance vm_state to ERROR
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] Traceback (most recent call last):
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2443, in do_terminate_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._delete_instance(context, instance, bdms, quotas)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/hooks.py"", line 131, in inner
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] rv = f(*args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2412, in delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] quotas.rollback()
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 82, in exit
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] six.reraise(self.type, self.value, self.tb)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2390, in _delete_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] self._shutdown_instance(context, instance, bdms)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/compute/manager.py"", line 2335, in _shutdown_instance
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 189, in wrapper
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] res = method(self, ctx, volume_id, *args, **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/nova/volume/cinder.py"", line 309, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] connector)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 331, in terminate_connection
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] {'connector': connector})
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self.api.client.post(url, body=body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 223, in post
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] return self._cs_request(url, 'POST', **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 187, in _cs_request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] **kwargs)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] File ""/opt/rackstack/879.28/nova/lib/python2.6/site-packages/cinderclient/client.py"", line 170, in request
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] raise exceptions.from_response(resp, body)
2014-08-29 15:54:03.251 8766 TRACE nova.compute.manager [instance: <uuid>] ClientException: DELETE on http://localhost:8081/volumes/<volume_uuid>/export?force=False returned '409' with 'Volume '<volume_uuid>' is currently attached to '<ip>'' (HTTP 409) (Request-ID: req-d8a81cfc-5ba2-4bfb-b519-c92a2","Xen: Attempt to find and cleanup orphaned SR during delete

Before some patches merged to better handle failures to boot an instance
from a volume it was possible to have an attached volume with an SR and
plugged PBD but no VM.  And the destroy method in xenapi short circuits
if no VM is present because everything is looked up from there in order
to be cleaned up.

This adds a check to see if a volume is supposed to be attached during
destroy if no VM is present and attempts to look it up via the SR uuid
and clean it up.

I would like to say that this patch is unnecessary, as it's sort of
hackish, but it's possible that some deployments have instances in this
state and this will allow them to cleanup without manually running
commands on the hypervisor.

Closes-bug: #1367918
Change-Id: I7745af7e22164a96af800ae796bda077e49bf291"
1366,ba87499dda0c897aa08afadfc2911ff80bdae2f9,1407020818,,1.0,26,13,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,841,1270,1313801,neutron,ba87499dda0c897aa08afadfc2911ff80bdae2f9,0,0,"Feature, “Colocate strings used in REST client methods as URIs for APIs to Cisco CSR device, to make it easier to find and maintain them.”","Bug #1313801 in neutron: ""Cisco VPN move URI strings to constants""","Colocate strings used in REST client methods as URIs for APIs to Cisco CSR device, to make it easier to find and maintain them. File is:
neutron/services/vpn/device_drivers/cisco_csr_rest_client.py","Move Cisco VPN RESTapi URI strings to constants

Cisco VPN RESTapi URI strings are currently spread out
throughout the cisco vpn device driver code. This create
challenge towards consistently using REST resource identifiers
as this code gets enhanced in the future. This change moves
those URIs as string contents towards the top of the device
driver file.

This addresses the review comment received during the
cisco-vpnaas BP implementation.

Change-Id: I17e02cca95b1d14d9218f1a2745780bd14c9f89d
Closes-Bug: #1313801"
1367,bb23a3730e5b2281e0a223a86c89f2a0bfcb4897,1401225504,,1.0,12,2,2,2,1,0.863120569,False,,,,,True,,,,,,,,,,,,,,,,,927,1363,1323813,nova,bb23a3730e5b2281e0a223a86c89f2a0bfcb4897,1,0,"“According to AWS docs
(http://docs.aws.amazon.com/AWSEC2/latest/APIReference/
api-error-codes.html)
“","Bug #1323813 in OpenStack Compute (nova): ""Invalid ec2 error code for absent volumes and snapshots""","ec2-api returns InvalidVolumeID.NotFound and InvalidSnapshotID.NotFound for absent volumes and snapshots.
But AWS returns InvalidVolume.NotFound and InvalidSnapshot.NotFound as it is documented in http://docs.aws.amazon.com/AWSEC2/latest/APIReference/api-error-codes.html
For example this affects Tempest. Tempest expects correct (AWS version) errors in waitXXXStatus functions and raises an error if other error cames for absent objects. So it make difficult writting tests.","Fix EC2 not found errors for volumes and snapshots.

According to AWS docs
(http://docs.aws.amazon.com/AWSEC2/latest/APIReference/
api-error-codes.html)

Change-Id: I43af230b975263bb2a14e0cc0bd8718f55bd8a55
Closes-Bug: #1323813"
1368,bb2c4ef9a26b12c68ca1e6710d0c92ce00c464cb,1383585290,,1.0,57,6,3,3,1,0.67955736,True,10.0,11022748.0,147.0,36.0,False,9.0,3951050.333,11.0,6.0,5.0,1188.0,1191.0,5.0,1112.0,1115.0,0.0,430.0,430.0,0.001926782,0.83044316,0.83044316,53,381,1247902,neutron,bb2c4ef9a26b12c68ca1e6710d0c92ce00c464cb,0,0,I think we need to handle collections such as… Feature,"Bug #1247902 in neutron: ""Bulk create need to notify dhcp agent""","I bulk created subnets and ports by neutron api.
Using these ports boot instance. I found that the vms could not get ip address.
Then I saw the source code and found that _send_dhcp_notification function only handle ['network', 'subnet', 'port'] resource.
I think we need to handle collections such as  ['networks', 'subnets', 'ports'] here.","After bulk create send DHCP notification

In the past send_dhcp_notification only handle single resource
because of VALID_RESOURCES = ['network', 'subnet', 'port'].
When I use bulk created ports to boot instance, the VMs could
not get IP address from DHCP agent. So I add some special handle
in function notify().

Add unit test for bulk create dhcp notification.

Modify a paramter for code style.

Change-Id: Ibf2b3380dc3081beaf0f2f68042b022f2726bbe7
Closes-Bug: #1247902"
1369,bb406281d8e36c29874bee3d9a3d1d5eae4ccdd5,1390602948,,1.0,6,6,1,1,1,0.0,True,4.0,997712.0,55.0,19.0,False,17.0,665705.0,28.0,7.0,78.0,1328.0,1340.0,78.0,1143.0,1155.0,72.0,623.0,631.0,0.1,0.854794521,0.865753425,327,736,1272565,neutron,bb406281d8e36c29874bee3d9a3d1d5eae4ccdd5,0,0,Bad log severity: Debug severity is more appropriate,"Bug #1272565 in neutron: ""Validation should not log bad user input at error level""","I noticed this while reviewing Ic2c87174.  When I read through log
files, I don't want to see errors like this that come from validating
bad user input.  Debug severity is more appropriate.","Reduce severity of log messages in validation methods

I noticed this while reviewing Ic2c87174.  When I read through log
files, I don't want to see errors like this that come from validating
bad user input.  Info severity is more appropriate.

Change-Id: Ib8a4dd08570923c6cade6447b52bb73d20558258
Closes-Bug: #1272565"
1370,bb4f44654f6765c4e1fbcf92375c273494151099,1400770285,,1.0,4,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,896,1332,1321080,neutron,bb4f44654f6765c4e1fbcf92375c273494151099,1,1,,"Bug #1321080 in neutron: ""[OSSA 2014-021] auth token is exposed in meter http.request (CVE-2014-4615)""","auth token is exposed in meter http.request
# curl -i -X GET -H 'X-Auth-Token: 258ab6539b3b4eae8b3af307b8f5eadd' -H 'Content-Type: application/json' -H 'Accept: application/json' -H 'User-Agent: python-ceilometerclient' http://0.0.0.0:8777/v2/meters/http.request
-----------
snip..
{""counter_name"": ""http.request"", ""user_id"": ""0"", ""resource_id"": ""ip-9-37-74-33:8774"", ""timestamp"": ""2014-05-16T17:42:16.851000"", ""recorded_at"": ""2014-05-16T17:42:17.039000"", ""resource_metadata"": {""request.CADF_EVENT:initiator:host:address"": ""9.44.143.6"", ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"", ""request.RAW_PATH_INFO"": ""/v2/9af97e383dad44969bd650ebd55edfe0/servers/060c76a5-0031-430d-aa1e-01f9b3db234b"", ""request.REQUEST_METHOD"": ""DELETE"", ""event_type"": ""http.request"", ""request.HTTP_X_TENANT_ID"": ""9af97e383dad44969bd650ebd55edfe0"", ""request.CADF_EVENT:typeURI"": ""http://schemas.dmtf.org/cloud/audit/1.0/event"", ""request.HTTP_X_PROJECT_NAME"": ""ibm-default"", ""host"": ""nova-api"", ""request.SERVER_PORT"": ""8774"", ""request.REMOTE_PORT"": ""55258"", ""request.HTTP_X_USER_ID"": ""0"", ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478"", ""request.CADF_EVENT:action"": ""delete"", ""request.CADF_EVENT:target:typeURI"": ""service/compute/servers/server"", ""request.HTTP_USER_AGENT"": ""Mozilla/5.0 (X11; Linux x86_64; rv:24.0) Gecko/20100101 Firefox/24.0"",
snip...
auth token is masked in ""request.CADF_EVENT:initiator:credential:token"": ""4724 xxxxxxxx 8478"".
But it is exposed in  ""request.HTTP_X_AUTH_TOKEN"": ""4724d3dd6b984079a58eecf406298478""","remove token from notifier middleware

notifier middleware is capturing token and sending it to MQ. this
is not advisable so we should filter it out.

Closes-Bug: #1321080
Change-Id: Ia1bfa1bd24989681db1d2f385defc12e69a01f8d"
1371,bb5228cf79ff5bea543abc321b2dfad9fea5e9a5,1394552042,,1.0,23,1,2,2,1,0.870864469,True,10.0,10432482.0,56.0,44.0,False,13.0,6605818.0,17.0,6.0,382.0,1611.0,1938.0,292.0,1335.0,1579.0,25.0,1348.0,1348.0,0.017071569,0.885751806,0.885751806,580,1001,1290503,cinder,bb5228cf79ff5bea543abc321b2dfad9fea5e9a5,1,1,key_error. Add default value,"Bug #1290503 in Cinder: ""KeyError: 'user_identity' showing up all over cinder-api logs""","Saw this while looking at logs for bug 1290468:
http://logs.openstack.org/85/78385/3/gate/gate-grenade-dsvm/2b66f90/logs/new/screen-c-api.txt.gz
You'll see a ton of errors like this:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 705, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/new/cinder/cinder/openstack/common/log.py"", line 669, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 467, in format
    s = self._fmt % record.__dict__
KeyError: 'user_identity'
Logged from file middleware.py, line 100
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiS2V5RXJyb3I6IFxcJ3VzZXJfaWRlbnRpdHlcXCdcIiBBTkQgZmlsZW5hbWU6bG9ncypzY3JlZW4tYy1hcGkudHh0IiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxMzk0NDc2MTE4MTQ2fQ==
Thinking it's related to this:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L370
Does that need to be popped off kwargs?
From the review https://review.openstack.org/#/c/55938/ it looks like Ben and Doug were questioning it.
Also wondering if that's somehow impacted by the logging format condition used here:
https://github.com/openstack/cinder/blob/master/cinder/openstack/common/log.py#L657
Because logging_context_format_string is the format that uses user_identity.
This started showing up on 3/7 which is when the lazy translation code was enabled in Cinder so it looks like that is exposing the bug.","Adding domain to context

Add support for upcoming ""domain"" concept in Keystone V3 API in
the context.

This fix ensures there is a user_identity in the context for the
corresponding new attribute in the log format string added in oslo. The
oslo change has the same change-id as this one.

Closes-Bug: #1290503
Change-Id: Ic2cf3e52cfcc0b8adccdf9c59afaa4014708a303"
1372,bb67b0137e8faebc8892c9176775f02f6dbb920b,1392996974,,1.0,2,1,2,2,1,0.918295834,True,2.0,486307.0,28.0,7.0,False,53.0,95548.0,85.0,3.0,21.0,2324.0,2327.0,21.0,1874.0,1877.0,21.0,2267.0,2270.0,0.002969762,0.306155508,0.306560475,334,744,1273496,nova,bb67b0137e8faebc8892c9176775f02f6dbb920b,1,0,Bad driver name,"Bug #1273496 in OpenStack Compute (nova): ""libvirt iSCSI driver sets is_block_dev=False""","Trying to use iSCSI with libvirt/Xen, attaching volumes to instances was failing. I tracked this down to the libvirt XML looking like:
<disk type=""block"" device=""disk"">
  <driver name=""file"" type=""raw"" cache=""none""/>
  <source dev=""/dev/disk/by-path/ip-192.168.8.11:3260-iscsi-iqn.1986-03.com.sun:02:ecd142ab-b1c7-6bcf-8f91-f55b6c766bcc-lun-0""/>
  <target bus=""xen"" dev=""xvdb""/>
  <serial>e8c640c6-641b-4940-88f2-79555cdd5551</serial>
</disk>
The driver name should be ""phy"", not ""file"".
More digging lead to the iSCSI volume driver in nova/virt/libvirt/volume.py, which does:
class LibvirtISCSIVolumeDriver(LibvirtBaseVolumeDriver):
    """"""Driver to attach Network volumes to libvirt.""""""
    def __init__(self, connection):
        super(LibvirtISCSIVolumeDriver,
              self).__init__(connection, is_block_dev=False)
Surely is_block_dev should be ""True"" for iSCSI?? Changing this makes the problem go away - now pick_disk_driver_name() in nova/virt/libvirt/utils.py does the right thing and my volume attaches successfully.
Am I missing something here... ?","libvirt: setting a correct driver name for iscsi volumes

Setting LibvirtISCSIVolumeDriver class to be identified
as a block device to determine the correct driver name.

Change-Id: Idfb89d051ccadab1d4c4b05b817b085e094ff992
Closes-Bug: #1273496"
1373,bb83f1a743150779e190705d2875a2319720c60c,1394150257,,1.0,19,21,2,2,1,0.811278124,True,6.0,352729.0,45.0,11.0,False,181.0,10537.0,922.0,4.0,65.0,1952.0,2013.0,65.0,1777.0,1838.0,4.0,1910.0,1910.0,0.000663922,0.253751162,0.253751162,524,945,1287945,nova,bb83f1a743150779e190705d2875a2319720c60c,1,1,,"Bug #1287945 in OpenStack Compute (nova): ""Instance doesn't have a task state""","I am getting with the latest version of the trunk:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/poll.py"", line 97, in wait
    readers.get(fileno, noop).cb(fileno)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
    service.start()
  File ""/opt/stack/nova/nova/service.py"", line 180, in start
    self.manager.init_host()
  File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
    self._init_instance(context, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
    if instance.task_state == task_states.DELETING:
AttributeError: 'dict' object has no attribute 'task_state'
Removing descriptor: 6
2014-03-04 13:59:14.304 ERROR nova.openstack.common.threadgroup [-] 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup Traceback (most recent call last):
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 117, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     x.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/threadgroup.py"", line 49, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self.thread.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self._exit_event.wait()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return hubs.get_hub().switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     return self.greenlet.switch()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     result = function(*args, **kwargs)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/openstack/common/service.py"", line 480, in run_service
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     service.start()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/service.py"", line 180, in start
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     self.manager.init_host()
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 824, in init_host
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     self._init_instance(context, instance)
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup   File ""/opt/stack/nova/nova/compute/manager.py"", line 684, in _init_instance
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup     if instance.task_state == task_states.DELETING:
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup AttributeError: 'dict' object has no attribute 'task_state'
2014-03-04 13:59:14.304 TRACE nova.openstack.common.threadgroup
I get this exception when creating a snapshot, put a breakpoint in the virt layer (in this case vmware_images/snapshot), stop nova-cpu, restart nova-cpu, delete the snapshot.
nova-cpu crashes.","Use instance object instead of _instance_update()

Currently, if a user takes a snapshot of a VM, stops nova-cpu
before the snapshot operation is finished, and restart nova-cpu:
nova-cpu will crash with the error:
AttributeError: 'dict' object has no attribute 'task_state'
This happens because the _instance_update() method returns a dict
and not an Instance object.

This patch fixes the issue by not using _instance_update()
in the compute manager which is returning a dict and using
the instance object instead.

Change-Id: I3af0b09aeff7790b147a684f7c90af17d8ee7ae2
Closes-Bug: #1287945"
1374,bc115eaa477d30826ac17986a6e45844b456e206,1397637391,,2.0,39,6,4,2,1,0.837647157,True,1.0,3054756.0,13.0,2.0,False,6.0,2406215.0,10.0,1.0,10.0,1930.0,1937.0,10.0,1601.0,1608.0,10.0,9.0,16.0,0.006970849,0.006337136,0.010773131,652,1078,1294598,cinder,bc115eaa477d30826ac17986a6e45844b456e206,1,1,,"Bug #1294598 in Cinder: ""vmware: APIs are retried unnecessarily for non-session/non-connection related problems""",The vmdk cinder drivers retry the VIM APIs including login API even during errors which are unrelated to connection or session overload problems. The VIM APIs need to retried only during session overload or a stale session scenario.,"vmware: Fix problems with VIM API retry logic

Currently the VIM APIs including session creation API are retried for
cases which are unrelated to connection or session overload problems.
This change fix the retry logic as follows:
* Add an exception to indicate connection problem and raise it
  appropriately from the VIM API client
* Modify the expected exceptions in the retry decorator to include
  only connection and session overload related exceptions

This change also fixes the base class of VimFaultException
(VolumeBackendAPIException -> VimException). This change is required
so that we can handle the exceptions thrown by the VIM API client in
a consistent manner. Currently if we need to handle VIM API related
exceptions, we have to catch both VimException and VimFaultException.
For example, the API for checking active session raises
VimFaultException in some cases and we are handling only VimException.
Due to this, when session expires, the session re-establishment and API
retry fails (intermittently) since the check for active session throws
VimFaultException in some cases which is not handled.

Change-Id: I72cfc7777c3ce693b8598633f822c12c2cee2235
Closes-Bug: #1294598
Closes-Bug: #1302514"
1375,bc115eaa477d30826ac17986a6e45844b456e206,1397637391,,2.0,39,6,4,2,1,0.837647157,True,1.0,3054756.0,13.0,2.0,False,6.0,2406215.0,10.0,1.0,10.0,1930.0,1937.0,10.0,1601.0,1608.0,10.0,9.0,16.0,0.006970849,0.006337136,0.010773131,743,1170,1302514,cinder,bc115eaa477d30826ac17986a6e45844b456e206,1,1,,"Bug #1302514 in Cinder: ""vmware: error observed - "" 'The session is not authenticated.' to caller""""","Observed Error message in logs : "" 'The session is not authenticated.' to caller""
After above message I am un-able to create volumes , all volumes created are in error state.
Please see attached logs for reference","vmware: Fix problems with VIM API retry logic

Currently the VIM APIs including session creation API are retried for
cases which are unrelated to connection or session overload problems.
This change fix the retry logic as follows:
* Add an exception to indicate connection problem and raise it
  appropriately from the VIM API client
* Modify the expected exceptions in the retry decorator to include
  only connection and session overload related exceptions

This change also fixes the base class of VimFaultException
(VolumeBackendAPIException -> VimException). This change is required
so that we can handle the exceptions thrown by the VIM API client in
a consistent manner. Currently if we need to handle VIM API related
exceptions, we have to catch both VimException and VimFaultException.
For example, the API for checking active session raises
VimFaultException in some cases and we are handling only VimException.
Due to this, when session expires, the session re-establishment and API
retry fails (intermittently) since the check for active session throws
VimFaultException in some cases which is not handled.

Change-Id: I72cfc7777c3ce693b8598633f822c12c2cee2235
Closes-Bug: #1294598
Closes-Bug: #1302514"
1376,bc16517ac7351b715d0202f7beb5a5e0d7ff8f97,1386630020,,1.0,1,1,1,1,1,0.0,True,1.0,24020.0,6.0,3.0,False,12.0,654912.0,18.0,3.0,1056.0,478.0,1208.0,903.0,443.0,1052.0,914.0,446.0,1053.0,0.740890688,0.36194332,0.853441296,167,569,1259336,cinder,bc16517ac7351b715d0202f7beb5a5e0d7ff8f97,1,1,Missing exception,"Bug #1259336 in Cinder: ""Unhandled exception when ""None"" is passed in as new_size to extend""","Currently the extend method in the cinder volume_actions extension checks for KeyError and ValueError, however the negative tests in Tempest include passing None in to the new_size which results in an unhandled trace in the test logs.
Add TypeError to the list of exceptions we look for.","Catch TypeError when new_size is None on extend

The volume_actions extend method does not catch/handle TypeError
exceptions.  The tempest volume_actions test includes a negative
test that doesn't pass in a new-size so the result is we get an
unhandled exception/trace in the log files for these runs.

Change-Id: I8b699a28e06e62126da02c3318d3129412dffa6b
Closes-Bug: #1259336"
1377,bc40e85c0278afcca74892fc818da1e005f13fc7,1410099931,,1.0,4,9,2,2,1,0.89049164,False,,,,,True,,,,,,,,,,,,,,,,,1279,1742,1366548,nova,bc40e85c0278afcca74892fc818da1e005f13fc7,0,0,Refactoring “the instance is updated so there is no need for the save”,"Bug #1366548 in OpenStack Compute (nova): ""libvirt: spawning an instance may have an additional 4 db writes""",instance save is used by the driver when it does not need to be. each instance save will invoke a db access. after the spawn method is called the instance is updated so there is no need for the save,"libvirt: Unnecessary instance.save(s) called

When spawning an instance there may a considerable amount of
instance.save() called. This is not necessary as the compute
manager will update the instance after the spawn method has
successfully completed.

Change-Id: I61610b3869caeb7822eab3f2d09335744b75db28
Closes-bug: #1366548"
1378,bd40fbe304a18f8a231b519a0e1708b5ba63ad6f,1406635340,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1114,1569,1349810,neutron,bd40fbe304a18f8a231b519a0e1708b5ba63ad6f,1,0,"“Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177.""","Bug #1349810 in neutron: ""Wrong order of tables for dropping in downgrade """,Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177. Now table ml2_brocadenetworks has foreign key and downgrade of 492a106273f8_brocade_ml2_mech_dri fails. Error log: http://paste.openstack.org/show/88898/,"Fix wrong order of tables in downgrade

Heal migration fix bug https://bugs.launchpad.net/neutron/+bug/1336177.
Now table ml2_brocadenetworks has foreign key and downgrade
of 492a106273f8_brocade_ml2_mech_dri fails. To fix this change order
of tables in downgrade is needed.

Closes-bug: #1349810

Change-Id: Ida2ba66d35a952381f213080d417e475e18c196d"
1379,bd4a85d67f091382752d75b95f9cfd076431f30e,1381185278,1.0,1.0,40,26,2,2,1,0.684038436,True,1.0,455307.0,16.0,9.0,False,2.0,5787423.0,3.0,5.0,12.0,1860.0,1868.0,11.0,1673.0,1681.0,0.0,197.0,197.0,0.00234192,0.463700234,0.463700234,1641,1235450,1235450,neutron,bd4a85d67f091382752d75b95f9cfd076431f30e,1,1,“vulnerability”,"Bug #1235450 in neutron: ""[OSSA 2013-033] Metadata queries from Neutron to Nova are not restricted by tenant (CVE-2013-6419)""","The neutron metadata service works in the following way:
Instance makes a GET request to http://169.254.169.254/
This is directed to the metadata-agent which knows which router(namespace) he is running on and determines the ip_address from the http request he receives.
Now, the neturon-metadata-agent queries neutron-server  using the router_id and ip_address from the request to determine the port the request came from. Next, the agent takes the device_id (nova-instance-id) on the port and passes that to nova as X-Instance-ID.
The vulnerability is that if someone exposes their instance_id their metadata can be retrieved. In order to exploit this, one would need to update the device_id  on a port to match the instance_id they want to hijack the data from.
To demonstrate:
arosen@arosen-desktop:~/devstack$ nova list
+--------------------------------------+------+--------+------------+-------------+------------------+
| ID                                   | Name | Status | Task State | Power State | Networks         |
+--------------------------------------+------+--------+------------+-------------+------------------+
| 1eb33bf1-6400-483a-9747-e19168b68933 | vm1  | ACTIVE | None       | Running     | private=10.0.0.4 |
| eed973e2-58ea-42c4-858d-582ff6ac3a51 | vm2  | ACTIVE | None       | Running     | private=10.0.0.3 |
+--------------------------------------+------+--------+------------+-------------+------------------+
arosen@arosen-desktop:~/devstack$ neutron port-list
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                       |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
| 3128f195-c41b-4160-9a42-40e024771323 |      | fa:16:3e:7d:a5:df | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.1""} |
| 62465157-8494-4fb7-bdce-2b8697f03c12 |      | fa:16:3e:94:62:47 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| 8473fb8d-b649-4281-b03a-06febf61b400 |      | fa:16:3e:4f:a3:b0 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.2""} |
| 92c42c1a-efb0-46a6-89eb-a38ae170d76d |      | fa:16:3e:de:9a:39 | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------+
arosen@arosen-desktop:~/devstack$ neutron port-show  62465157-8494-4fb7-bdce-2b8697f03c12
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| device_id             | 1eb33bf1-6400-483a-9747-e19168b68933                                            |
| device_owner          | compute:None                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.4""} |
| id                    | 62465157-8494-4fb7-bdce-2b8697f03c12                                            |
| mac_address           | fa:16:3e:94:62:47                                                               |
| name                  |                                                                                 |
| network_id            | 5f68c45d-b729-4e21-9ded-089848eb4ef2                                            |
| security_groups       | 3e29d8e7-0195-4438-a49a-9706736b888d                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | 0f9d696fc73d4110ab492ca105881b9b                                                |
+-----------------------+---------------------------------------------------------------------------------+
arosen@arosen-desktop:~/devstack$ neutron port-show  92c42c1a-efb0-46a6-89eb-a38ae170d76d
+-----------------------+---------------------------------------------------------------------------------+
| Field                 | Value                                                                           |
+-----------------------+---------------------------------------------------------------------------------+
| admin_state_up        | True                                                                            |
| allowed_address_pairs |                                                                                 |
| device_id             | eed973e2-58ea-42c4-858d-582ff6ac3a51                                            |
| device_owner          | compute:None                                                                    |
| extra_dhcp_opts       |                                                                                 |
| fixed_ips             | {""subnet_id"": ""d5cbaa98-ecf0-495c-b009-b5ea6160259b"", ""ip_address"": ""10.0.0.3""} |
| id                    | 92c42c1a-efb0-46a6-89eb-a38ae170d76d                                            |
| mac_address           | fa:16:3e:de:9a:39                                                               |
| name                  |                                                                                 |
| network_id            | 5f68c45d-b729-4e21-9ded-089848eb4ef2                                            |
| security_groups       | 3e29d8e7-0195-4438-a49a-9706736b888d                                            |
| status                | ACTIVE                                                                          |
| tenant_id             | 0f9d696fc73d4110ab492ca105881b9b                                                |
+-----------------------+---------------------------------------------------------------------------------+
From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm2.novalocal
arosen@arosen-desktop:~/devstack$ neutron port-update 92c42c1a-efb0-46a6-89eb-a38ae170d76d   --device_id=1eb33bf1-6400-483a-9747-e19168b68933
 From vm2 (eed973e2-58ea-42c4-858d-582ff6ac3a51):
$ curl http://169.254.169.254/latest/meta-data/hostname
vm1.novalocal
In order to fix this issue I believe we need to also pass the tenant-id in the metadata request to nova. When nova receives the request it will now have to query it's database using the instance_id and check that the tenant_id's match. Using the tenant_id solves this issue as the user is not allowed to specify or update this field.","Add X-Tenant-ID to metadata request

Previously, one could update a port's device_id to be that of
another tenant's instance_id and then be able to retrieve that
instance's metadata. In order to prevent this X-Tenant-ID is now
passed in the metadata request to nova and nova then checks that
X-Tenant-ID also matches the tenant_id for the instance against it's
database to ensure it's not being spoofed.

DocImpact - When upgrading OpenStack nova and neturon, neutron
            should be updated first (and neutron-metadata-agent
            restarted before nova is upgraded) in order to minimize
            downtime. This is because there is also a patch to nova
            which has checks X-Tenant-ID against it's database
            therefore neutron-metadata-agent needs to pass that
            before nova is upgraded for metadata to work.

Change-Id: I2b8fa2f561a7f2914608e68133abf15efa95015a
Closes-Bug: #1235450"
1380,bd5c5d5bbd808b4f83da58dce433cac711575bee,1407047003,,1.0,23,1,2,2,1,0.543564443,False,,,,,True,,,,,,,,,,,,,,,,,1056,1502,1340552,nova,bd5c5d5bbd808b4f83da58dce433cac711575bee,1,1,"""The feature above was put in by this commit: https://github.com/openstack/nova/commit/dc716bd0ce77b56f4aabe54d6633b7f3bf9b0a5d. I agree with your proposed fix.”","Bug #1340552 in OpenStack Compute (nova): ""Volume detach error when use NFS as the cinder backend""","Tested Environment
--------------------------
OS: Ubuntu 14.04 LST
Cinder NFS driver:
volume_driver=cinder.volume.drivers.nfs.NfsDriver
Error description
--------------------------
I used NFS as the cinder storage backend and successfully attached multiple volumes to nova instances.
However, when I tried to detach one them, I found following error on nova-compute.log.
-----------Error log------
2014-07-07 17:48:46.175 3195 ERROR nova.virt.libvirt.volume [req-a07d077f-2ad1-4558-91fa-ab1895ca4914 c8ac60023a794aed8cec8552110d5f12 fdd538eb5dbf48a98d08e6d64def73d7] Couldn't unmount the NFS share 172.23.58.245:/NFSThinLun2
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Traceback (most recent call last):
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/virt/libvirt/volume.py"", line 675, in disconnect_volume
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     utils.execute('umount', mount_path, run_as_root=True)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/utils.py"", line 164, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     return processutils.execute(*cmd, **kwargs)
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume   File ""/usr/local/lib/python2.7/dist-packages/nova/openstack/common/processutils.py"", line 193, in execute
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume     cmd=' '.join(cmd))
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume ProcessExecutionError: Unexpected error while running command.
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Command: sudo nova-rootwrap /etc/nova/rootwrap.conf umount /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Exit code: 16
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stdout: ''
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume Stderr: 'umount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\numount.nfs: /var/lib/nova/mnt/16a381ac60f3e130cf26e7d6eb832cb6: device is busy\n'
2014-07-07 17:48:46.175 3195 TRACE nova.virt.libvirt.volume
-----------End of the Log--
For NFS volumes, every time you detach a volume, nova tries to umount the device path.
/nova/virt/libvirt/volume.py in
Line 632: class LibvirtNFSVolumeDriver(LibvirtBaseVolumeDriver):
Line 653: 	def disconnect_volume(self, connection_info, disk_dev):
Line 661:		utils.execute('umount', mount_path, run_as_root=True)
This works when the device path is not busy.
If the device path is busy (or in use), it should output a message to log and continue.
The problem is, Instead of output a log message, it raise exception and that cause the above error.
I think the reason is, the ‘if’ statement at Line 663 fails to catch the device busy massage from the content of the exc.message. It looking for the ‘target is busy’ in the exc.message, but umount error code returns ‘device is busy’.
Therefore, current code skip the ‘if’ statement and run the ‘else’ and raise the exception.
How to reproduce
--------------------------
(1)	Prepare a NFS share storage and set it as the storage backend of you cinder
(refer http://docs.openstack.org/grizzly/openstack-block-storage/admin/content/NFS-driver.html)
In cinder.conf
volume_driver=cinder.volume.drivers.nfs.NfsDriver
nfs_shares_config=<path to your nfs share list file>
(2)	Create 2 empty volumes from cinder
(3)	Create a nova instance and attach above 2 volumes
(4)	Then, try to detach one of them.
You will get the error in nova-compute.log “Couldn't unmount the NFS share <your NFS mount path on nova-compute>”
Proposed Fix
--------------------------
I’m not sure about any other OSs who outputs the ‘target is busy’ in the umount error code.
Therefore, first fix comes to my mind is fix the ‘if’ statement to:
Before fix;
if 'target is busy' in exc.message:
After fix;
if 'device is busy' in exc.message:","Fix for volume detach error when use NFS as the cinder backend

For NFS volumes, every time you detach a volume, nova tries to umount
the device path. If the device path is busy (or in use), it should
output a message to log and continue.
In current code, if the device is busy, it cannot catch the ‘device is busy’
message returned by umount, because it looking for the ‘target is busy’.
 Therefore, current code skip the ‘if’ statement and run the ‘else’ and
raise the exception.
Fix: Add ‘device is busy’ to if statement.

Add a mock test to check the behaviour of the
virt.libvirt.volume.LibvirtNFSVolumeDriver.disconnect_volume
when it has umount errors.

Closes-Bug: #1340552

Change-Id: Iac946c37064c5f5bf5a102305de40d21d16846c1"
1381,bdecc8d2339e3e0dd87c7258244ac8568b5b965e,1386950599,,1.0,18,2,2,2,1,0.881290899,True,10.0,4146246.0,67.0,30.0,False,77.0,92546.0,158.0,4.0,2.0,3112.0,3113.0,2.0,2626.0,2627.0,0.0,2209.0,2209.0,0.000146113,0.322910579,0.322910579,185,588,1260771,nova,bdecc8d2339e3e0dd87c7258244ac8568b5b965e,0,0,"Implementation: Define ""supported_instances""","Bug #1260771 in OpenStack Compute (nova): ""Fake compute driver cannot deploy image with hypervisor_type attribute""","The fake compute driver does not provide the attribute supported_instances to ImagePropertiesFilter scheduler filter.
So ImagePropertiesFilter refuses to deploy images with hypervisor_type=fake property on fake computes.
Consequently, fake computes can not be used in multi hypervisor_types deployments because in this case hypervisor_type property on image is mandatory to avoid mixing one hypervisor_type image with another hypervisor_type compute.","Define ""supported_instances"" for fake compute

Allow fake computes to deploy images with hypervisor_type
defined and set to ""fake"" and by extension to be used in
multi hypervisor_type deployments.
Mixing fake and ""real"" computes on the same OpenStack
deployment allows to create many fake instances and some
working instances for testing applications on top of
OpenStack (ie: fake instances for stress tests, working
ones for functional tests).

Closes-bug: #1260771
Change-Id: Idc337c4a7ed024f236ca2b60d91e2c30f7d54536"
1382,bdfc450f59a6b73c1548734e0a5cca5a9e6256ab,1398448722,,1.0,48,1,2,2,1,0.88864667,True,8.0,1045899.0,56.0,16.0,False,84.0,22811.0,222.0,4.0,0.0,796.0,796.0,0.0,729.0,729.0,0.0,767.0,767.0,0.000626566,0.481203008,0.481203008,655,1081,1294812,cinder,bdfc450f59a6b73c1548734e0a5cca5a9e6256ab,0,0,Implement validate_connector in FibreChannelDriver,"Bug #1294812 in Cinder: ""FibreChannelDriver doesn't implement validate_connector""","The base FibreChannelDriver doesn't implement the validate_connector() method.  It falls back to the parent class which simply does a pass.   The FC Driver should do a check on the connector and ensure that is has wwnns, wwpns to ensure that the child classes  can actually do a volume attach.   Without wwnns, wwpns, no FC driver can successfully do initialize_connection.","Implement validate_connector in FibreChannelDriver

The base FibreChannelDriver didn't implement the
validate_connector() method. It fell back to the
parent class which simply does a pass.  Now the
FCDriver checks to ensure that the connector has
wwnns and wwpns and raises an exception if either
one is empty or not set.

Closes-Bug: #1294812

Change-Id: I1e63c4718d24bdb3ead78de871971cdc498f6b01"
1383,be340d1bab015c47650687f97393c6c9015fb537,1401137125,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,910,1346,1322139,neutron,be340d1bab015c47650687f97393c6c9015fb537,1,1,“fix openvswitch requirement check” (b53c094d83df881156aa646002f95c9643b1b7a5),"Bug #1322139 in neutron: ""VXLAN kernel requirement check for openvswitch agent is not working ""","on RHEL7 beta agent set to use VXLAN tunneling does not start.
I'm using rdo packages, if you want I can check the upstream version somewhere, but seems that code that checks the version is still in github.
in openvswitch-agent.log I see:
2014-05-21 13:53:21.762 1814 ERROR neutron.plugins.openvswitch.agent.ovs_neutron_agent [req-ce4eadcb-4cbb-4c09-a404-98ecb5383fa5 None] Agent terminated
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent Traceback (most recent call last):
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 231, in _check_ovs_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 551, in check_ovs_vxlan_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     'kernel', 'VXLAN')
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent   File ""/usr/lib/python2.7/site-packages/neutron/agent/linux/ovs_lib.py"", line 529, in _compare_installed_and_required_version
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent     raise SystemError(msg)
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent SystemError: Unable to determine kernel version for Open vSwitch with VXLAN support. To use VXLAN tunnels with OVS, please ensure that the version is 1.10 or newer!
2014-05-21 13:53:21.762 1814 TRACE neutron.plugins.openvswitch.agent.ovs_neutron_agent
It seems that the minimum kernel version required to use VXLAN is set to 3.13 (shouldn't 3.9 be enough ?). RHEL7 ships only 3.10. Vxlan module however is present and working, and even if the only module properly working is in 3.13 kernel, the check doesn't take into consideration backported features.","fix openvswitch requirement check

With VXLAN enabled on openvswitch, neutron checks
module version of the openvswitch kernel module.
If the pattern to extract the version matches twice
(eg. for path and version) the agent dies.

This patch ensures, that only the version is checked
against the pattern.

Change-Id: I879624f6b0936cab59e02958ae3a89950df773bb
Closes-Bug: #1322139"
1384,be4102b0577937c0193703ace9ae7591197341de,1385093749,,1.0,7,105,4,3,1,0.77178928,True,11.0,12498678.0,113.0,26.0,False,38.0,340928.25,108.0,3.0,19.0,576.0,595.0,19.0,533.0,552.0,1.0,532.0,533.0,0.001702128,0.453617021,0.454468085,102,501,1255925,cinder,be4102b0577937c0193703ace9ae7591197341de,0,0, Remove unused reservation methods from db.api ,"Bug #1255925 in Cinder: ""unused reservation methods from db api""","There are several unused reservation methods are not called directly, so it could be removed from the db API.
Here are the methods:
* reservation_create https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L695
* reservation_get https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L702
* reservation_get_all_by_project https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L707
* reservation_destroy https://github.com/openstack/cinder/blob/master/cinder/db/api.py#L712","Remove unused reservation methods from db.api

There are several unused reservation methods are not called directly,
so it could be removed from the db API.

closes-bug: #1255925

Change-Id: I8dffa1d4222da162e55ddd6cfce656b4620f3fad"
1385,be5d39eacc84c11f96357a624ffa1cdeda8a568d,1379854238,,1.0,41,7,4,2,1,0.757630382,True,12.0,15202482.0,98.0,44.0,False,42.0,553203.75,100.0,7.0,1220.0,3627.0,3627.0,1106.0,3029.0,3029.0,416.0,2538.0,2538.0,0.068304668,0.415888616,0.415888616,1585,1228847,1228847,nova,be5d39eacc84c11f96357a624ffa1cdeda8a568d,1,1,,"Bug #1228847 in OpenStack Compute (nova): ""VMware: VimException: Exception in __deepcopy__ Method not found""","When an exception occurs in the VMWare driver, for example when there are no more IP addresses available, then the following exception is returned:
2013-09-22 05:26:22.522 ERROR nova.compute.manager [req-b29710eb-5cb9-4de1-adca-919119b10460 demo demo] [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Error: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] Traceback (most recent call last):
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1038, in _build_instance
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     set_access_ip=set_access_ip)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/claims.py"", line 53, in __exit__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     self.abort()
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/compute/claims.py"", line 107, in abort
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     LOG.debug(_(""Aborting claim: %s"") % self, instance=self.instance)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 228, in __mod__
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     return copied._save_parameters(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/openstack/common/gettextutils.py"", line 186, in _save_parameters
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     self.params = copy.deepcopy(other)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 190, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = _reconstruct(x, rv, 1, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 334, in _reconstruct
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     state = deepcopy(state, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 257, in _deepcopy_dict
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y[deepcopy(key, memo)] = deepcopy(value, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 163, in deepcopy
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     y = copier(x, memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/usr/lib/python2.7/copy.py"", line 285, in _deepcopy_inst
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     return x.__deepcopy__(memo)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]   File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 195, in vim_request_handler
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]     _(""Exception in %s "") % (attr_name), excep)
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c] VimException: Exception in __deepcopy__ Method not found: 'VimService.VimPort.__deepcopy__'
2013-09-22 05:26:22.522 TRACE nova.compute.manager [instance: 7f425853-63a9-4d84-8a66-38d6494b9b4c]
2013-09-22 05:37:01.926 DEBUG nova.virt.vmwareapi.driver [req-8c58e23f-3970-4c4c-bfbf-39060c0da3ba demo demo] 'VMwareAPISession' object has no attribute 'vim' from (pid=2206) __del__ /opt/stack/nova/nova/virt/vmwareapi/driver.py:705","VMWare: bug fix for Vim exception handling

The exception handling in the Vim module was too broad and did not treat
the suds.MethodNotFound exception. This would result in a exception
that ""__deepcopy__"" is not supported in the VIM module.

In the case of an exception the VMwareAPISession class may be deleted.
The patch also ensures that the deletion is able to logout if necessary.

Closes-bug: #1228847

Change-Id: Ic85be9b3407c444db4b3a074108c01d9141c61be"
1386,be81901b615b45d6aed4287df8c285a1f0aa72b0,1407409129,,1.0,5,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1160,1617,1353885,neutron,be81901b615b45d6aed4287df8c285a1f0aa72b0,1,0,"API change. “The L2Pop feature seems to have broken due to OVS Concurrent DeferredBridge implementation approved here:
https://review.openstack.org/77578
“","Bug #1353885 in neutron: ""L2Pop on OVS broken due to DeferredBridge introduction""","The L2Pop feature seems to have broken due to OVS Concurrent DeferredBridge implementation approved here:
https://review.openstack.org/77578
On the OVS Agent logs on compute hosts, could see that dynamic add_tunnel_port creation is failing:
2014-08-07 01:05:37.618 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-0561d1e7-87a9-43cf-b7d4-c60e560d34f5 ^[[00;36mNone None^[[01;31m] ^[[01;35m^[[01;31mException during message handling: add_tunnel_port^[[00m
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 45, in add_fdb_entries
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    self.fdb_add(context, fdb_entries)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 347, in fdb_add
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    agent_ports, self.tun_br_ofports)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return method(*args, **kwargs)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/l2population_rpc.py"", line 179, in fdb_add_tun
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    lvm.network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1061, in setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    network_type)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1017, in _setup_tunnel_port
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    ofport = br.add_tunnel_port(port_name,
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 486, in __getattr__
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    raise AttributeError(name)
^[[01;31m2014-08-07 01:05:37.618 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mAttributeError: add_tunnel_port","DeferredBridge to allow add_tunnel_port passthru

The DeferredBridge should allow add_tunnel_port
method as passthrough in addition to add_port
and delete_port. L2Pop uses add_tunnel_port to
dynamically establish tunnel endpoints on cloud
nodes.

Closes-Bug: #1353885

Change-Id: I2e6d82ee79814e500604f4951e7d89eab662047a"
1387,befa0b9184eb0c6248d06efa5b02be8217f1722e,1389013119,1.0,1.0,292,17,9,6,1,0.744409979,True,37.0,19947150.0,524.0,102.0,False,30.0,1558803.889,46.0,8.0,46.0,359.0,382.0,38.0,314.0,329.0,24.0,223.0,226.0,0.03742515,0.335329341,0.339820359,1658,1237427,1237427,neutron,befa0b9184eb0c6248d06efa5b02be8217f1722e,0,0,Bug or feature3 “Implement local ARP responder onto OVS agent“,"Bug #1237427 in neutron: ""l2-pop ML2 mechanism driver need ARP responder with the OVS agent""","The l2-pop mechanism driver uses an ARP responder to avoid ARP broadcast but it only works with the LB agent.
We need to implement an ARP responder for the OVS agent also.","Implement local ARP responder onto OVS agent

With ML2 plugin and l2-pop mechanism driver, it's possible to locally
answer to the ARP request of the VM and avoid ARP broadcasting emulation
on overlay which is costly.

When this functionality is enabled, the OVS flows logic evolves to [1].
This functionality was introduce in 2.1 OVS branch [2].

A README is added to describe l2-pop mechanism driver and the agents
particularities.

[1] https://wiki.openstack.org/wiki/Ovs-flow-logic#OVS_flows_logic_with_local_ARP_responder
[2] http://git.openvswitch.org/cgi-bin/gitweb.cgi?p=openvswitch;a=commitdiff;h=f6c8a6b163af343c66aea54953553d84863835f7

DocImpact: New OVS agent flag 'arp_responder' set to false by default
Closes-Bug: #1237427
Change-Id: Ic28610faf2df6566d8d876fcd7aed333647970e2"
1388,bf322d27a0eaec92a9ed6f42c60c2168456825f8,1393254557,,1.0,11,10,2,2,1,0.702466551,True,5.0,4233241.0,74.0,20.0,False,185.0,1128244.5,925.0,5.0,69.0,3613.0,3646.0,64.0,3030.0,3060.0,69.0,2661.0,2694.0,0.009425071,0.358421974,0.362865221,472,887,1284345,nova,bf322d27a0eaec92a9ed6f42c60c2168456825f8,0,0,Make less calls,"Bug #1284345 in OpenStack Compute (nova): ""Some network API methods unnecessarily trigger multiple get_instance_nw_info() calls""","Network manager methods add_fixed_ip_to_instance() and remove_fixed_ip_from_instance() both return with updated nw_info models. The corresponding network API methods however returns nothing, which has the following effect:
Both API methods have the @refresh_cache decorator that tries to update instance info cache from the decorated method's return value. In absence of a return value, it will make a new rpc call to to get the missing nw_info model. By changing the two API methods so that they return the models that they in fact already get, these extra calls can be avoided altogether.
In addition, having the API methods return updated nw_info models make it possible to further improve as in compute manager, calls to these methods are immediately followed by calls to get updated nw_info.","Make compute manager use network api method return values

Calls to network api methods add_fixed_ip_to_instance() and
remove_fixed_ip_from_instance() return updated nw_info models
so there is no need to make separate calls to get this info.

This change refactors the affected code to avoid such extra
calls. Related methods in the neutron api are updated to return
the same information as their nova-network api counterparts.

Change-Id: I39dcf257ed5c101b13c2243266eed265c64e3c98
Closes-Bug: #1284345"
1389,bf4a0199a73374d786e3a5bda770fd8545ebc4e9,1410763591,,1.0,19,1,2,2,1,0.468995594,False,,,,,True,,,,,,,,,,,,,,,,,1315,1781,1369431,neutron,bf4a0199a73374d786e3a5bda770fd8545ebc4e9,1,1,,"Bug #1369431 in neutron: ""Don't create ipset chain if corresponding security group has  no member""","when a security group has bellow rule, it should not create ipset chain:
security group id is: fake_sgid, it has rule bellow:
{'direction': 'ingress', 'remote_group_id': 'fake_sgid2'}
but the security group:fake_sgid2 has no member, so when the port in security group:fake_sgid should not create corresponding ipset chain
root@devstack:/opt/stack/neutron# ipset list
Name: IPv409040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet hashsize 1024 maxelem 65536
Size in memory: 16520
References: 1
Members:
20.20.20.11
Name: IPv609040f9f-cb86-4f72-a
Type: hash:ip
Revision: 2
Header: family inet6 hashsize 1024 maxelem 65536
Size in memory: 16504
References: 1
Members:
because the security group:09040f9f-cb86-4f72-af74-4de4f2b86442 has no ipv6 member, so it should't create ipset chain:IPv609040f9f-cb86-4f72-a","Don't create unused ipset chain

when a security group don't have members, it should not create corresponding
ipset chain.

Change-Id: Ia04ffb3ac539c9a89a882e6dd91f373cb67c6f8b
Closes-bug: #1369431"
1390,bf5d003af694ea1eb95fceea8c36e961092cb516,1410240733,,1.0,52,68,3,3,1,0.994174043,False,,,,,True,,,,,,,,,,,,,,,,,1243,1702,1362405,nova,bf5d003af694ea1eb95fceea8c36e961092cb516,1,1,"“Commit 77b4012a02c6b4827a7a6b112f35deb9ce94c395 added further
    validation on the quota update controller that isn't aware of the
    'force' parameter. This”","Bug #1362405 in OpenStack Compute (nova): ""'Force' option broken for quota update""","This change broke the ability to force quotas below the current in-use value by adding new validation checks:
https://review.openstack.org/#/c/28232/
$ nova quota-update --force --cores 0 132
ERROR (BadRequest): Quota limit must be greater than 1. (HTTP 400) (Request-ID: req-ff0751a9-9e87-443e-9965-a30768f91d9f)","Fix 'force' parameter for quota-update

Commit 77b4012a02c6b4827a7a6b112f35deb9ce94c395 added further
validation on the quota update controller that isn't aware of the
'force' parameter. This validation also duplicated the existing
validation that *did* honour 'force'.

This change merges the two validation methods and ensures that the
validation is skipped when the 'force' parameter is passed in the
request.

Change-Id: I25e3ce857a83cb50789a2a61bb01c796128ff4f8
Closes-bug: #1362405"
1391,bf8bf46cbbe7428c9a12886604fe5aa10019aaec,1388979112,,1.0,29,1,2,2,1,0.353359335,True,1.0,16869.0,11.0,3.0,False,10.0,776986.0,21.0,3.0,24.0,651.0,652.0,24.0,555.0,556.0,16.0,250.0,251.0,0.02556391,0.377443609,0.378947368,253,657,1266334,neutron,bf8bf46cbbe7428c9a12886604fe5aa10019aaec,0,0, Quota extension is not enabled,"Bug #1266334 in neutron: ""Quota extension is not enabled for bigswitch plugin""",Any quota request made to neutron fails with the bigswitch plugin. Enabling the quota extension will fix this.,"Enables quota extension on BigSwitch plugin

Adds quotas to supported extensions in
BigSwitch plugin and adds db migration script
to create quota tables for BigSwitch plugin.

Closes-Bug: #1266334
Change-Id: I255b318fde792ef59fe2cf456df4be87135c65cd"
1392,bfb66019edd197141ea1462ba78dd27a2ed0e40d,1382645934,,1.0,4,4,2,2,1,1.0,True,1.0,175631.0,8.0,5.0,False,15.0,3176801.5,31.0,5.0,149.0,1358.0,1386.0,147.0,1222.0,1250.0,131.0,947.0,966.0,0.117962466,0.847184987,0.864164433,28,340,1244238,cinder,bfb66019edd197141ea1462ba78dd27a2ed0e40d,1,1,,"Bug #1244238 in Cinder: ""GlusterFS: errors when trying to boot instance from cloned volume""","I am working with gluster as cinder's backend on 2 computes for Havana.
when I try to boot an instance from a cloned volume (cinder create 10 --source-volid f7416ba6-af45-47d3-a333-478447a1ab54 --display-name from_vol1) we get a rootwrap error in volumes log and the instance moves to status ERROR.
more info that might be helpful, I tried booting an instance from a newly created volume and a volume created from image and instance is started correctly.
This error is for cloned from volume only.
[root@cougar06 ~(keystone_admin)]# nova list
+--------------------------------------+------+--------+------------+-------------+----------+
| ID                                   | Name | Status | Task State | Power State | Networks |
+--------------------------------------+------+--------+------------+-------------+----------+
| 4dc50e69-9d84-4b19-b7ec-4bf0628d751b | na   | ERROR  | None       | NOSTATE     |          |
+--------------------------------------+------+--------+------------+-------------+----------+
[root@cougar06 ~(keystone_admin)]#
2013-10-24 16:44:00.413 2483 ERROR cinder.openstack.common.rpc.common [req-26521926-e9cb-4308-aacd-425ba2a1932a a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] ['Traceback (most recent call last):\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection\n    conn_info = self.driver.initialize_connection(volume, connector)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection\n    info = self._qemu_img_info(path)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info\n    info = image_utils.qemu_img_info(path)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info\n    out, err = utils.execute(*cmd, run_as_root=True)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute\n    return processutils.execute(*cmd, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute\n    cmd=\' \'.join(cmd))\n', 'ProcessExecutionError: Unexpected error while running command.\nCommand: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\nExit code: 1\nStdout: \'\'\nStderr: ""Could not open \'/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452\': No such file or directory\\n""\n']
2013-10-24 16:44:04.763 2483 ERROR cinder.openstack.common.rpc.amqp [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Exception during message handling
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 808, in wrapper
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 605, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     conn_info = self.driver.initialize_connection(volume, connector)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 851, in initialize_connection
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     info = self._qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/glusterfs.py"", line 132, in _qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     info = image_utils.qemu_img_info(path)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/image/image_utils.py"", line 191, in qemu_img_info
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     out, err = utils.execute(*cmd, run_as_root=True)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 142, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     return processutils.execute(*cmd, **kwargs)
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp     cmd=' '.join(cmd))
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp ProcessExecutionError: Unexpected error while running command.
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Exit code: 1
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stdout: ''
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n""
2013-10-24 16:44:04.763 2483 TRACE cinder.openstack.common.rpc.amqp
2013-10-24 16:44:04.765 2483 ERROR cinder.openstack.common.rpc.common [req-53eb21ec-fdee-44f4-85fe-37ea71ebf1a1 a660044c9b074450aaa45fba0d641fcc e27aae2598b94dca88cd0408406e0848] Returning exception Unexpected error while running command.
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf env LC_ALL=C LANG=C qemu-img info /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452
Exit code: 1
Stdout: ''
Stderr: ""Could not open '/var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-0466799b-0810-4c69-a894-0f395fe89452': No such file or directory\n"" to caller
the volume does appear as if it exists and available in cinder list
[root@cougar06 ~(keystone_admin)]# cinder list
/usr/lib/python2.6/site-packages/babel/__init__.py:33: UserWarning: Module backports was already imported from /usr/lib64/python2.6/site-packages/backports/__init__.pyc, but /usr/lib/python2.6/site-packages is being added to sys.path
  from pkg_resources import get_distribution, ResolutionError
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
|                  ID                  |   Status  | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
| 0466799b-0810-4c69-a894-0f395fe89452 | available |   from_vol   |  10  |     None    |   true   |             |
| 1e36f3ac-27ef-46ea-b5fa-686c4da9f449 | available |     test     |  10  |     None    |  false   |             |
| 5d658297-5037-4203-9482-b072a2bc7526 | available |  from_vol1   |  10  |     None    |   true   |             |
| f7416ba6-af45-47d3-a333-478447a1ab54 | available |   from_img   |  10  |     None    |   true   |             |
| f9d6b98f-8394-4a01-9424-f23897382d87 | available |    dafna     |  10  |     None    |  false   |             |
+--------------------------------------+-----------+--------------+------+-------------+----------+-------------+
[root@cougar06 ~(keystone_admin)]#
but if I look under mnt its not there:
root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/
conversion/  mnt/         .novaclient/ tmp/
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-
volume-1e36f3ac-27ef-46ea-b5fa-686c4da9f449                                       volume-f9d6b98f-8394-4a01-9424-f23897382d87
volume-f7416ba6-af45-47d3-a333-478447a1ab54                                       volume-f9d6b98f-8394-4a01-9424-f23897382d87.24804bb3-3846-45f8-8f24-5320e6d57184
volume-f7416ba6-af45-47d3-a333-478447a1ab54-clone                                 volume-f9d6b98f-8394-4a01-9424-f23897382d87.73262c7c-e762-44cf-85d4-14693ab0dd31
volume-f7416ba6-af45-47d3-a333-478447a1ab54.info                                  volume-f9d6b98f-8394-4a01-9424-f23897382d87.info
[root@cougar06 ~(keystone_admin)]# ls -l /var/lib/cinder/mnt/792e7ed79ec67a83b6a55e1479a7c82f/volume-^C","GlusterFS: set correct filename when cloning volume

When cloning a volume, the volume[name] field was populated
with incorrect data, resulting in an unexpected filename containing
the volume data.  This results in failures of later operations on
that cloned volume.

Change-Id: I067ed44cebdc8e91e9ded326953fd0c99d003f05
Closes-Bug: #1244238"
1393,bfbc29f19037feb89408206625849bde14a7e84a,1401333383,,1.0,183,20,7,3,1,0.809874941,False,,,,,True,,,,,,,,,,,,,,,,,913,1349,1322379,cinder,bfbc29f19037feb89408206625849bde14a7e84a,1,1,,"Bug #1322379 in Cinder: ""NetApp netapp_server_port config is ignored""","The netapp_server_port setting in cinder.conf is ignored by both the NetApp NFS and iSCSI drivers. Ironically it is marked as a required flag.
NetAppDirectNfsDriver._get_client() creates a client from NaServer but no port argument is passed to init(). NaServer.set_transport_type() sets the port based on the configured transport_type (URI scheme), http=80 and https=443. netapp_server_port appears to be largely unused.
A quick work-around is to set the client port from self.configuration.netapp_server_port in NetAppDirectNfsDriver._get_client() immediately after creating the client. This may not be the cleanest solution but it solves the immediate problem I had.
There is a similar problem in NetAppDirectISCSIDriver._create_client().
It seems to me that a better solution would be to add a port argument to NaServer.init() and fall back to the scheme-based defaults if no arg is supplied. That may have ramifications that I am unaware of.","NetApp fix to set non default server port in api

The non default netapp_server_port config option was not
getting set in api even if specified in cinder.conf. Its
made non mandatory and set if specified in the configuration.

Change-Id: I62943427e0caac2742cce4ce56c1a49917f9210d
Closes-bug: #1322379"
1394,bfdae32efbeffcb74e7b2a0c48cb89cbf4c11329,1409069158,,1.0,12,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1247,1706,1362595,nova,bfdae32efbeffcb74e7b2a0c48cb89cbf4c11329,1,1,,"Bug #1362595 in OpenStack Compute (nova): ""move_vhds_into_sr - invalid cookie""","When moving VHDs on the filesystem a coalesce may be in progress.  The result of this is that the VHD file is not valid when it is copied as it is being actively changed - and the VHD cookie is invalid.
Seen in XenServer CI: http://dd6b71949550285df7dc-dda4e480e005aaa13ec303551d2d8155.r49.cf1.rackcdn.com/36/109836/4/23874/run_tests.log
2014-08-28 12:26:37.538 |     Traceback (most recent call last):
2014-08-28 12:26:37.543 |       File ""tempest/api/compute/servers/test_server_actions.py"", line 251, in test_resize_server_revert
2014-08-28 12:26:37.550 |         self.client.wait_for_server_status(self.server_id, 'VERIFY_RESIZE')
2014-08-28 12:26:37.556 |       File ""tempest/services/compute/json/servers_client.py"", line 179, in wait_for_server_status
2014-08-28 12:26:37.563 |         raise_on_error=raise_on_error)
2014-08-28 12:26:37.570 |       File ""tempest/common/waiters.py"", line 77, in wait_for_server_status
2014-08-28 12:26:37.577 |         server_id=server_id)
2014-08-28 12:26:37.583 |     BuildErrorException: Server e58677ac-dd72-4f10-9615-cb6763f34f50 failed to build and is in ERROR status
2014-08-28 12:26:37.589 |     Details: {u'message': u'[\'XENAPI_PLUGIN_FAILURE\', \'move_vhds_into_sr\', \'Exception\', ""VDI \'/var/run/sr-mount/16f5c980-eeb6-0fd3-e9b1-dec616309984/os-images/instancee58677ac-dd72-4f10-9615-cb6763f34f50/535cd7f2-80a5-463a-935c-9c4f52ba0ecf.vhd\' has an invalid footer: \' invalid cook', u'code': 500, u'created': u'2014-08-28T11:57:01Z'}","XenAPI: run vhd-util repair if VHD check fails

We can hit issues with corrupted VHDs if we copy a VHD while XenServer
is performing other operations. This happens because there are times
when we copy the VHD chains while XenServer is still performing a
coalesce of the VHD chain.

In most cases, vhd-util should be able to safely repair any metadata
corruption. It can copy the copy of the VHD footer at the front of the
VHD file and add it at the bottom on the VHD file. There is no VM data
loss, due to the way the coalesce happens, but the chain will be bigger
than it would be both before and after the coalesce.

This does not, however, ensure that snapshots are valid before uploading
them to glance. But should you launch a corrupted snapshot, this change
would fix up the snapshot, and allow it to boot correctly.

Closes-Bug: #1362595

Change-Id: I88b737d7e97964a9db5ccf2c39dea7fd0701ead4"
1395,bfdec043f1429ac4aa884e9422861b4e6c1ca815,1398464954,,1.0,21,14,2,2,1,0.951762676,True,1.0,1162433.0,23.0,6.0,False,4.0,103837.0,8.0,4.0,322.0,1917.0,1997.0,299.0,1599.0,1658.0,254.0,964.0,1003.0,0.215189873,0.814345992,0.847257384,837,1266,1312822,neutron,bfdec043f1429ac4aa884e9422861b4e6c1ca815,1,0,“NSX 4.2 GA has tweaked the naming for certain resources. Edge”,"Bug #1312822 in neutron: ""NSX: fix API mappings""","NSX: change api mapping for Service Cluster to Edge Cluster
NSX 4.2 GA has tweaked the naming for certain resources. Edge
Cluster vs Service CLuster is one of them.","NSX: fix API payloads for dhcp/metadata setup

NSX 4.2 GA has tweaked the way dhcp and metadata
ports are configured. This patch takes care of
that.

Closes-bug: #1312822

Change-Id: I4e8496bf721c0f35df4228ec9c6f8066275e8c7a"
1396,c011b7668f83b25f1b8e026a909243af96f8f85c,1386236071,,1.0,23,1,2,2,1,0.41381685,True,4.0,437545.0,30.0,14.0,False,3.0,4953030.0,3.0,3.0,0.0,1286.0,1286.0,0.0,1137.0,1137.0,0.0,495.0,495.0,0.001647446,0.817133443,0.817133443,147,549,1258065,neutron,c011b7668f83b25f1b8e026a909243af96f8f85c,1,1,,"Bug #1258065 in neutron: ""The implementation of utils.str2dict fails to convert a dict with more than 2 elements""","from neutron.common import utils
print utils.str2dict('inside_addr=10.0.1.2,inside_port=22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp')
returns
{'inside_addr': '10.0.1.2', 'inside_port': '22,outside_addr=172.16.0.1,outside_port=2222,protocol=tcp'}
expected value should be
{'outside_port': '2222', 'inside_addr': '10.0.1.2', 'protocol': 'tcp', 'inside_port': '22', 'outside_addr': '172.16.0.1'}
The reason is that in the third line of the implementation below,  string.split(',', 1) only splits out two key-value pairs.
quote from neutron/common/utils.py:181:
def str2dict(string):
    res_dict = {}
    for keyvalue in string.split(',', 1):
        (key, value) = keyvalue.split('=', 1)
        res_dict[key] = value
    return res_dict
a quick fix might be remove "",1"" from string.split. But it turns out that str2dict/dict2str may also fail when input values containing characters like '=' or ','. A better fix might be using json encode/decode to deal with it.","Fix str2dict and dict2str's incorrect behavior

Closes-bug: #1258065

Change-Id: Idf14c077eeeda6f18f534ad8f62cd741d0c0a2eb"
1397,c04709b127d6f2dea32c5faa37cc1e27f6792cd9,1404726823,,1.0,0,15,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,970,1409,1329156,cinder,c04709b127d6f2dea32c5faa37cc1e27f6792cd9,0,0,“Remove cinder.context warning logging”,"Bug #1329156 in Cinder: ""Lots of cinder.context warning in scheduler log""","There have been quite a few warning log in cinder scheduler log, something like:
2014-06-11 09:41:39.441 3074 WARNING cinder.context [-] Arguments dropped when creating context: {'user': u'1284c7ced03d4b17830b0c1911b0cfb2', 'tenant': u'a28cee075f3f49528afd48a79646533a', 'user_identity': u'1284c7ced03d4b17830b0c1911b0cfb2 a28cee075f3f49528afd48a79646533a - - -'}
http://logs.openstack.org/25/77125/8/check/check-tempest-dsvm-full/bd92be7/logs/screen-c-sch.txt.gz?level=WARNING#_2014-06-11_09_26_45_485
It's quite annoy and not really helpful.","Remove cinder.context warning logging

The code removed creates a large volume of log entries that are of
limited use. Removing them should improve the readability of the logs.

Change-Id: I2a7f20cfa6cd57aa7ae05bf9685947207cbb9502
Closes-Bug: #1329156"
1398,c08211b19e2b9c38e537e155d11ffced37af0e85,1399338334,,1.0,11,1,2,2,1,0.650022422,False,,,,,True,,,,,,,,,,,,,,,,,917,1353,1322958,cinder,c08211b19e2b9c38e537e155d11ffced37af0e85,1,1,,"Bug #1322958 in Cinder: ""UnboundLocalError: local variable 'volume' referenced before assignment""","When the driver was not initialized, I uploaded a volume as  image into glance, I got some error as follow:
[01;31mException during message handling: local variable 'volume' referenced before assignment^[[00m
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 721, in copy_volume_to_image
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    if (volume['instance_uuid'] is None and
^[[01;31m2014-05-25 00:09:40.599 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mUnboundLocalError: local variable 'volume' referenced before assignment
As we can see the code from cinder/volume/manager.py,  the method require_driver_initialized occurs exception so that the method volume_get  will be not executed,  and the variable 'volume' is not defined.
 def copy_volume_to_image(self, context, volume_id, image_meta):
        payload = {'volume_id': volume_id, 'image_id': image_meta['id']}
        try:
            # NOTE(flaper87): Verify the driver is enabled
            # before going forward. The exception will be caught
            # and the volume status updated.
            utils.require_driver_initialized(self.driver)
            volume = self.db.volume_get(context, volume_id)
            image_service, image_id = \
                glance.get_remote_image_service(context, image_meta['id'])
            self.driver.copy_volume_to_image(context, volume, image_service,
                                             image_meta)
            LOG.debug(_(""Uploaded volume %(volume_id)s to ""
                        ""image (%(image_id)s) successfully""),
                      {'volume_id': volume_id, 'image_id': image_id})
        except Exception as error:
            with excutils.save_and_reraise_exception():
                payload['message'] = unicode(error)
        finally:
            if (volume['instance_uuid'] is None and
                    volume['attached_host'] is None):
                self.db.volume_update(context, volume_id,
                                      {'status': 'available'})
            else:
                self.db.volume_update(context, volume_id,
                                      {'status': 'in-use'})","Add exception handling for copy_volume_to_image()

The method require_driver_initialized occurs exception so that the
method volume_get will be not executed, and the variable 'volume'
will be not defined. In the ""finally"" code segment, the
volume['instance_uuid'] will get the UnboundLocalError, this patch
gets volume before require_driver_initialized to avoid the above
problem.

Change-Id: Ic8167383eb67c5016c9853da274455e0f202dc4d
Closes-Bug: #1322958"
1399,c0c3c8361771091c1ef1d4906c2552f9d92e7715,1400748257,,1.0,45,3,3,3,1,0.415373501,False,,,,,True,,,,,,,,,,,,,,,,,909,1345,1322105,neutron,c0c3c8361771091c1ef1d4906c2552f9d92e7715,1,1,,"Bug #1322105 in neutron: ""NVP FWaaS occurs error when removing a rule which is shared by two firewalls""","Bugs reproduce process:
1. create a firewall rule and attache it to a firewall policy
2. create two firewalls with the firewall policy attached alternatively on two routers
3. remove the firewall rule from the firewall policy
it would occur the following error:
 Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/api/v2/resource.py"", line 87, in resource
    result = method(request=request, **args)
  File ""/home/stack/neutron/neutron/api/v2/base.py"", line 201, in _handle_action
    return getattr(self._plugin, name)(*arg_list, **kwargs)
  File ""/home/stack/neutron/neutron/plugins/vmware/plugins/service.py"", line 1077, in remove_rule
    context, fwr['id'], edge_id)
  File ""/home/stack/neutron/neutron/plugins/vmware/vshield/edge_firewall_driver.py"", line 270, in delete_firewall_rule
    vcns_rule_id = rule_map.rule_vseid
AttributeError: 'NoneType' object has no attribute 'rule_vseid'
2014-05-22 16:21:22,244     INFO [neutron.plugins.vmware.vshield.tasks.tasks] TaskManager terminated
}}}
Traceback (most recent call last):
  File ""/home/stack/neutron/neutron/tests/unit/vmware/vshield/test_fwaas_plugin.py"", line 650, in test_remove_rule_with_firewalls
    expected_body=attrs)
  File ""/home/stack/neutron/neutron/tests/unit/db/firewall/test_db_firewall.py"", line 295, in _rule_action
    self.assertEqual(res.status_int, expected_code)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 322, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""/home/stack/neutron/.venv/local/lib/python2.7/site-packages/testtools/testcase.py"", line 412, in assertThat
    raise MismatchError(matchee, matcher, mismatch, verbose)
MismatchError: 500 != 200
It is because when deleting the corresponding vcns_edge_firewallrule_binding entry, it query based on id instead of (edge_id, id) which leads to deleting the other rule_binding entry.","Fix NVP FWaaS occurs error when deleting a shared rule

It is due to the lack of edge_id query index  when deleting the
vcns_edge_firewallrule_binding entry, which leads to deleting
rule binding entrys on both edges.
Closes-Bug: #1322105

Change-Id: I1d15a06bd2a6c42f29c8258d4b9344c8e9338638"
1400,c0e1d5bd2944a011c0cbaba61d284aedec086b60,1392991420,0.0,1.0,89,24,3,2,1,0.895735243,True,8.0,11961084.0,78.0,21.0,False,38.0,461616.0,82.0,2.0,14.0,828.0,830.0,14.0,567.0,569.0,14.0,597.0,599.0,0.012886598,0.513745704,0.515463918,20,332,1243704,glance,c0e1d5bd2944a011c0cbaba61d284aedec086b60,0,0,it becomes easier to log the error message – feature,"Bug #1243704 in Glance: ""Log image_id with all BadStoreURI error messages""","In the following migration scripts, not all BadStoreUri error messages are logged with image_id. Hence it might become difficult to track the record which has got BadStoreUri.  Including the image_id with the error log messages would help in tracking the faulty records easily when BadStoreUri exception is raised.
015_quote_swift_credentials.py
017_quote_encrypted_swift_credentials.py","Log 'image_id' with all BadStoreURI error messages

- handle all BadStoreUri exceptions raised by 'legacy_parse_uri'
  function in
one location so that it becomes easier to log the error message with
'image_id'
- add tests for 'legacy_parse_uri'

Closes-Bug: #1243704

Change-Id: Ifc5de11832860ed51c1eb359d6f5cf78de8c0ba4"
1401,c0e546ace478277cc2911b1c5bef1e082b76b546,1382743352,,1.0,29,4,4,4,1,0.631379067,True,4.0,727990.0,32.0,16.0,False,88.0,55119.0,145.0,5.0,1.0,2199.0,2200.0,1.0,1863.0,1864.0,1.0,2041.0,2042.0,0.000310463,0.316982304,0.317137535,32,344,1244829,nova,c0e546ace478277cc2911b1c5bef1e082b76b546,1,1,"Broken method, I dont see a evolution context","Bug #1244829 in OpenStack Compute (nova): ""destroy() method broken on Docker virt driver""","Just saw that virt drivers now take an extra arg ""context"" on the destroy method. For some reason, it has not been added to the docker driver... The destroy method with the driver enabled currently fails with the following error:
2013-10-25 23:03:20.764 ERROR nova.openstack.common.rpc.amqp [req-75ff872a-fd4d-4b63-a587-93b9fd3ede4b demo demo] Exception during message handling
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 353, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 243, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 229, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 294, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 271, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 258, in decorated_function
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1792, in terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     do_terminate_instance(instance, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1784, in do_terminate_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     reservations=reservations)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/hooks.py"", line 105, in inner
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     rv = f(*args, **kwargs)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1757, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     user_id=user_id)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1729, in _delete_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     self._shutdown_instance(context, db_inst, bdms)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1662, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     requested_networks)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 1652, in _shutdown_instance
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp     context=context)
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp TypeError: destroy() got an unexpected keyword argument 'context'
2013-10-25 23:03:20.764 TRACE nova.openstack.common.rpc.amqp","Fixes the destroy() method for the Docker virt driver

* Added the missing ""context"" argument in nova/virt/driver.py
* Added the missing argument in the test_virt_drivers.destroy() as well so it
will make sure none of the other drivers will miss it.

Change-Id: I083b935da95c28ad4f9879036ef2bf7c1fbbb58e
Closes-Bug: #1244829"
1402,c15a794a832e0453075d363df949684c0fb86657,1387185468,,1.0,16,1,2,2,1,0.997502546,True,2.0,4418860.0,48.0,26.0,False,6.0,349466.0,15.0,12.0,182.0,2327.0,2342.0,180.0,2073.0,2088.0,141.0,577.0,583.0,0.224328594,0.913112164,0.922590837,183,586,1260682,neutron,c15a794a832e0453075d363df949684c0fb86657,0,0,Handle a new exception,"Bug #1260682 in neutron: ""LBaaS: stacktraces in q-svc while running tempest lbaas api tests""","Followin stacktraces appeared after merging https://review.openstack.org/#/c/40381:
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     context, obj_id['monitor_id'], obj_id['pool_id'], status)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     assoc = self._get_pool_health_monitor(context, id, pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp     monitor_id=id, pool_id=pool_id)
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c
2013-12-12 14:34:39.961 6522 TRACE neutron.openstack.common.rpc.amqp
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] Returning exception Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c to caller
2013-12-12 14:34:39.963 6522 ERROR neutron.openstack.common.rpc.common [req-bc48bc7a-b9dd-429f-a522-ab6ff56adfc9 None None] ['Traceback (most recent call last):\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data\n    **args)\n', '  File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch\n    neutron_ctxt, version, method, namespace, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 171, in update_status\n    context, obj_id[\'monitor_id\'], obj_id[\'pool_id\'], status)\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 651, in update_pool_health_monitor\n    assoc = self._get_pool_health_monitor(context, id, pool_id)\n', '  File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 635, in _get_pool_health_monitor\n    monitor_id=id, pool_id=pool_id)\n', 'PoolMonitorAssociationNotFound: Monitor 7cea505d-d5cb-4d3f-958a-6dd14edb65e1 is not associated with Pool 2ce72926-fcbb-442e-b9e6-7724ec6c472c\n']
2013-12-12 14:34:40.265 6522 ERROR neutron.openstack.common.rpc.amqp [-] Exception during message handling
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp Traceback (most recent call last):
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/amqp.py"", line 438, in _process_data
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     **args)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/common/rpc.py"", line 45, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     neutron_ctxt, version, method, namespace, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/services/loadbalancer/drivers/haproxy/plugin_driver.py"", line 174, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     context, model_mapping[obj_type], obj_id, status)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 194, in update_status
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     v_db = self._get_resource(context, model, id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp   File ""/opt/stack/new/neutron/neutron/db/loadbalancer/loadbalancer_db.py"", line 212, in _get_resource
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp     raise loadbalancer.MemberNotFound(member_id=id)
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp MemberNotFound: Member 21f6ce73-fff8-4a7e-b798-527a4bb64b92 could not be found
2013-12-12 14:34:40.265 6522 TRACE neutron.openstack.common.rpc.amqp","LBaaS: handle NotFound exceptions in update_status callback

LBaaS agent may send update_status requests to server on objects
which were already deleted from db: this is due to creating and
deleting objects with a high rate (like tempest API tests do).
As a result errors and stacktraces appear in server and agent logs.
The proposed solution is to catch NotFound exceptions and print a warning.

Change-Id: I4446b678893d0bda578ad7ccdf3f109cb1c91b4d
Closes-Bug: #1260682"
1403,c167db9536c2d1ad22ac7463d448d092ca5b530e,1389395694,,1.0,3,2,1,1,1,0.0,True,1.0,1469061.0,25.0,7.0,False,16.0,14102617.0,22.0,3.0,925.0,2432.0,3106.0,759.0,1830.0,2389.0,863.0,2226.0,2856.0,0.123217342,0.317598403,0.407444381,261,665,1266620,nova,c167db9536c2d1ad22ac7463d448d092ca5b530e,1,1,Wrong message,"Bug #1266620 in OpenStack Compute (nova): ""bad english in string: ""Disabled reason contains invalid characters or is too long""""","This string: ""Disabled reason contains invalid characters or is too long""
found in
nova/api/openstack/compute/contrib/services.py:177, nova/api/openstack/compute/plugins/v3/services.py:159
should be fixed.
From the spanish translation team:
Se recomienda que esta cadena sea reestructurada en su versión original, particularmente ""Disabled reason"".
Essentially, the string isn't the best English and should be re-written so it can be translated properly - particularly the use of ""Disabled reason"".","Improve error message in services API

The awkward grammar of the original error message was making i18n
translations hard.

Change-Id: I2ff98f2d3585d5f1ccf0deb552e73c452393f124
Closes-Bug: #1266620"
1404,c17253bde86f62952351d42db4dba926b33269db,1380977984,,1.0,26,21,1,1,1,0.0,True,2.0,199013.0,7.0,2.0,False,7.0,1308304.0,9.0,2.0,1171.0,787.0,1913.0,854.0,766.0,1577.0,965.0,760.0,1682.0,0.155280502,0.1223276,0.270535284,1640,1235399,1235399,nova,c17253bde86f62952351d42db4dba926b33269db,0,0,Test files,"Bug #1235399 in OpenStack Compute (nova): ""test_cell_update_without_type_specified test fails sporadically""","The test_cell_update_without_type_specified test fails sporadically:
======================================================================
2013-10-04 18:17:30.357 | FAIL: nova.tests.api.openstack.compute.contrib.test_cells.CellsTest.test_cell_update_without_type_specified
2013-10-04 18:17:30.358 | tags: worker-3
2013-10-04 18:17:30.358 | ----------------------------------------------------------------------
2013-10-04 18:17:30.358 | Empty attachments:
2013-10-04 18:17:30.359 |   pythonlogging:''
2013-10-04 18:17:30.359 |   stderr
2013-10-04 18:17:30.359 |   stdout
2013-10-04 18:17:30.360 |
2013-10-04 18:17:30.360 | Traceback (most recent call last):
2013-10-04 18:17:30.360 |   File ""nova/tests/api/openstack/compute/contrib/test_cells.py"", line 290, in test_cell_update_without_type_specified
2013-10-04 18:17:30.360 |     self.assertEqual(cell['type'], 'parent')
2013-10-04 18:17:30.361 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-04 18:17:30.361 |     self.assertThat(observed, matcher, message)
2013-10-04 18:17:30.361 |   File ""/home/jenkins/workspace/gate-nova-python26/.tox/py26/lib/python2.6/site-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-04 18:17:30.362 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-04 18:17:30.362 | MismatchError: 'child' != 'parent'
2013-10-04 18:17:30.362 | ======================================================================
More info:
https://jenkins02.openstack.org/job/gate-nova-python26/6256/console
http://logstash.openstack.org/#eyJzZWFyY2giOiJAbWVzc2FnZTpcIk1pc21hdGNoRXJyb3I6ICdjaGlsZCcgIT0gJ3BhcmVudCdcIiIsImZpZWxkcyI6W10sIm9mZnNldCI6MCwidGltZWZyYW1lIjoiOTAwIiwiZ3JhcGhtb2RlIjoiY291bnQiLCJ0aW1lIjp7InVzZXJfaW50ZXJ2YWwiOjB9LCJzdGFtcCI6MTM4MDkxMTY5ODYzMH0=","Fix races in v3 cells extension tests

A global was used for a cell list and tests would update this global
and fail depending on order run, etc.

Yes, this is the same fix as commit bb2c3c5 except for the v3
extension. Yes, copied and pasted code is utterly evil. No, we
apparently will never learn.

Closes-Bug: #1235399
Change-Id: I3406d0440aed38cbf3a9bedf24420fb867876b08"
1405,c1827a074654d2e8d07e93945fa605ad3b6c18fb,1397452651,,1.0,52,3,2,2,1,0.994030211,True,3.0,2983467.0,46.0,9.0,False,9.0,4972.0,15.0,2.0,10.0,251.0,253.0,10.0,251.0,253.0,10.0,251.0,253.0,0.009666081,0.221441125,0.223198594,815,1243,1309286,neutron,c1827a074654d2e8d07e93945fa605ad3b6c18fb,1,1,,"Bug #1309286 in neutron: ""OFAgent: Apply the OVS's patch ""Fixing lost vlan ids on interfaces""""","Port the patch for the following bug of OVS to OFAgent.
https://bugs.launchpad.net/nova/+bug/1240849","OFAgent: Fixing lost vlan ids on interfaces

Port the following patch to OFAgent.
commit: 60cb0911712ad11688b4d09e5c01ac39c49f5aea
https://review.openstack.org/#/c/66375/

Closes-Bug: #1309286

Change-Id: I04f99ef7928cd07959b29eb64b5744df397979b8"
1406,c199a14039ddcde63896456ee25db68da1afcc8f,1389105838,,1.0,4,4,1,1,1,0.0,True,3.0,2567881.0,35.0,16.0,False,5.0,4157715.0,6.0,7.0,18.0,2766.0,2770.0,18.0,2052.0,2056.0,15.0,2501.0,2503.0,0.002292921,0.35855546,0.358842075,262,666,1266634,nova,c199a14039ddcde63896456ee25db68da1afcc8f,0,0,tests,"Bug #1266634 in OpenStack Compute (nova): ""unnecessary parameter is found in quota-classes request body""","Nova quota-classes request should be in the format like:
body = {'quota_class_set': {'instances': 50, 'cores': 50,
                            'ram': 51200, 'floating_ips': 10,
                            'fixed_ips': -1, 'metadata_items': 128,
                            'security_groups': 10,
                            'security_group_rules': 20,
                            'key_pairs': 100,
                            }}
But in some unit test cases, the id parameter is found in the body, it should be deleted.
 def test_quotas_update_as_admin(self):
     body = {'quota_class_set': {'instances': 50, 'cores': 50,
                                 'ram': 51200, 'floating_ips': 10,
                                 'fixed_ips': -1, 'metadata_items': 128,
                                 ★'id': 'test_class', ★
                                 'security_groups': 10,
                                 'security_group_rules': 20,
                                 'key_pairs': 100}}","Fix test case with wrong parameter in test_quota_classes

The 'id' parameter is filled in the quota-classes request body in
unit test, but it is not necessary. 'id' is passed to the server
by the URL but not the body. This patch fixes this bug.

Change-Id: I4014264e16031382277f6f1b520556a2a2690ad8
Closes-Bug: #1266634"
1407,c19ea390ecbe045728444d49f40920329a1d0743,1393712078,,1.0,5,4,2,2,1,0.764204507,True,1.0,1397040.0,20.0,3.0,False,17.0,1518886.0,24.0,4.0,0.0,2864.0,2864.0,0.0,2408.0,2408.0,0.0,2756.0,2756.0,0.00013369,0.368582888,0.368582888,1535,1223309,1223309,nova,c19ea390ecbe045728444d49f40920329a1d0743,1,1,“Added os-security-groups prefix”,"Bug #1223309 in OpenStack Compute (nova): ""v3 security groups's attribute without prefix in create's response""","Both for xml and json:
{
    ""server"": {
        ""admin_pass"": ""%(password)s"",
        ""id"": ""%(id)s"",
        ""links"": [
            {
                ""href"": ""http://openstack.example.com/v3/servers/%(uuid)s"",
                ""rel"": ""self""
            },
            {
                ""href"": ""http://openstack.example.com/servers/%(uuid)s"",
                ""rel"": ""bookmark""
            }
        ],
        ""security_groups"": [{""name"": ""test""}]
    }
}
<?xml version='1.0' encoding='UTF-8'?>
<server xmlns:atom=""http://www.w3.org/2005/Atom"" xmlns=""http://docs.openstack.org/compute/api/v1.1"" id=""%(id)s"" admin_pass=""%(password)s"">
  <metadata/>
  <atom:link href=""%(host)s/v3/servers/%(uuid)s"" rel=""self""/>
  <atom:link href=""%(host)s/servers/%(uuid)s"" rel=""bookmark""/>
  <security_groups>
   <security_group name=""test"" />
  </security_groups>
</server>
'security_groups' should be 'os-security-groups:security_groups'","Added os-security-groups prefix

Added os-security-groups prefix to the security_groups attribute in json response.

Change-Id: Id2a4a92da0c9751abc5c37f06fc2484ba80a8a09
Closes-Bug: #1223309"
1408,c1a64c94ef848f825e6a36581814511a81945428,1392398555,,1.0,8,6,1,1,1,0.0,True,3.0,1606741.0,27.0,7.0,False,30.0,3610.0,44.0,4.0,84.0,2685.0,2746.0,70.0,2252.0,2302.0,69.0,2225.0,2271.0,0.009508286,0.302363488,0.30861179,413,828,1280379,nova,c1a64c94ef848f825e6a36581814511a81945428,0,0,tests,"Bug #1280379 in OpenStack Compute (nova): ""VolumeOpsTestCase class is incorrectly inheriting from HyperVAPITestCase""","The VolumeOpsTestCase class in test_hypervapi.py inherits from HyperVAPITestCase, resulting in all the tests from the base class being executed twice.
The patch that introduced the VolumeOpsTestCase is here:
https://github.com/openstack/nova/commit/d143540ad1b69ec93c2b7bfadd1f654c4d8c7a34","Fixes the Hyper-V VolumeOpsTestCase base class

The VolumeOpsTestCase base class inherits incorrectly from
HyperVAPITestCase, with the unwanted side effect of executing all
base class tests as well.

This patch generalizes a common base class from HyperVAPITestCase
and fixes the inheritance issue.

Closes-Bug: #1280379

Change-Id: Ic81beb8397138c16c8d2d88a9c9d05d2b76e0e83"
1409,c1be8381e19390b0edca28c8ab3f77e6226f25e0,1384966299,0.0,1.0,6,2,1,1,1,0.0,True,6.0,7015980.0,48.0,20.0,False,14.0,2443120.0,22.0,5.0,1.0,3438.0,3439.0,1.0,2687.0,2688.0,1.0,3122.0,3123.0,0.000301205,0.470331325,0.470481928,1779,1252693,1252693,nova,c1be8381e19390b0edca28c8ab3f77e6226f25e0,1,0,“SQLite doesn't provide a native implementation of BOOLEAN data type.”,"Bug #1252693 in OpenStack Compute (nova): ""Running of unit tests fails if SQLAlchemy >= 0.8.3 is used""","If SQLAlchemy >= 0.8.3 is used, running of unit tests fails. The following error is printed to stderr multiple times:
DBError: (IntegrityError) constraint failed u'UPDATE instance_system_metadata SET   updated_at=?, deleted_at=?, deleted=? WHERE instance_system_metadata.id = ?' ('2013-  11-19 10:37:44.378444', '2013-11-19 10:37:44.377819', 11, 11)
A few of our migrations change the type of deleted column from boolean to int. MySQL and SQLite don't have native boolean data type. SQLAlchemy uses int columns (e.g. in case of MySQL - tinyint) + CHECK constraint (something like CHECK (deleted in (0, 1))) to emulate boolean data type.
In our migrations when the type of column `deleted` is changed from boolean to int, the corresponding CHECK constraint is dropped too. But starting from SQLAlchemy version 0.8.3, those CHECK constraints aren't dropped anymore. So despite the fact that column deleted is of type int now, we still restrict its values to be either 0 or 1.
Migrations changing the data type of deleted columns rely on SQL rendered for CHECK constraints (e.g. https://github.com/openstack/nova/blob/master/nova/db/sqlalchemy/migrate_repo/versions/152_change_type_of_deleted_column.py#L172). There was a patch in SQLAlchemy 0.8.3 release that slightly changed the way DDL statements are rendered ((http://docs.sqlalchemy.org/en/latest/changelog/changelog_08.html#change-487183f04e6da9aa27d8817bca9906d1)). Unfortunately, due to the fact that nova migrations depend on such implementation details, this change to SQLAlchemy broke our code.
We must fix our migrations to work properly with new SQLAlchemy versions too (0.8.3+).","Fix migrations changing the type of deleted column

SQLite doesn't provide a native implementation of BOOLEAN data type.
SQLAlchemy emulates BOOLEAN data type for SQLite using INT column +
CHECK constraint.
We have a few migrations changing the type of column 'deleted' from
BOOLEAN to INT. Due to limitations of ALTER in SQLite, in order to do
that, we omit the original 'deleted' column as well as the corresponding
CHECK constraint when recreating the table. Omitting of the constraint
is more tricky. It's implemented by analyzing the SQL text used for its
rendering. SQLAlchemy versions 0.8.3+ slightly changed the way
constraints are rendered, so we have to update our migrations changing
the type of 'deleted' column to work with SQLAlchemy 0.8.3+ on SQLite
backend (we aren't using SQLite in production, but we still need it for
running of unit tests).

Closes-Bug: #1252693
Change-Id: I52f2b5f90a2f9191767fadc7b1eacae236c30e98"
1410,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1398531573,,2.0,4,18,1,1,1,0.0,True,4.0,1896320.0,29.0,10.0,False,6.0,10585349.0,5.0,3.0,19.0,57.0,74.0,19.0,57.0,74.0,4.0,16.0,19.0,0.00313087,0.010644959,0.012523482,838,1267,1313116,cinder,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1,0,“update version response to remove PDF and WADL links”,"Bug #1313116 in Cinder: ""cinder: update version response to remove PDF and WADL links""","The links in the GET /v2 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v2/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}","Fix broken version responses

The version responses link to non-existing PDF and WADL files.
Remove the WADL link and replace the PDF link with a HTML link to
docs.openstack.org.

Change-Id: Id514caee32df430b616609144f52d1a58c93d9cd
Closes-Bug: #1313118
Closes-Bug: #1313116"
1411,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1398531573,,2.0,4,18,1,1,1,0.0,True,4.0,1896320.0,29.0,10.0,False,6.0,10585349.0,5.0,3.0,19.0,57.0,74.0,19.0,57.0,74.0,4.0,16.0,19.0,0.00313087,0.010644959,0.012523482,839,1268,1313118,cinder,c1c10205423db25bc6b69e0e23b4f56aa2dfbd25,1,0,,"Bug #1313118 in Cinder: ""cinder v1: links in versions response don't work""","The links in the GET /v1 response do not work and should be removed. Replace the PDF link with a link to docs.openstack.org.
{
   ""version"":{
      ""status"":""CURRENT"",
      ""updated"":""2012-01-04T11:33:21Z"",
      ""media-types"":[
         {
            ""base"":""application/xml"",
            ""type"":""application/vnd.openstack.volume+xml;version=1""
         },
         {
            ""base"":""application/json"",
            ""type"":""application/vnd.openstack.volume+json;version=1""
         }
      ],
      ""id"":""v1.0"",
      ""links"":[
         {
            ""href"":""http://23.253.228.211:8776/v1/"",
            ""rel"":""self""
         },
         {
            ""href"":""http://jorgew.github.com/block-storage-api/content/os-block-storage-1.0.pdf"",
            ""type"":""application/pdf"",
            ""rel"":""describedby""
         },
         {
            ""href"":""http://docs.rackspacecloud.com/servers/api/v1.1/application.wadl"",
            ""type"":""application/vnd.sun.wadl+xml"",
            ""rel"":""describedby""
         }
      ]
   }
}","Fix broken version responses

The version responses link to non-existing PDF and WADL files.
Remove the WADL link and replace the PDF link with a HTML link to
docs.openstack.org.

Change-Id: Id514caee32df430b616609144f52d1a58c93d9cd
Closes-Bug: #1313118
Closes-Bug: #1313116"
1412,c1c72d1694927e4dea1c6f09bd0cea67c2643e7b,1387792623,,1.0,16,2,2,2,1,0.852405179,True,9.0,2562305.0,68.0,26.0,False,32.0,97215.0,58.0,2.0,6.0,222.0,228.0,6.0,222.0,228.0,0.0,189.0,189.0,0.000925069,0.175763182,0.175763182,218,621,1263647,glance,c1c72d1694927e4dea1c6f09bd0cea67c2643e7b,1,1, Fix the incorrect log message when creating images ,"Bug #1263647 in Glance: ""log image id incorectly when creating image ""","when I use ""nova image-create my_server_id  name"" to create an image, the log /var/log/glance/api.log record the below texts:
<glance.common.wsgi.Resource object at 0x38a7150>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-23 20:24:19.168 20884 INFO glance.registry.api.v1.images [0f0f1d19-9d63-4f2a-88d1-f48c0e28f6f4 e0f87f6761614850b064f9717ac83a36 3b520afd03d94532b64fef4d863230f6] Successfully created image None
but the image id in DB is not None but an uuid.","Fix the incorrect log message when creating images

When using ""nova image-create my_server_id name"" to create an image, the
glance api log image id is incorrect in the method create().

Change-Id: I938279526ed5adf5b95ec5f67e78c6465f890898
Closes-bug: #1263647"
1413,c1eb61b98a37a10775affa845185388f05e3ceb4,1391555072,,1.0,1,4,2,2,1,0.721928095,True,2.0,1484074.0,38.0,7.0,False,18.0,594620.0,20.0,6.0,2.0,1190.0,1190.0,2.0,1055.0,1055.0,2.0,649.0,649.0,0.003968254,0.85978836,0.85978836,1821,1276367,1276367,Neutron,c1eb61b98a37a10775affa845185388f05e3ceb4,0,0,"""Removes an incorrect and unnecessary return""-- There is a comment later that said ""We're going to revert this""","Bug #1276367 in neutron: ""Unnecessary return statement in ovs_lib  ""","[openstack-dev] [neutron] unnecessary return statement in ovs_lib
Thu Jan 16 02:48:46 UTC 2014
Came across the following issue while looking at ovs_lib [1]:
The BaseOVS class has the add_bridge() method which after creating an OVS
bridge, returns an OVSBridge object. BaseOVS class is only used by
OVSBridge defined in the same file. OVSBridge has a create() method that
calls the add_bridge() nethod mentioned earlier but do not use the return
value. (See the methods add_bridge and create below.)
What seems odd is the return statement at the end of add_bridge() which is
not used anywhere and doesn't make much sense as far as I can see but I may
be missing something. The OVSBase is never directly used anywhere in
Neutron directory. Of course the return does not do any harm beyond
creating an unused object but it looks to me that it should be removed
unless there is a good reason (or a potential future use case) for it.
class BaseOVS(object):
        ...
    def add_bridge(self, bridge_name):
        self.run_vsctl([""--"", ""--may-exist"", ""add-br"", bridge_name])
        return OVSBridge(bridge_name, self.root_helper)
class OVSBridge(BaseOVS):
        ...
    def create(self):
        self.add_bridge(self.br_name)
[1]
https://github.com/openstack/neutron/blob/master/neutron/agent/linux/ovs_lib.py","Removes an incorrect and unnecessary return

The current return statement creates a new object that is
not used anywhere and does not provide a functionality

Change-Id: Id53f6fbc8cc6fb38419e5616a352279f1a9b917f
Closes-Bug: #1276367"
1414,c1ed203ccb816ac0a3a0e015d2790ed3aee04564,1401357688,,1.0,97,31,2,2,1,0.954434003,False,,,,,True,,,,,,,,,,,,,,,,,934,1370,1324450,neutron,c1ed203ccb816ac0a3a0e015d2790ed3aee04564,0,0,"""add delete operations”","Bug #1324450 in neutron: ""add delete operations for the ODL MechanismDriver""","The delete operations (networks, subnets and ports) haven't been managed since the 12th review of the initial support.
It seems sync_single_resource only implements create and update operations.","Add delete operations for the ODL MechanismDriver

This commit adds delete operations (networks, subnets and ports) for the ODL MechanismDriver.
It also modifies sync_single_resource to reduce db operations.

Change-Id: I03ca04c83ac2ef9c879fbd87e74bae495daea16d
Closes-Bug: #1324450
Partial-Bug: #1325184"
1415,c20f50b419ca95c7df35a2b2a546a8f36f311d98,1412243075,,1.0,15,2,2,2,1,0.873981048,False,,,,,True,,,,,,,,,,,,,,,,,1375,1845,1376651,nova,c20f50b419ca95c7df35a2b2a546a8f36f311d98,1,0,“versions of libvirt that are older than the minimum supported version”,"Bug #1376651 in OpenStack Compute (nova): ""Enforce the minimum required libvirt in nova""","In a number of cases now users are reporting bugs for features that don't work in Nova with versions of libvirt that are older than the minimum supported version. Most recently in Juno libvirt 0.9.11 is required but user reported bugs about a problem in 0.9.8
https://bugs.launchpad.net/nova/+bug/1376307
People clearly aren't seeing the log error message about their unsupported libvirt version, so we should turn this into a fatal exception that blocks nova-compute startup.","libvirt: abort init_host method on libvirt that is too old

Nova has a declared minimum supported libvirt and the init_host
method will log an error message if it sees an older version is
installed. Unfortunately people are frequently not seeing this
error message and then filing bugs about things that are broken
on older libvirt versions.

Turn the log message into an exception so that nova-compute
process startup is completely aborted so people will see the
problem immediately rather than some unpredictable time later.

Closes-bug: #1376651
Change-Id: I87d5a41145b05cc2be3559ab528497fe8355ba85"
1416,c250696ba12ead71e741cf8f608ce5212b0cc7bb,1403687739,,1.0,17,4,2,2,1,0.958711883,False,,,,,True,,,,,,,,,,,,,,,,,1006,1448,1334142,nova,c250696ba12ead71e741cf8f608ce5212b0cc7bb,1,1,"“The git commit 873aad92944f8840e772d65eda4b3320d65a9ce7[1] has moved
    ""brctl addif"" command, but the commit did not move its error check.”","Bug #1334142 in OpenStack Compute (nova): ""A server creation fails due to adding interface failure""","http://logs.openstack.org/72/61972/27/gate/gate-tempest-dsvm-full/ed1ab55/logs/testr_results.html.gz
pythonlogging:'': {{{
2014-06-25 06:45:11,596 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 202 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers 0.295s
2014-06-25 06:45:11,674 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.077s
2014-06-25 06:45:12,977 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.300s
2014-06-25 06:45:12,978 25675 INFO     [tempest.common.waiters] State transition ""BUILD/scheduling"" ==> ""BUILD/spawning"" after 1 second wait
2014-06-25 06:45:14,150 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 200 GET http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.171s
2014-06-25 06:45:14,153 25675 INFO     [tempest.common.waiters] State transition ""BUILD/spawning"" ==> ""ERROR/None"" after 3 second wait
2014-06-25 06:45:14,221 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 400 POST http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f/action 0.066s
2014-06-25 06:45:14,404 25675 INFO     [tempest.common.rest_client] Request (DeleteServersTestXML:test_delete_server_while_in_verify_resize_state): 204 DELETE http://127.0.0.1:8774/v2/aedc849c2c1742b8b1077c85d609b127/servers/f9fb672b-f2e6-4303-b7d1-2c5aa324170f 0.182s
}}}
Traceback (most recent call last):
  File ""tempest/api/compute/servers/test_delete_server.py"", line 97, in test_delete_server_while_in_verify_resize_state
    resp, server = self.create_test_server(wait_until='ACTIVE')
  File ""tempest/api/compute/base.py"", line 247, in create_test_server
    raise ex
BadRequest: Bad request
Details: {'message': 'The server could not comply with the request since it is either malformed or otherwise incorrect.', 'code': '400'}","Move the error check for ""brctl addif""

The git commit 873aad92944f8840e772d65eda4b3320d65a9ce7[1] has moved
""brctl addif"" command, but the commit did not move its error check.

Now the error check sees the result of ""ip addr show dev <interface>
scope global"" command wrongly, and the command outputs ""Dump was
interrupted and may be inconsistent"" sometimes because of a kernel change.
As the result, gate failures happen sometimes.

This patch moves the error check to right place.

[1]: https://github.com/openstack/nova/commit/873aad92944f8840e772d65eda4b3320d

Change-Id: I92ee96e3aa34f7cd154a8ce4b33852b96cabe8c9
Closes-Bug: #1334142"
1417,c27d2dcfa9ee1c91176bb98b0be5765948266e68,1383117485,0.0,1.0,12,12,2,2,1,0.249882293,True,2.0,10194158.0,62.0,20.0,False,38.0,73267.0,77.0,5.0,1391.0,1380.0,2566.0,1253.0,1352.0,2400.0,558.0,1350.0,1703.0,0.086278747,0.208519833,0.26300355,1736,1246206,1246206,nova,c27d2dcfa9ee1c91176bb98b0be5765948266e68,1,1,“fix missing datastore”,"Bug #1246206 in OpenStack Compute (nova): ""VMware: regex support missing for ESX driver """,Datatsore regex support is missing for the ESX driver,"VMware: fix missing datastore regex with ESX driver

When using shared datastore and the ESX driver the administrator
is unable to indicate which datastore to use.

Note: there were extensive tests added for the VC driver for the
regex support. These are invoked by passing the flag for the regex
when the ESX driver is created.

Change-Id: Ifb0a4a8df2b26f3521b1301462d1f2c3eeba5c14
Closes-bug: #1246206"
1418,c283576770b1bb96fd3dd012c6ced22b2a8eb903,1391292245,,1.0,6,3,1,1,1,0.0,True,4.0,3177308.0,62.0,22.0,False,6.0,501957.0,8.0,7.0,329.0,2125.0,2327.0,329.0,1680.0,1882.0,312.0,1887.0,2078.0,0.043279867,0.261061947,0.287472345,355,766,1275352,nova,c283576770b1bb96fd3dd012c6ced22b2a8eb903,0,0,tests,"Bug #1275352 in OpenStack Compute (nova): ""test_migration_utils runs sqlite tests even if no sqlite configured""","There are several jobs that only run if you have sqlite configured in nova.tests.db.test_migration_utils but there are two which will run and fail if you don't have sqlite configured:
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_check_shadow_table_with_unsupported_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 391, in test_check_shadow_table_with_unsupported_type
    engine = self.engines['sqlite']
KeyError: 'sqlite'
======================================================================
ERROR: nova.tests.db.test_migration_utils.TestMigrationUtils.test_util_drop_unique_constraint_with_not_supported_sqlite_type
----------------------------------------------------------------------
_StringException: pythonlogging:'': {{{INFO [nova.tests.db.test_migrations] Creating DB2 schema nova.novatstu}}}
Traceback (most recent call last):
  File ""/root/nova-es/nova/tests/db/test_migration_utils.py"", line 215, in test_util_drop_unique_constraint_with_not_supported_sqlite_type
    for i in xrange(0, len(values)):
UnboundLocalError: local variable 'values' referenced before assignment
Those should be skipped if sqlite isn't configured like the other sqlite-specific tests.","Skip sqlite-specific tests if sqlite is not configured

There are several tests in test_migration_utils which only work if
sqlite is configured for migration tests, but two of them are missing
the conditional checks for skipping the test.

Also renames test_check_shadow_table_with_unsupported_type to
test_check_shadow_table_with_unsupported_sqlite_type since it's specific
to sqlite.

Closes-Bug: #1275352

Change-Id: I46f1b95e654bc702c76790350a8929d03c04e0a1"
1419,c28956a6872b34a93891c985c93aad5e242563b6,1402881092,,1.0,20,4,2,2,1,0.41381685,False,,,,,True,,,,,,,,,,,,,,,,,975,1414,1329882,nova,c28956a6872b34a93891c985c93aad5e242563b6,1,1,"“This looks like a clear regression caused by https://review.openstack.org/#/c/82455/.""","Bug #1329882 in OpenStack Compute (nova): ""Snapshot is deleted after instance is terminated""",NovaImageBuilder runs Anaconda and other installers inside a nova instance.  After the install the instance is shut down and a snapshot is taken.  After the snapshot finishes being saved the instance is terminated.  In the last week I have noticed that the snapshot gets deleted also.,"Should not delete active snapshot when instance is terminated

The instance might be deleted when doing snapshot, need to check
whether the snapshot is active state before delete it.

Change-Id: Ic12b8dfd05ad7f36831dd073cef746f094d8f2d0
Closes-Bug: #1329882"
1420,c28c87db4c2b20e0f6c1375d2d4433e06c4e1743,1381697497,1.0,1.0,14,2,2,2,1,0.988699408,True,5.0,3874544.0,26.0,15.0,False,4.0,4166372.0,4.0,12.0,501.0,3994.0,4219.0,412.0,3484.0,3640.0,429.0,2992.0,3176.0,0.068253968,0.475079365,0.504285714,1673,1238374,1238374,nova,c28c87db4c2b20e0f6c1375d2d4433e06c4e1743,1,1, ,"Bug #1238374 in OpenStack Compute (nova): ""TypeError in periodic task 'update_available_resource'""","this occurs while I creating an instance under my devstack env:
2013-10-11 02:56:29.374 ERROR nova.openstack.common.periodic_task [-] Error during ComputeManager.update_available_resource: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task Traceback (most recent call last):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/periodic_task.py"", line 180, in run_periodic_tasks
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     task(self, context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/manager.py"", line 4859, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     rt.update_available_resource(context)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     return f(*args, **kwargs)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/compute/resource_tracker.py"", line 313, in update_available_resource
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     self.pci_tracker.clean_usage(instances, migrations, orphans)
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task   File ""/opt/stack/nova/nova/pci/pci_manager.py"", line 285, in clean_usage
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task     for dev in self.claims.pop(uuid):
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task TypeError: 'NoneType' object is not iterable
2013-10-11 02:56:29.374 TRACE nova.openstack.common.periodic_task","Check for None when cleaning PCI dev usage

The PCI manager needs to check for instance UUIDs mapped to
None when cleaning up it's internal claims/allocations maps.

Also adds a check so the claims dict doesn't map to None in
the first place.  There was already a check like this when
setting a value in the allocations dict.

Closes-Bug: #1238374

Change-Id: I7ac010f7d50ad85c145e536cfd48e07c8f4602d3"
1421,c2aa27d6f86d9ead2a9653b07de425669351d93d,1402074129,,1.0,11,7,2,2,1,0.852405179,False,,,,,True,,,,,,,,,,,,,,,,,953,1390,1327145,nova,c2aa27d6f86d9ead2a9653b07de425669351d93d,1,1,“Change https://review.openstack.org/#/c/62314/ mistakenly made the conversion of the instance object in compute/rpcapi.py rescue_image()”,"Bug #1327145 in OpenStack Compute (nova): ""rescue_instance RPC has reverted to passing a dict""",Change https://review.openstack.org/#/c/62314/  mistakenly made the conversion of the instance object in compute/rpcapi.py   rescue_image() unconditional on the RPC version.   From 3.9 onwards this should be passed as an object,"Fix error in rescue rpcapi that prevents sending objects

The change restores the rescue_instance rpc's ability to send
instance objects from version 3.11 onwards, that was inadvertently
broken by https://review.openstack.org/#/c/62314

Unit test modified to check that rpc version passes instance object
instead of dict.

Change-Id: Icffea82da623d3aab4661554b33bd5959bfe2bd8
Closes-Bug: #1327145"
1422,c2ae5fb5659faeadc6f4d8afe27eb68e3c0ade0c,1381319683,,1.0,1,1,1,1,1,0.0,True,4.0,19414036.0,58.0,21.0,False,127.0,55843.0,517.0,7.0,138.0,3155.0,3234.0,138.0,2743.0,2822.0,88.0,2126.0,2158.0,0.014230892,0.340102335,0.34521906,1657,1237303,1237303,nova,c2ae5fb5659faeadc6f4d8afe27eb68e3c0ade0c,1,1,“Update log message for add_host_to_aggregate“ bug in comments,"Bug #1237303 in OpenStack Compute (nova): ""Log message does not contain blank space""","liugya@liugya-ubuntu:~/devstack$ nova  aggregate-add-host  1 liugya-ubuntu
Aggregate 1 has been successfully updated.
+----+------+-------------------+--------------------+----------------------------------+
| Id | Name | Availability Zone | Hosts              | Metadata                         |
+----+------+-------------------+--------------------+----------------------------------+
| 1  | agg1 | zone1             | [u'liugya-ubuntu'] | {u'availability_zone': u'zone1'} |
+----+------+-------------------+--------------------+----------------------------------+
liugya@liugya-ubuntu:~/devstack$ nova  aggregate-add-host  2 liugya-ubuntu
ERROR: Cannot perform action 'add_host_to_aggregate' on aggregate 2. Reason: Host already in availability zonezone1.. (HTTP 409) (Request-ID: req-fe50ad9a-f20d-456c-8c1f-f236b8329b0c)
Two issues:
1) contain two period at the end of the log message
2) Should be availability zone zone1","Update log message for add_host_to_aggregate

When add host to aggregates with availability zone, if the host
is already in one availability zone, then the host cannot be added
to another availability zone.

When user do above operation, nova api will report error message,
but the error message does not include blank space between
<availability zone> and zone name, also the error message include
two periods.

The fix was adding blank space and removes one period for the message.

Change-Id: I7307288be194d3ce6de05b77433bbe2626ae46d9
Closes-bug: #1237303"
1423,c31ae4987c5ae2015cc5379a4717490b75c62c9e,1381769060,,1.0,3,3,1,1,1,0.0,True,2.0,670506.0,10.0,3.0,False,121.0,355434.0,337.0,2.0,1.0,1868.0,1868.0,1.0,1471.0,1471.0,1.0,1614.0,1614.0,0.000317007,0.255983516,0.255983516,1684,1239747,1239747,nova,c31ae4987c5ae2015cc5379a4717490b75c62c9e,1,1, ,"Bug #1239747 in OpenStack Compute (nova): ""Logging ""None"" in place of path in libvirt driver""","The get_instance_disk_info method in the nova.virt.libvirt.driver logs a None value if a disk path is undefined. The log message is intended to show a path that does not represent a path, but in the case the path does not actually exist, it gives the following uninformative message:
2013-10-14 06:25:21.404 45722 DEBUG nova.virt.libvirt.driver [-] skipping None since it looks like volume get_instance_disk_info /usr/lib/python2.7/dist-packages/nova/virt/libvirt/driver.py:4299
When the path is not defined it should log the instance it is looking at instead.","Improve logging messages in libvirt driver

This is a trivial change to correct an uninformative log message.
There are two checks in get_instance_disk_info that look at a disk.
One checks for the type of a disk and logs its path if it is not a file.
The other checks for the path and logs the instance name it belongs to
if it does not exist. The first will log None if the path does not exist,
so reverse the order of the checks.

Closes-Bug: #1239747
Change-Id: Ieb52ba0584ad789dc55f40e0172cf5ed41e86fbe"
1424,c32302c5d5f35df39b49d32fcba47a4d36b3783a,1388820253,0.0,1.0,50,33,2,2,1,0.163860513,True,2.0,248789.0,13.0,4.0,False,23.0,343796.0,42.0,3.0,11.0,1243.0,1249.0,11.0,1065.0,1071.0,4.0,1088.0,1088.0,0.00385208,0.838983051,0.838983051,250,654,1265893,cinder,c32302c5d5f35df39b49d32fcba47a4d36b3783a,0,0,Update V2 API,"Bug #1265893 in Cinder: ""Update V2 API to return detailed volume information on create_volume""","Currently the V2 API returns volume summary after volume create which requires the cinderclient to invoke additional GET request to get additional details.
This bug is to update the summary method to detail and remove the additional get from cinderclient.","Update V2 API to return detailed volume information on create

Current implementation returns only summary information, so
cinderclient requires additional GET call to get details.
Updated the api to return the details by default so the GET in
cinderclient can be removed.

Closes-Bug: #1265893

Change-Id: I56d4d79c4a942d8bf53318e46737674dc0bf9b56"
1425,c32986966dfd034c3d706b2e9ab2820a2c3cfc3e,1394662396,,1.0,39,1,5,4,1,0.800742987,True,7.0,5292922.0,114.0,27.0,False,157.0,1213014.6,638.0,5.0,25.0,1565.0,1570.0,25.0,1536.0,1541.0,20.0,1527.0,1527.0,0.002768987,0.201476793,0.201476793,597,1021,1291489,nova,c32986966dfd034c3d706b2e9ab2820a2c3cfc3e,1,1,,"Bug #1291489 in OpenStack Compute (nova): ""list-secgroup fail if no secgroups defined for server""","No issues if there are atleast 1 secgroup defined for the server.
If no secgroups are defined for the server, it fails with 400 error.
$ nova --debug list-secgroup vp25q00cs-osfe11b124f4.isg.apple.com
.
.
.
RESP: [400] CaseInsensitiveDict({'date': 'Wed, 12 Mar 2014 17:08:11 GMT', 'content-length': '141', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61'})
RESP BODY: {""badRequest"": {""message"": ""The server could not comply with the request since it is either malformed or otherwise incorrect."", ""code"": 400}}
DEBUG (shell:740) The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
Traceback (most recent call last):
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 737, in main
    OpenStackComputeShell().main(map(strutils.safe_decode, sys.argv[1:]))
  File ""/Library/Python/2.7/site-packages/novaclient/shell.py"", line 673, in main
    args.func(self.cs, args)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/shell.py"", line 1904, in do_list_secgroup
    groups = server.list_security_group()
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 328, in list_security_group
    return self.manager.list_security_group(self)
  File ""/Library/Python/2.7/site-packages/novaclient/v1_1/servers.py"", line 883, in list_security_group
    base.getid(server), 'security_groups', SecurityGroup)
  File ""/Library/Python/2.7/site-packages/novaclient/base.py"", line 61, in _list
    _resp, body = self.api.client.get(url)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 229, in get
    return self._cs_request(url, 'GET', **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 213, in _cs_request
    **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 195, in _time_request
    resp, body = self.request(url, method, **kwargs)
  File ""/Library/Python/2.7/site-packages/novaclient/client.py"", line 189, in request
    raise exceptions.from_response(resp, body, url, method)
BadRequest: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)
ERROR: The server could not comply with the request since it is either malformed or otherwise incorrect. (HTTP 400) (Request-ID: req-20cb1b69-a69c-435c-9e85-3eec2fb2ae61)","Fix security group list when not defined for an instance

If an instance has an empty security group list an exception
is thrown due to iteration attempt of None.

To fix this, this patch modifies get_instance_security_group
to return an empty list instead of None.

Change-Id: I7efb3c3a58d7e20501d126c6d194479ca1d14166
Closes-Bug: #1291489"
1426,c3330931113ac2edf5961a653e5c2cfe459c13a0,1381441113,,1.0,46,1,2,2,1,0.25387844,True,11.0,87451.0,37.0,12.0,False,22.0,140873.0,57.0,4.0,162.0,2722.0,2804.0,162.0,2235.0,2317.0,148.0,2448.0,2519.0,0.023741236,0.390216699,0.401529637,1671,1238281,1238281,nova,c3330931113ac2edf5961a653e5c2cfe459c13a0,1,1,“ This addresses a minor (albeit important) logic error in the migration script”,"Bug #1238281 in OpenStack Compute (nova): ""Migration script deletes wrong compute node stats""","The nova DB migration script for 215 executes:
result = conn.execute(
        'update compute_node_stats set deleted = id, '
        'deleted_at = current_timestamp where compute_node_id not in '
        '(select id from compute_nodes where deleted <> 0)')
Need to remove the 'not' part so that we delete the right stats since the nested select finds all DELETED nodes.  The current SQL actually deletes all active compute nodes' stats.","Fix nova DB 215 migration script logic error

This addresses a minor (albeit important) logic error in the migration script
for https://review.openstack.org/#/c/46379/ - that script currently deletes
compute node stats for active compute nodes.  This patch reverses that logic
so it deletes the correct set of compute node stats.

Also included is a migration test case to demonstrate the behavior.

Change-Id: I77afcf443357d0767ac933d791a565289eabee9a
Closes-Bug: #1238281"
1427,c363dae6a2b878db6801b502cced1fcc6aad2d0c,1406264615,,1.0,94,2,3,2,1,0.467276983,False,,,,,True,,,,,,,,,,,,,,,,,1100,1551,1348288,nova,c363dae6a2b878db6801b502cced1fcc6aad2d0c,1,1,,"Bug #1348288 in OpenStack Compute (nova): ""Resource tracker should report virt driver stats""","sha1 Nova at: 106fb458c7ac3cc17bb42d1b83ec3f4fa8284e71
sha1 ironic at: 036c79e38f994121022a69a0bc76917e0048fd63
The ironic driver passes stats to nova's resource tracker in get_available_resources(). Sometimes these appear to get through to the database without modification, sometimes they seem to be replaced entirely by other stats generated by the resource tracker. The correct behaviour should be to combine the two.
As an example, the following query on the compute_nodes table in nova's database shows the contents for a tripleo system (all nodes are ironic):
mysql> select hypervisor_hostname, stats from compute_nodes;
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| hypervisor_hostname                  | stats                                                                                                                                                                       |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 0588e463-748a-4248-9110-6e18988cfa4e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 4615c72a-9ea0-433e-8c52-308163112f89 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | {""cpu_arch"": ""amd64"", ""ironic_driver"": ""ironic.nova.virt.ironic.driver.IronicDriver""}                                                                                       |
| 18794409-10d0-4946-9356-66cd5ab8472e | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
| ed281d00-c16a-474d-8adb-ef525a9045fa | {""num_task_None"": 1, ""io_workload"": 0, ""num_instances"": 1, ""num_vm_active"": 1, ""num_vcpus_used"": 24, ""num_os_type_None"": 1, ""num_proj_505908300744403496b2e64b06606529"": 1} |
+--------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
29 rows in set (0.00 sec)
The nodes with ironic stats have not had any instances created on them, the ones with resource tracker stats but no ironic stats have instances on them. This is shown by the following:
ironic node-list
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| uuid                                 | instance_uuid                        | power_state | provision_state | maintenance |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+
| dc5c63b6-d576-46c9-aa16-0537450cdbd8 | None                                 | power off   | None            | False       |
| fadb50bf-26ec-420c-a13f-f182e38569d6 | a703ae98-2398-445b-91bf-48f368c5b82a | power on    | active          | False       |
| 9adf4551-24f0-43a7-9267-a20cfa309137 | None                                 | power off   | None            | False       |
| f2f0ecb1-6234-4975-808f-a17534c9ae6c | 308b1375-f8b8-42e5-bf20-143303975135 | power on    | active          | False       |
| 1bd13fc5-4938-4781-9680-ad1e0ccec77c | 4e8c16c4-d0f6-43b8-8c8f-d110d89ac16f | power on    | active          | False       |
| a1b81342-ed57-4310-8d5b-a2aa48718f1f | 01cbe85a-1b12-40ca-ac99-5e7b062b1b50 | power on    | active          | False       |
| 18794409-10d0-4946-9356-66cd5ab8472e | 4d764e7e-a123-4390-9c73-0c05a52f5f23 | power on    | active          | False       |
| ec6b5dc6-de38-4e23-a967-b87c10da37e3 | 87636b3b-b7b0-4e79-bd5f-5189eb5b1134 | power on    | active          | False       |
| ac52fd79-e0b9-4749-b794-590d5c181b4a | 03c06c2f-b606-4ddd-a205-573ea036b5b8 | power on    | active          | False       |
| 4e014e26-2f90-4a91-a6f0-c1978df88369 | 0189aa0e-5d05-4f6d-8a77-d5869b5c79f2 | power on    | active          | False       |
| ed281d00-c16a-474d-8adb-ef525a9045fa | 11e03336-fb12-4448-a8de-b38f82d8b282 | power on    | active          | False       |
| bac38b1d-f7f9-4770-9195-ff204a0c05c3 | None                                 | power off   | None            | False       |
| 62cc33f7-701b-47f6-8f50-3f7c1ca0f0a3 | ab08556b-ce7f-46f3-bb10-91771426d977 | power on    | active          | False       |
| 8135a1be-c8d8-4cea-a381-8ab8be8b15c7 | 4051cc97-bb28-4dbf-b018-d9a61e73a269 | power on    | active          | False       |
| 6f653502-d8ed-4763-b418-3ccfcc430c24 | 855e24b6-f9ae-441b-8520-42d4df9f8703 | power on    | active          | False       |
| 2c2d5b87-1be2-4e47-aabe-6822c569446c | 141e6055-3e2c-4376-aa8b-39ad5c63e8bc | power on    | active          | False       |
| 8f73d8dc-5d8c-47b0-a866-b829edc3667f | a62cab3f-56f7-47f2-813b-23a3255cad15 | power on    | active          | False       |
| 0588e463-748a-4248-9110-6e18988cfa4e | 8fe18a1b-d753-4558-a9e1-b24f552f8e12 | power on    | active          | False       |
| e2cb81ca-314f-4436-80fd-e154ca3e9ccc | 839a77e1-2a9e-4db4-94d9-d68903fe028c | power on    | active          | False       |
| ffe5a5bf-7151-468c-b9bb-980477e5f736 | 205b6dbf-5f75-4d2c-a3b0-1be45d93d493 | power on    | active          | False       |
| 752966ea-17f8-4d6d-87a4-03c91cb65354 | 09d2ad8c-f813-4b1a-9501-530462240657 | power on    | active          | False       |
| 006aff97-d3e6-49c8-93f0-4f4c5af1231d | aa6c8024-ba80-4dc8-810c-c6fae57218c7 | power on    | active          | False       |
| 7a7266a9-d72e-49be-b51a-4053ed251b41 | a76d7b2f-cf8c-4673-96e4-dcd9f2ea3bb5 | power on    | active          | False       |
| 88a39f5d-6174-47c9-9817-13d08bf2e079 | 9a2904d2-d364-4084-a40a-3fbc65d90059 | power on    | active          | False       |
| addf8ff8-52fe-49da-a4b2-5688554e9161 | a86316c0-bdf0-4d6e-81ba-f44da63c906c | power on    | active          | False       |
| b4b7f7ad-4adc-4dc9-9afb-a9966e2be141 | 03b0d70d-42c4-426e-adab-09df927f30bb | power on    | active          | False       |
| 680e6aa7-9a84-41de-94ba-b761d48b4087 | 808dbf2e-8f40-4d6d-9d7b-2c74e3194a6d | power on    | active          | False       |
| 4615c72a-9ea0-433e-8c52-308163112f89 | None                                 | power off   | None            | False       |
| af7f79bf-b2c1-405b-9bc7-5370b93b08cf | None                                 | power off   | None            | False       |
+--------------------------------------+--------------------------------------+-------------+-----------------+-------------+","Fix Resource tracker should report virt driver stats

If the virt driver provides any data for resource stats it is
lost whenever the resource tracker updates its own view of stats.
Moreover, if the resource tracker has not instances to track it
only reports the driver's view, which might be nothing.

This fix adds the driver's view of stats to the resource tracker
stats to make sure they are correctly handled.

Change-Id: Icb19148660bca542a8120ecab064551d67ac28af
Closes-bug: #1348288"
1428,c36ddaf930ba309be3c127dc1836f28a8ab97c5d,1394843530,,1.0,26,2,2,2,1,0.67694187,True,3.0,612734.0,54.0,6.0,False,12.0,121707.5,20.0,3.0,21.0,1339.0,1341.0,21.0,1121.0,1123.0,16.0,739.0,740.0,0.017525773,0.762886598,0.763917526,502,921,1286565,neutron,c36ddaf930ba309be3c127dc1836f28a8ab97c5d,1,1,"I think cisco is not a specific platform, is 1 of their plugins","Bug #1286565 in neutron: ""Cisco Nexus: maximum recursion error in ConnectionContext.__del__""","If DevStack is configured for the Cisco Nexus plugin with the latest DevStack, the following
infinite recursion error is observed:
Exception RuntimeError: 'maximum recursion depth exceeded' in <bound method
ConnectionContext.__del__ of <neutron.openstack.common.rpc.amqp.ConnectionContext
object at 0x403a3d0>> ignored
An investigation shows that this failure triggered when the DB base plugin's
_is_native_pagination_supported method is called. The infinite recursion begins
when this exception is raised:
             raise AttributeError(
                 _(""'%(model)s' object has no attribute '%(name)s'"") %
                {'model': self._model, 'name': name})
in the PluginV2.__getattr__ method in
neutron/plugins/cisco/network_plugin.py. The problem is that self._model
object is being % mod'd as a string in the unicode message, and this
causes many levels of recursion into deepcopy (a deepcopy for all objects
embedded in this object).","Cisco Nexus: maximum recursion error in ConnectionContext.__del__

If DevStack is configured for the Cisco Nexus plugin, the following
error is observed:

Exception RuntimeError: 'maximum recursion depth exceeded' in <bound
method ConnectionContext.__del__ of
<neutron.openstack.common.rpc.amqp.ConnectionContext object at
0x403a3d0>> ignored

The root cause of the problem is that the Cisco Nexus plugin's
PluginV2.__gettattr__ method, a model object is being passed
as a value for a unicode %s format mod. Because the neutron server
has ""lazy gettext"" (deferred interpretation of unicode objects) enabled,
this causes many layers of recursive calls to deepcopy.

The fix is to pass a string object for the unicode %s mod field.

Change-Id: I0a07a0ab417add68e44cb1bca722cb0b4a71205b
Closes-Bug: #1286565"
1429,c3b4f24883c04cd139292ecf9cee31cff43e3973,1379412742,,1.0,0,12,1,1,1,0.0,True,2.0,696323.0,28.0,14.0,False,2.0,1098623.0,2.0,5.0,3.0,1082.0,1082.0,3.0,983.0,983.0,3.0,314.0,314.0,0.011594203,0.913043478,0.913043478,1575,1227079,1227079,neutron,c3b4f24883c04cd139292ecf9cee31cff43e3973,0,0,Bug in test,"Bug #1227079 in neutron: ""Midonet plugin, add multiple subnets tests""",The plugin is skipping the unit tests for multiple subnets,"Remove code that bypasses some midonet plugin unit tests

Multiple subnets unit tests were skipped

Closes-Bug: #1227079
Change-Id: Iffe731722d0e6c515bc64923673218ddde98c1f6"
1430,c3c9f580393aea658571b00b3afd0b729dffe89b,1406260660,1.0,1.0,163,22,4,4,1,0.532773275,False,,,,,True,,,,,,,,,,,,,,,,,1101,1552,1348306,neutron,c3c9f580393aea658571b00b3afd0b729dffe89b,1,1,,"Bug #1348306 in neutron: ""L3 agent restart disrupts fip namespace causing connectivity loss""","When the L3 agent restarts, it does not preserve the link local addresses used for each router.  For this reason, it has to reassign them and rewire everything.  This is very disruptive to network connectivity.  Connectivity should be preserved as much as possible.
This was an expected backlog item for the new DVR feature.","Preserve link local IP allocations for DVR fip ns across restart

The L3 agent allocates link local address pairs used in connecting the
routers to the floating ip namespace.  When those allocations are
forgetten by restarting the L3 agent they all get rewired on restart.
This change preserves the allocations using a file in the local file
system.  Storing them in the database would be overkill and would
affect system performance.

Change-Id: I39614c7ea2a7dcc35bf969c90045adc5926ea9df
Closes-Bug: #1348306
Partially-Implements: blueprint neutron-ovs-dvr
Co-Authored-By: Rajeev Grover <rajeev.grover@hp.com>"
1431,c47900064f2f3b4384cfa3fd6fe21f452514fa36,1394866257,,1.0,6,38,5,4,1,0.762042273,True,14.0,3722986.0,159.0,47.0,False,246.0,139285.0,1364.0,9.0,1069.0,4319.0,4588.0,815.0,3643.0,3787.0,491.0,2843.0,3062.0,0.064626297,0.373571522,0.402338106,533,954,1288392,nova,c47900064f2f3b4384cfa3fd6fe21f452514fa36,1,1,Bug. Remove some callls,"Bug #1288392 in OpenStack Compute (nova): ""instances get stuck in ERROR/deleting when Neutron is unavailable""","We had a temporary outage of Neutron, and many instances got stuck in this state. 'nova delete' on them does not work until nova-compute is forcibly restarted.
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                                                                                                                                                      |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                                                                                                                                                     |
| OS-EXT-AZ:availability_zone          | nova                                                                                                                                                                                                                       |
| OS-EXT-SRV-ATTR:host                 | ci-overcloud-novacompute1-4q2dbhdklrkq                                                                                                                                                                                     |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ci-overcloud-novacompute1-4q2dbhdklrkq.novalocal                                                                                                                                                                           |
| OS-EXT-SRV-ATTR:instance_name        | instance-00003f80                                                                                                                                                                                                          |
| OS-EXT-STS:power_state               | 1                                                                                                                                                                                                                          |
| OS-EXT-STS:task_state                | deleting                                                                                                                                                                                                                   |
| OS-EXT-STS:vm_state                  | error                                                                                                                                                                                                                      |
| OS-SRV-USG:launched_at               | 2014-03-05T03:54:49.000000                                                                                                                                                                                                 |
| OS-SRV-USG:terminated_at             | -                                                                                                                                                                                                                          |
| accessIPv4                           |                                                                                                                                                                                                                            |
| accessIPv6                           |                                                                                                                                                                                                                            |
| config_drive                         |                                                                                                                                                                                                                            |
| created                              | 2014-03-05T03:46:25Z                                                                                                                                                                                                       |
| default-net network                  | 10.0.58.225                                                                                                                                                                                                                |
| fault                                | {""message"": ""Connection to neutron failed: Maximum attempts reached"", ""code"": 500, ""details"": ""  File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 253, in decorated_function |
|                                      |     return function(self, context, *args, **kwargs)                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2038, in terminate_instance                                                                                               |
|                                      |     do_terminate_instance(instance, bdms)                                                                                                                                                                                  |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/lockutils.py\"", line 249, in inner                                                                                                  |
|                                      |     return f(*args, **kwargs)                                                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2036, in do_terminate_instance                                                                                            |
|                                      |     self._set_instance_error_state(context, instance['uuid'])                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__                                                                                                 |
|                                      |     six.reraise(self.type_, self.value, self.tb)                                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2026, in do_terminate_instance                                                                                            |
|                                      |     reservations=reservations)                                                                                                                                                                                             |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/hooks.py\"", line 103, in inner                                                                                                                       |
|                                      |     rv = f(*args, **kwargs)                                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 2005, in _delete_instance                                                                                                 |
|                                      |     user_id=user_id)                                                                                                                                                                                                       |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/openstack/common/excutils.py\"", line 68, in __exit__                                                                                                 |
|                                      |     six.reraise(self.type_, self.value, self.tb)                                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1975, in _delete_instance                                                                                                 |
|                                      |     self._shutdown_instance(context, db_inst, bdms)                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 1884, in _shutdown_instance                                                                                               |
|                                      |     network_info = self._get_instance_nw_info(context, instance)                                                                                                                                                           |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/compute/manager.py\"", line 902, in _get_instance_nw_info                                                                                             |
|                                      |     instance)                                                                                                                                                                                                              |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/api.py\"", line 48, in wrapper                                                                                                                |
|                                      |     res = f(self, context, *args, **kwargs)                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 445, in get_instance_nw_info                                                                                        |
|                                      |     result = self._get_instance_nw_info(context, instance, networks)                                                                                                                                                       |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 452, in _get_instance_nw_info                                                                                       |
|                                      |     nw_info = self._build_network_info_model(context, instance, networks)                                                                                                                                                  |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/network/neutronv2/api.py\"", line 1010, in _build_network_info_model                                                                                  |
|                                      |     data = client.list_ports(**search_opts)                                                                                                                                                                                |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 112, in with_params                                                                                                  |
|                                      |     ret = self.function(instance, *args, **kwargs)                                                                                                                                                                         |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 307, in list_ports                                                                                                   |
|                                      |     **_params)                                                                                                                                                                                                             |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1251, in list                                                                                                        |
|                                      |     for r in self._pagination(collection, path, **params):                                                                                                                                                                 |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1264, in _pagination                                                                                                 |
|                                      |     res = self.get(path, params=params)                                                                                                                                                                                    |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1237, in get                                                                                                         |
|                                      |     headers=headers, params=params)                                                                                                                                                                                        |
|                                      |   File \""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/neutronclient/v2_0/client.py\"", line 1229, in retry_request                                                                                               |
|                                      |     raise exceptions.ConnectionFailed(reason=_(\""Maximum attempts reached\""))                                                                                                                                              |
|                                      | "", ""created"": ""2014-03-05T04:22:04Z""}                                                                                                                                                                                      |
| flavor                               | h1.large (872d8f61-c45a-45c3-87da-466d9f0f241b)                                                                                                                                                                            |
| hostId                               | 0bab209cc6f26a8d5c4bc76e3da39d4fa68e5fefc6e5c0eada7a90d2                                                                                                                                                                   |
| id                                   | ae9c75e3-51d2-43a3-8b20-34375b4c72d3                                                                                                                                                                                       |
| image                                | tripleo-precise-1393812840.template.openstack.org (114e4b92-567e-4348-9ed8-e88281104208)                                                                                                                                   |
| key_name                             | -                                                                                                                                                                                                                          |
| metadata                             | {}                                                                                                                                                                                                                         |
| name                                 | tripleo-precise-tripleo-test-cloud-2125650.slave.openstack.org                                                                                                                                                             |
| os-extended-volumes:volumes_attached | []                                                                                                                                                                                                                         |
| security_groups                      | default, default                                                                                                                                                                                                           |
| status                               | ERROR                                                                                                                                                                                                                      |
| tenant_id                            | 64d2d3bc07084ef1accd4e3502909c77                                                                                                                                                                                           |
| tripleo-bm-test network              | 192.168.1.78                                                                                                                                                                                                               |
| updated                              | 2014-03-05T04:22:04Z                                                                                                                                                                                                       |
| user_id                              | 35ef3ce265cb4a25b5303f3daa143f4e                                                                                                                                                                                           |
+--------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+","Remove unneeded call to fetch network info on shutdown

There is no reason to fetch the network_info from the network api on shutdown
since we can just pull it out of the local cache. There are probably other
places where this call to the network api call can be removed but this patch
just handles the case for shutdown.

Note: the following test was removed 'test_terminate_no_fixed_ips' as
the call to the network api is no longer done so this test would be the
same as test_terminate_no_network.

Closes-bug: #1288392

Change-Id: Ifbf751739c215e566926719f481c03e2c064163a"
1432,c47b4f388cb40d5792a206ac42ebdaebd5bde0d1,1386792700,,1.0,6,13,1,1,1,0.0,True,3.0,1962329.0,26.0,11.0,False,46.0,537694.0,99.0,6.0,28.0,2934.0,2934.0,28.0,2534.0,2534.0,28.0,1984.0,1984.0,0.004244731,0.290544496,0.290544496,174,577,1260075,nova,c47b4f388cb40d5792a206ac42ebdaebd5bde0d1,1,1,,"Bug #1260075 in OpenStack Compute (nova): ""VMware: NotAuthenticated occurred in the call to RetrievePropertiesEx""","The VMware Minesweeper CI occasionally runs into this error when trying to boot an instance:
2013-12-11 04:50:15.048 20785 DEBUG nova.virt.vmwareapi.driver [-] Task [ReconfigVM_Task] (returnval){
   value = ""task-322""
   _type = ""Task""
 } status: success _poll_task /opt/stack/nova/nova/virt/vmwareapi/driver.py:926
Reconfigured VM instance to enable vnc on port - 5986 _set_vnc_config /opt/stack/nova/nova/virt/vmwareapi/vmops.py:1461
Instance failed to spawn
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1461, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 628, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 435, in spawn
    upload_folder, upload_name + "".vmdk"")):
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 1556, in _check_if_folder_file_exists
    ""browser"")
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 173, in get_dynamic_property
    property_dict = get_dynamic_properties(vim, mobj, type, [property_name])
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 179, in get_dynamic_properties
    obj_content = get_object_properties(vim, None, mobj, type, property_names)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim_util.py"", line 168, in get_object_properties
    options=options)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 187, in vim_request_handler
    fault_checker(response)
  File ""/opt/stack/nova/nova/virt/vmwareapi/error_util.py"", line 99, in retrievepropertiesex_fault_checker
    exc_msg_list))
VimFaultException: Error(s) NotAuthenticated occurred in the call to RetrievePropertiesEx
Full logs here for a CI build where this occurred are available here: http://162.209.83.206/logs/35303/31/","VMware: use session.call_method to invoke api's

All api invocations should use session.call_method, which has a
retry mechanism to handle session failures. Few methods in vmops
that are not using this wrapper to invoke api's, fail
intermittently when the session is not authenticated.

This patch fixes those methods. This issue is more pronounced
during VMware CI (Minesweeper), where random tempest tests fail.

The existing tests for vmops and those added for the retrying
session in https://review.openstack.org/#/c/58890/ should cover
this change, as we are re-routing the api invocation.

Closes-Bug: #1260075
Change-Id: I2beb7c92a4c3f53fd2120629e8f72bfd54d13740"
1433,c47bb5b89e04403b46c224c1bda13d3d0e11eedd,1381971210,,1.0,10,5,5,4,1,1.0,True,1.0,1046020.0,12.0,7.0,False,14.0,2843026.0,26.0,6.0,12.0,1208.0,1217.0,12.0,1088.0,1097.0,3.0,1048.0,1048.0,0.003639672,0.954504095,0.954504095,1646,1236648,1236648,cinder,c47bb5b89e04403b46c224c1bda13d3d0e11eedd,1,0,“__metaclass__ is incompatible for python 3”,"Bug #1236648 in Cinder: ""__metaclass__ is incompatible for python 3""","Some class uses __metaclass__ for abc.ABCMeta.
six be used in general for python 3 compatibility.
For example
import abc
import six
six.add_metaclass(abc.ABCMeta)
class FooDriver:
    @abc.abstractmethod
    def bar():
        pass","Apply six for metaclass

__metaclass__ cannot be used in python3.
six be used in general for python 3 compatibility.

Change-Id: I1d21c82163a1c00bbf4fbf3c9dd513f1c0591b00
Closes-Bug: #1236648"
1434,c49ec8b3ba10bb414a0f135d29ff3685e26d58b3,1396547977,,1.0,0,6,1,1,1,0.0,True,1.0,113089.0,17.0,5.0,False,11.0,17970.0,15.0,5.0,1170.0,1904.0,1906.0,997.0,1428.0,1430.0,709.0,836.0,836.0,0.653173873,0.7700092,0.7700092,733,1160,1302091,neutron,c49ec8b3ba10bb414a0f135d29ff3685e26d58b3,0,0,Bug in test,"Bug #1302091 in neutron: ""Redundant SG rule create calls in unit tests""","The following test cases in TestSecurityGroups of test_extension_security_group.py file
are making multiple calls to create a SG rule.
test_create_security_group_rule_min_port_greater_max()
test_create_security_group_rule_ports_but_no_protocol()
test_create_security_group_rule_port_range_min_only()
test_create_security_group_rule_port_range_max_only()
test_create_security_group_rule_icmp_type_too_big()
test_create_security_group_rule_icmp_code_too_big()
The redundant calls can be removed.","Redundant SG rule create calls in unit tests

This patch removes the redundant calls in the test cases.

Closes-Bug: #1302091
Change-Id: I509843b427dc454c0e6b2fd2cbff1bbbb284337f"
1435,c4d0e55098952ad3180f74357a9256da43bf7d15,1409736111,,1.0,5,4,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1260,1720,1364849,nova,c4d0e55098952ad3180f74357a9256da43bf7d15,1,1,The BIC is not the pc”Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68”,"Bug #1364849 in OpenStack Compute (nova): ""VMware driver doesn't return typed console""","Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68 added typed consoles, and updated drivers to use them. However, when it touched the VMware driver, it modified get_vnc_console in VMwareVMOps, but not in VMwareVCVMOps, which is the one which is actually used.
Incidentally, VMwareVMOps has now been removed, so this type of confusion should not happen again.","VMware: Fix return type of get_vnc_console()

Change I8f6a857b88659ee30b4aa1a25ac52d7e01156a68 changed the return type of
get_vnc_console(), but didn't update it in VMwareVCVMOps.

Closes-Bug: #1364849
Change-Id: I25edf0a19fa79439ed375ad128d0050718d04cb3"
1436,c4f0c4ba967af71d31629beff72a0f1f748202cb,1398184353,,1.0,39,12,4,3,1,0.691251587,True,6.0,2437181.0,94.0,20.0,False,5.0,2775476.0,6.0,3.0,8.0,433.0,437.0,8.0,427.0,431.0,8.0,408.0,412.0,0.007718696,0.35077187,0.354202401,826,1255,1311260,neutron,c4f0c4ba967af71d31629beff72a0f1f748202cb,1,1,,"Bug #1311260 in neutron: ""SDN-VE controller expects information not in Neutron request""","The SDN-VE controller expects the port-id information for removing router interfaces.  Furthermore, the controller requires the use of string 'null' when the external gateway info is present and set to {} in Neutron update_router requests. The controller cannot accept "":"" as part of the incoming requests and therefore the requests need to be manipulated to replace the "":"" with ""_"" before sending requests to the controller.","Fixes bugs for requests sent to SDN-VE controller

This patchset fixes a couple of bugs wrt processing requests
before sending them to the backend controller and adjusts the
requests based on the controller requirements. It also corrects
typos, add quotas and changes the default for a configuration
parameter.

Change-Id: I4b64c2b49ff4854949afc1e54cba1057f376b058
Closes-Bug: #1311260
DocImpact: The default tenant type is chganged from OF to OVERLAY"
1437,c5402ef4fc509047d513a715a1c14e9b4ba9674f,1405497734,1.0,1.0,241,47,4,4,1,0.604967124,True,36.0,22288525.0,252.0,102.0,False,54.0,7419424.25,89.0,2.0,14.0,1580.0,1591.0,14.0,1512.0,1523.0,7.0,1563.0,1567.0,0.000998752,0.19525593,0.195755306,1485,1215772,1215772,nova,c5402ef4fc509047d513a715a1c14e9b4ba9674f,1,0,“the root cause is nova not support cinderclient v2 yet”,"Bug #1215772 in OpenStack Compute (nova): ""Nova Attach/Detach Volume can't work if volume endpoint set to v2""","Cannot attach or detach volume if use v2 volume endpoint.
How to reproduce this bug:
1, Remove the volume v1 endpoint from keystone, use ""keystone endpoint-delete""
2. Create the v2 volume endpoint via CLI command: keystone endpoint-create
3.export OS_VOLUME_API_VERSION=2
4. run 'cinder list' ok, and create one volume
5. run 'nova volume-attach' or 'nova detach' command will be failed. return 500 error.
After investigated, I found the root cause is nova not support cinderclient v2 yet, we need make some code change to let nova to support cinderclient v2.","Code change for nova support cinder client v2

Use v2 volume endpoint to attach/detach volume would be failed,
due to nova not supporting cinder client v2 yet. This patch is for
nova support cinder client v2.

Implements bp support-cinderclient-v2
Closes-Bug: #1215772
Co-Authored-By: Mike Perez <thingee@gmail.com>
Co-Authored-By: Yaguang Tang <yaguang.tang@canonical.com>

Change-Id: Id8abbbb4d9b0c8c49ab51fc3d958ef0d487467f8"
1438,c55736d9fc941ae3f00a29e945b8881be7813e52,1407515387,,1.0,3,3,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1167,1624,1354499,nova,c55736d9fc941ae3f00a29e945b8881be7813e52,1,0,“In c5402ef4fc509047d513a715a1c14e9b4ba9674f we added support for the new cinder V2 API.”,"Bug #1354499 in OpenStack Compute (nova): ""boot from volume fails when upgrading using cinder v1 API""","In c5402ef4fc509047d513a715a1c14e9b4ba9674f we added support for the new cinder V2 API.
When a user who was previously using the Cinder v1 API (which would have been required) updates to the new code the immediate defaults cause the cinder v2 API to be chosen. This is because we now default cinder_catalog_info to 'volumev2:cinder:publicURL'. So if a user was using the previous default value of 'volumev2:cinder:publicURL' their configuration would now be broken.
Given the new deprecation code hasn't been released yet I think we need to wait at least one release before we can make this change to our cinder_catalog_info default value.","Use v1 as default for cinder_catalog_info

In c5402ef4fc509047d513a715a1c14e9b4ba9674f we recently
added support for the Cinder v2 client. This change modified
the default value of the cinder_catalog_info config such
that an end user who was previously using the Cinder V1
API via the default config setting (by not setting it) would
have a broken Nova -> cinder configuration upon upgrade.

We should hold off on changing the default cinder_catalog_info
for one release to allow for proper deprecation.

Change-Id: I040b2c87ad0a2be92f31264e293794d97c27c965
Closes-bug: #1354499"
1439,c57f11321220a21af10d884c87ab989dc7dc2e69,1394688173,,1.0,23,5,2,2,1,0.905928216,True,6.0,622328.0,123.0,20.0,False,8.0,97763.0,20.0,3.0,722.0,2202.0,2538.0,616.0,1877.0,2152.0,243.0,471.0,564.0,0.256033578,0.495278069,0.592864638,602,1026,1291690,neutron,c57f11321220a21af10d884c87ab989dc7dc2e69,1,1,,"Bug #1291690 in neutron: ""delete router interface fail if neutron and nvp out of sync""","It's similar to https://bugs.launchpad.net/neutron/+bug/1251422, but wrt routers.
If we delete a router from neutron that is already deleted in nvp, it throw 404 error. The correct behavior should be to delete it from neutron, if it's already deleted in nvp.
rainbow:~ bhuvan$ neutron router-interface-delete tempest-router 67056b2d-a924-4456-9050-ed0baa0eaf1a
404-{u'NeutronError': {u'message': u'Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on subnet 67056b2d-a924-4456-9050-ed0baa0eaf1a', u'type': u'RouterInterfaceNotFoundForS
ubnet', u'detail': u''}}
neutron server log. Note: 404 error from nvp  is logged at INFO level. It should be a WARNING.
2014-03-12 22:42:26,149 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest-admin with project_id : csi-tenant-tempest and ro
les: csi-tenant-admin,csi-role-admin
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Matched PUT /routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Route path: '/routers/:(id)/remove_router_interface.:(format)', defaults: {'action': u'remove_router_interface', 'c
ontroller': <wsgify at 68316752 wrapping <function resource at 0x410fb18>>}
2014-03-12 22:42:26,151 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'remove_router_interface', 'controller': <wsgify at 68316752 wrapping <function resource at
 0x410fb18>>, 'id': u'd6f3c0c6-6884-467f-9a84-5a64b88f8936', 'format': u'json'}
2014-03-12 22:42:26,208 (neutron.api.v2.resource): INFO resource resource remove_router_interface failed (client error): Router d6f3c0c6-6884-467f-9a84-5a64b88f8936 has no interface on s
ubnet 67056b2d-a924-4456-9050-ed0baa0eaf1a
2014-03-12 22:42:26,210 (neutron.wsgi): INFO log write 17.199.81.86 - - [12/Mar/2014 22:42:26] ""PUT /v2.0/routers/d6f3c0c6-6884-467f-9a84-5a64b88f8936/remove_router_interface.json HTTP/1
.1"" 404 329 0.066915","NSX: Fix router-interface-delete returns 404 when router not in nsx

Previously, if one would run router-interface-delete and the router
was not found in NSX. Neutron would raise a 404 error and remove the interface
from the database. In this case it would be better if we did not raise
a 404 error as the caller shouldn't be aware that the backend is out of sync.

Closes-bug: #1291690

Change-Id: I2400f8b31817e6dd4bc8aa08f07e1613fc2deeae"
1440,c586d635387e9baa3c0857afb56d05137fcddd7c,1378977816,1.0,1.0,82,14,6,2,1,0.931976803,True,10.0,7372985.0,80.0,25.0,False,55.0,61673.0,124.0,7.0,1175.0,2796.0,2796.0,1071.0,2523.0,2523.0,377.0,1903.0,1903.0,0.062811565,0.316384181,0.316384181,1444,1199954,1199954,nova,c586d635387e9baa3c0857afb56d05137fcddd7c,1,1, ,"Bug #1199954 in OpenStack Compute (nova): ""VCDriver: Failed to resize instance""","Steps to reproduce:
nova resize <UUID> 2
Error:
 ERROR nova.openstack.common.rpc.amqp [req-762f3a87-7642-4bd3-a531-2bcc095ec4a5 demo demo] Exception during message handling
  Traceback (most recent call last):
    File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
      **args)
    File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
      result = getattr(proxyobj, method)(ctxt, **kwargs)
    File ""/opt/stack/nova/nova/exception.py"", line 99, in wrapped
      temp_level, payload)
    File ""/opt/stack/nova/nova/exception.py"", line 76, in wrapped
      return f(self, context, *args, **kw)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 218, in decorated_function
      pass
    File ""/opt/stack/nova/nova/compute/manager.py"", line 204, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 269, in decorated_function
      function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 246, in decorated_function
      e, sys.exc_info())
    File ""/opt/stack/nova/nova/compute/manager.py"", line 233, in decorated_function
      return function(self, context, *args, **kwargs)
    File ""/opt/stack/nova/nova/compute/manager.py"", line 2633, in resize_instance
      block_device_info)
    File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 410, in migrate_disk_and_power_off
      dest, instance_type)
    File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 893, in migrate_disk_and_power_off
      raise exception.HostNotFound(host=dest)
  HostNotFound:","VMware: fix VM resize bug

The following operations would fail due to invalid references, either by name
or UUID:
    - resize
    - resize-confirm
    - resize-revert

Closes-bug: #1199954

Change-Id: I39b804ed3c0c57455547d9ece0ef779cf73a5926"
1441,c58f2393b1c88268179205e349a2638f481bd492,1398358664,1.0,1.0,43,16,3,3,1,0.727817875,True,13.0,7701695.0,163.0,59.0,False,138.0,212935.6667,480.0,4.0,131.0,1964.0,1973.0,131.0,1838.0,1847.0,128.0,1941.0,1948.0,0.016405952,0.246979524,0.24786977,791,1218,1307416,nova,c58f2393b1c88268179205e349a2638f481bd492,1,1,,"Bug #1307416 in OpenStack Compute (nova): ""Unshelve instance needs handling exceptions""","There are some error cases not handled when we unshelve an instance in the conductor.
   nova/conductor/manager.py#823
if the key 'shelved_image_id' is not defined the current code will raise an KeyError not handled.
Also when the 'shelved_image_id' is set to None (which is not the expected behavior), the error is not correctly handled and the message could be confusing.","Improve conductor error cases when unshelving

This commit handles different cases when the key 'shelved_image_id'
is not defined or None. Also it tries to improve the error messages.

Closes-Bug: #1307416
Change-Id: Ia2d0a183eefccdfd6f612d86f60cd2930c1a4b23"
1442,c5ae5ad2637a561ccbc7484045e99d8140822b8d,1410498339,,1.0,14,2,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1294,1760,1367881,neutron,c5ae5ad2637a561ccbc7484045e99d8140822b8d,1,1,,"Bug #1367881 in neutron: ""l2pop RPC code throwing an exception in fdb_chg_ip_tun()""","I'm seeing an error in the l2pop code where it's failing to add a flow for the ARP entry responder.
This is sometimes leading to DHCP failures for VMs, although a soft reboot typically fixes that problem.
Here is the trace:
2014-09-10 15:10:36.954 9351 ERROR neutron.agent.linux.ovs_lib [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 None] OVS flows could not be applied on bridge br-tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib Traceback (most recent call last):
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     return method(*args, **kwargs)
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib     for mac, ip in after:
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.954 9351 TRACE neutron.agent.linux.ovs_lib
2014-09-10 15:10:36.955 9351 ERROR oslo.messaging.rpc.dispatcher [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Exception during message handling: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 55, in update_fdb_entries
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     self.fdb_update(context, fdb_entries)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 212, in fdb_update
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     getattr(self, method)(context, values)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 407, in _fdb_chg_ip
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     self.local_ip, self.local_vlan_map)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/common/log.py"", line 36, in wrapper
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     return method(*args, **kwargs)
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/venvs/openstack/local/lib/python2.7/site-packages/neutron/agent/l2population_rpc.py"", line 250, in fdb_chg_ip_tun
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher     for mac, ip in after:
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher TypeError: 'NoneType' object is not iterable
2014-09-10 15:10:36.955 9351 TRACE oslo.messaging.rpc.dispatcher
2014-09-10 15:10:36.957 9351 ERROR oslo.messaging._drivers.common [req-de0c2985-1fac-46a8-a42b-f0bad5a43805 ] Returning exception 'NoneType' object is not iterable to caller
I don't know this code well enough to suggest a fix - whether it's checking the return from agent_ports.items() better, or that there is a bug elsewhere, so any help would be appreciated.","Properly handle empty before/after notifications in l2pop code

Change-Id: I8644bb7cc2afb3b181397a478f96927990c0a4ca
Closes-Bug: #1367881"
1443,c5de5e7ab166e29304a01d7e310ae6ed32d22090,1402840748,,1.0,101,29,5,2,1,0.765087853,False,,,,,True,,,,,,,,,,,,,,,,,976,1415,1330065,nova,c5de5e7ab166e29304a01d7e310ae6ed32d22090,1,1,"“Also during spawing, if a datastore in maintenance mode gets choosen, since it had the largest disk space, the spawn would fail.”","Bug #1330065 in OpenStack Compute (nova): ""VMWare - Driver does not ignore Datastore in maintenance mode""","A datastore can be in maintenance mode. The driver does not ignore it both in stats update and while spawing instances.
During stats update, a wrong stats update is returned if a datastore is in maintenance mode.
Also during spawing, if a datastore in maintenance mode gets choosen, since it had the largest disk space, the spawn would fail.
The driver should ignore datastore in maintenance mode","VMWare Driver - Ignore datastore in maintenance mode

A datastore can be part of a datastore cluster. Any
datastore which is part of a cluster can be put in maintenance
mode. Instances cannot reside on a datastore if it is in
maintenance mode.
This fix ignores a datastore in the following scenarios
1. Stats update - Ignore a datastore during stats update.
2. Ignore a datastore during instance spawn.

Change-Id: Id0cb3f5fdb4d0bc4884cf5405fadc433d4a2b6ba
Closes-Bug: #1330065"
1444,c5e17f4dc2277ed422b060f860443d71a47fc440,1394740972,,2.0,5,1,2,2,1,0.650022422,True,1.0,93295.0,13.0,5.0,False,21.0,95907.0,60.0,4.0,705.0,3593.0,3969.0,635.0,2594.0,2927.0,693.0,3350.0,3717.0,0.091412013,0.441385669,0.489726027,612,1037,1292181,nova,c5e17f4dc2277ed422b060f860443d71a47fc440,1,1,,"Bug #1292181 in OpenStack Compute (nova): ""Cells: compute.instance.exists notification prior to rebuild has incorrect OS information""","Prior to a rebuild, nova issues a compute.instance.exists notification to capture usage for the period between the beginning of the day and the rebuild.
It should have the info for the instance prior to the rebuild, so if you are rebuilding from Centos 6.0 to Ubuntu 10.04, it should list a os_distribution of centos and an os_version of 6.0 Instead it's listing a distribution of Ubuntu and an os_version of 10.04 (i.e. the info for after the rebuild).","Don't sync [system_]metadata down to cells on instance.save()

A cell should be authoritative for its metadata and system_metadata.
rebuild relis on system_metadata not being updated until the cell
updates it itself.  update_instance_metadata relies on metadata not
being updated so that it can calculate the diff properly.

And before the objects conversion took place the compute.api.update()
method that instance.save() replaced was not used for syncing metadata
or system_metadata.  So this change goes back to previous behaviour for
cells.

Change-Id: I339da9d1ff6d0e59f6a4ab68fd396bdfca7a224b
Closes-bug: #1292181
Closes-bug: #1292185"
1445,c5e17f4dc2277ed422b060f860443d71a47fc440,1394740972,,2.0,5,1,2,2,1,0.650022422,True,1.0,93295.0,13.0,5.0,False,21.0,95907.0,60.0,4.0,705.0,3593.0,3969.0,635.0,2594.0,2927.0,693.0,3350.0,3717.0,0.091412013,0.441385669,0.489726027,613,1038,1292185,nova,c5e17f4dc2277ed422b060f860443d71a47fc440,1,1,,"Bug #1292185 in OpenStack Compute (nova): ""Cells: metadata update calls don't get to the virt driver""","When updating the metadata for an instance, the values should end up in xenstore for a guest to be able to query.  This is related to https://bugs.launchpad.net/nova/+bug/1292181 but with instance objects metadata is now synced down to cells earlier than it used to be.  This causes an issue with the metadata diff detection at the cell level so new keys are not pushed to the virt driver.  This causes them to not be set in xenstore in the xenserver driver.","Don't sync [system_]metadata down to cells on instance.save()

A cell should be authoritative for its metadata and system_metadata.
rebuild relis on system_metadata not being updated until the cell
updates it itself.  update_instance_metadata relies on metadata not
being updated so that it can calculate the diff properly.

And before the objects conversion took place the compute.api.update()
method that instance.save() replaced was not used for syncing metadata
or system_metadata.  So this change goes back to previous behaviour for
cells.

Change-Id: I339da9d1ff6d0e59f6a4ab68fd396bdfca7a224b
Closes-bug: #1292181
Closes-bug: #1292185"
1446,c5e186f466a4347b72868db8adbde9450215f107,1407254183,,1.0,27,1,2,2,1,0.371232327,False,,,,,True,,,,,,,,,,,,,,,,,1149,1604,1352786,neutron,c5e186f466a4347b72868db8adbde9450215f107,1,1,"“this was caused because the l3 service plugin was
    erroneously calling a method on self”","Bug #1352786 in neutron: ""router-update fails with 500 for dvr routers""","1.create a dvr with name say dvr1
2.add router gateway
3.now perform router update neutron router-update dvr1 --name dvr2
Actual Results:
Request Failed: internal server error while processing your request"" error is seen,trace related to L3RouterPlugin logs are seen
Expected Results:
router name should be updated","Fix 500 error  during router-update for dvr routers

This was caused because the l3 service plugin was
erroneously calling a method on self, whereas the
method is implemented by the core plugin.

Closes-bug: #1352786

Change-Id: I0746eee314730370b2df4bef6d9fd41680e2e3d1"
1447,c606043c64b31ba3289002dd9ac90b7566e1bca2,1402913076,,1.0,38,4,2,2,1,0.591672779,False,,,,,True,,,,,,,,,,,,,,,,,978,1417,1330431,nova,c606043c64b31ba3289002dd9ac90b7566e1bca2,1,1,,"Bug #1330431 in OpenStack Compute (nova): ""wrong lock name when operating instance events""","We use wrong lock name at here:
    def prepare_for_instance_event(self, instance, event_name):
        """"""Prepare to receive an event for an instance.
        This will register an event for the given instance that we will
        wait on later. This should be called before initiating whatever
        action will trigger the event. The resulting eventlet.event.Event
        object should be wait()'d on to ensure completion.
        :param instance: the instance for which the event will be generated
        :param event_name: the name of the event we're expecting
        :returns: an event object that should be wait()'d on
        """"""
        @utils.synchronized(self._lock_name)
        def _create_or_get_event():
            if instance.uuid not in self._events:
                self._events.setdefault(instance.uuid, {})
            return self._events[instance.uuid].setdefault(
                event_name, eventlet.event.Event())
        LOG.debug('Preparing to wait for external event %(event)s',
                  {'event': event_name}, instance=instance)
        return _create_or_get_event()
We should invoke self._lock_name, not pass it as name.
So will get log message as below:
2014-06-16 17:44:59.022 DEBUG nova.openstack.common.lockutils [req-97211458-bae1-473b-a3ad-47fd153ae30a admin admin] Got semaphore ""<function _lock_name at 0x7fe6a7edec08>"" from (pid=30672) lock /opt/stack/nova/nova/openstack/common/lockutils.py:168
Same problem for pop_instance_event and clear_events_for_instance","Fix wrong lock name for operating instance external events

This patch correct the wrong lock name for operating instance events.
Currently it use function object InstanceEvents._lock_name as the lock
name, the lock name will be like ""<function _lock_name at 0x7fe6a7edec08>"".
So when process the server external event, it will use one lock for all
the events that may belong to different instances. This isn't efficient.
This patch correct it to invoke InstanceEvents._lock_name to get the lock
name as '<instance-uuid>-events'. Then the lock is per instance.

Change-Id: I57712afe893933fde6ea7ab3f5d6328f04c65a8a
Closes-Bug: #1330431"
1448,c61c67c5afd100749b7b55b251bd4e4e3bb556a2,1386087418,,1.0,14,9,2,2,1,0.666578358,True,2.0,5142202.0,23.0,11.0,False,139.0,352539.0,415.0,4.0,70.0,784.0,850.0,70.0,722.0,788.0,18.0,628.0,646.0,0.00280567,0.092882457,0.095540461,135,536,1257355,nova,c61c67c5afd100749b7b55b251bd4e4e3bb556a2,1,1, live migration fails when using non-image backed disk ,"Bug #1257355 in OpenStack Compute (nova): ""live migration fails when using non-image backed disk""","running live migration with --block-migrate fails if the disk was resized before (aka detached from the cow image). This is because nova.virt.libvirt.driver.py uses disk_size, not virt_disk_size for re-creating the qcow2 file on the destination host. in the case of qcow2 files, qemu-img however needs to get the virt_disk size passed down, otherwise the block migration step will not be able to convert all blocks.","Setup destination disk from virt_disk_size

When running live-migration --block-migrate on a qcow2 backed
VM without cow image, the destination qcow2 file should be created
with the virtual disk size. For raw images, the virt_disk_size
is set to disk_size to ensure that virt_disk_size is always the
size of the disk that should be re-created.

Update unit tests to be more strict and check for sizes to be correct.

Closes-Bug: #1257355

Change-Id: Ie3be46024f06b9f59af92f5e3918a1958386d4f1"
1449,c6c4a20777921dc1b21e80edb96ccd957a054c68,1396273855,,1.0,13,12,6,3,1,0.912616554,True,10.0,390318.0,74.0,13.0,False,45.0,1137990.0,78.0,3.0,1150.0,1257.0,1258.0,979.0,1081.0,1082.0,691.0,768.0,769.0,0.657794677,0.730988593,0.731939163,686,1112,1297875,neutron,c6c4a20777921dc1b21e80edb96ccd957a054c68,0,0,tests,"Bug #1297875 in neutron: ""some tests call ""called_once_with_args"" with no assert, those lines are ignored""","A few tests use ""called_once_with_args""  instead of mock's ""assert_called_once_with_args""
without checking the result.
That means that we're not asserting for that to happen.
Those tests need to be fixed.
[majopela@f20-devstack neutron]$ grep "".called_once_with"" * -R | grep -v assert
neutron/tests/unit/test_dhcp_agent.py:            disable.called_once_with_args(network.id)
neutron/tests/unit/test_dhcp_agent.py:                uuid5.called_once_with(uuid.NAMESPACE_DNS, 'localhost')
neutron/tests/unit/test_post_mortem_debug.py:        mock_print_exception.called_once_with(*exc_info)
neutron/tests/unit/test_db_migration.py:                mock_open.write.called_once_with('a')
neutron/tests/unit/test_agent_netns_cleanup.py:                ovs_br_cls.called_once_with('br-int', conf.AGENT.root_helper)
neutron/tests/unit/test_metadata_agent.py:            self.eventlet.wsgi.server.called_once_with(","fixes tests using called_once_ without assert

A few tests were using mock's called_once, or called_once_with_args
instead of assert_called_once or assert_called_once_with_args. Those
methods return a bool that needs to be actively checked.

The tests are fixed to avoid them from passing if the call condition
is not met.

Change-Id: I21e5257b26b2a08cc8f0b108233d1d5cc0b97b89
Closes-bug: #1297875"
1450,c70749b9c8d9575ba28b2c012615d6bae146ccc3,1391765635,,1.0,37,16,2,2,1,0.941423108,True,3.0,8981315.0,54.0,16.0,False,142.0,150826.5,589.0,2.0,67.0,2297.0,2350.0,64.0,1634.0,1685.0,67.0,2097.0,2150.0,0.009326567,0.287752023,0.295021259,377,790,1277422,nova,c70749b9c8d9575ba28b2c012615d6bae146ccc3,1,1,wrong quota calculation,"Bug #1277422 in OpenStack Compute (nova): ""Quotas change incorrectly when a resizing instance is deleted""","When deleting a resizing instance that has not yet finished resizing, quotas should be adjusted for the old flavor type. Instead it incorrectly use values from the new flavor.
This was originally reported and fixed in bug 1099729 but has since resurfaced with the move to objects (commit  dce64683291ba2cdb5e6617e01ccc2909254acb4). This was made possible by a prior change (commit a56f0b33069b919ebb24c4afdcc6b6c31592c98e) that accidentally removed the test put in place to guard against this error ever happening again.","Fix wrong quota calculation when deleting a resizing instance

When deleting a resizing instance that has not yet finished resizing,
quotas should be adjusted for the old flavor type. Instead it
incorrectly use values from the new flavor.

This was previously fixed in bug 1099729, but has since resurfaced
with the change to use objects in delete methods.

Change-Id: I9bc68abc0d86784c642483dd0cc46c6b9c6a0c69
Closes-Bug: #1277422"
1451,c70c2719d700902854ef0381cb725722ac2da05e,1396339529,1.0,1.0,4,8,3,3,1,0.873479028,True,2.0,321554.0,55.0,12.0,False,17.0,392392.6667,45.0,4.0,1178.0,1376.0,1397.0,1007.0,1171.0,1192.0,714.0,825.0,841.0,0.673258004,0.777777778,0.792843691,713,1139,1300628,neutron,c70c2719d700902854ef0381cb725722ac2da05e,0,0,Refactoring “This needs to be removed because it's resulting in the entire portbinding_db schema for one small function.”,"Bug #1300628 in neutron: ""BigSwitch ML2 driver uses portbindingsports table""",The Big Switch ML2 driver references the deprecated portbindings_db in the port location tracking code. This needs to be removed because it's resulting in the entire portbinding_db schema for one small function.,"Stop using portbindings_db in BSN ML2 driver

Avoids using the portbindings_db in the Big Switch
ML2 driver since ML2 has deprecated that database for
its own version that tracks the same information.

Also eliminates unnecessary 'binding_host' field since
it is now always the same as as the 'portbinding:host_id'
field.

Closes-Bug: #1300628
Change-Id: I17d47cb446cdd2e989c3e0d01b797a81309faaa7"
1452,c756d3c3ca9e90fab5e1d3ff37af33972747ba70,1411406291,,1.0,1,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1346,1814,1372571,neutron,c756d3c3ca9e90fab5e1d3ff37af33972747ba70,0,0,Bug in test,"Bug #1372571 in neutron: ""Neutron cannot possibly pass unit tests under Python 2.6""","On neutron/tests/unit/test_api_v2.py:148, _get_collection_kwargs() uses collections.OrderedDict, which does not exist in Python 2.6.  As the ordering here is clearly unimportant, this should be converted to be just a plain dict.","Eliminate OrderedDict from test_api_v2.py

Neutron cannot possibly be passing tests under Python 2.6, as
neutron/tests/unit/test_api_v2.py is referencing
collections.OrderedDict, which does not exist in Python 2.6.
Since there is no reason to use an OrderedDict in this case,
this replaces it with a simple dict.

Change-Id: I1b9886f508c4c8b96cf50c50f157c1960da433fc
Closes-Bug: #1372571"
1453,c75aada35b13da33d08f51d14fd36640d8b735fa,1393472322,1.0,1.0,79,55,2,2,1,0.88846679,True,3.0,505552.0,50.0,8.0,False,11.0,620988.0,15.0,4.0,11.0,610.0,611.0,11.0,530.0,531.0,11.0,363.0,364.0,0.013745704,0.416953036,0.418098511,490,908,1285473,neutron,c75aada35b13da33d08f51d14fd36640d8b735fa,0,0,Use x to avoid inconsistencies,"Bug #1285473 in neutron: ""Use database session from the context in Cisco N1kv plugin""",Use database session from the context wherever possible during database transactions to avoid inconsistencies in the Cisco N1kv plugin.,"Use database session from the context in N1kv plugin

Avoid inconsistencies by using context.session for database
transactions wherever possible in the Cisco N1KV plugin.

Change-Id: Ic0784cbbf50beae6bb3b124c959ae90f3affb604
Closes-Bug: #1285473"
1454,c75cd9a8b9da86b9d9e7ffd6512fe13b1913fd85,1402558791,,1.0,11,2,2,2,1,0.619382195,True,4.0,720666.0,24.0,6.0,False,159.0,1372559.0,482.0,1.0,1.0,1232.0,1232.0,1.0,1231.0,1231.0,1.0,1222.0,1222.0,0.000250063,0.152913228,0.152913228,883,1319,1318891,nova,c75cd9a8b9da86b9d9e7ffd6512fe13b1913fd85,1,1,,"Bug #1318891 in OpenStack Compute (nova): ""detach_pci_devices failed""","when detach a pci device from instance
the method  _detach_pci_devices will check if pci device detached
>      for hdev in [d for d in guest_config.devices
>                        if d.type == 'pci']:
guest_config.devices will have more device not only pci, like disk
in LibvirtConfigGuestDisk has no attribute type
  File ""/home/xiaoding/nova/nova/tests/virt/libvirt/test_libvirt.py"", line 684, in test_detach_pci_devices
    conn._detach_pci_devices(FakeDomain(), pci_devices)
  File ""/home/xiaoding/nova/nova/virt/libvirt/driver.py"", line 2774, in _detach_pci_devices
    if d.type == 'pci']:
AttributeError: 'LibvirtConfigGuestDisk' object has no attribute 'type'
https://review.openstack.org/#/c/93383/","Fix detaching pci device failed

after booting an instance with flavor has pci device,
and then resize the instance, it will raise an ""AttributeError"".
this because in the method _detach_pci_devices(),
guest_config.devices will have more device not only pci, like disk.

Closes-bug: #1318891

Change-Id: Id52883a4f1fa7d05d61b31674acd1ca57918c3e1"
1455,c77868f44b1e72be2b65f697a1f0f3b32126e581,1380081304,,1.0,12,11,2,1,1,0.666578358,True,1.0,73477.0,15.0,6.0,False,8.0,36338.0,13.0,3.0,266.0,638.0,758.0,255.0,618.0,728.0,255.0,595.0,704.0,0.247821878,0.57696031,0.682478219,1468,1212710,1212710,cinder,c77868f44b1e72be2b65f697a1f0f3b32126e581,1,1, ,"Bug #1212710 in Cinder: ""cinder-scheduler fails to transmit the image_id parameter to cinder-volume""","This break the create volume from image feature with RBD backend and perhaps other backends as well?
On a grizzly setup with both glance and cinder configured with RBD
backend, I cannot find a way to create a new volume from a (raw) image.
The resulting volume is correctly created but remains full of NULL bytes.
$ glance show 92af7175-9478-42c4-8ed0-b336362ff3f7
URI: https://example.com:9292/v1/images/92af7175-9478-42c4-8ed0-b336362ff3f7
Id: 92af7175-9478-42c4-8ed0-b336362ff3f7
Public: Yes
Protected: No
Name: precise-server-cloudimg-amd64.raw
Status: active
Size: 2147483648
Disk format: raw
Container format: bare
Minimum Ram Required (MB): 0
Minimum Disk Required (GB): 0
Owner: 8226a848c4eb43e69b1a0f00c582e31f
Created at: 2013-08-11T07:22:13
Updated at: 2013-08-11T07:29:10
$ cinder create 10 --image-id 92af7175-9478-42c4-8ed0-b336362ff3f7
--display-name boot-from-volume
In the following logs, cinder-scheduler gets the correct image_id
parameter but cinder-volume gets None instead which looks weird.
==> cinder-scheduler.log <==
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'5e5eca077ae149a6a64b1760bc55cc55', u'_context_read_deleted': u'no',
u'args': {u'request_spec': {u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'volume_properties':
{u'status': u'creating', u'volume_type_id': None, u'display_name':
u'boot-from-volume', u'availability_zone': u'nova', u'attach_status':
u'detached', u'source_volid': None, u'metadata': {}, u'volume_metadata':
[], u'display_description': None, u'snapshot_id': None, u'user_id':
u'01d6651d807649718e01be2999b11af0', u'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'id':
u'79e98020-555c-4179-97ff-867a79a37160', u'size': 10}, u'
volume_type': {}, u'image_id': u'92af7175-9478-42c4-8ed0-b336362ff3f7',
u'source_volid': None, u'snapshot_id': None}, u'volume_id':
u'79e98020-555c-4179-97ff-867a79a37160', u'filter_properties': {},
u'topic': u'cinder-volume', u'image_id':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'snapshot_id': None},
u'_context_tenant': u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_auth_token': '<SANITIZED>', u'_context_is_admin': False,
u'version': u'1.2', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f',
u'_context_timestamp': u'2013-08-11T22:34:22.569488', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'm
ethod': u'create_volume', u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'adm
in'], 'timestamp': u'2013-08-11T22:34:22.569488', 'auth_token':
'<SANITIZED>', 'remote_address': u'1.2.3.4', 'quota_class': None,
'is_admin': False, 'user': u'01d66
51d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] Making
asynchronous cast on cinder-volume.sun2-test...
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp]
UNIQUE_ID is b3bc44f910054661a7a532bd6bfbe5bb.
==> cinder-volume.log <==
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] received
{u'_context_roles': [u'Member', u'_member_', u'admin'],
u'_context_request_id': u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac',
u'_context_quota_class': None, u'_unique_id':
u'b3bc44f910054661a7a532bd6bfbe5bb', u'args': {u'request_spec': None,
u'volume_id': u'79e98020-555c-4179-97ff-867a79a37160',
u'allow_reschedule': True, u'filter_properties':
u'92af7175-9478-42c4-8ed0-b336362ff3f7', u'source_volid': None,
u'image_id': None, u'snapshot_id': None}, u'_context_tenant':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_auth_token':
'<SANITIZED>', u'_context_timestamp': u'2013-08-11T22:34:22.569488',
u'_context_is_admin': False, u'version': u'1.4', u'_context_project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', u'_context_user':
u'01d6651d807649718e01be2999b11af0', u'_context_
read_deleted': u'no', u'_context_user_id':
u'01d6651d807649718e01be2999b11af0', u'method': u'create_volume',
u'_context_remote_address': u'1.2.3.4'}
2013-08-12 00:34:23    DEBUG [cinder.openstack.common.rpc.amqp] unpacked
context: {'user_id': u'01d6651d807649718e01be2999b11af0', 'roles':
[u'Member', u'_member_', u'admin'], 'timestamp':
u'2013-08-11T22:34:22.569488', 'auth_token': '<SANITIZED>',
'remote_address': u'1.2.3.4', 'quota_class': None, 'is_admin': False,
'user': u'01d6651d807649718e01be2999b11af0', 'request_id':
u'req-a0539c82-8777-4dd3-a6ce-87d5d37120ac', 'project_id':
u'8226a848c4eb43e69b1a0f00c582e31f', 'read_deleted': u'no', 'tenant
': u'8226a848c4eb43e69b1a0f00c582e31f'}
2013-08-12 00:34:23    DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating lv of size 10G
2013-08-12 00:34:23     INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating
2013-08-12 00:34:23    DEBUG [cinder.utils] Running cmd (subprocess):
rbd --help
2013-08-12 00:34:23    DEBUG [cinder.utils] Running cmd (subprocess):
rbd create --pool volumes --size 10240
volume-79e98020-555c-4179-97ff-867a79a37160 --new-format
2013-08-12 00:34:23    DEBUG [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: creating export
2013-08-12 00:34:24     INFO [cinder.volume.manager] volume
volume-79e98020-555c-4179-97ff-867a79a37160: created successfully
2013-08-12 00:34:24     INFO [cinder.volume.manager] Clear capabilities
cinder.conf:
[DEFAULT]
rootwrap_config = /etc/cinder/rootwrap.conf
api_paste_confg = /etc/cinder/api-paste.ini
iscsi_helper = tgtadm
volume_name_template = volume-%s
volume_group = cinder-volumes
verbose = True
auth_strategy = keystone
state_path = /var/lib/cinder
lock_path = /var/lock/cinder
volumes_dir = /var/lib/cinder/volumes
rabbit_host=127.0.0.1
sql_connection=mysql://cinder:cinder@127.0.0.1/cinder?charset=utf8
api_paste_config=/etc/cinder/api-paste.ini
debug=True
rabbit_userid=nova
osapi_volume_listen=0.0.0.0
rabbit_virtual_host=/
scheduler_driver=cinder.scheduler.simple.SimpleScheduler
rabbit_hosts=127.0.0.1:5672
rabbit_ha_queues=False
rabbit_password=secret
rabbit_port=5672
rpc_backend=cinder.openstack.common.rpc.impl_kombu
sql_idle_timeout=3600
volume_driver=cinder.volume.drivers.rbd.RBDDriver
rbd_user=volumes
max_gigabytes=25000
rbd_pool=volumes
rbd_secret_uuid=XXXX
glance_api_version=2
glance-api.conf:
[DEFAULT]
verbose = True
debug = True
default_store = rbd
bind_host = 0.0.0.0
bind_port = 9292
log_file = /var/log/glance/api.log
backlog = 4096
sql_connection = mysql://glance:glance@127.0.0.1/glance
sql_idle_timeout = 3600
workers = 2
show_image_direct_url = True
registry_host = 0.0.0.0
registry_port = 9191
registry_client_protocol = http
notifier_strategy = noop
rabbit_host = localhost
rabbit_port = 5672
rabbit_use_ssl = false
rabbit_userid = guest
rabbit_password = guest
rabbit_virtual_host = /
rabbit_notification_exchange = glance
rabbit_notification_topic = notifications
rabbit_durable_queues = False
filesystem_store_datadir = /var/lib/glance/images/
rbd_store_ceph_conf = /etc/ceph/ceph.conf
rbd_store_user = images
rbd_store_pool = images
rbd_store_chunk_size = 8
delayed_delete = False
scrub_time = 43200
scrubber_datadir = /var/lib/glance/scrubber
image_cache_dir = /var/lib/glance/image-cache/
[keystone_authtoken]
auth_host = 127.0.0.1
auth_port = 35357
auth_protocol = http
admin_tenant_name = services
admin_user = glance
admin_password = secret
[paste_deploy]
flavor=keystone+cachemanagement","Pass correct args to vol_rpc create_volume calls

In the chance and simple scheduler, create volume was originally using
snapshot_id and image_id for request_spec and filter_properties. This
corrects that by passing the correct arguments and keyword arguments to
create_volume.

Change-Id: Icbcfbfb28f36e1f75519bf5ad6fcbcc12a9b4ec1
Closes-Bug: #1212710"
1456,c860280a92c2d0e32897d71aa4c6f083ee13345c,1410449539,,1.0,20,5,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,1305,1771,1368404,nova,c860280a92c2d0e32897d71aa4c6f083ee13345c,1,1,,"Bug #1368404 in OpenStack Compute (nova): ""Uncaught 'libvirtError: Domain not found' errors during destroy""","Some uncaught libvirt errors may result in instances being set to ERROR state and is causing sporadic gate failures. This can happen for any of the code paths that use _destroy().  Here is a recent example of a failed resize:
[req-06dd4908-382e-455e-854e-e4d42a4bf62b TestServerAdvancedOps-724416891 TestServerAdvancedOps-711228572] [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Setting instance vm_state to ERROR
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] Traceback (most recent call last):
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 5902, in _error_out_instance_on_exception
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     yield
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 3658, in resize_instance
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 5468, in migrate_disk_and_power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self.power_off(instance, timeout, retry_interval)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2400, in power_off
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self._destroy(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 998, in _destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     timer.start(interval=0.5).wait()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 121, in wait
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     return hubs.get_hub().switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 293, in switch
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     return self.greenlet.switch()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/openstack/common/loopingcall.py"", line 81, in _inner
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     self.f(*self.args, **self.kw)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 971, in _wait_for_destroy
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     dom_info = self.get_info(instance)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 3922, in get_info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     dom_info = virt_dom.info()
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 183, in doit
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 141, in proxy_call
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     rv = execute(f, *args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 122, in execute
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     six.reraise(c, e, tb)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 80, in tworker
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     rv = meth(*args, **kwargs)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 1068, in info
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]     if ret is None: raise libvirtError ('virDomainGetInfo() failed', dom=self)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d] libvirtError: Domain not found: no domain with matching uuid '525f4f95-f631-4fbb-a884-20c37711fb0d' (instance-00000097)
2014-09-05 01:08:37.123 26984 TRACE nova.compute.manager [instance: 525f4f95-f631-4fbb-a884-20c37711fb0d]","Make sure libvirt VIR_ERR_NO_DOMAIN errors are handled correctly

Some libvirt VIR_ERR_NO_DOMAIN errors leak outside of the libvirt
driver, which may result in instances being set to ERROR state.

Make sure such errors are either ignored (during destroy) or
otherwise turned into InstanceNotFound errors.

Change-Id: I0ac2989062247a05162825760367488f4f90bd04
Closes-Bug: #1368404"
1457,c883862751d3081bbc05eb4b40fc15abd4fa5b1b,1378777352,,1.0,12,4,2,2,1,0.954434003,True,1.0,1279474.0,6.0,2.0,False,26.0,1676417.0,39.0,2.0,353.0,2570.0,2719.0,352.0,2249.0,2398.0,349.0,2440.0,2586.0,0.058557805,0.408398862,0.432825832,1456,1206884,1206884,nova,c883862751d3081bbc05eb4b40fc15abd4fa5b1b,0,0,Test files,"Bug #1206884 in OpenStack Compute (nova): ""Enable v3 test_create_multiple_servers integrated test""","Need to enable the integrated test_create_multiple_servers v3 version of the test
when the V3 multiple create extension merges","Enable test_create_multiple_servers test for V3 API

Now that the multiple create extension has merged we can enable
this test for the V3 API. Need to also override post_server
for the test client for the V3 API as the data return when requesting
a reservation id in the V3 API has changed

Change-Id: I7f34f1e90b872c4679b5c54fe9e4b0f4420910ab
Closes-Bug: #1206884"
1458,c892a0f398b664bacf8a2a0302f51e3d9b96619d,1401362541,1.0,1.0,91,6,2,2,1,0.98700443,False,,,,,True,,,,,,,,,,,,,,,,,931,1367,1324131,neutron,c892a0f398b664bacf8a2a0302f51e3d9b96619d,0,0,“Radware LBaaS driver should support HA backend”,"Bug #1324131 in neutron: ""Radware LBaaS driver should support HA backend""","Radware LBaaS driver should be able to work with a backend that was configured in HA  mode.
The driver should try and call the other node in the HA pair and see if it is active.","Radware LBaaS driver is able to flip to a secondary backend node

Change-Id: Ifbfef493d5339f61dcf58dddcc8e3830aaf06bf1
Closes-Bug: #1324131"
1459,c8bdff1533e54787d9a3dd98fe57a1e2e0d82e73,1402079155,,1.0,2,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,962,1400,1328321,neutron,c8bdff1533e54787d9a3dd98fe57a1e2e0d82e73,1,1,,"Bug #1328321 in neutron: ""Big Switch: Consistency watchdog calling wrong method""","The consistency watch dog is calling the wrong method for a health check which is raising an exception. However, since it is in a greenthread, the exception is silently discarded so the watchdog dies without any indication.","Big Switch: Call correct method in watchdog

Updates the consistency watchdog fuction to call
the correct rest method.

Closes-Bug: #1328321
Change-Id: I86ce0af36f6764a3f1e789602cef52758caedc8b"
1460,c8ded6429616b798947072a62cb1b5ee4ea51209,1385416510,1.0,1.0,40,3,2,2,1,0.968164732,True,8.0,463661.0,37.0,14.0,False,39.0,222856.0,153.0,5.0,136.0,2444.0,2509.0,136.0,2137.0,2202.0,133.0,2318.0,2381.0,0.020029895,0.346636771,0.356053812,69,397,1248799,nova,c8ded6429616b798947072a62cb1b5ee4ea51209,1,1,,"Bug #1248799 in OpenStack Compute (nova): ""vm would be stuck in unshelving when unshelve fails""","when unshelve a vm, if this vm has been offloaded, the process would involve re-scheduling.
in nova/conductor/manager.py def  unshelve_instance(self, context, instance):
elif instance.vm_state == vm_states.SHELVED_OFFLOADED:
            try:
                with compute_utils.EventReporter(context, self.db,
                        'get_image_info', instance.uuid):
                    image = self._get_image(context,
                            sys_meta['shelved_image_id'])
            except exception.ImageNotFound:
                with excutils.save_and_reraise_exception():
                    LOG.error(_('Unshelve attempted but vm_state not SHELVED '
                                'or SHELVED_OFFLOADED'), instance=instance)
                    instance.vm_state = vm_states.ERROR
                    instance.save()
            filter_properties = {}
            hosts = self._schedule_instances(context, image,
                                             filter_properties,instance)           <<<<<this re-scheduling would cause exception,when it occurs,the
         <<<<<<instance will be stuck in task_state: unshelving forever
            host = hosts.pop(0)['host']
            self.compute_rpcapi.unshelve_instance(context, instance, host,
                    image)","instance state will be stuck in unshelving when unshelve fails

When unshelve an instance, if this instance has been offloaded,
the conductor manager will involve re-schedule for the instance.
If re-schedule failed to find a target host for the unsleved
instance, then the instance will be stuck in unshelving state.

This patch fix the issue as this: If re-schedule failed to find a
target host for unshelve the instance, then conductor manager will
try to rollback the instance to unshelve state.

Change-Id: If49b2d2c9263b853c745ce25fe146ade51948123
Closes-Bug: #1248799"
1461,c8ff2a10697303d8a04980d258a12449d12d587a,1386836002,1.0,1.0,73,24,6,3,1,0.818939757,True,2.0,4861797.0,21.0,9.0,False,12.0,3822221.333,23.0,4.0,179.0,752.0,811.0,177.0,673.0,731.0,138.0,318.0,354.0,0.2224,0.5104,0.568,173,575,1259965,neutron,c8ff2a10697303d8a04980d258a12449d12d587a,0,0,Feature: Need to make server return also members and monitors which are in pending states.,"Bug #1259965 in neutron: ""[LBaaS] Creation of a health monitor after creating a VIP doesn't work correctly""","Description
===========
After making the following steps:
1. Create a pool
2. Create members
3. Create a VIP
The attempt to create a health monitor and associate it with the created pool
leads to the situation when the health monitor is shown as active
but it is not being added to /opt/stack/data/neutron/lbaas/<pool_id>/conf:
# neutron lb-healthmonitor-show c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
+----------------+------------------------------------
| Field          | Value
+----------------+------------------------------------
| admin_state_up | True
| delay          | 3
| expected_codes | 200
| http_method    | GET
| id             | c6ddc6e3-4a91-4a81-a110-fc3a44f311a0
| max_retries    | 3
| pools          | {""status"": ""ACTIVE"", ""status_description"": null, ""pool_id"": ""50497a9f-1439-431b-ac50-dd4cca0216fa""}
| tenant_id      | dcf66bfd01aa4b82b10d8fb263ef375d
| timeout        | 3
| type           | HTTP
| url_path       | /
+----------------+------------------------------------
# cat /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/conf
global
        daemon
        user nobody
        group nogroup
        log /dev/log local0
        log /dev/log local1 notice
        stats socket /opt/stack/data/neutron/lbaas/50497a9f-1439-431b-ac50-dd4cca0216fa/sock mode 0666 level user
defaults
        log global
        retries 3
        option redispatch
        timeout connect 5000
        timeout client 50000
        timeout server 50000
frontend 5842ed27-02b9-499b-8512-bcf071a81ab2
        option tcplog
        bind 10.0.0.4:80
        mode http
        default_backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        option forwardfor
backend 50497a9f-1439-431b-ac50-dd4cca0216fa
        mode http
        balance roundrobin
        option forwardfor
        server 78bbb52b-06e9-4eb6-9af9-95ecdf41c83d 10.0.0.3:80 weight 1","LBaaS: fix handling pending create/update members and health monitors

When agent requests loadbalancer logical config from server,
server returns only active pool members and health_monitors.
Need to make server return also members and monitors which are in pending states.

Also a small refactoring moving ACTIVE_PENDING set to common place

Change-Id: I8e10004f199f982b055da18ea7a0e5e4d11fa7fb
Closes-Bug: #1259965"
1462,c91842af547f9d70ac2a538477d89da1e3efafa9,1384907684,0.0,1.0,102,16,4,2,1,0.844473572,True,4.0,10172428.0,51.0,20.0,False,45.0,717830.25,109.0,2.0,15.0,1863.0,1870.0,15.0,1581.0,1588.0,15.0,1766.0,1773.0,0.002414001,0.266596258,0.267652384,1786,1252967,1252967,nova,c91842af547f9d70ac2a538477d89da1e3efafa9,1,1, ,"Bug #1252967 in OpenStack Compute (nova): ""VMware: delete vm snapshot after nova snapshot""","When performing a nova snapshot, the vmware driver takes a VM snapshot and copies out the snapshotted disk. After the operation, the VM snapshot should be deleted.","VMware: delete vm snapshot after nova snapshot

When performing a nova snapshot, the vmware driver takes a VM snapshot
and copies out the snapshotted disk. After the operation, the VM
snapshot should be deleted, to prevent growing disk chain(s) on each nova
snapshot.

Closes-Bug: #1252967
Change-Id: I7861ccce0153e45eea0316a2e8dd824210db1660"
1463,c9226a858201a6c3691390259b46a643e8dc8b2f,1394054651,,1.0,2,1,1,1,1,0.0,True,1.0,26411.0,19.0,3.0,False,7.0,179694.0,7.0,2.0,4.0,435.0,437.0,4.0,354.0,356.0,4.0,230.0,232.0,0.005543237,0.256097561,0.258314856,535,956,1288420,neutron,c9226a858201a6c3691390259b46a643e8dc8b2f,0,0, IBM Plugin need db migration,"Bug #1288420 in neutron: ""IBM Plugin need db migration for ext gw mode""",The plugin requires one more Alembic migration script; needs to be added to the list of plugins in the script for ext_gw_mode.,"Adds the missing migration for gw_ext_mode

Requires adding the plugin to the script for
db migration for the ext_gw_mode in routers table.

Change-Id: Iefa121b5c3a60ed804bf3927d91e64d843a28fd1
Closes-Bug: #1288420"
1464,c94b71c2d42a651f3b24d5f0ac1819bfa2a79a12,1391569458,,1.0,4,5,3,3,1,0.852792488,True,3.0,1710471.0,46.0,13.0,False,53.0,709221.0,111.0,5.0,337.0,2639.0,2817.0,337.0,2034.0,2212.0,320.0,2371.0,2536.0,0.044220967,0.326766772,0.349497176,365,777,1276398,nova,c94b71c2d42a651f3b24d5f0ac1819bfa2a79a12,0,0,There is a bug in tests. Change the place of the configuration to satisfy the tests,"Bug #1276398 in OpenStack Compute (nova): ""nova.tests.api.ec2.test_cloud is missing consoleauth_manager import""","If you run nova unit tests outside of a virtualenv, like with using nosetests on python 2.6 (which should be supported but not really enforced), then the ec2 test_cloud test fails due to not having the consoleauth_manager config option in scope:
Traceback (most recent call last):
  File ""/root/nova/nova/tests/api/ec2/test_cloud.py"", line 178, in setUp
    self.consoleauth = self.start_service('consoleauth')
  File ""/root/nova/nova/test.py"", line 295, in start_service
    svc = self.useFixture(ServiceFixture(name, host, **kwargs))
  File ""/usr/lib/python2.6/site-packages/testtools/testcase.py"", line 628, in useFixture
    fixture.setUp()
  File ""/root/nova/nova/test.py"", line 174, in setUp
    self.service = service.Service.create(**self.kwargs)
  File ""/root/nova/nova/service.py"", line 272, in create
    manager = CONF.get(manager_cls, None)
  File ""/usr/lib64/python2.6/_abcoll.py"", line 336, in get
    return self[key]
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1626, in __getitem__
    return self.__getattr__(key)
  File ""/usr/lib/python2.6/site-packages/oslo/config/cfg.py"", line 1622, in __getattr__
    raise NoSuchOptError(name)
NoSuchOptError: no such option: consoleauth_manager
There is a mailing list thread related to this, but not for ec2:
http://lists.openstack.org/pipermail/openstack-dev/2013-September/014896.html
Simply importing this fixes the problem:
CONF.import_opt('consoleauth_manager', 'nova.consoleauth.manager')","Move consoleauth_manager option into nova.service and fix imports

Moves the consoleauth_manager option into nova.service like the other
manager options in commit 39ce4032.

The thinking for having it in nova.service is that's where
CONF.get('%_manager'...) is called.  It also makes no sense for the
option to be declared in nova.consoleauth.manager because if you change
this config option, then you don't want nova.consoleauth.manager loaded.

Closes-Bug: #1276398

Change-Id: I85e089239228920e9e58284cf6ff52e43bf85ab0"
1465,c97069dc9a73344ebdc7b686133269850a81b3b2,1412726770,,1.0,28,8,2,2,1,0.997772472,False,,,,,True,,,,,,,,,,,,,,,,,1393,1863,1379609,neutron,c97069dc9a73344ebdc7b686133269850a81b3b2,1,0,Change requirements.:  “it does not delete the older profile bindings and maintains them”,"Bug #1379609 in neutron: ""Cisco N1kv: Fix add-tenant in update network profile""","During cisco-network-profile-update, if a tenant id is being added to the network profile, the current behavior is to remove all the tenant-network profile bindings and add the new list of tenants. This works well with horizon since all the existing tenant UUIDs, along with the new tenant id, are passed during update network profile.
If you try to update a network profile and add new tenant to the network profile via CLI, this will replace the existing tenant-network profile bindings and add only the new one.
Expected behavior is to not delete the existing tenant bindings and instead only add new tenants to the list.","Cisco N1kv: Fix update network profile for add tenants

This patch makes sure that while updating network profile to add new
tenants, it does not delete the older profile bindings and maintains
them, while adds only the new tenant ids to the bindings.

Change-Id: I862eb1c400e022334a2f6a4078425448bb144843
Closes-bug: #1379609"
1466,c9a020f121a85ff88b890e964dc8c1f6d0ff5843,1393329609,,1.0,17,11,3,3,1,0.968672472,True,5.0,3877252.0,78.0,19.0,False,69.0,105513.0,156.0,4.0,74.0,3725.0,3732.0,74.0,3048.0,3055.0,74.0,2784.0,2791.0,0.010075228,0.374126814,0.375067168,371,783,1276731,nova,c9a020f121a85ff88b890e964dc8c1f6d0ff5843,0,0,Suggestion:  should not rely,"Bug #1276731 in OpenStack Compute (nova): ""simple_tenant_usage extension should not rely on looking up flavors""","The simple_tenant_usage extension gets the flavor data from the instance and then looks up the flavor from the database to return usage information. Since we now store all of the flavor data in the instance itself, we should use that information instead of what the flavor currently says is right. This both (a) makes it more accurate and (b) avoids us failing to return usage info if a flavor disappears.","Use instance data instead of flavor in simple_tenant_usage extension

The simple_tenant_usage extension gets the flavor data from the instance
and then looks up the flavor from the database to return usage information.
Since we now store all of the flavor data in the instance itself,
we should use that information instead of what the flavor currently
says is right. This both (a) makes it more accurate and (b)
avoids us failing to return usage info if a flavor disappears.

Change-Id: I1472d78e8e920013a1db5c76e04715fa04b696c2
Closes-Bug: #1276731"
1467,c9a034c838e2c5a1f636656ead9aa3b6f7d06603,1399006147,,1.0,52,20,10,4,1,0.776805114,True,4.0,1746143.0,30.0,7.0,False,69.0,1537654.4,206.0,1.0,82.0,258.0,280.0,82.0,252.0,274.0,75.0,229.0,246.0,0.06255144,0.189300412,0.203292181,1841,1315237,1315237,Glance,c9a034c838e2c5a1f636656ead9aa3b6f7d06603,1,1, ,"Bug #1315237 in Glance: ""Glance should not call configure and configure_add two times at startup""","Currently at startup Glance is first creating the stores and then verifying the default store (cmd/api.py)
 30         glance.store.create_stores()
 31         glance.store.verify_default_store()
In both of these calls, the store objects are created (store/base.py) which means that if the methods configure() and configure_add() methods are defined in the store, these methods will be called two times: 1 time in the ""create_stores()"" phase and 1 time in the verify_default_store() phase.
It turns out that the configure()/configure_add() method can contain remote calls (which can potentially be expensive): in which case we do not want them to happen two times. Having configure/configure_add called only one time will speed up the Glance startup time for some of the stores.","Do not call configure several times at startup

At startup, Glance first creates the stores and then verifies
the default store. Each of these operations will indirectly
trigger the creation of store objects which is triggering configure
and configure_add calls.
configure and configure_add can potentially contain remote calls
going through the network: we should avoid having these methods
called two times for nothing.

This patch adds a parameter in the store init method to specify
if the configuration of the store should happen. In case of verify
default store, the configuration should not happen.

Closes-Bug: #1315237

Change-Id: I329730a3351cd7b78de27a88a7f669ae32b2f100"
1468,c9b6c15d5de4d5ee326d7a870c2b2668f7909efa,1382449740,1.0,1.0,44,7,3,3,1,0.985868122,True,2.0,409931.0,26.0,3.0,False,6.0,4329206.333,21.0,2.0,155.0,808.0,910.0,155.0,761.0,863.0,96.0,191.0,256.0,0.202505219,0.400835073,0.536534447,1714,1242338,1242338,neutron,c9b6c15d5de4d5ee326d7a870c2b2668f7909efa,1,1, ,"Bug #1242338 in neutron: ""Trying to remove a load balancer pool (which contains members) via horizon ends with error""","I've tried to remove a pool that has 2 members and a health monitor, operation failed with the following popup:
""Error: Unable to delete pool. 409-{u'NeutronError': {u'message': u'Pool f5004d04-4461-4a9a-aa7c-04a9bdfde974 is still in use', u'type': u'PoolInUse', u'detail': u''}}""
I did expect this operation to fail, I just didn't expect it to be available in horizon while the pool still has other objects associated with it and I didn't expect it to leave the pool in ""PENDING_DELETE"" status.
The exception from the log file:
2013-10-20 16:12:13.564 22804 ERROR neutron.services.loadbalancer.drivers.haproxy.agent_manager [-] Unable to destroy device for pool: f5004d04-4461-4a9a-aa7c-04a9bdfde974
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Traceback (most recent call last):
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/agent_manager.py"", line 244, in destroy_device
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self.driver.destroy(pool_id)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/services/loadbalancer/drivers/haproxy/namespace_driver.py"", line 92, in destroy
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     ns.garbage_collect_namespace()
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 141, in garbage_collect_namespace
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self.netns.delete(self.namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 440, in delete
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     self._as_root('delete', name, use_root_namespace=True)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 206, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     kwargs.get('use_root_namespace', False))
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 65, in _as_root
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     namespace)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/ip_lib.py"", line 76, in _execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     root_helper=root_helper)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager   File ""/usr/lib/python2.6/site-packages/neutron/agent/linux/utils.py"", line 61, in execute
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager     raise RuntimeError(m)
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager RuntimeError:
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Command: ['sudo', 'neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ip', 'netns', 'delete', 'qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974']
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Exit code: 255
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stdout: ''
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager Stderr: 'Cannot remove /var/run/netns/qlbaas-f5004d04-4461-4a9a-aa7c-04a9bdfde974: Device or resource busy\n'
2013-10-20 16:12:13.564 22804 TRACE neutron.services.loadbalancer.drivers.haproxy.agent_manager","LBaaS: Fix incorrect pool status change

Avoid incorrect status change when deleting the pool.
We can check for the conditions prior putting the pool
to PENDING_DELETE state, in case delete conditions are met
it is safe to change the state to PENDING_DELETE.
Also, change create_vip and update_vip operations to respect
PENDING_DELETE and avoid race conditions.

Change-Id: I9f526901eb85bdb83cf4ff8131460eb592c900f8
Closes-Bug: #1242338"
1469,c9e5bb4fe8b23743f4e2a064395584c87d1232ba,1409922271,1.0,1.0,6,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1274,1736,1366011,cinder,c9e5bb4fe8b23743f4e2a064395584c87d1232ba,1,1,,"Bug #1366011 in Cinder: ""Some tcp options are ignored""","The values for tcp_keepalive, tcp_keepidle, tcp_keepalive_count and tcp_keepalive_interval in cinder.conf are ignored.","Some tcp configuration paramters are ignored

The values for tcp_keepalive, tcp_keepidle, tcp_keepalive_count and
tcp_keepalive_interval in cinder.conf are not being picked up.

Change-Id: Ie0a232ae6b9b9a34d57b48f3b0d8694f90504517
Closes-bug: #1366011"
1470,ca00b1739431fb9fe8f81919dc4644f4f467fa9d,1407999227,1.0,1.0,48,12,4,4,1,0.881024496,False,,,,,True,,,,,,,,,,,,,,,,,1156,1613,1353309,neutron,ca00b1739431fb9fe8f81919dc4644f4f467fa9d,1,0,"“The following commit bumped the RPC version of l3-agent.
L3-agent tried to get service plugin list when it starts.
commit d6f014d0922e03864fd72efbcde04322711c2510”","Bug #1353309 in neutron: ""l3 agent is failing with unsupported version endpoint does not support rpc version 1.3""","my devstack script is failing with the above mentioned error. It is causing my CI to fail.
the stack trace is
2014-08-06 12:31:44.935 TRACE neutron Traceback (most recent call last):
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/bin/neutron-l3-agent"", line 10, in <module>
2014-08-06 12:31:44.935 TRACE neutron     sys.exit(main())
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1787, in main
2014-08-06 12:31:44.935 TRACE neutron     manager=manager)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/service.py"", line 264, in create
2014-08-06 12:31:44.935 TRACE neutron     periodic_fuzzy_delay=periodic_fuzzy_delay)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/service.py"", line 197, in __init__
2014-08-06 12:31:44.935 TRACE neutron     self.manager = manager_class(host=host, *args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 1706, in __init__
2014-08-06 12:31:44.935 TRACE neutron     super(L3NATAgentWithStateReport, self).__init__(host=host, conf=conf)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 430, in __init__
2014-08-06 12:31:44.935 TRACE neutron     self.plugin_rpc.get_service_plugin_list(self.context))
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/l3_agent.py"", line 142, in get_service_plugin_list
2014-08-06 12:31:44.935 TRACE neutron     version='1.3')
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/log.py"", line 36, in wrapper
2014-08-06 12:31:44.935 TRACE neutron     return method(*args, **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/rpc.py"", line 170, in call
2014-08-06 12:31:44.935 TRACE neutron     context, msg, rpc_method='call', **kwargs)
2014-08-06 12:31:44.935 TRACE neutron   File ""/opt/stack/neutron/neutron/common/rpc.py"", line 196, in __call_rpc_method
2014-08-06 12:31:44.935 TRACE neutron     return func(context, msg['method'], **msg['args'])
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 152, in call
2014-08-06 12:31:44.935 TRACE neutron     retry=self.retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-08-06 12:31:44.935 TRACE neutron     timeout=timeout, retry=retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 404, in send
2014-08-06 12:31:44.935 TRACE neutron     retry=retry)
2014-08-06 12:31:44.935 TRACE neutron   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 395, in _send
2014-08-06 12:31:44.935 TRACE neutron     raise result
2014-08-06 12:31:44.935 TRACE neutron RemoteError: Remote error: UnsupportedVersion Endpoint does not support RPC version 1.3
Please help.","Skip FWaaS config mismatch check if RPC method is unsupported

In this commit FWaaS config check skip is skipped
if neutron server does not support this API.

Commit d6f014d introduced FWaaS config mismatch check between
server and agent. It added a new RPC method get_service_plugin_list
and bumped l3-agent RPC version to 1.3, but this version RPC is
only supported by L3 router service plugin and
it breaks existing plugins using L3 router mixin.

Bumping l3-agent RPC version requires detailed investigation on all
affected plugins and it can be done by plugin maintainer later.

Change-Id: I388a24b0c6a507203674ef108bb14cea0534f98c
Closes-Bug: #1353309"
1471,ca668a0b9ec5f6412cd842b89d523ba1a8369782,1389854054,,1.0,11,12,1,1,1,0.0,True,5.0,3288176.0,95.0,42.0,False,9.0,3757851.0,9.0,11.0,99.0,748.0,785.0,97.0,655.0,690.0,79.0,387.0,406.0,0.113960114,0.552706553,0.57977208,169,571,1259646,neutron,ca668a0b9ec5f6412cd842b89d523ba1a8369782,0,0,Refactoring code,"Bug #1259646 in neutron: ""Clean up ML2 Manager""","Some things need cleanup in the ML2Manager.
1.) In the current ML2 Manager, we are using sys.exit(1) if the network_type isn't found in self.drivers:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L70
Here we should probably throw an exception. When running unit test, if we hit this condition the unit tests will exit as well.
2.) We should also be mindful of using the reserved keyword 'type' and rename type in this case to something else:
https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/managers.py#L49","Clean up ML2 Manager

Some things need cleanup in the ML2Manager.

1) Replace sys.exit(1) with raise SystemExit(1)
2) Replace reserved keyword type with network_type

Change-Id: I921bfaec7d3e31503942b3ca4a1b2218c44b14ac
Closes-Bug: #1259646"
1472,cad9e77091dfa896ad59233341e8947eca13f66b,1408698305,,1.0,49,12,3,2,1,0.308309662,False,,,,,True,,,,,,,,,,,,,,,,,1224,1683,1360113,nova,cad9e77091dfa896ad59233341e8947eca13f66b,0,0,Bug in test,"Bug #1360113 in OpenStack Compute (nova): ""V2 hypervisors Unit Test tests hypervisors API as non Admin""","hypervisors API are Admin API and unit tests should test those accordingly. But All the V2 hypervisors Unit tests (https://github.com/openstack/nova/blob/master/nova/tests/api/openstack/compute/contrib/test_hypervisors.py) tests those as a  non Admin API.
Issue is in fake_policy.py where V2 hypervisors API is not marked as Admin role unlike V3. https://github.com/openstack/nova/blob/master/nova/tests/fake_policy.py#L221","Fix V2 unit tests to test hypervisor API as admin

Hypervisors API are Admin API and unit tests should test those accordingly.
But All the V2 hypervisors Unit tests tests those as a non Admin API

This patch fix this issue and add more non-admin unit tests

Change-Id: I5c29828d567e2044786d4bcbff4970219c6e8c5f
Closes-Bug: #1360113"
1473,caf7ecaef75f4d9eba0d85db8917be5a9118792d,1394218216,3.0,1.0,21,1,2,2,1,0.439496987,True,1.0,267499.0,23.0,2.0,False,1.0,532.0,8.0,2.0,40.0,1243.0,1246.0,40.0,1036.0,1039.0,32.0,668.0,671.0,0.035560345,0.720905172,0.724137931,558,979,1289138,neutron,caf7ecaef75f4d9eba0d85db8917be5a9118792d,1,1,bad call,"Bug #1289138 in neutron: ""BigSwitch: Consistency watchdog doesn't work""","The consistency watchdog for the BigSwitch plugin server manager currently does not work. It incorrectly calls rest_call on a servers list rather than calling it on the server pool object.
https://github.com/openstack/neutron/blob/7255e056092f034daaeb4246a812900645d46911/neutron/plugins/bigswitch/servermanager.py#L554","BigSwitch: Fix rest call in consistency watchdog

Fixes the rest call in the consistency watchdog
in the BigSwitch plugin that was causing it to
raise an exception.

Closes-Bug: #1289138
Change-Id: Id4dff993426573be6984eb1e8f6ef2aabfff0779"
1474,cb0df591a9508e863ad5d5d71190eca349dc551f,1378721209,5.0,1.0,421,14,11,7,1,0.821211465,True,11.0,17838313.0,64.0,26.0,False,18.0,3171.818182,21.0,6.0,98.0,1236.0,1239.0,67.0,1111.0,1114.0,8.0,276.0,276.0,0.029801325,0.917218543,0.917218543,1425,1177973,1177973,neutron,cb0df591a9508e863ad5d5d71190eca349dc551f,1,1, ,"Bug #1177973 in neutron: ""OVS L2 agent polling is too cpu intensive""","On a Devstack-deployed, single-node install, the ovs l2 agent is using an order of magnitude more cpu than any other service.  On a nested-virt VM running on a 2.5GHz host, with no VM's provisioned:
 - WIth L2 agent running, 10-100% cpu usage recorded, averaging ~40%
 - With L2 agent stopped, 0-10% cpu usage recorded
Casual inspection with something like top or htop will show the excessive cpu usage, but won't give a good indication of the culprit.  Enabling the CUTIME and CSTIME stats is necessary to highlight the problem, as all the time taken by the agent is in subprocess-invoked polling whose execution time is not directly included in the parent's cpu usage.","Add the option to minimize ovs l2 polling

This change adds the ability to monitor the local ovsdb for
interface changes so that the l2 agent can avoid unnecessary
polling.  Minimal changes are made to the agent so the risk
of breakage should be low.  Future efforts to make the agent
entirely event-based may be able to use OvsdbMonitor as a
starting point.

By default polling minimization is not done, and can only be
enabled by setting 'minimize_polling = True' in the ovs
section of the l2 agent's config file.

Closes-Bug: #1177973

Change-Id: I26c035b48a74df2148696869c5a9affae5ab3d27"
1475,cb1d1e4241c6ecb99797a46053c43b2e4b3b52e7,1383151063,,1.0,13,7,1,1,1,0.0,True,3.0,311312.0,24.0,10.0,False,9.0,683990.0,14.0,2.0,68.0,634.0,657.0,68.0,613.0,636.0,48.0,608.0,613.0,0.043286219,0.537985866,0.542402827,1718,1242511,1242511,cinder,cb1d1e4241c6ecb99797a46053c43b2e4b3b52e7,1,1, ,"Bug #1242511 in Cinder: ""GPFS driver creates volume file on local disk when GPFS is unmounted""","I enable GPFS driver on one cinder volume node. After I unmount GPFS from this node, the GPFS driver can continue to handle creating volume request, so that the volume file is created on local disk, instead of GPFS NSD.
[root@zhaoqin-RHEL-GPFS1 gpfs]# mmumount fs1
Sun Oct 20 11:35:53 CDT 2013: mmumount: Unmounting file systems ...
[root@zhaoqin-RHEL-GPFS1 gpfs]# mount | grep gpfs
[root@zhaoqin-RHEL-GPFS1 gpfs]#
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder create 1
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|     attachments     |                  []                  |
|  availability_zone  |                 nova                 |
|       bootable      |                false                 |
|      created_at     |      2013-10-20T16:36:11.236108      |
| display_description |                 None                 |
|     display_name    |                 None                 |
|          id         | 78906f80-4809-49e9-aceb-b21d6fdf226b |
|       metadata      |                  {}                  |
|         size        |                  1                   |
|     snapshot_id     |                 None                 |
|     source_volid    |                 None                 |
|        status       |               creating               |
|     volume_type     |                 None                 |
+---------------------+--------------------------------------+
[root@zhaoqin-RHEL-GPFS1 gpfs]# ls /gpfs/fs1
volume-78906f80-4809-49e9-aceb-b21d6fdf226b
Then, I attempt to remove this volume file using cinder command, but fails.
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder delete 78906f80-4809-49e9-aceb-b21d6fdf226b
[root@zhaoqin-RHEL-GPFS1 gpfs]# cinder list
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |     Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+
| 78906f80-4809-49e9-aceb-b21d6fdf226b | error_deleting |     None     |  1   |     None    |  false   |             |
+--------------------------------------+----------------+--------------+------+-------------+----------+-------------+","Examine if GPFS is mounted before writing data.

If GPFS is not mounted, throw an exception and log the error message. That will
prevent create_volume(), copy_image_to_volume() or _clone_image() to create
file on local disk, which can not be removed by the driver code itself.

Change-Id: If5d4c7a88e82362d2a87068202926b1ca82d9dff
Closes-Bug: #1242511"
1476,cb7b189f4cd425cad3cbea6ca71986216416cec7,1403731116,,1.0,4,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1001,1443,1333325,glance,cb7b189f4cd425cad3cbea6ca71986216416cec7,0,0,"“This is a sync of processutils and lockutils from oslo-incubator
along with their dependencies.”","Bug #1333325 in Glance: ""glance-api workers should default to number of CPUs available""","The docs recommend setting the 'workers' option equal to the number of CPUs on the host but defaults to 1.  I proposed a change to devstack to set workers=`nproc` but it was decided to move this into glance itself:
https://review.openstack.org/#/c/99739/
Note that nova changed in Icehouse to default to number of CPUs available also, and Cinder will most likely be doing the same for it's osapi_volume_workers option.
This will have a DocImpact and probably UpgradeImpact is also necessary since if you weren't setting the workers value explicitly before the change you'll now have `nproc` glance API workers by default after restarting the service.","Use (# of CPUs) glance workers by default

The config docs have historically recommended that the number of
glance  workers should be set to the number of CPUs available for
best performance, so we should make this the default.

Commit 75c96a48fc7e5dfb59d8258142b01422f81b0253 did the same thing in
Nova in Icehouse and the plan is to do the same thing for Cinder
and Trove workers.

The config files are updated to match the help string in the code.

There is no upgrade impact since glance-api.conf previously
hard-coded the workers value to 1 so anyone upgrading to this
will still get whatever value was set in glance-api.conf prior
to this change.

DocImpact: glance workers will now be equal to the number of
           CPUs available by default if not explicitly specified
           in glance-api.conf and/or glance-registry.conf.

UpgradeImpact: There is no upgrade impact to glance-api workers
           since glance-api.conf previously hard-coded the workers
           value to 1 so anyone upgrading to tihs will still get
           whatever value was set in glance-api.conf prior to
           this change. There is an upgrade impact to the
           glance-registry workers since glance-registry.conf
           did not hard-code the workers value to 1 before change
           I0cee0d284eef9ce5dcb26720499f2c4d31eaca0f, which is
           overwritten here. So anyone upgrading to this change
           that does not have workers specified in
           glance-registry.conf will now be running multiple
           workers by default when they restart the glance
           registry service.

Closes-Bug: #1333325

Change-Id: I6795c6e22268bb3fb67331edc7af641aefa904cc"
1477,cb93eb60abf10af6fbb1ee438a6e8a51853e6788,1398738055,1.0,1.0,87,7,2,2,1,0.978998737,True,7.0,1944482.0,59.0,12.0,True,,,,,,,,,,,,,,,,,843,1272,1313992,glance,cb93eb60abf10af6fbb1ee438a6e8a51853e6788,1,1,,"Bug #1313992 in Glance: ""VMware store should handle upload of files with unknown Content-Length""","Nova needs to upload streamOptimized disks to Glance. These streamOptimized disks are converted/compressed on the fly by
vCenter. Consequently, it is not possible to know the size of the Glance image before upload. Without specifying the size
or size zero, vCenter will reject the request (Broken Pipe).","Use Chunked transfer encoding in the VMware store

Nova needs to upload streamOptimized disks to Glance. These
streamOptimized disks are converted/compressed on the fly by
vCenter. Consequently, it is not possible to know the size of
the Glance image before upload. Without specifying the size
or size zero, vCenter will reject the request (Broken Pipe).

This patch adds the Chunked transfer encoding which allows
to not specify a Content-Length header.

Change-Id: I579084460e7f61ab4042632d17ec0f045fa6f5af
Closes-Bug: #1313992"
1478,cb99c08e968685c6f59bab2e49f86d56a2ec509d,1394159984,,1.0,7,3,1,1,1,0.0,True,1.0,4252765.0,53.0,5.0,False,1.0,1407381.0,1.0,4.0,873.0,1781.0,1831.0,751.0,1478.0,1507.0,352.0,774.0,789.0,0.384113166,0.843307943,0.859630033,467,882,1284277,neutron,cb99c08e968685c6f59bab2e49f86d56a2ec509d,0,0,Add some logic to manage errors,"Bug #1284277 in neutron: ""nsx: deal with 503 errors coming from the controller""","If the NSX controller returns a 503, like below:
http://paste.openstack.org/show/69125/
(full log):
http://208.91.1.172/logs/neutron/69361/12/401769/logs/screen-q-svc.txt.gz?level=TRACE#_2014-02-23_01_09_35_875
Tempest failures may be observed, like:
http://208.91.1.172/logs/neutron/69361/12/401769","NSX: Add ability to retry on 503's returned by the controller

There are a number of circumstances where the NSX controller
may return 503. Currently the API client does not retry, so
this patch adds a retry logic with timeout.

Closes-bug: #1284277

Change-Id: I85df087d5ae409e6cb5c35eb171e89346abe81f4"
1479,cbb057169094e9d633162cc5645fa19a82325d49,1379539807,1.0,1.0,10,4,2,2,1,0.985228136,True,2.0,462887.0,19.0,9.0,False,7.0,1402673.5,15.0,5.0,7.0,1108.0,1108.0,7.0,1006.0,1006.0,6.0,956.0,956.0,0.006923838,0.946587537,0.946587537,1576,1227366,1227366,cinder,cbb057169094e9d633162cc5645fa19a82325d49,1,1,,"Bug #1227366 in Cinder: ""GPFS Driver does not  limit clone depth for snapshots""",GPFS Driver does not  limit clone depth for snapshots - there is a potential for snapshot clones to grow without bound regardless of the setting of gpfs_max_clone_depth config flag.,"GPFS Driver missing clone depth limit for snapshots

GPFS driver is fixed to handle snapshot clones correctly. Previously,
these were allowed to grow without respecting limit defined in config
flag gpfs_max_clone_depth.  This change adds the depth check operation
in create_snapshot.

To ensure that all clone files are cleaned up, the delete_snapshot
method now marks snapshots to be deleted with ts file extension
and attempts to delete the snapshot.  If the snapshot cannot be
deleted because it has clone children, it will be deleted when the
child is deleted.

Closes-Bug: #1227366

Change-Id: I4fb2a720b55dbe033159e6fb341f6e2f1508776e"
1480,cbbb9de51b52793f6d424c58199b22a422d39aed,1393534546,,1.0,42,3,3,3,1,0.887224409,True,11.0,6038968.0,135.0,33.0,False,173.0,201479.0,647.0,5.0,77.0,2831.0,2843.0,77.0,2264.0,2276.0,77.0,2729.0,2741.0,0.010451561,0.365804636,0.367412569,287,693,1268569,nova,cbbb9de51b52793f6d424c58199b22a422d39aed,1,1, race condition,"Bug #1268569 in OpenStack Compute (nova): ""Double removal of floating IP in nova-network""","It is possible to send two DELETE requests via api and in_use.floating_ips will be decreased by two. It can be changed even to value below zero:
Logs:
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0797279
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47613)
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: 172.16.0.2 ""DELETE /v2/46f4516b6fbd461eb30f17409
36c2167/os-floating-ips/1 HTTP/1.1"" status: 202 len: 209 time: 0.0804379
<182>Dec 19 17:52:08 node-2 nova-nova.osapi_compute.wsgi.server INFO: (19517) accepted ('172.16.0.2', 47615)
<0>Dec 19 17:52:08 node-2 <BF><180>nova-nova.db.sqlalchemy.api WARNING: Change will make usage less than 0 for the fol
lowing resources: ['floating_ips']
Database:
mysql> select resource,in_use from quota_usages;
+-----------------+--------+
| resource | in_use |
+-----------------+--------+
| security_groups | 0 |
| instances | 0 |
| ram | 0 |
| cores | 0 |
| fixed_ips | 0 |
| floating_ips | -1 |
+-----------------+--------+
6 rows in set (0.00 sec)","Add lock on API layer delete floating IP

There are potential race condition when multiple nova-api
service is running. We need to prevent the concurrent access to
the same floating ip, though db layer function is ok for floating
ip, the quota will be released more than one time and it will
lead to potential problem later

So the solution: check whether the db is *really* updated with the
value we want, which means the second operation will fail and
return None, so network api layer can handle it

Change-Id: I028938a4c387d21a8880ae168b161a763e7361ff
Closes-Bug: #1268569"
1481,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1386324369,,2.0,7,6,1,1,1,0.0,True,1.0,287862.0,7.0,5.0,False,11.0,8432973.0,11.0,5.0,190.0,1308.0,1455.0,189.0,1144.0,1291.0,26.0,1053.0,1053.0,0.021969081,0.857607811,0.857607811,133,533,1257293,cinder,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1,0,This bug affects only Topology  Version 2,"Bug #1257293 in Cinder: ""[messaging] QPID broadcast RPC requests to all servers for a given topic""","According to the oslo.messaging documentation, when a RPC request is made to a given topic, and there are multiple servers for that topic, only _one_ server should service that RPC request.  See http://docs.openstack.org/developer/oslo.messaging/target.html
""topic (str) – A name which identifies the set of interfaces exposed by a server. Multiple servers may listen on a topic and messages will be dispatched to one of the servers in a round-robin fashion.""
In the case of a QPID-based deployment using topology version 2, this is not the case.  Instead, each listening server gets a copy of the RPC and will process it.
For more detail, see
https://bugs.launchpad.net/oslo/+bug/1178375/comments/26","Sync rpc fix from oslo-incubator

Sync the following fixes from oslo-incubator:

ef406a2 Create a shared queue for QPID topic consumers
e227c0e Properly reconnect subscribing clients when QPID broker restarts

Change-Id: I934a6ea6e9eb510513640870750ea37e6f076df9
Closes-bug: #1251757
Closes-bug: #1257293"
1482,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1386324369,,2.0,7,6,1,1,1,0.0,True,1.0,287862.0,7.0,5.0,False,11.0,8432973.0,11.0,5.0,190.0,1308.0,1455.0,189.0,1144.0,1291.0,26.0,1053.0,1053.0,0.021969081,0.857607811,0.857607811,1767,1251757,1251757,cinder,cbd7acfc294c4bf2ae0014bcf04646382bed469d,1,0,“Sync the following fixes from oslo-incubator:” The bug was in Oslo and Cinder only need to sync so it didn’t cause the bug.,"Bug #1251757 in Cinder: ""On restart of QPID broker, fanout no longer works""","When the QPID broker is restarted, RPC servers attempt to re-connect.  This re-connection process is not done correctly for fanout subscriptions - two subscriptions are established to the same fanout address.
This problem is compounded by the fix to bug#1178375 https://bugs.launchpad.net/oslo/+bug/1178375
With this bug fix, when topology version 2 is used, the reconnect attempt uses a malformed subscriber address.
For example, I have a simple RPC server script that attempts to service ""my-topic"".   When it initially connects to the broker using topology-version 1, these are the subscriptions that are established:
(py27)[kgiusti@t530 work (master)]$ ./my-server.py --topology=1 --auto-delete server-02
Running server, name=server-02 exchange=my-exchange topic=my-topic namespace=my-namespace
Using QPID topology version 1
Enable auto-delete
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_489a3178fc704123b0e5e2fbee125247""}}
When I restart the qpid broker, the server reconnects using the following subscriptions
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
Recevr my-topic_fanout ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": false, ""type"": ""fanout""}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true, ""durable"": false}, ""durable"": true, ""name"": ""my-topic_fanout_b40001afd9d946a582ead3b7b858b588""}}
^^^^--- Note: subscribing twice to the same exclusive address!  (Bad!)
Recevr openstack/my-topic.server-02 ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic.server-02""}}
Recevr openstack/my-topic ; {""node"": {""x-declare"": {""auto-delete"": true, ""durable"": true}, ""type"": ""topic""}, ""create"": ""always"", ""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": false, ""durable"": false}, ""durable"": true, ""name"": ""my-topic""}}
When using topology=2, the failure case is a bit different.  On reconnect, the fanout addresses are lacking proper topic names:
Recevr amq.topic/topic/openstack/my-topic ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/fanout/ ; {""link"": {""x-declare"": {""auto-delete"": true, ""exclusive"": true}}}
Recevr amq.topic/topic/openstack/my-topic.server-02 ; {""link"": {""x-declare"": {""auto-delete"": true, ""durable"": false}}}
Note again - two subscriptions to fanout, and 'my-topic' is missing (it should be after that trailing /)
FYI - my test RPC server and client can be accessed here: https://github.com/kgiusti/oslo-messaging-clients","Sync rpc fix from oslo-incubator

Sync the following fixes from oslo-incubator:

ef406a2 Create a shared queue for QPID topic consumers
e227c0e Properly reconnect subscribing clients when QPID broker restarts

Change-Id: I934a6ea6e9eb510513640870750ea37e6f076df9
Closes-bug: #1251757
Closes-bug: #1257293"
1483,cbd89e75d34c048c15c52dc828b1df7443c540d6,1404902821,,1.0,156,149,12,10,1,0.448736846,False,,,,,True,,,,,,,,,,,,,,,,,1052,1498,1340145,neutron,cbd89e75d34c048c15c52dc828b1df7443c540d6,0,0,feature? ”Right now CommonDBMixin resides in db_base_plugin_v2.py so those plugins require to import it.”,"Bug #1340145 in neutron: ""Extract CommonDBMixin into a separate module""","Several service plugins are inheritig from CommonDBMixin which has a few utulity methods.
Right now CommonDBMixin resides in db_base_plugin_v2.py so those plugins require to import it.
In some cases it is undesirable and can lead to cycles in imports,
so CommonDBMixin needs to be extracted into a different module","Extract CommonDBMixin to a separate file

db_base_plugin_v2 imports too much modules that are not necessary
usually, so extract CommonDBMixin in different file.
Plus using db_base_plugin_v2 for some types of modules can lead to
cycles in imports, this refactoring should resolve the issue.

Closes-Bug: #1340145
Change-Id: Idb027d7c5cee2d5bc7598f805c56c55fd4aca048"
1484,cc134ab2dca6c28e75d4530517f309c3fabc969a,1403881197,0.0,1.0,48,9,2,2,1,0.934849024,False,,,,,True,,,,,,,,,,,,,,,,,298,706,1269990,nova,cc134ab2dca6c28e75d4530517f309c3fabc969a,1,1,LXC I think is OS code,"Bug #1269990 in OpenStack Compute (nova): ""LXC volume issues""","Few issues with LXC and volumes that relate to the same code.
* Hard rebooting a volume will make attached volumes disappear from libvirt xml
* Booting an instance specifying an extra volume (passing in block_device_mappings on server.create) will result in the volume not being in the libvirt xml
This is due to 2 places in the code where LXC is treated differently
1. nova.virt.libvirt.blockinfo  get_disk_mapping
2. nova.virt.libvirt.driver get_guest_storage_config","libvirt: persist lxc attached volumes across reboots and power down

Currently, attached volumes to a LXC instance will be lost, when
hard rebooting or powering it down.
Adjusting the get_storage_config() to include LXC volumes in the
domain configuration.
The change will add all volumes to the instance config, a part from
the root device mapping.
When (re)creating a LXC instance, get_bdms_to_connect() generator
from block_device module, will be used to exclude the root device
mapping for a domain configuration.

Co-authored-by: Sam Morrison<sorrison@gmail.com>
Closes-Bug: #1269990
Change-Id: I0aa227ede5fca065c6da15506d8cc4391d993e20"
1485,cc5388bbe81aba635fb757e202d860aeed98f3e8,1403883915,,1.0,51,13,2,2,1,0.982316608,False,,,,,True,,,,,,,,,,,,,,,,,893,1329,1320628,nova,cc5388bbe81aba635fb757e202d860aeed98f3e8,1,1,,"Bug #1320628 in OpenStack Compute (nova): ""Double powering-off state confuses the clients and causes gate failure""","http://logs.openstack.org/52/73152/8/check/check-tempest-dsvm-full/9352c04/console.html.gz#_2014-05-13_18_12_38_547
At client side the only way to know an instance action is doable is to making sure the status  is a permanent status like ACTIVE or SHUTOFF and no action in progress, so the task-state is None.
In the above linked case tempest stopped the instance and the instance reached the  ""SHUTOFF/None"".
'State transition ""ACTIVE/powering-off"" ==> ""SHUTOFF/None"" after 10 second wait'
Cool, at this time we can start the instance right ? No, other attribute needs to be checked.
The start attempt was rewarded with 409 :
 u'Instance 7bc9de3b-1960-476f-b964-2ab2da986ec7 in task_state powering-off. Cannot start while the instance is in this  state'
The below line indicates the  task state, silently moved back to ""SHUTOFF/powering-off""  , before the 'start' attempt.
2014-05-13 18:09:13,610 State transition ""SHUTOFF/powering-off"" ==> ""SHUTOFF/None"" after 1 second wait
Please do not set the 'None' task-state when the 'powering-off' is not finished.","synchronize 'stop' and power state periodic task

This change adds the synchronized decorator to the stop function and
_sync_instance_power_state to address a race condition when powering
off and updating the database for an instance. The power off and
database update should be protected so the periodic task sees a
consistent view when it checks power state from the driver and
VM state from the database in order to take action.

In the bug, a stopped instance is in the middle of being rebuilt,
and while it is in the process of being returned to the original
STOPPED state, _sync_power_states sees the combination of the
power state from the driver being 'on' and the vm_state from
the database as being STOPPED. This patch aims to ensure power
'off' and vm_state set to STOPPED happen atomically in the
stop_instance function. Thus, the read by _sync_power_states
must also be synced on the instance UUID.

Change-Id: I8aa83ab8dca35878cf792ae2d46feaa9912ffd38
Closes-Bug: #1320628"
1486,cc70e3a28df3c8492769b305d248bb4d9bf32830,1381950856,4.0,1.0,112,2,3,3,1,0.937067751,True,9.0,1120022.0,40.0,11.0,False,129.0,16549.0,368.0,1.0,4.0,255.0,258.0,4.0,219.0,222.0,4.0,248.0,251.0,0.000787278,0.039206424,0.039678791,1700,1240670,1240670,nova,cc70e3a28df3c8492769b305d248bb4d9bf32830,1,1,,"Bug #1240670 in OpenStack Compute (nova): ""Nova compute service should be disabled, if libvirt service is unavailable.""","Nova compute service should be disabled on a node when a connection to libvirt is lost
and resumed when libvirtd become fuctional again.
This is in order to avoid new instances or migrations to be scheduled compute node, when it'd disconnected from libvirt.","Disable nova-compute on libvirt connectivity exceptions

The service will be disabled and re-enabled according to the success
or failure of establishing a connection to the libvirt service.

update_available_resources, periodic task, will probe the connection
even if the service is disabled.

This is in order to avoid migrations or new instances to be scheduled
on a disconnected from libvirt compute node.

Closes-Bug: #1240670
Change-Id: Ib8d67838ceb73c5b1cdc9498c17b335e9e5bb6f3"
1487,ccb68ab1cd50f00546794c4bda9acb2580d8b3a4,1393506868,,1.0,14,16,2,2,1,1.0,True,5.0,1882234.0,51.0,13.0,False,65.0,52818.0,208.0,5.0,214.0,2856.0,2892.0,212.0,2407.0,2442.0,205.0,2747.0,2774.0,0.02761024,0.368315239,0.371934057,491,909,1285482,nova,ccb68ab1cd50f00546794c4bda9acb2580d8b3a4,0,0,add a sample file,"Bug #1285482 in OpenStack Compute (nova): ""There is not any API sample file for ""unshelve a server"" API""","Now there is not any API sample file of ""unshelve a server"" API,
and OpenStack API documentation[1] also does not describe the API.
[1]: http://api.openstack.org/api-ref-compute-ext.html","Add API sample files of ""unshelve a server"" API

Now there are not API sample files of ""unshelve a server"" and
""shelve-offload"" APIs, and OpenStack API documentation also
does not show these APIs.
This patch adds these API sample files.

DocImpact
Closes-Bug: #1285482

Change-Id: Idf797eb6723b94abae71a77c12bc2bb9b330fa28"
1488,ccd650732729451aa8e5ce3401f9570c70c4f066,1411739383,,1.0,48,38,3,3,1,0.681078317,False,,,,,True,,,,,,,,,,,,,,,,,1361,1830,1374461,neutron,ccd650732729451aa8e5ce3401f9570c70c4f066,1,1,,"Bug #1374461 in neutron: ""potential lock wait timeout issue when creating a HA router""","Currently the failures during the creation of resources related to the creation of a HA router are handled my a try/except to avoid a potential lock wait timeout. This has been done in order to keep the RPC calls outside the transactions.
All the related resources are created in the _create_router_db but this method is called inside a transaction which is started is the create_router method. Moreover the try/except mechanism used to rollback the router creation will not work since we are in a already opened transaction.","Moves the HA resource creations outside of transaction

Currently the HA resources are created in the
_create_router_db which includes calls to
the plugin and generates RPC calls. Even if the
resource creations are outside of any transaction
from the _create_router_db point of view, this
method is  called in a transaction from the
create_router method.
This patch moves the resource creations to the
create_router method outside the transaction.
The failures are handled as previously with
a try/expect.

Change-Id: If8fcfd012f8e992175e49bbefb2ae667881a620a
Closes-bug: #1374461"
1489,ccfe47d4b9c2abaa25b3b2a2dc655c32beb0b377,1394375189,,1.0,3,2,1,1,1,0.0,True,1.0,234948.0,43.0,7.0,False,2.0,11845674.0,2.0,7.0,14.0,718.0,719.0,14.0,632.0,633.0,14.0,468.0,469.0,0.01607717,0.502679528,0.50375134,647,1073,1294434,neutron,ccfe47d4b9c2abaa25b3b2a2dc655c32beb0b377,0,0,"Implementation, ","Bug #1294434 in neutron: ""Add missing ondelete foreignkey constraint to Cisco N1kv tables""","Current DB migration script to populate N1kv tables is missing the ondelete=""CASCADE"" option in the ForeignKey constraint for network and port binding tables. This causes the network and port delete calls to fail with Integrity error. This occurs when the database is populated with db migration scripts.
super(N1kvNeutronPluginV2, self).delete_network(context, id)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/db/db_base_plugin_v2.py"", line 1013, in delete_network
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     context.session.delete(network)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self.commit()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self._prepare_impl()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     self.session.flush()
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 616, in _wrap
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource     raise exception.DBError(e)
2014-03-09 06:51:31.033 8487 TRACE neutron.api.v2.resource DBError: (IntegrityError) (1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`cisco_neutron`.`cisco_n1kv_network_bindings`, CONSTRAINT `cisco_n1kv_network_bindings_ibfk_1` FOREIGN KEY (`network_id`) REFERENCES `networks` (`id`))') 'DELETE FROM networks WHERE networks.id = %s' ('12fb857a-cb23-44f0-81a4-9cc8fc669de0',)","Add missing ondelete option to Cisco N1kv tables

Update the DB migration script, which creates the Cisco N1kv tables,
to include the ondelete='CASCADE' option in network and port
binding tables. This allows proper clean up of network and port
binding entries on respective delete operations, when the tables
are populated via DB migration script.

Closes-Bug: #1294434

Change-Id: I5dad7bd718b75cfd8825b08336e834d64bddd196"
1490,cd0371280dd204a10673d5d1b70ba0b9f2757a22,1389117933,,1.0,12,4,2,2,1,0.337290067,True,2.0,348162.0,19.0,10.0,False,144.0,135628.0,440.0,4.0,14.0,1394.0,1403.0,14.0,1238.0,1247.0,14.0,1316.0,1325.0,0.002148997,0.188681948,0.189971347,256,660,1266534,nova,cd0371280dd204a10673d5d1b70ba0b9f2757a22,0,0,Fixing tests,"Bug #1266534 in OpenStack Compute (nova): ""baselineCPU parser break unit tests in nova""","The fix for https://bugs.launchpad.net/nova/+bug/1259796 break nova unit tests. The original test nova.tests.virt.libvirt.test_libvirt.LibvirtConnTestCase.test_cpu_features_bug_1217630 and more tests that parse xml are broken. Details here:
http://paste.openstack.org/show/60471/","libvirt: stop overwriting LibvirtConfigCPU in get_host_capabilities

Currently, LibvirtConfigCPU configuration is being overwritten with baselineCPU
output. However, baselineCPU never reports the ARCH and other relevant fields,
that is being set in LibvirtConfigCPU by the LibvirtConfigCaps.
The absense of the arch field is causing the bug below.
Fixing it to simply update the LibvirtConfigCPU.

Change-Id: I9dd06fa44a232a333609fa115d05ad8822e21c0c
Closes-Bug: #1266534"
1491,cd33dbee09ca8335bcb5ba6975a8edd16258babe,1384152557,,1.0,58,12,2,2,1,0.999411065,True,3.0,2098476.0,24.0,9.0,False,8.0,2483659.0,15.0,5.0,6.0,1164.0,1165.0,6.0,1068.0,1069.0,5.0,468.0,468.0,0.011385199,0.889943074,0.889943074,1743,1249957,1249957,neutron,cd33dbee09ca8335bcb5ba6975a8edd16258babe,0,0,“This bug is created to add this support.” Feature,"Bug #1249957 in neutron: ""Midonet plugin does not associate port at floating IP creation""",The Midonet plugin currently does not support associating floating IPs with ports at floating IP creation time. This bug is created to add this support.,"Midonet to support port association at floating IP creation

The Midonet plugin currently does not support associating floating IPs with
ports at floating IP creation time. This bug is created to add this support.

Change-Id: Ie57ebffa5185f26138c04b9836067417c6dc1388
Closes-Bug: #1249957"
1492,cd3f4f7d24c92475ca698906c57f60375861b83f,1406629113,,1.0,3,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1112,1566,1349767,neutron,cd3f4f7d24c92475ca698906c57f60375861b83f,1,1,,"Bug #1349767 in neutron: ""lbaas is not backward compatible with havana config file""",In Havana user_group was taken from default section while currently it needs to be in [haproxy] section.,"Fix deprecated opt in haproxy driver

If Neutron was updated from Havana, lbaas config file defines user_group
and loadbalancer_state_path in default section but since Icehouse these
settings were moved to haproxy section.
This patch adds DEFAULT group to DeprecatedOpt.

Change-Id: Ic92d4bc81ff6a3b16799414d17ac52e7a4801796
Closes-bug: #1349767"
1493,cd4b4da8b6362e6621602a8b815bbeeea2000ff7,1387271431,,1.0,116,3,4,3,3,0.177744339,True,8.0,4235404.0,37.0,18.0,False,0.0,0.0,0.0,3.0,16.0,597.0,599.0,16.0,510.0,512.0,16.0,577.0,579.0,0.020190024,0.686460808,0.688836105,196,599,1261692,swift,cd4b4da8b6362e6621602a8b815bbeeea2000ff7,1,1,but the fixing commit solves many more issues!,"Bug #1261692 in OpenStack Object Storage (swift): ""swift-recon ignores zone filter if set to ""0""""","swift-recon has a filter to query only servers in a specific zone:
  -z ZONE, --zone=ZONE  Only query servers in specified zone
If zone is set to ""0"", the filter is not applied.","Add some tests for bin/swift-recon

Fix also minor bug in zone filtering when zone set to 0.

Moved bin/swift-recon to swift/cli/recon.py, which makes
it possible to import it without using some scary hacks.
bin/swift-recon is now created by setup.py install.

Closes-Bug: #1261692
Change-Id: Id0729991c8ece73604467480dbf93fec7d8eb196"
1494,cd542a7f1f2ea21d3694e70b8e6a6db0efbd3a01,1386241437,,1.0,3,3,2,2,1,0.918295834,True,2.0,1654998.0,19.0,8.0,False,5.0,4068821.0,7.0,4.0,4.0,2262.0,2262.0,4.0,2020.0,2020.0,3.0,544.0,544.0,0.006578947,0.896381579,0.896381579,1731,1245885,1245885,neutron,cd542a7f1f2ea21d3694e70b8e6a6db0efbd3a01,1,0,“Previous default for eswitch/daemon_endpoint would set a TCP port (5001)”,"Bug #1245885 in neutron: ""Mellanox Neutron Agent is using keystone port""","Mellanox Neutron agent is configuredby default  to contact Eswitch Daemon using port 5001 which is used by keystone.
It should be changed to use port 60001.","Change default eswitchd port to avoid conflict

Previous default for eswitch/daemon_endpoint would set a TCP port (5001)
which is in the range of the well known ports.
Change to unreserved port (60001) will avoid that conflict.

Change-Id: I711c4659f497e44a6041ec9958835c1f2f4174a7
Closes-Bug: #1245885"
1495,cda106c399a81d4027d3bb99f452fd4064405009,1377639927,,1.0,4,5,2,2,1,0.99107606,True,1.0,230882.0,17.0,7.0,False,25.0,89381.0,63.0,5.0,118.0,2036.0,2090.0,118.0,1882.0,1936.0,110.0,1194.0,1244.0,0.018980848,0.204343365,0.212893297,1489,1217586,1217586,nova,cda106c399a81d4027d3bb99f452fd4064405009,1,1,Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver raise NotImplementedError from the plug_vifs method which causes this failure on the compute node when it starts up,"Bug #1217586 in OpenStack Compute (nova): ""init_host fails due to powervm driver raising NotImplementedError for plug_vifs""","Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver raise NotImplementedError from the plug_vifs method which causes this failure on the compute node when it starts up:
http://paste.openstack.org/show/45241/
Looking at other virt drivers that don't support the plug_vifs method (hyperv and vmware), they simply pass, which is what the powervm driver use to do before that change.
The powervm driver should either revert to pass on plug_vifs or the nova compute manager should catch NotImplementedError from driver.plug_vifs and swallow it here:
https://github.com/openstack/nova/blob/2013.2.b2/nova/compute/manager.py#L638","powervm: revert driver to pass for plug_vifs

Change Ieaf256a5c0f54804686318d6cbd5877a5003c9eb made the powervm driver
plug_vifs method raise NotImplementedError rather than pass, which is
what the hyper-v and vmware drivers do for plug_vifs. This results in
the compute manager failing to init the host because it's not catching
NotImplementedError from driver.plug_vifs.

This patch simply changes the powervm driver plug_vifs method back to
the old behavior (pass), but at least logs a debug message this time.

Closes-Bug: #1217586

Change-Id: Iafe87cbb9b3cf728a47cb9cc43151798fcbb780f"
1496,ce2031afb2e42c756c065f423126ffd1a61b820a,1408408318,,1.0,8,8,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,1211,1670,1358552,nova,ce2031afb2e42c756c065f423126ffd1a61b820a,1,1,,"Bug #1358552 in OpenStack Compute (nova): ""Fail to remove logical volume""","The logical volume can not be removed when delete VM error. I look at the code, found that  parameter is a list in the libvirt's lvm, but in imagebackend, parameters passed is a string.
in the Libvirt's LVM
def remove_volumes(paths):  ## #the path is list
    """"""Remove one or more logical volume.""""""
    errors = []
    for path in paths:
        clear_volume(path)
        lvremove = ('lvremove', '-f', path)
        try:
            utils.execute(*lvremove, attempts=3, run_as_root=True)
        except processutils.ProcessExecutionError as exp:
            errors.append(str(exp))
    if errors:
        raise exception.VolumesNotRemoved(reason=(', ').join(errors))
in the imagebackend's LVM
 @contextlib.contextmanager
    def remove_volume_on_error(self, path):
        try:
            yield
        except Exception:
            with excutils.save_and_reraise_exception():
                lvm.remove_volumes(path)  ### the path is string","FIX: Fail to remove the logical volume

The logical volume can not be removed when delete VM error. I look at the
code, found that parameter is a list in the libvirt's lvm, but in
imagebackend, parameters passed is a string.

Change-Id: I222f6c5cfe5be63105f2fd8afd7ccd209ca2e5f7
Closes-Bug: #1358552"
1497,ce3f9e5fa9cd05f3ee3bb0cc7d06521d05901cf4,1400326960,,1.0,40,26,3,2,1,0.986447064,False,,,,,True,,,,,,,,,,,,,,,,,178,581,1260265,nova,ce3f9e5fa9cd05f3ee3bb0cc7d06521d05901cf4,0,0,Evolution: Remove traces of now unused host capabilities from scheduler ,"Bug #1260265 in OpenStack Compute (nova): ""BaremetalHostManager cannot distinguish baremetal hosts from other hosts""","BaremetalHostManager could distinguish baremetal hosts by checking ""baremetal_driver"" exists in capabilities or not. However, now BaremetalHostManager cannot, because capabilities are not reported to scheduler and BaremetalHostManager always receives empty capabilities. As a result, BaremetalHostManager just does the same thing as the original HostManager.","Remove traces of now unused host capabilities from scheduler

Host capabilities updates are no longer used by the scheduler. All host
state is instead fetched from the DB. Some left over fields are still kept
by the host manager but contain no data. This change remove the last
traces of host capabilities from the scheduler.

The baremetal host manager used to rely on capabilities info to
decide which host state class to use, VM or baremetal. This has been
broken since host capabilities were removed (bug 1260265). The logic to
decide on host state class to use is updated to use DB data instead and
a test is added to verify the same.

Change-Id: I350109492c2addf5fdfaadee7cec4784f244e102
Closes-Bug: #1260265"
1498,ce59e63249dfcd86e782d056407e38e845cbe19c,1401373050,,1.0,124,81,2,2,1,0.110145431,False,,,,,True,,,,,,,,,,,,,,,,,879,1314,1317935,neutron,ce59e63249dfcd86e782d056407e38e845cbe19c,1,1,,"Bug #1317935 in neutron: ""Neutron does not follow the RFC 3442 spec for DHCP""","When metadata networking is enabled, neutron will generate a dnsmasq configuration that includes classless routes.
The RFC states that those routes should include the default route if they are defined, and that the DHCP client should ignore the default router.
This causes standards-abiding DHCP clients like dhcpcd to not provide a default gateway when running on nova.
At https://github.com/openstack/neutron/pull/22 you can see an example of how to fix this.","Neutron does not follow the RFC 3442 spec for DHCP

When setting a gateway and additional host routes in neutron subnet, the
gateway is only sent to clients via the router dhcp option, dhcp clients
conforming to rfc3442 will ignore router option if
classless-static-routes are available. This patch ensures setting both
the router option and the classless-static-routes including the gateway

Change-Id: Ia00b9385025020f848872309ae42ddac08528f53
Closes-Bug: #1317935"
1499,ce7c8dfba800874fec7411393e6799d81064f939,1409834037,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1266,1726,1365474,cinder,ce7c8dfba800874fec7411393e6799d81064f939,1,1,,"Bug #1365474 in Cinder: ""gpfs driver not setting pool information properly""","In the current code, the configuration option 'gpfs_storage_pool' is assigned with default value of 'None'
And in the do_setup(),  it line self._storage_pool = pool or system, it get assigned with ""None"" if the user has not specified any value.
Following is seen in the c-vol log.
2014-09-04 17:43:44.039 TRACE cinder.volume.manager Traceback (most recent call last):
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 250, in init_host
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     self.driver.do_setup(ctxt)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     return f(*args, **kwargs)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/ibm/gpfs.py"", line 341, in do_setup
2014-09-04 17:43:44.039 TRACE cinder.volume.manager     raise exception.VolumeBackendAPIException(data=msg)
2014-09-04 17:43:44.039 TRACE cinder.volume.manager VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Invalid storage pool None specificed.
""gpfs_storage_pool"" option needs to assigned by default with ""system"" instead of ""None""","Set default pool value to system in gpfs driver

The configuration option gpfs_storage_pool is assigned with
default value of None. And it gets initialized with None
instead of system, where as the intention is to set the default
value as system.

This change sets default value of gpfs_storage_pool to system
instead of None.

DocImpact

Needs an update in gpfs driver documentation, as this patch
proposes a change in default value of configuration option
(gpfs_storage_pool)

Change-Id: Ib4c2a2e4593a940ae563d329fd59d2d5fe3d4520
Closes-Bug: #1365474"
1500,ce8893f4d3fae402494328bfbd07ea6642aae836,1406794217,,1.0,41,26,2,2,1,0.527642326,False,,,,,True,,,,,,,,,,,,,,,,,1130,1585,1350699,cinder,ce8893f4d3fae402494328bfbd07ea6642aae836,0,0,Bug in test,"Bug #1350699 in Cinder: ""FakeBackupService doesn't implement verify""","Testcase test_import_record_with_verify uses 'FakeBackupService' but there is no
verfiy function implemented in that class.","Fix unit test test_import_record_with_verify

The test case test_import_record_with_verify is intended to call a
backup service class with a defined verify function. This verify
function wasn't overloaded by the fake service.

Change-Id: Ib5738e9c463856dd71a47717b7abb8dff785b3b1
Closes-bug: #1350699"
1501,ce889d99f61aecd0b4002d44866bbfff8ff6a2c3,1387022408,,1.0,3,3,2,2,1,0.918295834,True,1.0,742369.0,5.0,2.0,False,7.0,1604.0,11.0,2.0,501.0,1081.0,1314.0,494.0,923.0,1155.0,485.0,934.0,1156.0,0.386634845,0.743834527,0.920445505,18,330,1243613,cinder,ce889d99f61aecd0b4002d44866bbfff8ff6a2c3,1,1,Typo in code,"Bug #1243613 in Cinder: ""Volume-transfer Creation doesn't support 'name' specified via xml format""","Volume-transfer creation provides to specify the 'name' param in cinder.
I tested it on json format, everything's ok.
But when I create it on xml format, the 'name' param is not transmitted to Cinder.
--------------
Here is input-body of my test:
    <transfer name=""transfer6"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
Here's the result, you can found the 'name' is 'None' in response.
<?xml version='1.0' encoding='UTF-8'?>
<transfer
    xmlns:os-volume-transfer=""http://docs.openstack.org/volume/ext/volume-transfer/api/v1.1"" auth_key=""9266c59563c84774"" created_at=""2013-10-23 07:59:07.231575"" id=""7e7d872b-791a-40a6-887f-9b718b4e04b0"" name=""None"" volume_id=""8d37404a-4d60-466c-a8b8-78501db208da""/>
--------------
The reason is due to the inconsistent processing between xml and json.
The CreateDeserializer of xml-format uses 'display_name' param, not 'name':
        --> attributes = ['volume_id', 'display_name']
But the processing of create() in volume-transfer, uses 'name' not 'display_name':
        --> name = transfer.get('name', None)
Therefore, no matter the 'name' is specified, its value in response will also be 'None'.","Transfer creation doesn't support 'name' via xml

Change the transfer create XML deserializer to use 'name' rather than
'display_name'. This is in line with cinderclient and the json code.

Change-Id: I97ba0c4411fe4a151261ed2f657465951bcb9d7b
Closes-Bug: #1243613"
1502,cecd7591533e2c046aedba3b8e5d14a5b2fa7fe9,1382121203,,1.0,37,2,4,3,1,0.821185724,True,9.0,3620317.0,55.0,18.0,False,16.0,1394831.0,21.0,2.0,7.0,799.0,805.0,5.0,752.0,756.0,0.0,184.0,184.0,0.002155172,0.398706897,0.398706897,2,314,1242933,neutron,cecd7591533e2c046aedba3b8e5d14a5b2fa7fe9,0,0,Add a feature. This allows instances to do SLAAC configuration,"Bug #1242933 in neutron: ""Security Groups does not allow RA packets in by default""","Provisioning a VM using Neutron, with RA's being broadcast by upstream switches, the VM was not getting the packets.
Changing the rules manually to:
ip6tables -I neutron-openvswi-ie90990dd-0 1 -p ipv6-icmp -j ACCEPT
Allowed RA packets through.","Pass in certain ICMPv6 types by default

This allows instances to do SLAAC configuration, without requiring
explicit security group rules to do so.

Closes-Bug: #1242933

Change-Id: I517c66a470296141c0024a64e39b6d40b0c0d581"
1503,ceee7b4ac9d027499dfd2568868f1192bfb77879,1396529579,0.0,1.0,34,38,7,7,1,0.677783411,True,2.0,129641.0,33.0,5.0,False,35.0,418607.2857,67.0,3.0,1173.0,1331.0,1333.0,1000.0,1148.0,1150.0,714.0,845.0,847.0,0.658986175,0.779723502,0.78156682,732,1159,1302007,neutron,ceee7b4ac9d027499dfd2568868f1192bfb77879,1,1,,"Bug #1302007 in neutron: ""l3 agent uses method concatenating two constants""","In l3 agent is method ns_name() that concatenates constant NS_PREFIX with router.id that doesn't change during router's lifecycle. It's ineffective, namespace name can be an instance attribute.","Set ns_name in RouterInfo as attribute

Closes-Bug: #1302007
Change-Id: I02a9b92eea06010569d3d9c5a987e69f89b23be5"
1504,cf16029b69f50af1bc2e8627af5bc65ffac24992,1381911262,,1.0,3,4,2,2,1,0.985228136,True,1.0,51929.0,8.0,4.0,False,23.0,627592.0,55.0,4.0,412.0,1080.0,1210.0,406.0,961.0,1090.0,398.0,931.0,1052.0,0.364051095,0.850364964,0.960766423,1697,1240395,1240395,cinder,cf16029b69f50af1bc2e8627af5bc65ffac24992,1,1, ,"Bug #1240395 in Cinder: ""storwize: empty io groups cannot be used""","The Storwize driver mistakenly checks on startup if vdisk_count > 0 for io groups, and fails if this is not the case.  This means that controllers with empty io groups cannot be used.","Storwize: Fix iogrp availability check

Do not check if vdisk_count is greater than zero to determine iogrp
availability. This is wrong, and leads to the driver not being able to
use empty iogrps.

Change-Id: Ia73189a11cfcc0641c1527293675361c4e78443f
Closes-Bug: #1240395"
1505,cf18199571b7965a8fe77db3d910b0e798f12946,1396505575,,1.0,45,3,3,2,1,0.510481955,True,2.0,349523.0,12.0,3.0,True,,,,,,,,,,,,,,,,,659,1085,1295239,cinder,cf18199571b7965a8fe77db3d910b0e798f12946,1,1, Upload volume to image fails,"Bug #1295239 in Cinder: ""VMware: Upload volume to image fails with GlanceConnectionFailed error""","Upload volume to image fails with the following error:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/opt/stack/cinder/cinder/volume/drivers/vmware/io_util.py"", line 108, in _inner
    data=self.input_file)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 311, in update
    _reraise_translated_image_exception(image_id)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 309, in update
    **image_meta)
  File ""/opt/stack/cinder/cinder/image/glance.py"", line 177, in call
    raise exception.GlanceConnectionFailed(reason=e)
GlanceConnectionFailed: Connection to glance failed: Error communicating with http://10.20.72.32:9292 timed out
When this happens, the volume gets stuck in “Uploading” state and never completes.","vmware: Force chunked transfer for upload-to-image

The upload-to-image operation downloads (using stream-optimized HTTP NFC
transfer) the virtual disk corresponding to the volume to a pipe from
where the glance image client reads the data for upload. Due to a recent
change in the glance image client, success in seeking the input file
object results in forgoing the chunked transfer in favor of regular
transfer. The glance image client expects an IOError if the input file
object is a pipe. Currently the pipe seek() is a NOP and this results in
upload-to-image failure. This changes fixes such failures by throwing
an IOError indicating an illegal seek.

Change-Id: I2e69fc4103559d49d2cee16ea04787a252dbf879
Closes-Bug: #1295239"
1506,cf3c1d10b10067cab226bf9a82e89017a4b27fec,1412704917,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1384,1854,1378461,nova,cf3c1d10b10067cab226bf9a82e89017a4b27fec,1,0,"“The VERSION of the nova.objects.NetworkRequest object was incorrectly
    still at 1.0, even though a 1.1 version had been incremented previously”","Bug #1378461 in OpenStack Compute (nova): ""nova.objects.network_request.NetworkRequest's version is incorrect""","from nova/objects/network_request.py:
class NetworkRequest(obj_base.NovaObject):
    # Version 1.0: Initial version
    # Version 1.1: Added pci_request_id
    VERSION = '1.0'
VERSION should be 1.1, per the comment above it.","Correct VERSION of NetworkRequest

The VERSION of the nova.objects.NetworkRequest object was incorrectly
still at 1.0, even though a 1.1 version had been incremented previously
that added a pci_request_id field to the object.  According to Dan on
IRC, there's no danger here, but this is good to clean up regardless.

From Dan: ""the only case where you'd have an issue is if you had a
conductor with the changeset before pci_request was added, and a compute
running the code after""

Change-Id: I3d8c1ac22fb50f25cee03bb230fccad2b18deaf5
Closes-bug: #1378461"
1507,cf6bf1c88687da5c18487ddfa2434a3943129847,1383897917,,1.0,65,10,7,4,1,0.93840421,True,6.0,1256982.0,21.0,6.0,False,35.0,1476282.857,47.0,4.0,6.0,2857.0,2859.0,6.0,2408.0,2410.0,4.0,2705.0,2707.0,0.000764526,0.413761468,0.414067278,1741,1249222,1249222,nova,cf6bf1c88687da5c18487ddfa2434a3943129847,1,1, ,"Bug #1249222 in OpenStack Compute (nova): ""Update quota-class-set and quota-set thrown 500 error""","When update quota-class-set without the parameter ""quota_class_set"",the server throws a 500 error:
 ""The server has either erred or is incapable of performing the requested operation.""
Because the server doesn't check whether the parameter ""quota_class_set"" is in request body,the KeyError is not catched.
   for key in body['quota_class_set'].keys():
       ......
I think we should catch the KeyError and transfer the KeyError to 400(HTTPBadRequest) instead of 500.
Update quota-set has the same problem.","Update quota-class-set/quota-set throw 500 error

The server doesn't check whether the parameter ""quota_class_set"" is in request
body.So the 500 error has been thrown.

We should catch the KeyError and transfer the KeyError to
400(HTTPBadRequest) instead of 500.

Change-Id: I431a836bd35f846790497d6179515e8d559e2cd0
Closes-Bug: #1249222"
1508,cf92bbead9b25ee40a3336bd76110b3733a4f7ee,1403112712,,1.0,6,1,2,2,1,0.863120569,False,,,,,True,,,,,,,,,,,,,,,,,986,1425,1331566,neutron,cf92bbead9b25ee40a3336bd76110b3733a4f7ee,1,1,,"Bug #1331566 in neutron: ""Neutron nova notifier does not support use of tenant name""",The neutron nova notifier (neutron/notifiers/nova.py) hardcodes the project_id to the novaclient as 'None'.  This prevents the ability to use a nova admin tenant name in place of a nova admin tenant id in the event that the tenant id is not known/available at the time that the neutron service is being configured.  An example of such a case is with tripleo (see related bug https://bugs.launchpad.net/tripleo/+bug/1293782).,"Add admin tenant name to nova notifier

This change introduces the ability to use the nova admin
tenant name with the nova notifier in place of the nova
admin tenant id which may not be available when the neutron
service is being configured as is the case with tripleo
installations where the neutron service is configured and
started before the nova admin tenant has been configured in
keystone and thus does not have a known id.

DocImpact
Introduces the nova_admin_tenant_name configuration entry as
an optional configurable value in neutron.conf.  If the
nova_admin_tenant_name is configured and the nova_admin_tenanat_id
is not configured, a performance impact may be seen because
keystone will become involved in communication between neutron
and nova.

Change-Id: I33701d4dc9bf61595e44a49050c405ebca779091
Closes-Bug: #1331566"
1509,cfdd9441b1545b5d89be6e8bca036ba185099f12,1395046498,,1.0,7,1,2,2,1,0.811278124,True,1.0,45989.0,11.0,3.0,False,57.0,68865.0,150.0,3.0,1839.0,3311.0,4541.0,1531.0,2730.0,3659.0,941.0,3191.0,3535.0,0.123638273,0.418952618,0.464102901,632,1058,1293435,nova,cfdd9441b1545b5d89be6e8bca036ba185099f12,1,1,Fix location,"Bug #1293435 in OpenStack Compute (nova): ""VMware: rescue with image that is not linked clone leaves rescue vmdk in the instance folder""","A rescue of an image that is not linked clone will leave the rescue image disk in the original VM folder. The disk will not be cleaned up properly.
This leads to a number of problems:
1. usage of unnecessary disk space
2. additional rescues for the same VM may fail","VMware: fix rescue disk location when image is not linked clone

A rescue of a VM that has an image which is not linked clone would
create the rescue disk in the original VM's folder and not in the
rescue VM's folder. When the unrescue takes place the rescue disk
will not be deleted.

The patch adds unit tests that validates that the rescue disk location
is correct.

Change-Id: I17dfcade0a162057219bc6d108987bec53f526f9
Co-authored-by: Vui Lam<vui@vmware.com>
Closes-bug: #1293435"
1510,cfea218390605e2fe34b225ffa75b8b5c141f0b9,1406855632,,1.0,15,10,8,4,1,0.977886897,False,,,,,True,,,,,,,,,,,,,,,,,1150,1607,1352907,neutron,cfea218390605e2fe34b225ffa75b8b5c141f0b9,1,1,,"Bug #1352907 in neutron: ""response of normal user update the ""shared"" property of network ""","I used a normal user to create a network successfully,then I wanted to update the ""shared"" property of the network.
It failed,and response 404 erorr,the message is :The resource could not be found.But I have created the network,it is so strange.
I check the policy.json of neutron, the rule is: ""update_network:shared"": ""rule:admin_only"", so the normal user can't update it.
So the error information is wrong.
Check the code:
    def update(self, request, id, body=None, **kwargs):
        """"""Updates the specified entity's attributes.""""""
      ......
      ......
        try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            msg = _('The resource could not be found.')
            raise webob.exc.HTTPNotFound(msg)
I think we couldn't provide the wrong response information to avoid giving away information,and there isn't any information that need to avoid giving away here, So I think it is a bug.
I suggest to modify the code like this:
       try:
            policy.enforce(request.context,
                           action,
                           orig_obj)
        except exceptions.PolicyNotAuthorized:
            # To avoid giving away information, pretend that it
            # doesn't exist
            # msg = _('The resource could not be found.')
            raise webob.exc.HTTPForbidden(exceptions.PolicyNotAuthorized.message)","Return 403 instead of 404 on attr policy failures

Return an HTTP Forbidden code (403) instead of an
HTTP Not Found code (404) if a tenant is trying to
update it's own object. This is a safe adjustment
since the tenant already knows this object exists
so pretending it doesn't isn't improving secuirty
as much as it is causing confusion.

Closes-Bug: #1352907
Change-Id: I021ba6f890dfbabddd53e75c63083f5da0ecfdec"
1511,cff35c1999997b6af62f4ef285d66f38b19433af,1394531600,,1.0,66,13,5,5,1,0.780389448,True,6.0,2306118.0,55.0,19.0,False,62.0,881216.0,241.0,2.0,0.0,370.0,370.0,0.0,359.0,359.0,0.0,333.0,333.0,0.000843882,0.28185654,0.28185654,200,603,1261747,glance,cff35c1999997b6af62f4ef285d66f38b19433af,0,0, user_storage_quota now accepts units with value,"Bug #1261747 in Glance: ""Openstack Glance: user_total_quota size unit bytes ""","Description of problem:
When you setting user_total_quota size is in bytes, unit should be in KB\MB\GB
Version-Release number of selected component (if applicable):
RHEL 6.5
python-glanceclient-0.12.0-1.el6ost.noarch
openstack-glance-2013.2-5.el6ost.noarch
python-glance-2013.2-5.el6ost.noarch","user_storage_quota now accepts units with value

user_storage_quota now accept value in B, KB, MB, GB
or TB.  The unit is optional. If no unit is specified
Bytes is used as default.

DocImpact

Change-Id: Icc3f672869a5947cbcae38de92993c88ce0ef4e1
Closes-Bug: #1261747"
1512,d00e0b839378aed4c337efa1a33f6abbb384c149,1395130925,,1.0,42,1,2,2,1,0.884115122,True,3.0,3300899.0,47.0,5.0,False,6.0,540515.0,11.0,2.0,242.0,464.0,592.0,237.0,398.0,523.0,199.0,250.0,353.0,0.201612903,0.253024194,0.356854839,212,615,1262885,neutron,d00e0b839378aed4c337efa1a33f6abbb384c149,1,1,Forgot to clean up? Bug: goes out of sync with db after being down ,"Bug #1262885 in neutron: ""haproxy-lbaas-agent goes out of sync with db after being down""","How to reproduce:
1) create a pool with a VIP
2) check which lbaas-agent that pool is scheduled to
3) shut down lbaas-agent on the node
4) remove the pool/VIP via the API
5) restart lbaas-agent
It turns out the VIP would remain on the lbaas-agent host forever.
Looks like the instance_mapping variable is reset to empty restart the agent restarted, so it essentially lost track of that removed pool/vip.","LBaaS: remove orphan haproxy instances on agent start

This change adds remove_orphans() handling to the haproxy
namespace driver. remove_orphans() is already called by
lbaas agent on start for all drivers

Closes-Bug: #1262885
Change-Id: I5deae8e56c2cd2deb1667e9646633fd59a94b34e"
1513,d0225509bdb7882324726312f4a4a9f63375203e,1403278433,0.0,1.0,21,14,2,2,1,0.898058793,False,,,,,True,,,,,,,,,,,,,,,,,1851,1333219,1333219,Nova,d0225509bdb7882324726312f4a4a9f63375203e,1,0,There is a partial bug and a bug-close differents which one3 The one that is in this table is not marked as a fix commit in Launchpad but it is the bug-close according to the Label.,"Bug #1333219 in OpenStack Compute (nova): ""Virt driver impls don't match ComputeDriver base class API""","There are a number of problems where the virt driver impls do not match the API defined by the base ComputeDriver class.
For example
 - Libvirt:  Adds 'SOFT' as default value for 'reboot' method but no other class does
 - XenAPI: set_admin_passwd takes 2 parameters but base class defines it with 3 parameters in a different order
 - VMWare: update_host_status method which doesn't exist in base class & is never called in entire codebase
 - All: names of parameters are not the same as names of parameters in the base class
 - ...more...
These inconsistencies are functional bugs in the worst, or misleading to maintainers in the best case. It should be possible to write a test using the python 'inspect' module which guarantees that the sub-class APis actually match what they claim to implement from the base class.","vmwareapi: make method signatures match parent class

Fix any inconsistencies in method signatures between the
vmwareapi driver impl and the ComputeDriver parent class.
Add a test case to prevent future regressions

Closes-bug: #1333219
Change-Id: Ic361a922a1aefa5b5a3470c0547e694d073d2d58"
1514,d02ae9225d8ba4f7373ecced49c2d7c1cd17a732,1385698908,,1.0,2,2,3,3,1,0.630929754,True,5.0,282000.0,27.0,12.0,False,9.0,6872122.0,11.0,3.0,18.0,709.0,717.0,18.0,671.0,679.0,10.0,664.0,664.0,0.009143807,0.552784705,0.552784705,99,498,1255908,cinder,d02ae9225d8ba4f7373ecced49c2d7c1cd17a732,0,0,Typo in comments,"Bug #1255908 in Cinder: ""Fix typo in cinder""","sesssion -> session     cinder\openstack\common\db\sqlalchemy\models.py
explicity -> explicitly   cinder\openstack\common\db\sqlalchemy\models.py
tranfers -> transfers
recurse -> recursive
parens -> parents  cinder\openstack\common\periodic_task.py
satisified -> satisfied cinder\taskflow\exceptions.py","Fix typo in cinder

tranfers -> transfers
recurse -> recursive
satisified -> satisfied

There are other typos will be fixed in Oslo. The commit is:
https://review.openstack.org/#/c/59319/

Change-Id: I6fc2be0414962f456a827d294e258637af665524
Closes-Bug: #1255908"
1515,d0948a1fb0a4c425310f0cf0aea5b28680dc4817,1395939682,,1.0,23,4,2,2,1,0.876716289,True,3.0,22362.0,18.0,8.0,False,202.0,63135.0,1024.0,6.0,367.0,3932.0,4072.0,350.0,3003.0,3133.0,363.0,3497.0,3633.0,0.047303444,0.454580897,0.472254711,1702,1240922,1240922,nova,d0948a1fb0a4c425310f0cf0aea5b28680dc4817,1,0,“As described in the bug - some hypervisors (libvirt) do not support this.”,"Bug #1240922 in OpenStack Compute (nova): ""VM don't resume after detaching volume""","Hi guys,
I have a suspend vm with an attached volume, if I detached volume while instance is in suspend state it can't be resumed properly.
It happens with both Windows and Linux vm's.
LibVirt error:
2494: error : qemuMonitorIORead:502 : Unable to read from monitor: Connection reset by peer
Packets versioning in Ubuntu 12.04:
ii  libvirt-bin                       1.0.2-0ubuntu11.13.04.2~cloud0              programs for the libvirt library
ii  libvirt-dev                       1.0.2-0ubuntu11.13.04.2~cloud0              development files for the libvirt library
ii  libvirt0                          1.0.2-0ubuntu11.13.04.2~cloud0              library for interfacing with different virtualization systems
ii  python-libvirt                    1.0.2-0ubuntu11.13.04.2~cloud0              libvirt Python bindings
ii  kvm                               1:84+dfsg-0ubuntu16+1.0+noroms+0ubuntu14.10 dummy transitional package from kvm to qemu-kvm
ii  qemu-common                       1.0+noroms-0ubuntu14.10                     qemu common functionality (bios, documentation, etc)
ii  qemu-kvm                          1.0+noroms-0ubuntu14.10                     Full virtualization on i386 and amd64 hardware
ii  nova-common                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - common files
ii  nova-compute                      1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node
ii  nova-compute-kvm                  1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute - compute node (KVM)
ii  python-nova                       1:2013.1.2-0ubuntu1~cloud0                  OpenStack Compute Python libraries
ii  python-novaclient                 1:2.13.0-0ubuntu1~cloud0                    client library for OpenStack Compute API","Disable volume attach/detach for suspended instances

As described in the bug - some hypervisors (libvirt) do not support
this. It is best to disable it in the API to provide a consistent user
experience.

Also adds a test to prevent an accidental regression.

Change-Id: I5b404cca22cffecbaf524e2511810e5341242052
Closes-bug: #1240922"
1516,d09d12ab2ba72a9e7fe42852a7cf837231053590,1397063154,1.0,1.0,7,6,2,2,1,0.995727452,True,2.0,450330.0,18.0,5.0,False,66.0,1337060.0,158.0,3.0,1312.0,1072.0,1844.0,1036.0,898.0,1466.0,1154.0,769.0,1413.0,0.736607143,0.491071429,0.901785714,771,1198,1305197,cinder,d09d12ab2ba72a9e7fe42852a7cf837231053590,1,1,,"Bug #1305197 in Cinder: ""volume manager must call driver.remove_export with elevated context""","There are a couple of places in the volume manager (initialize_connection and terminate_connection) where driver.remove_export() is called with a user context rather than an elevated context.
This appears to break the assumption used in delete_volume() where an elevated context is passed.
This causes the LVM LIO driver to fail with an error removing an export in an initialize_connection() error case:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 462, in _process_data
    **args)
  File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/manager.py"", line 801, in initialize_connection
    self.driver.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/drivers/lvm.py"", line 540, in remove_export
    self.target_helper.remove_export(context, volume)
  File ""/usr/lib/python2.7/site-packages/cinder/volume/iscsi.py"", line 232, in remove_export
    volume['id'])
  File ""/usr/lib/python2.7/site-packages/cinder/db/api.py"", line 232, in volume_get_iscsi_target_num
    return IMPL.volume_get_iscsi_target_num(context, volume_id)
  File ""/usr/lib/python2.7/site-packages/cinder/db/sqlalchemy/api.py"", line 116, in wrapper
    raise exception.AdminRequired()
AdminRequired: User does not have admin privileges","driver.create/remove_export() require elevated context

The volume manager should call driver.create_export()
and driver.remove_export() with an elevated context.

This is already done for remove_export() in some cases
but not in initialize_connection error paths, or for
terminate_connection.

This will at a minimum cause issues with the LVM LIO
driver as its create/remove_export methods uses database
queries requiring admin access (volume_get_iscsi_target_num,
iscsi_target_count_by_host).

Partial-Bug: #1300148
Closes-Bug: #1305197

Change-Id: I5c1091cf9720ebccefc328b64fbf2982b3aac397"
1517,d0a5e27288940c8e104330d784fed43575187afd,1398339866,,1.0,53,4,6,6,1,0.922165103,True,6.0,2385794.0,64.0,23.0,False,68.0,1084128.0,259.0,5.0,429.0,1518.0,1801.0,403.0,1426.0,1687.0,421.0,1487.0,1765.0,0.053689567,0.189312977,0.224681934,831,1260,1312132,nova,d0a5e27288940c8e104330d784fed43575187afd,1,1,,"Bug #1312132 in OpenStack Compute (nova): ""live migrate of stopped VM goes to error, not back to task_state=None ""","When the InstanceNotRunning exception is raised during the initial live-migrate, it should really just revert to task_state=None in the same way as if there was NoValidHost, or the compute service was down.","Ensure live-migrate reverts if server not running

When the check to ensure a VM is running before attempting the
live migrate fails, it raises InstanceNotRunning. It should really be
treated in the same way as MigrationPreCheckError.

Along with fixing this bug, I have ensured the expected exceptions in
migrate_server are in sync with the except clause in _live_migrate.

This goes through to the API, ensuring the errors are not treated as
internal server errors, but are given a slightly more appropriate error
return value.

Currently the API returns 400 for all these kinds of errors. While other
error codes may be more appropriate, this change is concentrating purely
on ensuring we treat all the exceptions in the same way.

Change-Id: Iafe5026ef5a095d76e9864b489571e7bcfeb55f0
Closes-Bug: #1312132"
1518,d1220a3c22fccc1d86eaafeebe7ef9c074d4fcb9,1384268726,0.0,1.0,13,1,2,2,1,0.749595257,True,1.0,6954258.0,41.0,15.0,False,4.0,1476725.0,5.0,8.0,181.0,929.0,929.0,180.0,862.0,862.0,134.0,308.0,308.0,0.252808989,0.578651685,0.578651685,1740,1249188,1249188,neutron,d1220a3c22fccc1d86eaafeebe7ef9c074d4fcb9,1,1, ,"Bug #1249188 in neutron: ""[neutron bandwidth metering] When I delete a label, router's tenant_id disappeared.""","[neutron bandwidth metering]
I delete a label using neutronclient CLI (neutron meter-label-delete).
Then router's tenant_id is omitted.
I don' t know why.
it's a bug maybe","Fix MeteringLabel model to not clear router's tenant id on deletion

foreign_keys parameter of orm.relationship should point to local columns.
Currently for MeteringLabel it points to Router.tenant_id column which causes
routers tenant_id clearing on label deletion.

Closes-Bug: #1249188
Change-Id: Iccc0daf4f6edd537fd7f9e4b2fc4be094543ca5d"
1519,d122b2e05a9e13d5caea3f8f6578bb473fbb9c5e,1386063049,3.0,1.0,338,80,15,7,1,0.826226233,True,7.0,12597576.0,31.0,11.0,False,135.0,126499.6,476.0,4.0,24.0,2384.0,2401.0,24.0,2068.0,2085.0,11.0,2277.0,2282.0,0.00177305,0.336583924,0.337322695,117,516,1256873,nova,d122b2e05a9e13d5caea3f8f6578bb473fbb9c5e,0,0,Server actions should raise – Feature,"Bug #1256873 in OpenStack Compute (nova): ""Catch InstanceIsLocked exception instead of invalidstate on server actions""","Server actions should raise ""Instance is locked"" while instance is locked by admin, the actions include:
1. reboot  in v2 and v3
2. delete  in v2 and v3
3. resize/confirm resize/revert resize in v2 and v3
4. shelve/unshelve/shelve_offload in v2 and v3
5. attach_volume/swap_volume/detach_volume in v2 and v3
6. rebuild in v2 and v3","Catch InstanceIsLocked exception on server actions

If the server is locked, server actions raise 409 error, and the error
message is like ""Instance is in an invalid state for 'pause(action name)'"". The
error message is inappropriate, two points:
1. the state is valid for pause
2. the message doesn't tell that the instance is locked

The patch fixes the problem, and will modify on v2 and v3.

Change-Id: I7e21037b9abca5a3fd0f64dd5621e88dcf1a3c21
Closes-Bug: #1256873"
1520,d144cb8ff422e16c5917c87471a46fd095fd1856,1411051183,,1.0,2,1,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1051,1496,1339968,neutron,d144cb8ff422e16c5917c87471a46fd095fd1856,1,1,,"Bug #1339968 in neutron: ""duplicate dhcp port when deleting original dhcp port and restart dhcp-agent""","my use case is as follows:
1.delete dhcp port
2.restart dhcp-agent
then, there will be two dhcp ports.","Only setup dhcp interface if dhcp is not active on network

When enabling (DhcpLocalProcess.enable()) dhcp for a network the agent
first sets dhcp interface, then checks if dhcp is curently active and
if it's true then the agent restarts dhcp.
Restart (DhcpBase.restart()) first disables dhcp and then enables it again
by calling DhcpLocalProcess.enable() recursively which in turn sets
dhcp interface again (it doesn't see the port created earlier as network
is not re-fetched from db). This leads to duplicate dhcp interface
for the network.

The fix is to only setup dhcp interface if dhcp is not active.

Closes-Bug: #1339968

Change-Id: I3a6d2fd5b18a97138ac5b699ecd4d4b30dbdbacb"
1521,d19c75c19d2de8b20e82e6de9413ba53671ad7fb,1401982998,,1.0,78,2,2,2,1,0.337290067,False,,,,,True,,,,,,,,,,,,,,,,,587,1009,1291007,nova,d19c75c19d2de8b20e82e6de9413ba53671ad7fb,1,1,,"Bug #1291007 in OpenStack Compute (nova): ""device_path not available at detach time for boot from volume""","When you do a normal volume attach to an existing VM and then detach it, the connection_info contains the following
connection_info['data']['device_path'] at libvirt volume driver disconnect_volume(self, connection_info, mount_device) time.
When you boot a VM from a volume, not an image, and then terminate the VM, the libvirt volume driver disconnect_volume's
connection_info['data'] doesn't contain the 'device_path' key.   The libvirt volume driver's need this information to correctly disconnect the LUN from the kernel.","libvirt: Save device_path in connection_info when booting from volume

If you boot an instance from a volume and later terminate it, the
libvirt volume driver disconnect_volume method does not have the
'device_path' key in connection_info['data'].  However, if you
attach a volume to an existing instance and then detach it,
the disconnect_volume method would have the 'device_path' key in
connection_info['data'].  Having the 'device_path' key would be
useful for some volume drivers to determine the device path of the
volume.  This patch saves the 'device_path' in connection_info['data']
when _create_domain_and_network is called, so it could be later used.

Change-Id: I8ebb5f3c2e7a81b11d776f8c0a15f3491ed273be
Closes-Bug: #1291007"
1522,d1b783fd673448381445dce82e9160ba2f715a7c,1382021928,,1.0,23,7,3,2,1,0.789690082,True,9.0,4409130.0,85.0,33.0,False,11.0,2561597.667,10.0,10.0,52.0,1463.0,1481.0,50.0,1386.0,1402.0,36.0,430.0,434.0,0.08008658,0.932900433,0.941558442,1701,1240790,1240790,neutron,d1b783fd673448381445dce82e9160ba2f715a7c,1,1,,"Bug #1240790 in neutron: ""Allow using ipv6 address with omiting zero""","Now neutron support ipv6 address like 2001:db8::10:10:10:0/120,
but don't support ipv6 address with omiting zero like 2001:db8:0:0:10:10:10:0/120
that will cause the exception ""'2001:db8:0:0:10:10:10:0/120' isn't a recognized IP subnet cidr, '2001:db8::10:10:10:0/120' is recommended""","Support uncompressed ipv6 address and abbreviated ipv4 address

Now neutron supports zero-compressed ipv6 address like
'2001:db8::10:10:10:0/120' and unabbreviated ipv4 address
like '10.0.0.0/24'.
This path will make neutron also support uncompressed ipv6
address and abbreviated ipv4 address. for example:
1, uncompressed ipv6 address like 'fe80:0:0:0:0:0:0:0/128'
2, ipv6 address with multiple consecutive zero like
   '2001:0db8:0:0:1::1/128' or '2001::0:1:0:0:1100/120'
3, abbreviated ipv4 address like '10/24'

Change-Id: I755e38ea11c139e15488ec60e7d5682a244ab114
Closes-Bug: #1240790"
1523,d1f1722e0edb63c73b60c80abafa63749349cd8e,1411339167,,1.0,4,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1341,1809,1372196,neutron,d1f1722e0edb63c73b60c80abafa63749349cd8e,1,1,0,"Bug #1372196 in neutron: ""ofagent: tenant triggerable LOG.info events""","ofagent's arp responder has some LOG.info which can be triggered by tenant's OSes.
they allow bad tenants flood host logs.","ofagent: Drop log level of tenant-triggerable events

To prevent evil tenants from flooding logs.

Closes-Bug: #1372196
Change-Id: I617e9fedf8a8c5e95c6067b716c83778d4d8cc7e"
1524,d23bc8fa6e2d8a735a2aa75224b1bc96a3b992f5,1398304033,,1.0,99,8,2,2,1,0.533643917,True,9.0,2873503.0,136.0,33.0,False,31.0,1774454.0,56.0,3.0,17.0,256.0,270.0,15.0,256.0,268.0,12.0,253.0,262.0,0.011082694,0.216538789,0.224211424,813,1241,1309195,neutron,d23bc8fa6e2d8a735a2aa75224b1bc96a3b992f5,1,1,,"Bug #1309195 in neutron: ""[OSSA 2014-019] IPv6 prefix shouldn't be added in the NAT table (CVE-2014-4167)""","SNAT rules with IPv6 prefixes are added into the NAT table, which causes failure with the call to iptables-restore:
Stderr: ""iptables-restore v1.4.18: invalid mask `64' specified\nError occurred at line: 22\nTry `iptables-restore -h' or 'iptables-restore --help' for more information.\n""","Install SNAT rules for ipv4 only

Change-Id: I37bd770aa0d54a985ac2bec708c571785084e0ec
Closes-Bug: #1309195"
1525,d2479f1e88e1eff777afbc922ae1da63afcc7716,1378142625,,1.0,1,5,3,3,1,0.920619836,True,4.0,1714408.0,32.0,11.0,False,65.0,413071.0,156.0,4.0,313.0,998.0,1088.0,310.0,922.0,1012.0,300.0,812.0,894.0,0.320895522,0.86673774,0.954157783,1500,1219097,1219097,cinder,d2479f1e88e1eff777afbc922ae1da63afcc7716,1,0,“fails due to db schema change”,"Bug #1219097 in Cinder: ""online volume migration fails due to db schema change""","After installing latest cinder code and applying db sync operation, online volume migration now fails with the following traceback:
2013-08-30 14:26:43.411 1399033 ERROR cinder.openstack.common.rpc.amqp [req-78555fa4-c79c-4160-993d-05d60b001222 d8a70c91de4d41cc9bf994c8d23e4479 af64382b5b1449189608c0a3c9c39a0c] Exception during message handling
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 433, in _process_data
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 686, in migrate_volume_completion
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     self.db.finish_volume_migration(ctxt, volume_id, new_volume_id)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/api.py"", line 198, in finish_volume_migration
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return IMPL.finish_volume_migration(context, src_vol_id, dest_vol_id)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/sqlalchemy/api.py"", line 120, in wrapper
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/db/sqlalchemy/api.py"", line 1121, in finish_volume_migration
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     filter_by(id=dest_vol_id).\
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.6/site-packages/sqlalchemy/orm/query.py"", line 2679, in delete
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     result = session.execute(delete_stmt, params=self._params)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/db/sqlalchemy/session.py"", line 485, in _wrap
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp     raise exception.DBError(e)
2013-08-30 14:26:43.411 1399033 TRACE cinder.openstack.common.rpc.amqp DBError: (IntegrityError) (1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`cinder`.`volume_admin_metadata`, CONSTRAINT `volume_admin_metadata_ibfk_1` FOREIGN KEY (`volume_id`) REFERENCES `volumes` (`id`))') 'DELETE FROM volumes WHERE volumes.id = %s' ('86475de8-daba-45e3-9c15-76ca0941efb6',)
There is new table in cinder db volume_admin_metadata with FK relationship with volumes.  This admin metadata needs to be handled in the same way as normal volume metadata.","Soft delete tmp migration volume

Previously the row for the temporary migration volume was deleted from
the database, which caused a foreign key violation for its admin
metadata. Regardless, the volume should have been soft deleted from the
database anyway, and this patch takes care of that.

Change-Id: I8e4942b0e21ec79c90aa95b18b5612b5b133fd62
Closes-Bug: #1219097"
1526,d26bd87919990923b6fc85779f03705514d8d0b3,1390846851,,1.0,2,2,1,1,1,0.0,True,6.0,8725718.0,165.0,41.0,False,2.0,14969100.0,3.0,4.0,12.0,1072.0,1073.0,12.0,929.0,930.0,12.0,531.0,532.0,0.017735334,0.725784447,0.727148704,333,743,1273355,neutron,d26bd87919990923b6fc85779f03705514d8d0b3,1,1,fails,"Bug #1273355 in neutron: ""Grizzly-havana upgrade fails when migrating from stamped db""","If grizzly is deployed without using quantum-db-manage and letting neutron to create tables, there are not created tables servicedefinitions and servicetypes. These tables are dropped later when using LoadBalancerPlugin. When creating db scheme with quantum-db-manage, these tables are created and dropped correctly.","Drop service* tables only if they exist

Tables servicedefinitions and servicetypes are not used by neutron but
created using db migration tool. If quantum-server in grizzly was
started without using migration tool then tables mentioned above are not
created. During migration from grizzly to havana alembic attempts to
drop these unused tables. This patch uses ""IF EXISTS"" clause when
dropping mentioned tables.

Closes-bug: #1273355
Change-Id: I33fcb3e1dc96ce37ec9c00987cb5a3a953ca691d"
1527,d281149ea0f860c7c862f7897710d2e1fcb681e7,1383137135,,1.0,3,3,1,1,1,0.0,True,2.0,143736.0,11.0,5.0,False,37.0,2238549.0,89.0,3.0,4.0,656.0,659.0,4.0,635.0,638.0,1.0,612.0,612.0,0.001768347,0.541998232,0.541998232,39,367,1246291,cinder,d281149ea0f860c7c862f7897710d2e1fcb681e7,0,0,Remove unnecesary calls (¿To improve the performance?) I think is just refactoring the code,"Bug #1246291 in Cinder: ""Remove unnecessary db calls made to fetch original metadata when delete flag is set to True""","Found in commit 75bcf70ed4698f0ffba4c6a7236a1e30b1214b57
Original metadata is retrieved unnecessarily from the database when delete flag is set to True in the following methods in cinder volume api.
update_volume_metadata
update_volume_admin_metadata
update_snapshot_metadata","Remove unused db calls to fetch original metadata

Remove unnecessary db calls made to fetch
original metadata when delete flag is set
to True.

Closes-Bug: #1246291

Change-Id: If7483b8b21f71efa5e7c211b031170bc03eb6b9b"
1528,d2874463010b129e62a568357cfa1df3ab4805e6,1394179691,0.0,1.0,33,7,2,2,1,0.881290899,True,2.0,3666658.0,20.0,7.0,False,38.0,1615229.5,51.0,2.0,14.0,768.0,782.0,13.0,768.0,781.0,1.0,763.0,764.0,0.000265498,0.101420417,0.101553166,1511,1220856,1220856,nova,d2874463010b129e62a568357cfa1df3ab4805e6,1,1, ,"Bug #1220856 in OpenStack Compute (nova): ""libvirt error when create VM with 2 NICs under DevStack""","Under a stock DevStack setup on bare metal, I started stack.sh, which creates a public network (w/o DHCP) and a private network (with DHCP) and a router.  On the host I created a br-ex and added eth3 to the bridge. The public network is connected via a physical switch, to another host (also running Devstack).
I am able to create VMs, and ping between them, and I can also (with the new VPNaaS feature under development) ping VMs over the public (provider?) network.
If I create a VM (seen this with cirros and others) and specify the private network, or create a VM with the public network, they launch fine. However, if I try to boot an instance with two NICs, the launch fails:
 localnet=`neutron net-list | grep private | cut -f 2 -d'|' | cut -f 2 -d' '`
 nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 mary --nic net-id=$localnet
pubnet=`neutron net-list | grep public | cut -f 2 -d'|' | cut -f 2 -d' '`
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 peter --nic net-id=$pubnet
nova boot --image cirros-0.3.1-x86_64-uec --flavor 1 paul --nic net-id=$localnet --nic net-id=$pubnet
nova list
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| ID                                   | Name  | Status | Task State | Power State | Networks            |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
| 0fc9c41c-fcd7-45a8-ae10-568b506a331f | mary  | ACTIVE | None       | Running     | private=10.2.0.4    |
| 76925e14-a0b7-4285-9197-5ff0e92f5bb4 | paul  | ERROR  | None       | NOSTATE     |                     |
| c8cd45a4-10d9-4d8c-9a5a-f806e63683c0 | peter | ACTIVE | None       | Running     | public=172.24.4.235 |
+--------------------------------------+-------+--------+------------+-------------+---------------------+
Looking at the logs, I see that the screen-n-cpu.log reports an error and has a traceback:
2013-09-04 18:01:47.963 ERROR nova.compute.manager [req-187bbd89-f40d-4637-b549-0edfc9b66419 admin admin] [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Instance failed to spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] Traceback (most recent call last):
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/compute/manager.py"", line 1293, in _spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1699, in spawn
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     block_device_info)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2697, in _creat\
e_domain_and_network
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain = self._create_domain(xml, instance=instance, power_on=power_on\
)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2652, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain.XMLDesc(0))
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 2647, in _creat\
e_domain
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     domain.createWithFlags(launch_flags)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 17\
9, in doit
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     result = proxy_call(self._autowrap, f, *args, **kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 13\
9, in proxy_call
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     rv = execute(f,*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/local/lib/python2.7/dist-packages/eventlet/tpool.py"", line 77\
, in tworker
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     rv = meth(*args,**kwargs)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]   File ""/usr/lib/python2.7/dist-packages/libvirt.py"", line 581, in createW\
ithFlags
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4]     if ret == -1: raise libvirtError ('virDomainCreateWithFlags() failed',\
 dom=self)
2013-09-04 18:01:47.963 9855 TRACE nova.compute.manager [instance: 76925e14-a0b7-4285-9197-5ff0e92f5bb4] libvirtError: internal error Cannot instantiate filter due to unresolvable\
 variables: DHCPSERVER
There is no DHCP server on the public network, but there is one on the local network.
This is consistently reproducible and occurs with other image types.
The nova code is off the master branch (havana) with the SHA1 of 86c97ff from 6 days ago.","libvirt: Make nwfilter driver use right filterref

This fixes Bug #1220856 which occurs with Libvirt NWFilterFirewall
Driver.  While creating a VM with multiple NICs including one connects
to DHCP serving network and another to DHCP non-serving network,
NWFilterFirewall Driver will use the same base filter that allows DHCP
for all NICs. This leads a libvirt launch error due to lack of
'dhcp_server' parameter which is needed to define allow-DHCP filters.

This patch makes NWFilterFirewall Driver use right base filter for
each NIC depends on 'dhcp_server' config.

Closes-bug: #1220856
Change-Id: I7f9a7c281f152985478b2ec295f0644ba475fd76"
1529,d2a81315316a95dbb6888f778e4f207582c626c3,1394676256,,1.0,2,2,2,2,1,1.0,True,1.0,1820261.0,10.0,2.0,False,20.0,2062438.0,30.0,2.0,4.0,503.0,506.0,4.0,394.0,397.0,1.0,356.0,356.0,0.001679261,0.299748111,0.299748111,571,992,1290234,glance,d2a81315316a95dbb6888f778e4f207582c626c3,0,0,do not use xx in python3,"Bug #1290234 in Glance: ""do not use __builtin__ in Python3""","__builtin__ does not exist in Python 3, use six.moves.builtins instead.","Do not use __builtin__ in python3

__builtin__ does not exist in Python 3, use
six.moves.builtins instead.

Change-Id: If7d2838b0b90ac2d82c4d62c6dfb3c011f937a0b
closes-bug: #1290234"
1530,d2ab9079ada5135598a89aa65f3ef8b63bf28003,1407781333,,1.0,4,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1129,1584,1350542,nova,d2ab9079ada5135598a89aa65f3ef8b63bf28003,1,1,,"Bug #1350542 in OpenStack Compute (nova): ""resource tracker reports negative value for free hard disk space""","When overcommiting on hard disk usage, the audit logs report negative amounts of free disk space. While technically correct, it may confuse the user, or make the user think there is something wrong with the tracking.
The patch to fix this will be in a similar vein to https://review.openstack.org/#/c/93261/","Adjust audit logs to avoid negative disk info

When requesting more disk than is available, the audit
logs will report negative values. This may confuse the
user or make him/her think the tracking is innaccurate.

This patch seeks to remove any confusion. It is in the
same vein as https://review.openstack.org/#/c/93261/
which addresses the same issue in regards to cpu and
memory audit logs.

Change-Id: I184985573b20144717ac40f1f3c26709030d5a03
Closes-bug: #1350542"
1531,d311066dd43bdc4598fa76837d78ecfac16f5e58,1391261613,,1.0,0,6,1,1,1,0.0,True,3.0,152220.0,18.0,7.0,False,61.0,121196.0,142.0,2.0,0.0,777.0,777.0,0.0,716.0,716.0,0.0,738.0,738.0,0.000725689,0.53628447,0.53628447,344,755,1274601,cinder,d311066dd43bdc4598fa76837d78ecfac16f5e58,0,0,No BFC for this bug,"Bug #1274601 in Cinder: ""GlanceMetadataNotFound exception not through during delete volume""","delete_volume implementation would like to log a warning ""no glance metadata found for volume %s"" in case of lack of metadata by catching a GlanceMetadataNotFound exception.
Nevertheless this exception is never throw by the SQL backend implementation.
So this log cannot be display correctly.","Remove a catching exception during delete_volume

GlanceMetadataNotFound Exception was catching during
a call to volume_glance_metadata_delete_by_volume().
This exception is never thrown by the implementation.
This catching has been removed.

Change-Id: Ifa15df5d9c4517f75a1a050ff5aa34225f33931b
Closes-Bug: #1274601"
1532,d37907af5b71024024fb70d9454e4b3e05875105,1388760688,,1.0,21,29,3,3,1,0.304221198,True,4.0,728413.0,33.0,7.0,False,23.0,42612.0,78.0,2.0,12.0,692.0,694.0,12.0,530.0,532.0,12.0,537.0,539.0,0.011850501,0.490428441,0.492251595,240,644,1265446,glance,d37907af5b71024024fb70d9454e4b3e05875105,1,1,,"Bug #1265446 in Glance: ""v2 upload returns '500' when quota exceeded on local filesystem storage""","Steps to reproduce:
1) Configure glance to use local filesystem backend, make sure the quota is explicitly configured.
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) Upload image data. Make sure the image data size > quota
 glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-upload --file $HOME/testImg.qcow2 $img_id
Request returned failure status.
HTTPInternalServerError (HTTP 500)
Same thing happens if you use curl:
1) Configure glance to use local filesystem backend
2) glance --os-auth-token ""$(cat ~/user-token)"" \
    --os-image-url http://127.0.0.1:9292 \
    --os-image-api-version 2 \
    image-create --container-format bare \
        --disk-format qcow2 --name localfs-image
3) curl -i -X PUT -H 'Transfer-Encoding: chunked' \
    -H 'X-Auth-Token: $(cat ~/user-token)' \
    -H 'Content-Type: application/octet-stream' \
    -H 'User-Agent: python-glanceclient' \
    --data-binary @$HOME/testImg.qcow2 http://127.0.0.1:9292/v2/images/$img_id/file
HTTP/1.1 100 Continue
HTTP/1.1 500 Internal Server Error
Content-Type: text/plain
Content-Length: 0
Date: Thu, 02 Jan 2014 07:27:26 GMT
Connection: close","Fix call to store.safe_delete_from_backend

When uploading data of a newly created image to a local filesystem
store, if the image data size exceeds the quota allocated to the user
then the operation fails with a ""500 Internal Server Error"".

This bug is triggered only when using chunked transfers to upload the
image data (curl and glanceclient both use chunked transfers).

When using chunked transfers the 'Content-Length' header is not set and
so the quota checking code in start of quota.ImageProxy.set_data() is
skipped. Furthermore, because of the missing size info, the store
downloads the entire image data without checking for the quota (the
quota is checked only after the image data has been downloaded and the
size determined).

If after the image data has been downloaded it is found that the quota
has been exceeded, then the StorageQuotaFull exception is raised and a
call to store.safe_delete_from_backend() made to clean up the image
data. But the 'location' and 'context' parameters to this call are given
in the wrong order and so this operation raises an (uncaught) exception,
giving rise to this bug.

This bug was not caught by existing tests because existing quota tests
don't use chunked transfer.

This commit fixes the order of the parameters and adds quota tests using
chunked transfers.

Change-Id: Ib8ab11f2115e3aead98af70788eaa45c2c33219c
Closes-bug: #1265446"
1533,d37b6c9ee8ddf2cc90197e5a442ba759eca5a0c0,1395094744,0.0,1.0,5,5,2,2,1,0.721928095,True,2.0,197969.0,43.0,9.0,False,10.0,293181.0,11.0,3.0,96.0,716.0,754.0,96.0,582.0,620.0,90.0,379.0,411.0,0.092198582,0.385005066,0.417426545,639,1065,1293818,neutron,d37b6c9ee8ddf2cc90197e5a442ba759eca5a0c0,1,1,Bug using root,"Bug #1293818 in neutron: ""Agents don't need root to list namespaces""","Given the expense of sudo (at scale) and rootwrap calls, agents should not be using root for commands that don't need it.  Listing namespaces is one of those.
(I could have sworn I already fixed this which is why I didn't fix it until today)","Don't use root to list namespaces

A bit of low hanging fruit.  I just noticed that this hadn't been
fixed yet.

Change-Id: Iea9210098b6acf4ab24a89287529ff82986faaad
Closes-Bug: #1293818"
1534,d39442007157c3074c199aba64424b2f4b08f461,1394977633,,1.0,24,5,2,2,1,0.578794625,True,1.0,122002.0,17.0,3.0,False,57.0,256966.0,149.0,3.0,1837.0,3308.0,4538.0,1529.0,2728.0,3658.0,939.0,3188.0,3532.0,0.12342437,0.418723739,0.463891807,621,1046,1292633,nova,d39442007157c3074c199aba64424b2f4b08f461,1,1,,"Bug #1292633 in OpenStack Compute (nova): ""VMware: Unable to boot from ISO when a 0 GB root disk size is specified""","When using VC Driver, booting an ISO will fail when a flavor with a 0 GB root disk size is specified.
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/compute/manager.py"", line 1703, in _spawn
    block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 598, in spawn
    admin_password, network_info, block_device_info)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 562, in spawn
    _create_virtual_disk(dest_vmdk_path, root_gb_in_kb)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 370, in _create_virtual_disk
    self._session._wait_for_task(vmdk_create_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 906, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A specified parameter was not correct.
The error message seen in vSphere client is ""The virtual disk size is too small"".
Log available here: http://paste.openstack.org/show/73516/","VMware: enable booting an ISO with root disk size 0

There may be cases where booting from an ISO does not require
any local disk. In this case the flavor may have the root disk
size set at 0.

Change-Id: Id290a0ede11c754f686d3b70322c84c03f161637
Closes-bug: #1292633"
1535,d3acac0f5bffca59441d9a4a12c89db1d45ec4cf,1411736024,,1.0,17,3,2,2,1,0.609840305,False,,,,,True,,,,,,,,,,,,,,,,,623,1048,1292733,nova,d3acac0f5bffca59441d9a4a12c89db1d45ec4cf,1,1,Ironic bug,"Bug #1292733 in OpenStack Compute (nova): ""Ironic: unplugging of instance VIFs fails if no VIFs associated with port""","During instance spawn, Ironic attempts to unplug any plugged VIFs from ports associated with an instance.  If there are no associated VIFs to unplug, instance spawn fails with a nova-compute errror:
2014-03-14 21:15:35.907 16640 TRACE nova.openstack.common.loopingcall HTTPBadRequest: Couldn't apply patch '[{'path': '/extra/vif_port_id', 'op': 'remove'}]'. Reason: u'vif_port_id'
The driver should be only attempt to unplug VIFs from ports that actually have them associated.","Ironic: Do not try to unplug VIF if not associated

During instance spawn, Ironic attempts to unplug any plugged VIFs from
ports associated with an instance. If there are no associated VIFs
to unplug, this would raise an exeception that would be logged into
n-cpu. This patch fix that behavior by making the driver check if the
port has an VIF associated with it before trying to remove it.

Change-Id: I5608b9eec625452edda9a0c9eb366ece2a95800f
Closes-Bug: #1292733"
1536,d3be7b040eaa61a4d0ac617026cf5c9132d3831e,1396321111,,1.0,8,3,4,1,1,0.933816945,True,4.0,346456.0,73.0,8.0,False,11.0,1351569.0,11.0,5.0,1175.0,1496.0,1514.0,1004.0,1263.0,1281.0,711.0,875.0,889.0,0.672332389,0.827195467,0.840415486,691,1117,1298459,neutron,d3be7b040eaa61a4d0ac617026cf5c9132d3831e,0,0,add missing migrations,"Bug #1298459 in neutron: ""db migration: some tables are not created for bigswitch plugin and bigswitch mech driver""","For bigswitch plugin, networkdhcpagentbindings and subnetroutes tables are not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_PLUGIN/logs/screen/screen-q-svc.log.gz
FOr bigswitch ML2 mech driver, neutron_ml2.consistencyhashes is not created by db migration.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-96-40296-13/BSN_ML2/logs/screen/screen-q-svc.log.gz","Add missing DB migrations for BSN ML2 plugin

Adds missing database migrations for the Big Switch
ML2 driver and plugin.

Closes-Bug: #1298459
Change-Id: I6a46a53e069353501d85c4c8400f98514d6f0162"
1537,d449b5d8556e67be08e016c94f9b1c523a69ce7e,1407856456,,1.0,3,4,2,2,1,0.985228136,False,,,,,True,,,,,,,,,,,,,,,,,1181,1638,1355882,nova,d449b5d8556e67be08e016c94f9b1c523a69ce7e,1,0,API change “Commit e00bdd7aa8c1ac9f1ae5057eb2f774f34a631845 change get_floating_ip_pools in a way that it now return a list of names”,"Bug #1355882 in OpenStack Compute (nova): ""get_floating_ip_pools for neutron v2 API inconsistent with nova network API""","Commit e00bdd7aa8c1ac9f1ae5057eb2f774f34a631845 change get_floating_ip_pools in a way that it now return a list of names rather than a list whose elements are in the form {'name': 'pool_name'}.
The implementation of this method in nova.network.neutron_v2.api has not been adjusted thus causing tempest.api.compute.floating_ips.test_list_floating_ips.FloatingIPDetailsTestJSON to always fail with neutron
The fix is straightforward.","Neutron v2 API: fix get_floating_ip_pools

Commit 7254f9b9dfbadadeb3aeda5d02bf37bfeb65e72d changed this method
in nova.network.api in a way that it now returns simply a list of
floating ip pool names, rather a list of dicts in the form

{'name': 'pool_name'}

The implementation of get_floating_ip_pools for neutron needs to
be changed accordingly. Otherwise nova's floating ip extension
will return a dict as floating ip pool names when neutron is enabled.

Change-Id: I27001c9f17dad4dc4ca8cd032e689736553d8225
Closes-Bug: #1355882"
1538,d46f4a5b6d17f20a9c1de9367af11c9f3e7a7ef0,1390755434,,1.0,0,22,2,1,1,0.945660305,True,2.0,6331704.0,32.0,16.0,False,63.0,177234.0,171.0,9.0,1685.0,3947.0,3950.0,1439.0,3203.0,3206.0,810.0,2571.0,2572.0,0.113236526,0.359117565,0.359257191,321,730,1272286,nova,d46f4a5b6d17f20a9c1de9367af11c9f3e7a7ef0,1,1,There is a bug. But they only remove code,"Bug #1272286 in OpenStack Compute (nova): ""VMware driver: Nova compute fails to start when multiple nova compute services are running per vCenter.""","Nova Compute fails to start when there are multiple nova compute services  running on different VMs(nova compute VMs) and each Vm managing multiple cluster in a vCenter and instances are provisioned on them.
Explanation:
Lets say, one nova compute vm(C1) is managing 5 clusters, and another(C2) managing 5 clusters. C1 manages n number of instances. Suppose in C2 compute service gets restarted, it fails to start.
Reason:
on the start up of the nova-compute,  it checks the instances reported by the driver are still associated with this host.  If they are  not, it destroys them.
method _destroy_evacuated_instances calls the driver list_instances, which lists all the instances in the vCenter, though they are managed by different compute. Instead it should return only the vms which are managed by c1/c2.
log file attached.","VMware: fix exception when using multiple compute nodes

When there is more than one compute node running and one of the
nodes restarts it terminates on an exception that the resource is
not found.

The cause of the issue is that a vif plug was being attempted for
a resource that did not exist. The vif plug should have raised an
""NotImplemented"" exception.

Change-Id: I5a3f1cc73a981173b6c2fa493de3aad10a7e97fd
Closes-bug: #1272286"
1539,d48dd03e4278ef3d6ed50521f595254167726d3c,1390831447,,1.0,6,3,1,1,1,0.0,True,2.0,1479221.0,24.0,5.0,False,44.0,41159.0,75.0,3.0,55.0,2099.0,2114.0,55.0,1698.0,1713.0,55.0,2044.0,2059.0,0.0078125,0.285295759,0.287388393,242,646,1265452,nova,d48dd03e4278ef3d6ed50521f595254167726d3c,1,1,,"Bug #1265452 in OpenStack Compute (nova): ""cache lock for image not consistent""","According to this bug https://bugs.launchpad.net/nova/+bug/1256306
for one image in _base dir 03d8e206-6500-4d91-b47d-ee74897f9b4e
2 locks were created
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-03d8e206-6500-4d91-b47d-ee74897f9b4e
-rw-r--r-- 1 nova nova 0 Oct 4 20:40 nova-_var_lib_nova_instances__base_03d8e206-6500-4d91-b47d-ee74897f9b4e
generally locks are used to protect concurrent data access, so the lock can't work as they expected (mutual access)
in current code fetch_image from glance use lock nova-xxxxx while copy image from _base to target directory use nova_var_lib_xxx
should they use same lock?","Fix cache lock for image not consistent

Currently 2 external (file) locks with different name
are created to protect image concurrent access for libvirt driver.
This may lead to image corruption in rare case. This patch change
the lock file name from nova-_var_lib_nova_instances_base_xxxx
to nova-xxxx to make the lock name consistent

Change-Id: I3e60c3bcf456d2392409aa56ba05f4ad8fdc0444
Closes-Bug: #1265452"
1540,d4d06dfd18d8d085ccd390d6ebae7e3808fa8d09,1379692045,,1.0,8,2,1,1,1,0.0,True,2.0,225981.0,16.0,7.0,False,1.0,1478972.0,2.0,3.0,133.0,929.0,1057.0,133.0,881.0,1009.0,3.0,301.0,301.0,0.010899183,0.822888283,0.822888283,1581,1228212,1228212,neutron,d4d06dfd18d8d085ccd390d6ebae7e3808fa8d09,0,0,Bug in test,"Bug #1228212 in neutron: ""TestNecPortBindingValidatePortInfo unit tests failing on longinteger conversion""","These two tests in neutron trunk are failing on my 64-bit xubuntu 12.04 using python 2.7:
http://paste.openstack.org/show/47323/
When I run the NECPluginV2._validate_portinfo method in python, I get this:
mriedem@ubuntu:~$ python
Python 2.7.3 (default, Aug  1 2012, 05:16:07)
[GCC 4.6.3] on linux2
Type ""help"", ""copyright"", ""credits"" or ""license"" for more information.
>>> datapath_id = '0x1234567890abcdef'
>>> dpid = int(datapath_id, 16)
>>> dpid
1311768467294899695L
>>> dpid > 0xffffffffffffffffL
False
>>> hex(dpid)
'0x1234567890abcdefL'
>>>
So the int(datapath_id, 16) conversion is returning it as a longinteger, so the 'L' is appended at the end.
http://docs.python.org/2/library/functions.html#int
""If the argument is outside the integer range, the function returns a long object instead.""
http://docs.python.org/2/reference/lexical_analysis.html#integers
I'm able to fix this in the tests by simply using assertIn rather than assertEqual:
self.assertIn('0x1234567890abcdef', portinfo['datapath_id'])
I'm not sure if this is a bug in the tests though or something that should be handled in the _validate_portinfo method, i.e. if you get past all of the current validation OK, then simply return the input argument in the dict that goes back.","Handle long integer conversion in NEC portinfo validation tests

There are a couple of tests for NEC portinfo validation that pass an id
which gets converted to a base-16 integer and then that gets converted
to a hex string. The problem is the integer conversion can make it a
long integer and then the hex built-in appends 'L' to the string, and
the tests do not account for that. This patch handles that case.

Closes-Bug: #1228212

Change-Id: Id3aed686329dae9cc4777213a4d2bcf406652e15"
1541,d4e5373240b97727ce9c4ec6636119bd5849bace,1402498145,,1.0,9,9,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,968,1406,1328889,neutron,d4e5373240b97727ce9c4ec6636119bd5849bace,0,0,Bug in test,"Bug #1328889 in neutron: ""DHCP Unit test uses invalid IPv6 prefix""","Unit tests for neutron's linux dhcp section use an invalid prefix and host address.
https://github.com/openstack/neutron/blob/master/neutron/tests/unit/test_linux_dhcp.py#L129","Fix invalid IPv6 address used in FakeV6 variables

Uses RFC 5180 benchmark scope to change bad IPv6 addresses

Closes-Bug: #1328889
Change-Id: I224ec8db28d62484f5f4142cfd361162fb37e1b6"
1542,d4ec8e13d7b43a21443f5b67cc16db68bded9bba,1389077318,,1.0,29,1,2,2,1,0.353359335,True,4.0,2939575.0,79.0,27.0,False,5.0,3279420.0,5.0,5.0,18.0,830.0,842.0,15.0,712.0,722.0,5.0,326.0,329.0,0.00896861,0.488789238,0.493273543,263,667,1266650,neutron,d4ec8e13d7b43a21443f5b67cc16db68bded9bba,0,0, should support Quota extension ,"Bug #1266650 in neutron: ""Ryu plugin should support Quota extension""","Now, start of VM fails because Ryu plugin does not  support Quota extension.
This problem and other problems due to Quota request will be solved by adding Quota support.","add quota support for ryu plugin

This patch adds Quota extension support to Ryu plugin and db migration
script.

Closes-Bug: #1266650
Change-Id: I4328ebf7bf307390461807bd8bb3318ac2f64845"
1543,d4fdac7ae596888b8e83d9559019eafb728cd3fe,1396360555,,1.0,2,1,1,1,1,0.0,True,2.0,300566.0,22.0,5.0,False,22.0,728665.0,25.0,5.0,1152.0,1562.0,1564.0,980.0,1310.0,1312.0,691.0,904.0,904.0,0.65098777,0.851364064,0.851364064,714,1140,1300808,neutron,d4fdac7ae596888b8e83d9559019eafb728cd3fe,1,1,"“This is due to bug #1291535 and this fix: https://git.openstack.org/cgit/openstack/neutron/commit/?id=b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c""","Bug #1300808 in neutron: ""Invalid version number '['3.13.0']' with Ubuntu trusty""","Running latest devstack on Trusty Ubuntu, the Neutron agent fails to start with the following backtrace:
2014-04-01 13:49:54.227 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['uname', '-r'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.232 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['uname', '-r']
Exit code: 0
Stdout: '3.13.0-20-generic\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.233 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Running command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version'] from (pid=14900) create_process /opt/stack/neutron/neutron/agent/linux/utils.py:48
2014-04-01 13:49:54.348 DEBUG neutron.agent.linux.utils [req-9f04e437-c667-40c2-8be4-aef96f4810de None None]
Command: ['sudo', '/usr/local/bin/neutron-rootwrap', '/etc/neutron/rootwrap.conf', 'ovs-vsctl', '--version']
Exit code: 0
Stdout: 'ovs-vsctl (Open vSwitch) 2.0.1\nCompiled Feb 23 2014 14:42:32\n'
Stderr: '' from (pid=14900) execute /opt/stack/neutron/neutron/agent/linux/utils.py:74
2014-04-01 13:49:54.349 DEBUG neutron.agent.linux.ovs_lib [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] Checking OVS version for VXLAN support installed klm version is None, installed Linux version is ['3.13.0'], installed user version is 2.0  from (pid=14900) check_ovs_vxlan_version /opt/stack/neutron/neutron/agent/linux/ovs_lib.py:541
2014-04-01 13:49:54.350 CRITICAL neutron [req-9f04e437-c667-40c2-8be4-aef96f4810de None None] invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron Traceback (most recent call last):
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/local/bin/neutron-openvswitch-agent"", line 10, in <module>
2014-04-01 13:49:54.350 TRACE neutron     sys.exit(main())
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 1360, in main
2014-04-01 13:49:54.350 TRACE neutron     agent = OVSNeutronAgent(**agent_config)
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 214, in __init__
2014-04-01 13:49:54.350 TRACE neutron     self._check_ovs_version()
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/plugins/openvswitch/agent/ovs_neutron_agent.py"", line 232, in _check_ovs_version
2014-04-01 13:49:54.350 TRACE neutron     ovs_lib.check_ovs_vxlan_version(self.root_helper)
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 550, in check_ovs_vxlan_version
2014-04-01 13:49:54.350 TRACE neutron     'kernel', 'VXLAN')
2014-04-01 13:49:54.350 TRACE neutron   File ""/opt/stack/neutron/neutron/agent/linux/ovs_lib.py"", line 507, in _compare_installed_and_required_version
2014-04-01 13:49:54.350 TRACE neutron     installed_kernel_version) >= dist_version.StrictVersion(
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/lib/python2.7/distutils/version.py"", line 40, in __init__
2014-04-01 13:49:54.350 TRACE neutron     self.parse(vstring)
2014-04-01 13:49:54.350 TRACE neutron   File ""/usr/lib/python2.7/distutils/version.py"", line 107, in parse
2014-04-01 13:49:54.350 TRACE neutron     raise ValueError, ""invalid version number '%s'"" % vstring
2014-04-01 13:49:54.350 TRACE neutron ValueError: invalid version number '['3.13.0']'
2014-04-01 13:49:54.350 TRACE neutron
q-agt failed to start
This is due to bug #1291535 and this fix: https://git.openstack.org/cgit/openstack/neutron/commit/?id=b2f65d9d447ddf2caf3b9c754bd00a5148bdf12c","Fix function parsing the kernel version

Change-Id: Id56efb476144a18cd9eb7279bc0f602760a86173
Closes-Bug: #1300808"
1544,d52f84d96a2fadcf90fdac7a3e265c45d2ec234a,1395165311,1.0,1.0,81,80,2,2,1,0.973088879,True,8.0,1132148.0,150.0,12.0,False,9.0,480592.5,11.0,2.0,3.0,1203.0,1203.0,3.0,1039.0,1039.0,3.0,720.0,720.0,0.004012036,0.723169509,0.723169509,35,347,1245208,neutron,d52f84d96a2fadcf90fdac7a3e265c45d2ec234a,0,0,Tests… ‘Should not’,"Bug #1245208 in neutron: ""LBaaS: unit tests for radware plugin driver should not employ multithreading""","Radware plugin driver uses task queue to perform interaction with the backend device.
Several operations such as lbaas objects deletion are performed in async manner.
In the unit test code actual object deletion happens in separate thread; it leads to a need for tricks like putting test thread to sleep.
Such unit tests are not reliable and could lead to failures that are hard to catch or debug.
Unit test code should be refactored in such way that it uses single-threaded strategy to perform driver operations.","Cancelling thread start while unit tests running

This change modifies the Radware driver and its unit testing code
to not start operations completion thread while unit tests are running.

The driver initialization changed not to start the operations completion thread,
the thread is started only when operation completion item is inserted into the queue
for the first time.
The operation completion functionality was moved to a new function which
is called by the operations completion thread run() function.
The run() function still have the functionality of popping operation completion
items out of the queue and push failed items back.

Unit testing code mocks the operation completion items queue
by calling the operations completion hanler new function when item
is added.

Start() and join() functions of the thread were mocked to do nothing.

All sleep() entrances were removed from the unit testing code.
All unnecessary mock_reset() calls were removed.

Change-Id: I72380bf223be690831aba1fc29c3dca910245516
Closes-Bug: #1245208"
1545,d532864d832656adaca433792ed50c8ed71db40a,1392758102,,1.0,1,1,1,1,1,0.0,True,2.0,372944.0,37.0,5.0,False,14.0,5499.0,81.0,3.0,663.0,1149.0,1521.0,579.0,990.0,1314.0,200.0,636.0,679.0,0.242460796,0.768395657,0.82026538,428,843,1281789,neutron,d532864d832656adaca433792ed50c8ed71db40a,1,1,wrong id,"Bug #1281789 in neutron: ""NSX: update_port gets nsx_status with wrong uuid""","2014-02-18 13:11:51.714 ERROR NeutronPlugin [-] Unable to update port id: 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7.
2014-02-18 13:11:51.714 TRACE NeutronPlugin Traceback (most recent call last):
2014-02-18 13:11:51.714 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1316, in update_port
2014-02-18 13:11:51.714 TRACE NeutronPlugin     nvp_port_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin   File ""/opt/stack/neutron/neutron/plugins/nicira/nvplib.py"", line 1019, in get_port_status
2014-02-18 13:11:51.714 TRACE NeutronPlugin     port_id=port_id, net_id=lswitch_id)
2014-02-18 13:11:51.714 TRACE NeutronPlugin PortNotFoundOnNetwork: Port 0bbbdf65-c654-4c30-b1b4-5d17b9b10ef7 could not be found on network 0d192d2e-e64f-410e-8d3b-cb2eef5a2655
2014-02-18 13:11:51.714 TRACE NeutronPlugin","NSX: get_port_status passed wrong id for network

The call to get_port_status in update_port was passing in the neutron
network_id instead of the nsx_network_id. These used to be the same
but now are different. This patch updates the code so that it now
passes in the correct uuid.

Change-Id: I7b0bd617f68291fa457a49e52d595bb8de1d4835
Closes-bug: #1281789"
1546,d534811cbcfe591688b5e9c6becbed2224be3c31,1380087681,,1.0,11,10,4,4,1,0.776742251,True,2.0,545783.0,32.0,14.0,False,74.0,711354.0,190.0,6.0,372.0,672.0,830.0,367.0,651.0,806.0,359.0,571.0,718.0,0.348162476,0.553191489,0.695357834,1598,1230069,1230069,cinder,d534811cbcfe591688b5e9c6becbed2224be3c31,1,1,“This patch fixes the calls to be according to the function definition.”,"Bug #1230069 in Cinder: ""Passing mandatory args as optional to create_volume""","Filter scheduler and create_volume flow both pass request_specs and filter_properties as optional arguments to create_volume in volume_rpcapi.  This does not fit how the call is defined.
Also, the create_volume call in the migration code in manager.py should not allow rescheduling.","Fix mandatory and optional args for create_volume

Calls to create_volume (both volume_rpcapi and scheduler_rpcapi) pass
mandatory args as optional and reverse. This patch fixes the calls to be
according to the function definition.

Also, fix the call to create_volume in the migration code where
allow_reschedule defaults to True, but we don't pass request_spec so it
prints a warning and disables it.

Change-Id: I9509f88866e0ae182f90ebdfca1fcf415ca60d19
Closes-Bug: #1230069"
1547,d54041f0b4763c00f661088f113e26e38c747730,1387367075,0.0,1.0,63,2,11,11,1,0.972013272,True,6.0,2571023.0,35.0,15.0,False,267.0,212018.0,1594.0,3.0,368.0,2318.0,2555.0,355.0,1964.0,2189.0,361.0,2201.0,2433.0,0.052524666,0.319500871,0.353163088,207,610,1262206,nova,d54041f0b4763c00f661088f113e26e38c747730,1,1,,"Bug #1262206 in OpenStack Compute (nova): ""xenapi: server gets deleted under certain live-migrate error conditions""","Any errors after the XenServer migrate command completes currently can cause the users VM to be deleted.
While there should be some cleanup performed, deleting the VM does not make sense for the XenAPI driver.","xenapi: stop server destroy on live_migrate errors

Currently where there are errors after live-migration has completed, the
server is destroyed, which for XenAPI driver, means the server no longer
exists, because XenServer has already deleted the source VM.

Closes-Bug: #1262206
Change-Id: I7f827f5d972da26f5d1edae7a22fb45a1f5dd6ca"
1548,d54c49a22e3ce147003cc9a926b7b72e8dc9c1a8,1385802568,,1.0,9,9,1,1,1,0.0,True,2.0,2235473.0,15.0,8.0,False,39.0,87173.0,83.0,3.0,34.0,721.0,731.0,34.0,661.0,671.0,24.0,672.0,672.0,0.020729685,0.558043118,0.558043118,109,508,1256483,cinder,d54c49a22e3ce147003cc9a926b7b72e8dc9c1a8,0,0,'Use model_query() instead of session.query’ for no reason,"Bug #1256483 in Cinder: ""Use model_query() instead of session.query in db.*****_destroy""","Use model_query() instead of session.query in  db.volume_destroy, db.volume_type_destroy, db.transfer_destroy and db.snapshot_destroy.","Use model_query() in db.*****_destroy

Use model_query() instead of session.query in db.volume_destroy,
db.volume_type_destroy, db.transfer_destroy and db.snapshot_destroy as
other methods use.
model_query function is the query helper which is used in most part of
cinder/db/sqlalchemy/api.py.

Change-Id: I97be252df9d40bb97231e3341d5cedec26af2d93
Closes-Bug: #1256483"
1549,d583d55c6a4e6c01c49411b7e5b519031bce0699,1389337273,0.0,1.0,153,69,4,4,1,0.524939787,True,16.0,5752692.0,220.0,48.0,False,21.0,2411259.5,31.0,6.0,0.0,1018.0,1018.0,0.0,908.0,908.0,0.0,512.0,512.0,0.001477105,0.757754801,0.757754801,279,684,1267682,neutron,d583d55c6a4e6c01c49411b7e5b519031bce0699,1,0,On some linux distribution,"Bug #1267682 in neutron: ""check_vxlan_support works incorrect on RHEL6.5""",RHEL6.5 has back ported vxlan support feature into its kernel 2.6 but check_vxlan_support still give NOT result.,"Check vxlan enablement via modinfo

On some linux distribution, for RHEL 6.5 as an example, vxlan is
enabled but the kernel version is still 2.6. And some linux kernel
version is higher than 3.8 or even 3.11, its vxlan mod may be
disabled. Under both situation, vxlan enablement checking via linux
kernel version may not be correct.

In this patch, we check vxlan enablement via modinfo: if vxlan module
exists and functional test pass, vxlan is enabled.

Closes-Bug: #1267682

Change-Id: Id52c04b4739d2d11fe52d4b1631cb4f39e6b577f"
1550,d59d5a1c72c2324b857225680f5578d381a2ec73,1392142597,1.0,1.0,9,9,1,1,1,0.0,True,4.0,3717677.0,59.0,13.0,False,5.0,2200097.0,13.0,4.0,646.0,1813.0,1813.0,572.0,1543.0,1543.0,183.0,659.0,659.0,0.235595391,0.845070423,0.845070423,389,802,1278991,neutron,d59d5a1c72c2324b857225680f5578d381a2ec73,0,0,Fix POSSIBLE deadlock,"Bug #1278991 in neutron: ""NSX: Deadlock can occur in sync code""",The sync code that syncs the operational status to the db calls out to nsx within a db transaction which can cause deadlock if another request comes in and eventlet context switches it.,"NSX: Fix possible deadlock in sync code

This patch removes the call to nsx from within the db transcation
which can cause deadlock if another request comes in during this time.

Change-Id: If16b53a77997bccfdff2554dead26f42e23eaec9
Closes-bug: #1278991"
1551,d5c0a37999f9e3a611a322baacabebc06b13283b,1394212575,1.0,1.0,58,6,7,5,1,0.84987111,True,13.0,6408108.0,279.0,43.0,False,55.0,37049.0,110.0,3.0,5.0,203.0,206.0,4.0,202.0,204.0,2.0,192.0,192.0,0.003243243,0.208648649,0.208648649,546,967,1288923,neutron,d5c0a37999f9e3a611a322baacabebc06b13283b,1,1,There is a bug. (Maybe make a feature(,"Bug #1288923 in neutron: ""Failover of a network from one dhcp agent to another breaks DNS""","Failing over a network from one dhcp agent to another results in a new IP address for the dhcp port.  This breaks dns for all vms on that network.    This can be reproduced by simply doing a ""neutron dhcp-agent-network-remove"" and then a ""neutron dhcp-agent-network-add"" and observing that the dhcp port ip address will change.","Provide way to reserve dhcp port during failovers

This change provides a way to save the dhcp port when failing
over a network from one dhcp agent to another.  When a
dhcp-agent-network-remove is issued, the dhcp port device_id is
marked as reserved which causes it to not be deleted. When a
subsequent dhcp-agent-network-add is issued, the reserved port
is used and the device_id is corrected.  This is desirable
in order to maintain the dhcp port ip address so that dns doesn't
get impacted. Unit test added.

Change-Id: I531d7ffab074b01adfe186d2c3df43ca978359cd
Closes-Bug: #1288923"
1552,d5ff12a700495ba6b685cae08a0a8f017e3a976a,1386110763,,1.0,11,1,3,3,1,0.980834038,True,5.0,1133689.0,46.0,21.0,False,113.0,47275.33333,326.0,4.0,580.0,2881.0,3198.0,540.0,2432.0,2734.0,174.0,1974.0,2044.0,0.025830258,0.291512915,0.301845018,124,523,1257146,nova,d5ff12a700495ba6b685cae08a0a8f017e3a976a,0,0, Need to handle…,"Bug #1257146 in OpenStack Compute (nova): ""Need to handle InstanceUserDataMalformed in create server v2 API""","This patch points out the need for this in the v2 API and the need for a test in the v3 API to cover InstanceUserDataMalformed for the server create call.
https://review.openstack.org/#/c/54202/6/nova/api/openstack/compute/servers.py
It was out of scope to add/cover that in that patch, so reporting it as a bug to keep track of it.","Handle InstanceUserDataMalformed in create server v2 api

The following patch adds missing support for handling
InstanceUserDataMalformed. In addition, it also adds test cases
in both the v2 and v3 api.

Change-Id: I1b474318248f64eed994f67c6d91db38d5579a63
Closes-bug: #1257146"
1553,d61eb32eeeaf213912383fb14bf86f2511518943,1378328337,1.0,1.0,13,11,3,2,1,0.995246747,True,4.0,434184.0,30.0,17.0,False,31.0,475987.0,83.0,3.0,3.0,2077.0,2078.0,3.0,1810.0,1811.0,2.0,1935.0,1935.0,0.000505902,0.326475548,0.326475548,1512,1220904,1220904,nova,d61eb32eeeaf213912383fb14bf86f2511518943,1,0,"“As some environments/architectures may have different PXE binary,this needs to be an option.” Change in the environment","Bug #1220904 in OpenStack Compute (nova): ""PXE dhcp_option needs to provide the bootfile_name ""","in the nova/virt/baremetal/driver.py the call to pxe.py pxe.get_pxe_config_file_path(instance) need to return the bootfile name not the path.
Line 84 reads only my_ip from nova conf. But, bootfile_path in line 512 (bootfile_path = self.driver.get_pxe_config_file_path(instance)) is getting set from pxe.py which is getting the value “/tftpboot/<instance-id>/config”. This value should actually be  “pxelinux.0” (or absolute location of pxelinux.0) .  baremetal does not find the bootfile in “/tftpboot/<instance-id>/config”, it finds it in /tftpboot/pxelinux.0 (where tftproot=/tftpboot set in nova.conf).
In pxe.py,
def get_pxe_config_file_path(instance):
    """"""Generate the path for an instances PXE config file.""""""
    return os.path.join(CONF.baremetal.tftp_root, instance['uuid'], 'config')","Fix the bootfile_name method call in baremetal

nova baremetal was supplying a pxelinux config file disk path, but
PXE clients need the binary to bootstrap, which is neither the config
file, nor a filesystem path - it's a path within TFTP server.

As some environments/architectures may have different PXE binary,
this needs to be an option.

DocImpact

Change-Id: I393552ae2d45e1ff5bc306fc8717fa4265ac827f
Closes-Bug: #1220904"
1554,d6214f122feb99be74b6654bf86d1677926fa628,1377661235,,1.0,92,15,5,5,1,0.904493205,True,5.0,3486210.0,46.0,15.0,False,44.0,561208.0,122.0,3.0,56.0,597.0,618.0,56.0,582.0,603.0,55.0,587.0,607.0,0.009567743,0.100461302,0.103878353,1490,1217679,1217679,nova,d6214f122feb99be74b6654bf86d1677926fa628,1,1,,"Bug #1217679 in OpenStack Compute (nova): ""there is an  Unexpected API Error when call remove_tenant_access action in nova v3 flavor_access api as an unadmin user""","When port flavor_access tempest tests into v3, I found if the remove_tenant_access and add_tenant_access called as non admin user an  Unexpected API Error arose.
I look into code, find out the issue is that flavors.add_flavor_access and flavors.remove_flavor_access require admin privilege in DB level but the policy doesn't require it. and the exception is not catched. I think there is the same issue in nova v2 api.
I also think we should remove the privilege check in DB level, but it need more tests, and can be remove in another patch or blue-print.
the tempest log is:
2013-08-28 11:56:08.154 2220 INFO tempest.common.rest_client [-] Request: POST http://192.168.1.101:8774/v3/flavors/155214353/action
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Headers: {'Content-Type': 'application/json', 'Accept': 'application/json', 'X-Auth-Token': '<Token omitted>'} _log_request /opt/stack/tempest/tempest/common/rest_client.py:295
2013-08-28 11:56:08.154 2220 DEBUG tempest.common.rest_client [-] Request Body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} _log_request /opt/stack/tempest/tempest/common/rest_client.py:299
2013-08-28 11:56:08.169 2220 INFO tempest.common.rest_client [-] Response Status: 500
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Headers: {'date': 'Wed, 28 Aug 2013 03:56:08 GMT', 'content-length': '202', 'content-type': 'application/json; charset=UTF-8', 'x-compute-request-id': 'req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab'} _log_response /opt/stack/tempest/tempest/common/rest_client.py:310
2013-08-28 11:56:08.169 2220 DEBUG tempest.common.rest_client [-] Response Body: {""computeFault"": {""message"": ""Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.\n<class 'nova.exception.AdminRequired'>"", ""code"": 500}} _log_response /opt/stack/tempest/tempest/common/rest_client.py:314
the nova log is:
2013-08-28 11:56:08.165 DEBUG routes.middleware [-] Matched POST /flavors/155214353/action from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Route path: '/flavors/:(id)/action', defaults: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-08-28 11:56:08.166 DEBUG routes.middleware [-] Match dict: {'action': u'action', 'controller': <nova.api.openstack.wsgi.Resource object at 0x4e72050>, 'id': u'155214353'} from (pid=12748) __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Action: 'action', body: {""add_tenant_access"": {""tenant_id"": ""9bfa07133a42464a8701e3cf367bbb4d""}} from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:927
2013-08-28 11:56:08.166 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Calling method <bound method FlavorActionController._add_tenant_access of <nova.api.openstack.compute.plugins.v3.flavor_access.FlavorActionController object at 0x50f5f50>> from (pid=12748) _process_stack /opt/stack/nova/nova/api/openstack/wsgi.py:928
2013-08-28 11:56:08.167 ERROR nova.api.openstack.extensions [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Unexpected exception in API method
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 469, in wrapped
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/flavor_access.py"", line 176, in _add_tenant_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     flavors.add_flavor_access(id, tenant, context)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/flavors.py"", line 245, in add_flavor_access
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return db.flavor_access_add(ctxt, flavorid, projectid)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/api.py"", line 1424, in flavor_access_add
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     return IMPL.flavor_access_add(context, flavor_id, project_id)
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/db/sqlalchemy/api.py"", line 106, in wrapper
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     nova.context.require_admin_context(args[0])
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/context.py"", line 195, in require_admin_context
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions     raise exception.AdminRequired()
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions AdminRequired: User does not have admin privileges
2013-08-28 11:56:08.167 TRACE nova.api.openstack.extensions
2013-08-28 11:56:08.167 INFO nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] HTTP exception thrown: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'>
2013-08-28 11:56:08.168 DEBUG nova.api.openstack.wsgi [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] Returning 500 to user: Unexpected API Error. Please report this at http://bugs.launchpad.net/nova/ and attach the Nova API log if possible.
<class 'nova.exception.AdminRequired'> from (pid=12748) __call__ /opt/stack/nova/nova/api/openstack/wsgi.py:1188
2013-08-28 11:56:08.168 INFO nova.osapi_compute.wsgi.server [req-17649b30-e3a7-489f-98ab-8cf0ccb0e0ab demo demo] 192.168.1.101 ""POST /v3/flavors/155214353/action HTTP/1.1"" status: 500 len: 409 time: 0.0126910","fix the an Unexpected API Error issue in flavor API

The flavors.add_flavor_access and flavors.remove_flavor_access
require admin privilege in DB level but the policy doesn't
require it and the exception isn't catched in the api. So the
bug arise. The privilege check in DB level is not necessary,
but it need more tests and can be remove in other patch or
blue-print.

This use extension_authorizer instead of soft_extension_authorizer
in the action of flavor_access, in order to raise exception.

Closes-Bug: #1217679
DocImpact

Change-Id: I0b1231f47fffaf99f330de02956bfe8d7cd4b920"
1555,d6286b3fd2f9535c023fd50e1825111c9a85b17f,1404965544,,1.0,59,44,16,7,1,0.879611977,False,,,,,True,,,,,,,,,,,,,,,,,1855,1340002,1340002,Glance,d6286b3fd2f9535c023fd50e1825111c9a85b17f,1,0,"""Some v2 exceptions raise unicodeError""","Bug #1340002 in Glance: ""Some exceptions raise unicodeError""","Thie bug is a generic case for bug 1339775 [0], there are a lot same cases in codebase which are outside of v2 stuff, but the result is smimilar, UnicodeError exception will be raised.
[0] https://bugs.launchpad.net/glance/+bug/1339775","Some exceptions raise UnicodeError

Use utils.exception_to_str to consistently cast the
exception to avoid the unicodeError stack trace.

Closes-Bug: #1340002
Related-Id: I54fce91c73adca846ec99ed50a57b95d16428ec1

Change-Id: I77478a8eeeefd85bf98138156429df53d0a4c079
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
1556,d62fb490b10d9372bd52189d4c688a7c1b495d8b,1395730059,1.0,1.0,28,8,8,8,1,0.913834463,True,6.0,4017913.0,98.0,21.0,False,143.0,200094.375,635.0,3.0,119.0,1080.0,1097.0,119.0,1077.0,1094.0,119.0,1065.0,1082.0,0.015627035,0.138820159,0.141033989,680,1106,1297052,nova,d62fb490b10d9372bd52189d4c688a7c1b495d8b,1,1,wrong behaviour,"Bug #1297052 in OpenStack Compute (nova): ""resize fail didn't show a correct info when --poll specified""","[root@controller ~]# nova resize --poll a9dd1fd6-27fb-4128-92e6-93bcab085a98 100
Instance resizing... 100% complete
Finished
but the instance is not finished yet and error logs in nova log","Make resize raise exception when no valid host found

When resize operation starts, it will ask conductor to find a
valid host and try to resize to that host. When there is no valid
host can be found ,current code return directly without error
notification, which will make upper layer code (e.g novaclient)
thinks the execution is succesfully performed and report 'Finished'.
But actually it isn't due to no valid host was found.

Change-Id: I3d66dbfc0a7346762b920e802aa3be92c43d93b7
Closes-Bug: #1297052"
1557,d632b66dc8b701ca777af4335b6505b4c4cd7828,1378213073,1.0,1.0,133,11,6,6,1,0.735581266,True,3.0,169272.0,20.0,7.0,False,12.0,651349.0,22.0,2.0,301.0,930.0,1077.0,290.0,849.0,993.0,49.0,235.0,246.0,0.184501845,0.870848708,0.911439114,1502,1220011,1220011,neutron,d632b66dc8b701ca777af4335b6505b4c4cd7828,1,1, ,"Bug #1220011 in neutron: ""Cannot clear binding:profile""","binding:profile attribute is a dict, but there is no way to clear it.
None is used to indicate to clear the corresponding attribute in general.
The validator for port binding should accept None.
It can be fixed by changing the type:dict to type:dict_or_none in the extension.
It is required by NEC plugin portbinding support ( https://blueprints.launchpad.net/neutron/+spec/nec-port-binding ) but it requires a change in the common code, so this bug will be fixed in a separate patch.","Allow None for binding:profile attribute

We need to pass None in binding:profile to allow an administrator
to clear binding:profile attribute.

Closes-Bug: #1220011

Adds dedicated unit tests to the plugins which uses binding:profile
attribute (Mellanox and NEC plugins at now).

This commit also adds common unit tests for binding:profile to
the common PortBindingTestCase class.
- create_port with binding:profile whose value is None or {}
- update_port with binding:profile whose value is None or {}
- Reject binding:profile from non-admin user

Note that _make_port() in BigSwitch plugin test is updated
to allow passing arg_list() from the base test class.

Fix a bug in NEC plugin that 500 is returned when putting
binding:profile None to a port whose binding:profile is
already None (Closes-Bug: #1220720)

Change-Id: I146afe961cd445a023adc7233588d8034fdb8437"
1558,d64478fd35a7024f46fa8dbf54103a7a0e30be5a,1389112686,,1.0,2,1,1,1,1,0.0,True,2.0,6128819.0,29.0,14.0,False,19.0,324799.0,25.0,10.0,37.0,1623.0,1646.0,18.0,1428.0,1437.0,1.0,1239.0,1239.0,0.001529052,0.948012232,0.948012232,265,669,1266804,cinder,d64478fd35a7024f46fa8dbf54103a7a0e30be5a,0,0,feature for postgre,"Bug #1266804 in Cinder: ""User/pass logged in PostgreSQL connection string""",PostgreSQL connection strings may contain passwords too which need to be filtered out (like it's done for MySQL).,"Hiding postgresql password in connection string

Closes-Bug: #1266804
Change-Id: Iafc820d6201acf5b563b62bd9f20a715901f1ab4"
1559,d65ca49990f1241a5a8afdc3ad0e1a99c878275a,1404133764,,1.0,83,5,3,3,2,0.672955106,False,,,,,True,,,,,,,,,,,,,,,,,1334,1802,1370999,nova,d65ca49990f1241a5a8afdc3ad0e1a99c878275a,1,0,"Platform. “The windows nova-agent now can trigger a gust reboot during resetnetwork, so the hostname is correctly updated.
“","Bug #1370999 in OpenStack Compute (nova): ""xenapi: windows agent unreliable due to reboots""","The windows nova-agent now can trigger a gust reboot during resetnetwork, so the hostname is correctly updated.
Also there was always a reboot during the first stages of polling for the agent version that can cause the need to wait for a call to timeout, rather than detecting a reboot.
Either way, we need to take more care to detect reboots while talking to the agent.","xenapi: deal with reboots while talking to agent

The latest version of the agent has started changing the hostname
through the official windows APIs, when resetnetwork is called. This
means a reboot might happen after resetnetwork.

A reboot happens when polling for the version. Until now we wait for the
call timeout, before fetching the new dom id. This change ensures that
if we spot a reboot, the plugin exits early rather than keeping polling
the wrong dom id.

Turns out its best to wait for the dom_id to change, before trying
to poll the agent again. Once the dom_id in xenapi has been updated,
the xenstore keys are always in place.

Trying too early leads to lots of reboot detections because we are
retrying with the old dom_id. XenServer continues to return the old
dom_id for a little while after the reboot.

Closes-Bug: #1370999

Change-Id: Id0bf5b64f2b271d162db5bbce50167ab1f665c87"
1560,d67a94b60261a856ab52e57a2490afaae6c5d91d,1407177284,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1144,1599,1352509,glance,d67a94b60261a856ab52e57a2490afaae6c5d91d,1,1,"""Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and missed a variable in the RBD store.”","Bug #1352509 in Glance: ""RBD store: need to use READ_CHUNKSIZE in the ImageIterator""","See https://bugs.launchpad.net/glance/+bug/1336168
Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and missed a variable in the RBD store.","Fix RBD store to use READ_CHUNKSIZE

Change 4e0c563b8f3a5ced8f65fcca83d341a97729a5d4 was incomplete and
missed a variable in the rbd store.

This patch fixes the issue by using store.READ_CHUNKSIZE instead of
store.chunk_size

Change-Id: I2dc5b1514d3e86ec129c1fc88de096d98a64f806
Closes-Bug: #1352509"
1561,d69e013519201af9af7683b1b6dfdf1efa226c7c,1383799527,,1.0,123,7,6,5,2,0.823133553,True,17.0,3700561.0,79.0,22.0,False,45.0,152164.1667,212.0,2.0,0.0,457.0,457.0,0.0,423.0,423.0,0.0,452.0,452.0,0.001312336,0.594488189,0.594488189,1738,1248818,1248818,Swift,d69e013519201af9af7683b1b6dfdf1efa226c7c,1,1,"""the Last-Modified header in Response didn't have a suitable
value""","Bug #1248818 in OpenStack Object Storage (swift): ""if-(un)modified-since handler not work with the value from last-modified header ""","~/swift$ git log | head -n1
commit 429cb8b7453662e325d6120457638827527483cd
$ python -c 'import swift; print swift.__version__'
1.10.0.47.g429cb8b
Uploading a new object returns last-modified header
 $ curl -i -XPUT --data-binary '1' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8f69afe6ec99d' http://127.0.0.
1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 201 Created
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Content-Length: 0
Etag: c4ca4238a0b923820dcc509a6f75849b
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:35:45 GMT
Then try to GET the object with if-modified-since header value from last-modified returns 200, not 304
$ curl -i -H'if-modified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381a8
f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 200 OK
Content-Length: 1
Accept-Ranges: bytes
Last-Modified: Thu, 07 Nov 2013 03:35:45 GMT
Etag: c4ca4238a0b923820dcc509a6f75849b
X-Timestamp: 1383795345.50550
Content-Type: application/x-www-form-urlencoded
Date: Thu, 07 Nov 2013 03:36:15 GMT
This leads the unnecessary download for most cases including the GET request from most of web browsers
Moreover, a GET request if-unmodified-since value returns 412, not 200
$ curl -i -H'if-unmodified-since: Thu, 07 Nov 2013 03:35:45 GMT' -H'x-auth-token:AUTH_tk035f3c147b5842a381
a8f69afe6ec99d' http://127.0.0.1:8080/v1/AUTH_test/cont1/obj1
HTTP/1.1 412 Precondition Failed
Content-Length: 92
Content-Type: text/html; charset=UTF-8
Date: Thu, 07 Nov 2013 03:36:30 GMT
<html><h1>Precondition Failed</h1><p>A precondition for this request was not met.</p></html>
For the ranged-GET request, the if-unmodified-since headers check the rest of contents are not modified and valid to continue the downloading, but fails always.
The reason the if-(un)modified-since header not working is last-modified header get the time value from x-timestamp value (floating type) and truncate it to integer value. So the last-modified header value always earlier than x-timestamp's one.","change the last-modified header value with valid one

the Last-Modified header in Response didn't have a suitable
value - an integer part of object's timestamp.
This leads that the the if-[un]modified-since header with the
value from last-modified is always earlier than timestamp
and results the content is always newer than value of these
conditional headers.
Patched code returns math.ceil() of object's timestamp
in Last-Modified header so the later conditional header works
correctly

Closes-Bug: #1248818
Change-Id: I1ece7d008551bf989da74d23f0ed6307c45c5436"
1562,d6e6c35ff653565aa65e049ed1de371235b261de,1407938373,,1.0,28,17,4,3,1,0.453044159,False,,,,,True,,,,,,,,,,,,,,,,,1443,1199308,1199308,nova,d6e6c35ff653565aa65e049ed1de371235b261de,1,1,Ambiguity33 “The exception class InvalidInstanceIDMalformed in exception.py could be changed to something like InvalidEC2IDMalformed.”,"Bug #1199308 in OpenStack Compute (nova): ""Ambiguous exception class for validate_ec2_id""","The validate_ec2_id() method is used to validate both the Instance ID as well as Volume ID for valid EC2 ID format.
However the exception class raised in both cases, if the respective ID were invalid is ""InvalidInstanceIDMalformed"".
This is ambiguous and needs to be fixed such that a clearer exception is seen in the stack trace.
The exception class InvalidInstanceIDMalformed in exception.py could be changed to something like InvalidEC2IDMalformed.","Split EC2 ID validator to validator per resource type.

Now there is only one method to validate EC2 id: validate_ec2_id().
It's used to validate both the Instance ID as well as Volume ID for
valid EC2 ID format. However exception class raised in both cases
is ""InvalidInstanceIDMalformed"" and EC2 error code is
InvalidInstanceID.Malformed.

This patch addresses this problem by adding two new methods:
* validate_instance_id()
* validate_volume_id()
These methods use validate_ec2_id() and raise appropriate
exception as well as appropriate EC2 error code based on
AWS EC2 Error Code documentation:
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/api-error-codes.html

Change-Id: I55591b8b643bb316e5001e645d71c13094106e96
Closes-Bug: #1199308"
1563,d6f014d0922e03864fd72efbcde04322711c2510,1398560128,1.0,1.0,35,5,9,9,1,0.870475092,True,2.0,883099.0,33.0,8.0,False,38.0,590733.7778,75.0,2.0,10.0,136.0,146.0,10.0,136.0,146.0,8.0,129.0,137.0,0.007588533,0.109612142,0.116357504,821,1249,1310857,neutron,d6f014d0922e03864fd72efbcde04322711c2510,1,0,“devstack currently has a bug re”,"Bug #1310857 in neutron: ""agent attempts to create firewall even if fwaas disabled""","Investigating some tempest failures in this area led me to this issue.  devstack currently has a bug re: configuration of fwaas.  This leads the service to be enabled, but the agent does get passed the relevant config files /w fwaas config.  On firewall creation, the following traceback appears while the firewall stays in PENDING_CREATE:
neutron.services.firewall.agents.l3reference.firewall_l3_agent [req-2b7a801e-7358-418e-b4e7-95b7b27aefc2 None] FWaaS RPC failure in create_firewall for fw: f24bd240-04d5-49f1-971c-8ae95e666ef0
neutron.services.firewall.agents.l3reference.firewall_l3_agent Traceback (most recent call last):
neutron.services.firewall.agents.l3reference.firewall_l3_agent   File ""/opt/stack/neutron/neutron/services/firewall/agents/l3reference/firewall_l3_agent.py"", line 133, in _invoke_driver_for_plugin_api
neutron.services.firewall.agents.l3reference.firewall_l3_agent     self.fwaas_driver.__getattribute__(func_name)(
neutron.services.firewall.agents.l3reference.firewall_l3_agent AttributeError: 'VPNAgent' object has no attribute 'fwaas_driver'
neutron.services.firewall.agents.l3reference.firewall_l3_agent","Exit Firewall Agent if config is invalid

When fwaas config file is not provided to the agent,
but the service is enabled in neutron.conf file the
agent should exit with an error message and should
not proceed further. This patch adds the necessary fix.

Change-Id: Iaced777e3a34e9405050252b17a203689e1c1fc0
Closes-Bug: #1310857"
1564,d6f50e8476feb3c76db485fa033d0b1c22ee024d,1377814560,,1.0,1,1,1,1,1,0.0,True,1.0,40696.0,10.0,6.0,False,7.0,252836.0,11.0,4.0,15.0,808.0,808.0,15.0,756.0,756.0,7.0,222.0,222.0,0.030888031,0.861003861,0.861003861,1507,1220521,1220521,neutron,d6f50e8476feb3c76db485fa033d0b1c22ee024d,1,1, ,"Bug #1220521 in neutron: ""incorrect format type in BigSwitch plugin log line""","The BigSwitch plugin incorrectly tries to log the response code it gets from an HTTP request as an integer when it should be a string, resulting in these errors in the output:
Traceback (most recent call last):
  File ""/usr/lib/python2.7/logging/__init__.py"", line 846, in emit
    msg = self.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 552, in format
    return logging.StreamHandler.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 723, in format
    return fmt.format(record)
  File ""/opt/stack/neutron/neutron/openstack/common/log.py"", line 516, in format
    return logging.Formatter.format(self, record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 464, in format
    record.message = record.getMessage()
  File ""/usr/lib/python2.7/logging/__init__.py"", line 328, in getMessage
    msg = msg % self.args
TypeError: %d format: a number is required, not str
Logged from file plugin.py, line 339","Fixes formatting exception from logging in BigSwitch plugin

Corrects the data type expected for the HTTP code that comes
back from the back-end controller in a debug log call.

Closes-Bug: #1220521
Change-Id: Ic71b24c724222ade5e695addf291c6488d665da9"
1565,d72914f739b1467ad849dd47fddd321965fed928,1385074693,,1.0,23,7,2,2,1,0.566509507,True,12.0,17340005.0,69.0,51.0,False,11.0,193881.0,22.0,6.0,4.0,1202.0,1202.0,4.0,1057.0,1057.0,4.0,1021.0,1021.0,0.004258944,0.870528109,0.870528109,1732,1245909,1245909,cinder,d72914f739b1467ad849dd47fddd321965fed928,1,1, ,"Bug #1245909 in Cinder: ""when creating the thin lvm pool, its size is not calculated correctly""","when the thin lvm pool needs to be created, it is incorrectly sized
a 64G VG results in a 64M POOL
the offending code seems to be here: https://github.com/openstack/cinder/blob/d951222c1440b31f5e994d43b4277344e0d8f63f/cinder/brick/local_dev/lvm.py#L333","LVM: Create thin pools of adequate size

Thin pools in LVM are quite different from volume groups or logical
volumes and their differences must be taken into account when providing
thin LVM support in Cinder.

When you create a thin pool, LVM actually creates 4 block devices.  You
can see this after thin pool creation with the following command:

    $ dmsetup ls

    volumes--1-volumes--1--pool       (253:4)
    volumes--1-volumes--1--pool-tpool (253:3)
    volumes--1-volumes--1--pool_tdata (253:2)
    volumes--1-volumes--1--pool_tmeta (253:1)

In the above command, a thin pool named 'volumes-1-pool' was created in
the 'volumes-1' volume group.  Despite this, the 'lvs' command will only
show one logical volume for the thin pool, which can be misleading if
you aren't aware of how thin pools are implemented.

When you create a thin pool, you specify on the command line a size for
the pool.  LVM will interpret this size as the amount of space requested
to store data blocks only.  In order to allow volume sharing and
snapshots, some amount of metadata must be reserved in addition to the
data request.  This amount is calculated by LVM internally and varies
depending on volume size and chunksize.  This is why one cannot simply
allocate 100% of a volume group to a thin pool - there must be some
remaining space for metadata or you will not be able to create volumes
and snapshots that are pool-backed.

This patch allocates 95% of a volume group's free space to the thin
pool.  By doing this, we allow LVM to successfully allocate a region for
metadata.  Additionally, any free space remaining will by dynamically
used by either data or metadata if capacity should become scarce.

The 95/5 split seems like a sane default.  This split can easily (and
probably should) be made user-configurable in the future if the user
expects an abnormal amount of volume sharing.

Change-Id: Id461445780c1574db316ede0c0194736e71640d0
Closes-Bug: #1245909"
1566,d72d925fc1c3b8620b81531ec74524aceb89d4e8,1393342445,0.0,1.0,34,2,3,2,1,0.723220893,True,6.0,4052724.0,113.0,35.0,False,2.0,953993.0,5.0,4.0,4.0,2660.0,2663.0,4.0,2226.0,2229.0,1.0,2589.0,2589.0,0.000268565,0.347791057,0.347791057,409,823,1280067,nova,d72d925fc1c3b8620b81531ec74524aceb89d4e8,1,1,Not catched exception,"Bug #1280067 in OpenStack Compute (nova): ""Missing catch InstanceNotFound that raise from Instance.save() in v3 API""","As the comment in https://review.openstack.org/#/c/70901/6/nova/api/openstack/compute/plugins/v3/pause_server.py
After apply instance look helper, some compute api will invoke instance.save(), it will raise InstanceNotFound also. We should catch them.","Missing catch InstanceNotFound in v3 API

After apply instance look helper, some compute api
will invoke instance.save(), it will raise
InstanceNotFound also.
This patch adds them.

Closes-Bug: #1280067
Change-Id: I73cb35c90f1e46a447ae1807a997713105e08bde"
1567,d7318ff7123ea851e9d6953ef2c66f793ae46ca1,1403712039,,1.0,4,4,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,878,1313,1317871,neutron,d7318ff7123ea851e9d6953ef2c66f793ae46ca1,1,1,,"Bug #1317871 in neutron: ""LBAAS :Deletion of pool is not deleting the lbaas pool directory/folder structure from the network node but files are getting deleted""","Steps to Reproduce:
Setup onto ICEHouse GA:
Build :2014.1-0ubuntu1~cloud0
Steps to Reproduce:
1. Create a lbaas site. (Pool,member,vip,health monitor)
2. Check the configuration onto the network node under
/var/lib/neutron/lbaas/$Pool_id/
conf  pid  sock
3. Delete all the lbaas and neutron resource created onto the controller.
4. Check the configuration onto the network node.
Actual Results: Directory structure for the pool id remain as its but the files got delete
Expected Results: all the configuration and files/folder should get deleted after deletion of lbaas resource.
Note: Working in Havana GA release.","Fix re-creation of the pool directory

Whenever vip is deleted the complete pool directory
is deleted, but since pool is still alive the get_stats
function recreates the directory. The fix prevents the
re-creation of the directory. Respective unit test case
is also modified

Change-Id: I577e8e3a51db361210aa83a6c7f9b4f61210e924
Closes-Bug: #1317871"
1568,d75a90ec1daff1444f20f6b68255890391bdb4e5,1396486331,0.0,1.0,107,2,2,2,1,0.132065356,True,2.0,148370.0,9.0,3.0,False,31.0,1661843.5,54.0,2.0,1377.0,1462.0,1622.0,1103.0,1107.0,1262.0,1228.0,1074.0,1230.0,0.78832585,0.68954458,0.789608724,720,1146,1301172,cinder,d75a90ec1daff1444f20f6b68255890391bdb4e5,1,0,"“Keys like display_name and display_description were deprecated for other actions in v2 like creating volumes, so for consistency they should work with updating.”","Bug #1301172 in Cinder: ""Can not update volume  by display_name, display_description in  cinder Block api v2""","Can not update volume  by display_name, display_description in  cinder Block api v2.
reason:
when updating volume , v2 API allows name instead of display_name,description Instead of display_description. but hasn't get
display_name,display_description,so cann't  update volume by the two prameters ,should get the two prameters value  first.
filepath:
cinder/api/v2/volumes.py
function :
@wsgi.serializers(xml=VolumeTemplate)
    def update(self, req, id, body):
add code:
        # NOTE(zhuozhe) fix bug : v2 API allows name instead of display_name
        # description Instead of display_description, so should get old parameter first
        valid_update_keys_v1 = (
            'display_name',
            'display_description',
        )
        for key in valid_update_keys_v1:
            if key in volume:
                update_dict[key] = volume[key]","Allow deprecated volume update keys in v2

Keys like display_name and display_description were deprecated for other
actions in v2 like creating volumes, so for consistency they should work
with updating.

DocImpact
Closes-Bug: #1301172
Change-Id: I19bc7c85352578bb57fa9fdaf1817f78e0ee2f2a"
1569,d7743bbdc3e992d9f9cceaeaa2919cd70b422364,1387440265,,1.0,22,16,5,5,1,0.858495892,True,1.0,2053826.0,23.0,9.0,False,6.0,2670871.0,8.0,6.0,3.0,621.0,621.0,3.0,550.0,550.0,3.0,311.0,311.0,0.00622084,0.485225505,0.485225505,1758,1250841,1250841,neutron,d7743bbdc3e992d9f9cceaeaa2919cd70b422364,0,0, “test in files”,"Bug #1250841 in neutron: ""Move FWaaS Noop driver to unit tests directory""","Per discussion during I summit, so as to avoid confusion and not be listed in providers.","Remove FWaaS Noop driver as default and move to unit tests dir

Remove the FWaaS Noop driver as the default and raise an exception
when the fwaas_driver.ini file has an enabled flag without any
associated driver. This communicates a misconfiguration clearly.
The Noop driver is moved to unit tests where it is used.
Also some cleanups in related area.

Closes-Bug: #1250841

Change-Id: Ib6345923df05994ceffc0b1cbf265b53c23e97f1"
1570,d7d4ef350c971c442954d772b639f8b673630896,1404359140,,1.0,13,7,2,2,1,0.970950594,False,,,,,True,,,,,,,,,,,,,,,,,1017,1460,1334965,nova,d7d4ef350c971c442954d772b639f8b673630896,1,1,,"Bug #1334965 in OpenStack Compute (nova): ""the headroom infomation  is wrong in the method of limit_check""","If multiple resources beyond the quotas ,  the headroom information  about the resources beyond the quotas is incomplete(quota.py).
for example:
If the lens of  path and content of the  injected files  exceed the limits, the above situation will appear.
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] overs: ['injected_file_content_bytes', 'injected_file_path_bytes']
2014-06-27 12:14:04.241 4547 INFO nova.quota [req-a94185e5-779f-4455-ba45-230eb70fb774 None] headroom: {'injected_file_path_bytes': 255}","the headroom infomation is incomplete

If multiple resources beyond the quotas in 'limit_check' from
nova/quota.py. The headroom information about the resources
beyond the quotas in OverQuota Exception is incomplete.

Change-Id: I790c22e3618a3558dfad64bf46e37e4e49f6385e
Closes-Bug: #1334965"
1571,d835163759527a651f3c4f2109ca0fdc3e968d37,1392607642,,1.0,1,1,1,1,1,0.0,True,2.0,2739355.0,26.0,8.0,False,147.0,220304.0,433.0,7.0,56.0,3523.0,3541.0,49.0,2847.0,2861.0,56.0,3280.0,3298.0,0.007738257,0.445424925,0.447868585,411,826,1280140,nova,d835163759527a651f3c4f2109ca0fdc3e968d37,1,1,Fix instance not found,"Bug #1280140 in OpenStack Compute (nova): ""cleanup_running_deleted_instances peroidic task failed with instance not found""","this is because the db  query is not including the deleted instance while
_delete_instance_files() in libvirt driver.
I can reproduce this bug both in master and stable havana.
reproduce steps:
1. create an instance
2. stop nova-compute
3. wait for nova-manage serivce list display xxx of nova-compute
4. modify the running_deleted_instance_poll_interval=60
running_deleted_instance_action = reap,
and start nova-compute and wait for this clean up peroidic task
5. a warnning will be given in the compute log:
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found.
the debug trace is:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1006)cleanup()
   1005 block_device_mapping = driver.block_device_info_get_mapping(
-> 1006 block_device_info)
   1007 for vol in block_device_mapping:
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1007)cleanup()
   1006 block_device_info)
-> 1007 for vol in block_device_mapping:
   1008 connection_info = vol['connection_info']
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1041)cleanup()
   1040
-> 1041 if destroy_disks:
   1042 self._delete_instance_files(instance)
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> s
--Call--
> /opt/stack/nova/nova/virt/libvirt/driver.py(4950)_delete_instance_files()
   4949
-> 4950 def _delete_instance_files(self, instance):
   4951 # NOTE(mikal): a shim to handle this file not using instance objects
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4953)_delete_instance_files()
   4952 # everywhere. Remove this when that conversion happens.
-> 4953 context = nova_context.get_admin_context()
   4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
ipdb> n
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(4954)_delete_instance_files()
   4953 context = nova_context.get_admin_context()
-> 4954 inst_obj = instance_obj.Instance.get_by_uuid(context, instance['uuid'])
   4955
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(1042)cleanup()
   1041 if destroy_disks:
-> 1042 self._delete_instance_files(instance)
   1043
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
--Return--
None
> /opt/stack/nova/nova/virt/libvirt/driver.py(931)destroy()
    930 self.cleanup(context, instance, network_info, block_device_info,
--> 931 destroy_disks)
    932
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1905)_shutdown_instance()
   1904 self.driver.destroy(context, instance, network_info,
-> 1905 block_device_info)
   1906 except exception.InstancePowerOffFailure:
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1906)_shutdown_instance()
   1905 block_device_info)
-> 1906 except exception.InstancePowerOffFailure:
   1907 # if the instance can't power off, don't release the ip
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1910)_shutdown_instance()
   1909 pass
-> 1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1911)_shutdown_instance()
   1910 except Exception:
-> 1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1914)_shutdown_instance()
   1913 # volume api calls, preserving current behavior
-> 1914 self._try_deallocate_network(context, instance,
   1915 requested_networks)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
2014-02-14 16:22:02.701 DEBUG nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Deallocating network for instance from (pid=19192) _deallocate_network /opt/stack/nova/nova/compute/manager.py:1531
2014-02-14 16:22:02.704 DEBUG oslo.messaging._drivers.amqpdriver [-] MSG_ID is e529a4edb22b480cb0641a62718e9b04 from (pid=19192) _send /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqpdriver.py:358
2014-02-14 16:22:02.705 DEBUG oslo.messaging._drivers.amqp [-] UNIQUE_ID is aed682f94730441aaa14e43a37c86227. from (pid=19192) _add_unique_id /opt/stack/oslo.messaging/oslo/messaging/_drivers/amqp.py:333
2014-02-14 16:22:02.718 WARNING nova.openstack.common.loopingcall [-] task run outlasted interval by 11.632922 sec
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> n
--Return--
None
> /opt/stack/nova/nova/compute/manager.py(1915)_shutdown_instance()
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
ipdb> l
   1910 except Exception:
   1911 with excutils.save_and_reraise_exception():
   1912 # deallocate ip and fail without proceeding to
   1913 # volume api calls, preserving current behavior
   1914 self._try_deallocate_network(context, instance,
-> 1915 requested_networks)
   1916
   1917 self._try_deallocate_network(context, instance, requested_networks)
   1918
   1919 for bdm in vol_bdms:
   1920 try:
ipdb> n
InstanceNotFound: Instance...found.',)
> /opt/stack/nova/nova/compute/manager.py(5225)_cleanup_running_deleted_instances()
   5224 self._shutdown_instance(context, instance, bdms,
-> 5225 notify=False)
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5227)_cleanup_running_deleted_instances()
   5226 self._cleanup_volumes(context, instance['uuid'], bdms)
-> 5227 except Exception as e:
   5228 LOG.warning(_(""Periodic cleanup failed to delete ""
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5228)_cleanup_running_deleted_instances()
   5227 except Exception as e:
-> 5228 LOG.warning(_(""Periodic cleanup failed to delete ""
   5229 ""instance: %s""),
ipdb> n
> /opt/stack/nova/nova/compute/manager.py(5230)_cleanup_running_deleted_instances()
   5229 ""instance: %s""),
-> 5230 unicode(e), instance=instance)
   5231 else:
ipdb> n
2014-02-14 16:22:25.917 WARNING nova.compute.manager [-] [instance: c32db267-21a0-41e7-9d50-931d8396d8cb] Periodic cleanup failed to delete instance: Instance c32db267-21a0-41e7-9d50-931d8396d8cb could not be found.","Fix InstanceNotFound error in _delete_instance_files

Currently the cleanup_running_deleted_instances peroidic task
failed with InstanceNotFound exception, this is because the
db query is not including the deleted instance while
_delete_instance_files() in libvirt driver.

Closes-bug: #1280140

Change-Id: Ie65ff255ad3c582a71db93b07304a21d4976a193"
1572,d869dee85aa9d32d1a397b954f6583d6bfa60c18,1385595502,,1.0,8,1,1,1,1,0.0,True,1.0,24801.0,5.0,2.0,False,40.0,3922.0,103.0,2.0,1043.0,677.0,1314.0,897.0,642.0,1157.0,901.0,638.0,1151.0,0.751666667,0.5325,0.96,1755,1250673,1250673,cinder,d869dee85aa9d32d1a397b954f6583d6bfa60c18,1,1, ,"Bug #1250673 in Cinder: ""base driver _do_iscsi_discovery relies on volume['host'] which breaks if called in multi-backend env""","The base driver file has a mechanism to attempt an iscsi target discovery in cases where the provider info is not present, this mechanism won't work in multi-backend mode because the sendtargets command is formed using volume['host'] to determine what ip to querie.  In the case of multi-backend this host entry will look something like:  ""cinder-host@backend-name"", the ""@backend-name"" is of course invalid for the iscsiadm call and will result in a failure.","Parse out '@' in volume['host'] to do discovery

The backup method of getting iscsi info is to use
iscsiadm discovery, however currently that method
just uses volume['host'] which in the case of
multi-backend will use ""host@backend-name"".

This will cause the discovery to fail of course, so
this change just parses out the '@' symbol if it's present
and avoids the problem in the first place.

This also beefs up the error logging and exception catching
a bit.

Parsing out the '@' symbol all the time should be safe as
the accepted valid chars for hostnames are digits, a-z and
hyphens.

Change-Id: Ic45a38bf4c56a4aec6847ab0d29e3b41d35bd3d2
Closes-Bug: #1250673"
1573,d87bd44d0211a633efd545dc7b2027a613897b85,1386254612,,1.0,16,1,2,2,1,0.522559375,True,4.0,7371224.0,23.0,13.0,False,141.0,208567.5,558.0,6.0,128.0,2045.0,2049.0,127.0,1628.0,1632.0,116.0,1864.0,1867.0,0.017226148,0.27458775,0.275029446,1644,1236585,1236585,nova,d87bd44d0211a633efd545dc7b2027a613897b85,1,1, ,"Bug #1236585 in OpenStack Compute (nova): ""tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since fails sporadically""","See: http://logs.openstack.org/69/49169/2/check/check-tempest-devstack-vm-full/473539f/console.html
2013-10-07 18:57:09.859 | ======================================================================
2013-10-07 18:57:09.859 | FAIL: tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | tempest.api.compute.servers.test_list_servers_negative.ListServersNegativeTestJSON.test_list_servers_by_changes_since[gate]
2013-10-07 18:57:09.860 | ----------------------------------------------------------------------
2013-10-07 18:57:09.860 | _StringException: Empty attachments:
2013-10-07 18:57:09.861 |   stderr
2013-10-07 18:57:09.861 |   stdout
2013-10-07 18:57:09.861 |
2013-10-07 18:57:09.862 | pythonlogging:'': {{{
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,059 Request: GET http://127.0.0.1:8774/v2/3b02395d4eec44958ffcc10ac2673fd2/servers?changes-since=2013-10-07T18%3A38%3A31.034080
2013-10-07 18:57:09.862 | 2013-10-07 18:38:33,490 Response Status: 200
2013-10-07 18:57:09.863 | 2013-10-07 18:38:33,490 Nova request id: req-906f2cf2-6508-4cd2-bf62-3595b18d223a
2013-10-07 18:57:09.863 | }}}
2013-10-07 18:57:09.863 |
2013-10-07 18:57:09.863 | Traceback (most recent call last):
2013-10-07 18:57:09.864 |   File ""tempest/api/compute/servers/test_list_servers_negative.py"", line 191, in test_list_servers_by_changes_since
2013-10-07 18:57:09.864 |     self.assertEqual(num_expected, len(body['servers']))
2013-10-07 18:57:09.864 |   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 322, in assertEqual
2013-10-07 18:57:09.865 |     self.assertThat(observed, matcher, message)
2013-10-07 18:57:09.865 |   File ""/usr/local/lib/python2.7/dist-packages/testtools/testcase.py"", line 417, in assertThat
2013-10-07 18:57:09.865 |     raise MismatchError(matchee, matcher, mismatch, verbose)
2013-10-07 18:57:09.865 | MismatchError: 3 != 2","Fix changes-since filter for list-servers API

On list-servers API, ""changes-since"" parameter filters out servers
which have been created at the same date as the specified timestamp.
For example, the following server is filtered out with ""changes-since=
2013-12-05T15:03:25"":

  mysql> select display_name, updated_at from instances;
  +--------------+---------------------+
  | display_name | updated_at          |
  +--------------+---------------------+
  | vm01         | 2013-12-05 15:03:25 |
  +--------------+---------------------+

This bug causes some Tempest test failures, if the timestamp of server
creations are the same as the ""changes-since"" timestamp in second unit.
In addition, the behavior would be wrong from the viewpoint of its name
which includes ""since"".

This patch chanegs the server list for including the ones created at
the specified timestamp.

Change-Id: Icc10a1363503b3553d810c6be20b86b3da7ac1a0
Closes-Bug: #1236585"
1574,d88b4cf31dbff942cf529e63ee5b09356970ed50,1408445671,,1.0,5,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1215,1674,1358702,nova,d88b4cf31dbff942cf529e63ee5b09356970ed50,0,0,Bug in test,"Bug #1358702 in OpenStack Compute (nova): ""Hyper-V unit test fails on Windows due to path separator inconsistency: nova.tests.virt.hyperv.test_pathutils.PathUtilsTestCase.test_lookup_config_drive_path""","The following test fails due to mismatching in the path separator.
FAIL: nova.tests.virt.hyperv.test_pathutils.PathUtilsTestCase.test_lookup_config_drive_path
----------------------------------------------------------------------
_StringException: Empty attachments:
  pythonlogging:''
Traceback (most recent call last):
  File ""C:\OpenStack\nova\nova\tests\virt\hyperv\test_pathutils.py"", line 48, i
 test_lookup_configdrive_path
    format_ext)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 321, in assertEqual
    self.assertThat(observed, matcher, message)
  File ""C:\Program Files (x86)\Cloudbase Solutions\OpenStack\Nova\Python27\lib\
ite-packages\testtools\testcase.py"", line 406, in assertThat
    raise mismatch_error
MismatchError: !=:
reference = 'C:/fake_instance_dir\\configdrive.vhd'
actual    = 'C:/fake_instance_dir/configdrive.vhd'","Fixes Hyper-V unit test path separator issue

Fixes an issue in a unit tests were path separators were not
handled in a platform independent way causing the test to fail
on Windows.

Change-Id: Ifca77a561e5a95af8655f2373af665292b2ca3cb
Co-Authored-By: Zsolt Dudas <zdudas@cloudbasesolutions.com>
Closes-Bug: #1358702"
1575,d8a11168c908fe6c6a07fbb30a5bc88a6df6e939,1384896347,,1.0,45,7,2,2,1,0.74441318,True,3.0,6421487.0,29.0,19.0,False,15.0,561677.5,34.0,8.0,166.0,1396.0,1409.0,164.0,1242.0,1255.0,148.0,1137.0,1143.0,0.127350427,0.972649573,0.977777778,1783,1252864,1252864,cinder,d8a11168c908fe6c6a07fbb30a5bc88a6df6e939,1,1, ,"Bug #1252864 in Cinder: ""GlusterFS: allow snapshot_delete when snapshot info does not exist""","The snapshot_delete operation will fail if the backing file for the snapshot does not exist.  This happens in legitimate cases such as when snapshot_create fails due to permissions problems.
The driver should allow the manager to delete the snapshot in this case where this is no action required for the driver to delete anything.","GlusterFS: Complete snapshot_delete when info doesn't exist

The snapshot_delete operation will fail if the snapshot info file
doesn't contain a record for the snapshot, or does not exist.
This happens in cases such as when snapshot_create fails to commit
anything to disk.

The driver should allow the manager to delete the snapshot
in this case, as there is no action required for the driver
to delete anything.

Closes-Bug: #1252864

Change-Id: I8686a1be09dbb7984072538bff6c026bb84eeb52"
1576,d90d71cfdae1d9c9c3b54cc33adfabce683c633c,1396988449,,1.0,22,2,2,2,1,0.650022422,True,3.0,42172.0,56.0,11.0,False,6.0,1544609.0,14.0,3.0,777.0,1652.0,2006.0,649.0,1343.0,1638.0,281.0,896.0,940.0,0.252688172,0.803763441,0.843189964,763,1190,1304647,neutron,d90d71cfdae1d9c9c3b54cc33adfabce683c633c,1,1,,"Bug #1304647 in neutron: ""KeyError in nsx sync nsx_router_id mapping not found""","2014-03-26 16:07:51,111 (neutron.plugins.vmware.common.nsx_utils): WARNING nsx_utils get_nsx_router_id Unable to find NSX router for Neutron router e90010ab-f630-4f81-9f6b-ac3da4b29aef
2014-03-26 16:07:51,111 (neutron.plugins.vmware.api_client.base): DEBUG base acquire_connection [0] Acquired connection https://17.176.14.50:443. 8 connection(s) available.
2014-03-26 16:07:51,112 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Issuing - request GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus
2014-03-26 16:07:51,320 (neutron.plugins.vmware.api_client.request): DEBUG request _issue_request [0] Completed request 'GET https://17.176.14.50:443//ws.v1/lrouter?relations=LogicalRouterStatus': 200 (0.207953929901 seconds)
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.base): DEBUG base release_connection [0] Released connection https://17.176.14.50:443. 9 connection(s) available.
2014-03-26 16:07:51,321 (neutron.plugins.vmware.api_client.eventlet_request): DEBUG eventlet_request _handle_request [0] Completed request 'GET /ws.v1/lrouter?relations=LogicalRouterStatus': 200
2014-03-26 16:07:51,322 (neutron.plugins.vmware.api_client.client): DEBUG client request Request returns ""<httplib.HTTPResponse instance at 0x3d97b00>""
2014-03-26 16:07:51,324 (neutron.openstack.common.loopingcall): ERROR loopingcall _inner in dynamic looping call
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/openstack/common/loopingcall.py"", line 123, in _inner
    idle = self.f(*self.args, **self.kw)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 606, in _synchronize_state
    scan_missing=scan_missing)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 380, in _synchronize_lrouters
    ctx, router, lrouter and lrouter.get('data'))
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 339, in synchronize_router
    self._nsx_cache.update_lrouter(lrouter)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 134, in update_lrouter
    self._update_resources(self._lrouters, [lrouter])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/vmware/common/sync.py"", line 86, in _update_resources
    item_id = item['uuid']
KeyError: 'uuid'","NSX: Fix KeyError in sync if nsx_router_id not found

Previously, a KeyError would occur in the sync code which would
cause the sync thread to stop running. This would occur if there
was a router entry in the database but no nsx_router_mapping and
the router was not found in nsx. Note: this should never happen unless
one did not run the db migration which introduced and migrated the data
for the NeutronNsxRouterMapping table.

Change-Id: I44f3e7de9323f594501db63d3ad33e80e617bfdc
Closes-bug: #1304647"
1577,d9237dff7ace30dab331151c736d7cca97618e04,1386229812,,1.0,1,1,1,1,1,0.0,True,1.0,4328927.0,20.0,9.0,False,92.0,1277715.0,234.0,8.0,5.0,3132.0,3133.0,5.0,2670.0,2671.0,5.0,2211.0,2212.0,0.000884173,0.325965223,0.326112585,146,548,1258048,nova,d9237dff7ace30dab331151c736d7cca97618e04,0,0,Typo in docstring,"Bug #1258048 in OpenStack Compute (nova): ""Wrong monkey_patch docstring  in nova/utils.py""","""Flags.monkey_patch"" should be ""CONF.monkey_patch"" in the docstring of monkey_patch function in nova/utils.py","Fix monkey_patch docstring bug

Modify Flags.monkey_patch to CONF.monkey_patch

Change-Id: I773a94d97d206bf78ac52de8e22d9e6db7650eb9
Closes-Bug: #1258048"
1578,d96a8f0e6b524c10e563418c2770069030455c25,1407407616,,1.0,12,1,2,2,1,0.89049164,False,,,,,True,,,,,,,,,,,,,,,,,1161,1618,1353949,nova,d96a8f0e6b524c10e563418c2770069030455c25,0,0,Feature “this exception is not handled in V3 api.”,"Bug #1353949 in OpenStack Compute (nova): ""nova.exception.ExternalNetworkAttachForbidden is not handled in V3""","When creating an instance and assigning a public network to it without admin authority, ExternalNetworkAttachForbidden
will be raised. But this exception is not handled in V3 api.
2014-08-07 19:40:55.032 ERROR nova.api.openstack.extensions [req-a3a824a2-d477-4720-98c7-d3161de268ba demo demo] Unexpected exception in API method
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions Traceback (most recent call last):
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/extensions.py"", line 473, in wrapped
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     return f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/validation/__init__.py"", line 39, in wrapper
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     return func(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/api/openstack/compute/plugins/v3/servers.py"", line 507, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     **create_kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/hooks.py"", line 131, in inner
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     rv = f(*args, **kwargs)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 1351, in create
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     legacy_bdm=legacy_bdm)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 967, in _create_instance
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 734, in _validate_and_build_base_options
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     requested_networks, max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/compute/api.py"", line 447, in _check_requested_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     max_count)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 709, in validate_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     neutron=neutron)
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions   File ""/opt/stack/nova/nova/network/neutronv2/api.py"", line 169, in _get_available_networks
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions     network_uuid=net['id'])
2014-08-07 19:40:55.032 TRACE nova.api.openstack.extensions ExternalNetworkAttachForbidden: It is not allowed to create an interface on external network 447d82c5-bf58-4f39-ac2f-a30227a464e2","Handle ExternalNetworkAttachForbidden exception

When creating an instance and assigning a public network
to it without admin authority, ExternalNetworkAttachForbidden
will be raised. But this exception is not handled in V3 api.

Change-Id: Id70d8eaa704fd82cef1afdb65756d75cb56fbdce
Closes-Bug: #1353949"
1579,d96fbffcab5fafe48f4a91ea6f0a24310d63d9c7,1406839800,,1.0,39,24,3,2,1,0.878746802,False,,,,,True,,,,,,,,,,,,,,,,,1134,1589,1351002,nova,d96fbffcab5fafe48f4a91ea6f0a24310d63d9c7,1,1,,"Bug #1351002 in OpenStack Compute (nova): ""live migration (non-block-migration) with shared instance storage and config drive fails""","Note: the reproduction case below has been fixed by not blocking migration on config drives.  However, the underlying issue of NFS not being marked
as shared storage still stands, since the 'is_shared_block_storage' data
is used elsewhere as well.
---------------------------------------------------------------------------
To reproduce:
1. Set up shared instance storage via NFS and use one of the file-based image backends
2. Boot an instance with a config drive
3. Attempt to live migrate said instance w/o doing a block migration
The issue is caused by the following lines in nova/virt/libvirt/driver.py:
        if not (is_shared_instance_path and is_shared_block_storage):
            # NOTE(mikal): live migration of instances using config drive is
            # not supported because of a bug in libvirt (read only devices
            # are not copied by libvirt). See bug/1246201
            if configdrive.required_by(instance):
                raise exception.NoLiveMigrationForConfigDriveInLibVirt()
The issue, I believe, was caused by commit bc45c56f1, which separated checks for shared instance directories and shared block storage backends like Ceph.  The issue is that if a deployer is not using Ceph, the call to self.image_backend.backend().is_shared_block_storage() returns False.  However, is_shared_block_storage should not even be considered if the image backend is a file-based one.","libvirt: support live migration with shared instances dir

If instance path is shared between compute nodes (e.g. over NFS) and image
backend uses files placed in the instance directory (e.g. Raw, Qcow2), live
migration should consider block storage of the instances shared.

Change-Id: I913c57152974562765502ab57be63ccb6dfe5208
Closes-Bug: #1351002"
1580,d97304d18cf594e5982899d1c9f82a297842cd1d,1384911584,,1.0,2,2,1,1,1,0.0,True,2.0,194712.0,36.0,9.0,False,41.0,150252.0,92.0,6.0,0.0,1297.0,1297.0,0.0,1156.0,1156.0,0.0,1115.0,1115.0,0.000853242,0.95221843,0.95221843,1784,1252915,1252915,cinder,d97304d18cf594e5982899d1c9f82a297842cd1d,1,1,Typo in method name,"Bug #1252915 in Cinder: ""Fix typo in cinder.volume.API""","cinder.volume.API has typo in its method name.
'_valid_availabilty_zone' should be '_valid_availability_zone'.","Fix typo in cinder.volume.API.

'_valid_availabilty_zone' --> '_valid_availability_zone'

Change-Id: Ib79772039d663f0eb4fc8994cc8edc5154d75a02
Closes-Bug: #1252915"
1581,d98ca642402bafbaa15ac3df588ed34d58d8456b,1405756449,1.0,1.0,14,1,7,3,1,0.946287461,False,,,,,True,,,,,,,,,,,,,,,,,1090,1541,1346673,neutron,d98ca642402bafbaa15ac3df588ed34d58d8456b,0,0,Bug in test,"Bug #1346673 in neutron: ""fixtures in neutron.tests.base blow away default database config""","Really trying to narrow this one down fully, and just putting this up because this is as far as I've gotten.
Basically, the lines in neutron/tests/base.py:
  line 159:        self.addCleanup(CONF.reset)
  line 182:    self.useFixture(self.messaging_conf)
cause cfg.CONF to get totally wiped out in the ""database"" config.  I don't yet understand why this is the case.
if you then run any test that extends BaseTestCase, and then run neutron/tests/unit/test_db_plugin.py -> NeutronDbPluginV2AsMixinTestCase in the same process, these two tests fail:
Traceback (most recent call last):
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py"", line 3943, in setUp
    self.plugin = importutils.import_object(DB_PLUGIN_KLASS)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/openstack/common/importutils.py"", line 38, in import_object
    return import_class(import_str)(*args, **kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/db_base_plugin_v2.py"", line 72, in __init__
    db.configure_db()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 45, in configure_db
    register_models()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 68, in register_models
    facade = _create_facade_lazily()
  File ""/Users/classic/dev/redhat/openstack/neutron/neutron/db/api.py"", line 34, in _create_facade_lazily
    _FACADE = session.EngineFacade.from_config(cfg.CONF, sqlite_fk=True)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 977, in from_config
    retry_interval=conf.database.retry_interval)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 893, in __init__
    **engine_kwargs)
  File ""/Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/sqlalchemy/session.py"", line 650, in create_engine
    if ""sqlite"" in connection_dict.drivername:
AttributeError: 'NoneType' object has no attribute 'drivername'
I'm getting this error running tox on a subset of tests, however it's difficult to reproduce as the subprocesses have to work out just right.
To reproduce, just install nose and do:
.tox/py27/bin/nosetests -v neutron.tests.unit.test_db_plugin:DbModelTestCase neutron.tests.unit.test_db_plugin:NeutronDbPluginV2AsMixinTestCase
That is, DbModelTestCase is a harmless test but because it runs base.BaseTestCase first, cfg.CONF gets blown away.
I don't know what the solution should be here, cfg.CONF shouldn't be reset but I don't know what ""messaging_conffixture.ConfFixture"" is or how ""CONF.reset"" was supposed to work as it blows away DB config.  The cfg.CONF in the first place seems to get set up via this path:
  <string>(7)exec2()
  /Users/classic/dev/redhat/openstack/neutron/neutron/tests/unit/test_db_plugin.py(26)<module>()
-> from neutron.api import extensions
  /Users/classic/dev/redhat/openstack/neutron/neutron/api/extensions.py(31)<module>()
-> from neutron import manager
  /Users/classic/dev/redhat/openstack/neutron/neutron/manager.py(20)<module>()
-> from neutron.common import rpc as n_rpc
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/rpc.py(22)<module>()
-> from neutron import context
  /Users/classic/dev/redhat/openstack/neutron/neutron/context.py(26)<module>()
-> from neutron import policy
  /Users/classic/dev/redhat/openstack/neutron/neutron/policy.py(55)<module>()
-> cfg.CONF.import_opt('policy_file', 'neutron.common.config')
  /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/config/cfg.py(1764)import_opt()
-> __import__(module_str)
  /Users/classic/dev/redhat/openstack/neutron/neutron/common/config.py(135)<module>()
-> max_overflow=20, pool_timeout=10)
> /Users/classic/dev/redhat/openstack/neutron/.tox/py27/lib/python2.7/site-packages/oslo/db/options.py(145)set_defaults()
-> conf.register_opts(database_opts, group='database')
e.g. oslo.db set_defaults() sets it up.","Call config_parse in base test setup

Some of the tests (e.g. NeutronDbPluginV2AsMixinTestCase) do not call
config_parse so if the database engine is not already been setup by another
test before the config object is reset on teardown, the database connection
will fail since the database config is then empty.

This patch adds a new setup_config method called during the base test case
setUp method which calls config_parse by default to load the default config.
Tests that couldn't use the default config were then modified to override the
setup_config method.

Some other unit tests were slightly adjusted to pass using the default config.

Closes-Bug: #1346673
Change-Id: I3724200fa932115c0a8c264640b1a9dbe431a1cc"
1582,d9a5a80bc06f7a25c259ff763a59dffd9514371e,1393534637,,1.0,29,2,4,2,1,0.776958071,True,7.0,121755.0,39.0,16.0,False,168.0,114834.0,527.0,3.0,853.0,1872.0,2474.0,609.0,1709.0,2101.0,800.0,1832.0,2388.0,0.107315113,0.245578778,0.320069668,493,912,1285735,nova,d9a5a80bc06f7a25c259ff763a59dffd9514371e,0,0,Evolution change. NO bug. Change in the decisions made,"Bug #1285735 in OpenStack Compute (nova): ""libvirt lvm volumes based on instance['name'] not instance['uuid']""","because libvirt lvm volumes are based on instance['name'], it means that the actual names used in lvm storage are based on an operator configuration variable: instance_name_template
the default is 'instance-%08x'
however this is site changable, and changable at any time. This creates 2 failure modes.
#1) operator changes this, the result is all volumes created before the change are no longer able to be cleaned up by nova
#2) operator has changed this to something that includes end user input, like %(display_name), which would allow one user to impact another (use A has display name ""bob"", user B has displayname ""bob_joe"") because of https://github.com/openstack/nova/blob/master/nova/virt/libvirt/driver.py#L1068
specifically:
            pattern = '%s_' % instance['name']
            def belongs_to_instance(disk):
                return disk.startswith(pattern)
#2 is a non default situation, and requires specific config by an adminstrator and specific naming by users, but it should be protected against.
A much better approach would be to use instance['uuid'] which has no operator or user impact on naming.","Use uuid instead of name for lvm backend

Because libvirt lvm volumes are based on instance['name'], it means
that the actual names used in lvm storage are based on an operator
configuration variable: instance_name_template (default is
'instance-%08x').

However this is site changeable, and changeable at any time. This
creates 2 failure modes.

1. Operator changes this, the result is all volumes created before the
change are no longer able to be cleaned up by nova

2. Operator has changed this to something that includes end user
input, like %(display_name), which would allow one user to impact
another (user A has display name ""bob"", user B has display name
""bob_joe"")

This changes the lvm backend to use instance['uuid'] as it's
identifier which we know is unique, non overlapping, and not
changeable based on whims of site policy.

It also provides limited backwards compatibility for cleaning up old
disks if the installation was using the default template (which is
safe), otherwise it logs a warning about possibly leaking a disk which
will need to be manually cleaned up. That should be removed once we
open the Juno tree.

UpgradeImpact - while there is no operator action required to upgrade
over this change, if the nova install included a non default value for
instance_name_template then old lvm volumes will need to be cleaned up
manually after old guests are destroyed.

Change-Id: Ib36b962971fd1f66ea9a0818e91fec59e118e686
Closes-Bug: #1285735"
1583,d9bcd597c67900472709cd11604afe1616e0af4e,1389259382,,1.0,8,2,1,1,1,0.0,True,4.0,199038.0,46.0,5.0,False,17.0,1964587.0,29.0,3.0,204.0,342.0,500.0,203.0,295.0,452.0,143.0,200.0,298.0,0.213967311,0.298662704,0.444279346,257,661,1266537,neutron,d9bcd597c67900472709cd11604afe1616e0af4e,1,1,Race condition,"Bug #1266537 in neutron: ""internal neutron server error on tempest VolumesActionsTest""","The following traces were found when  VolumesActionsTest has failed:
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/console.html
http://logs.openstack.org/18/63918/5/gate/gate-tempest-dsvm-neutron-pg-isolated/5a67370/logs/screen-q-svc.txt.gz?level=TRACE#_2014-01-04_11_56_37_489","Fix race condition on ml2 delete and update port methods

Synchronize access to ports table when deleting and updating
a port. Otherwise concurrent update/delete request for the same port
may cause neutron server to throw an exception and return
'500 Internal server error' for such requests.

Change-Id: I868002643147ce6baace5671cffb38b4f5e66729
Closes-Bug: #1266537"
1584,d9eea51187ddd9b951e9943030724489f1c21a26,1396226901,,1.0,3,1,1,1,1,0.0,True,1.0,125431.0,24.0,3.0,False,3.0,3548127.0,4.0,2.0,2.0,1245.0,1245.0,2.0,1064.0,1064.0,2.0,759.0,759.0,0.002859867,0.724499523,0.724499523,705,1131,1299946,neutron,d9eea51187ddd9b951e9943030724489f1c21a26,1,1,"""Added the missing plugin session.”","Bug #1299946 in neutron: ""OneConvergence plugin create_network fails for external networks""",create_network for networks with --router:external=True call doesn't add the network to the externalnetworks table. This is resulting in failures due to the missing entry in externalnetworks table. The reason is _process_l3_create() (called from create_network) expects to be invoked with a plugin session. Until recently process_l3_create was adding the entry to externalnetworks table even without the plugin session. Plugin third-party tests have been failing the last few days due to this.,"Invoke _process_l3_create within plugin session

_process_l3_create should be invoked with a plugin session, else
externalnetworks table is not populated for networks with
router:external=True. Added the missing plugin session.

Change-Id: I09c49dea9474f91a70aef829c1596fbb9089d6cf
Closes-Bug: #1299946"
1585,da139f615673172a1a54fb3065233d568db5e313,1382947700,0.0,1.0,25,1,2,2,1,0.391243564,True,4.0,2131714.0,24.0,9.0,False,2.0,256013.0,3.0,4.0,5.0,2018.0,2019.0,5.0,1836.0,1837.0,4.0,448.0,449.0,0.009920635,0.890873016,0.892857143,1724,1245408,1245408,neutron,da139f615673172a1a54fb3065233d568db5e313,1,0,"“The DHCP driver used to use device_delegate, but that has changed to device_manager.” Software evolved","Bug #1245408 in neutron: ""Midonet DhcpNoOpDriver references incorrect member""","The Midonet DhcpNoOpDriver currently references device_delegate, which is now named device_manager. This bug is to correct the naming.","Use correct device_manager member in dhcp driver

* The DHCP driver used to use device_delegate, but that has
  changed to device_manager. referencing device_delegate
  caused the cleanup of the dhcp agent namespaces to fail.
  This fixes the issue by using the correct member name,
  device_manager

Change-Id: Iab8ea0549383335ff66f3f13df1953d206eeb28f
Closes-Bug: #1245408"
1586,da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2,1396497765,1.0,1.0,8,3,1,1,1,0.0,True,1.0,128827.0,8.0,4.0,False,18.0,1948327.0,22.0,4.0,1228.0,1639.0,1649.0,959.0,1268.0,1278.0,1076.0,1239.0,1242.0,0.690384615,0.794871795,0.796794872,353,764,1275173,cinder,da13c6285bb0aee55cfbc93f55ce2e2b7d6a28f2,0,0,Refactoring code to make 1 less call. No one speaks about performance,"Bug #1275173 in Cinder: ""_translate_from_glance() can cause an unnecessary HTTP request""","I noticed when performing a ""nova image-show"" on a current (not deleted) image, two HTTP requests were issued. Why isn't the Image retrieved on the first GET request?
In fact, it is. The problem lies in _extract_attributes(), called by _translate_from_glance(). This function loops through a list of expected attributes, and extracts them from the passed-in Image. The problem is that if the attribute 'deleted' is False, there won't be a 'deleted_at' attribute in the Image. Not finding the attribute results in getattr() making another GET request (to try to find the ""missing"" attribute?). This is unnecessary of course, since it makes sense for the Image to not have that attribute set.","_translate_from_glance() can cause an unnecessary HTTP request

After returning from a get() call to python-glanceclient, cinder runs a
translation function on the returned Image to get the data it wants. Part of
this process is checking for an expected set of attributes, one of which is
the deletion time ('deleted_at'). However, if the image has not been deleted,
deleted_at key will not exist. This forces another call to glance to occur for
the same image. A similar problem exists for the checksum attribute, which does
not exist before an image is active. The fix here is to only consider
deleted_at and checksum if they are expected to be present.

This change was made in nova as change I67b7dd16

Change-Id: Iedc16cb9316f9610fdb8ac03f448bc375a4e6bfa
Closes-Bug: #1275173"
1587,da1fa5dc93e6e64d397e8d00769e34828669e010,1397680877,,1.0,25,37,2,2,1,0.740865686,True,3.0,1960730.0,37.0,14.0,False,154.0,439481.0,562.0,7.0,1856.0,4374.0,5579.0,901.0,3018.0,3608.0,1739.0,3907.0,5014.0,0.222563315,0.49987209,0.641468406,805,1233,1308715,nova,da1fa5dc93e6e64d397e8d00769e34828669e010,1,1,,"Bug #1308715 in OpenStack Compute (nova): ""Deadlock on quota_usages""","We are getting deadlocks for concurrent quota reservations that we did not see in grizzly:
see https://bugs.launchpad.net/nova/+bug/1283987
The deadlock handling needs to be fixed as per above, but we shouldn't be deadlocking, here. It seems this is due to bad indexes in the database:
mysql> show index from quota_usages;
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| Table        | Non_unique | Key_name                        | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
| quota_usages |          0 | PRIMARY                         |            1 | id          | A         |           8 |     NULL | NULL   |      | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_project_id      |            1 | project_id  | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_user_id_deleted |            1 | user_id     | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
| quota_usages |          1 | ix_quota_usages_user_id_deleted |            2 | deleted     | A         |           8 |     NULL | NULL   | YES  | BTREE      |         |               |
+--------------+------------+---------------------------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+
4 rows in set (0.01 sec)
mysql> explain select * from quota_usages where project_id='foo' and user_id='bar' and deleted=0;
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
| id | select_type | table        | type | possible_keys                                              | key                        | key_len | ref   | rows | Extra                              |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
|  1 | SIMPLE      | quota_usages | ref  | ix_quota_usages_project_id,ix_quota_usages_user_id_deleted | ix_quota_usages_project_id | 768     | const |    1 | Using index condition; Using where |
+----+-------------+--------------+------+------------------------------------------------------------+----------------------------+---------+-------+------+------------------------------------+
1 row in set (0.00 sec)
We should have an index on project_id/deleted and project_id/user_id/deleted instead of the current values.","Use one query instead of two for quota_usages

The second query was unnecessary and not using a proper index. It
seemed to be contributing to a deadlock issue as well.

Change-Id: I1031451b25140d71380c8149d3c83827eec0d4a9
Closes-Bug: #1308715
Co-Authored-by: Chris Behrens <cbehrens@codestud.com>"
1588,da794932ba5e627c2aaa58200116eeaf5234863b,1297724005,1.0,,22,22,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1813,1264561,1264561,Swift,da794932ba5e627c2aaa58200116eeaf5234863b,1,0,"“Most stock Linux systems come with a soft limit of nproc set to 1024, which can be low now”","Bug #1264561 in OpenStack Object Storage (swift): ""Quickly hit nproc limit""","Most stock Linux systems come with a soft limit of nproc set to 1024, which can be low now due to threads_per_disk giving us an effective multiplier on total number of object server processes.  If workers * threads_per_disk * disks > 1024, we get this error:
Dec 26 03:49:28 object-server ERROR __call__ error with HEAD /d95/236790/AUTH_sd-fuse/c0003125/lib/python2.6/site-packages/yum/pkgtag_db.pyc : #012Traceback (most recent call last):#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 631, in __call__#012    res = method(req)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 1870, in wrapped#012    return func(*a, **kw)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 686, in _timing_stats#012    resp = func(ctrl, *args, **kwargs)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 514, in HEAD#012    obj)#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 114, in _diskfile#012    kwargs.setdefault('threadpool', self.threadpools[device])#012  File ""/opt/ss/lib/python2.7/site-packages/swift/obj/server.py"", line 86, in <lambda>#012    lambda: ThreadPool(nthreads=self.threads_per_disk))#012  File ""/opt/ss/lib/python2.7/site-packages/swift/common/utils.py"", line 2054, in __init__#012    thr.start()#012  File ""/opt/ss/lib/python2.7/threading.py"", line 494, in start#012    _start_new_thread(self.__bootstrap, ())#012error: can't start new thread
Since Swift already raises/sets nofile and data limits for the processes at startup, add some logic to do the same to nproc.  The 8192 choice is arbitrary on my part, but seems a reasonable number.",i18n
1589,da84cb078e9820c7c9d64bdccd904e1b257ab96e,1389953374,,1.0,38,3,2,2,1,0.926212213,True,5.0,904952.0,25.0,11.0,False,43.0,5832820.5,89.0,3.0,43.0,1108.0,1115.0,43.0,939.0,946.0,42.0,1039.0,1045.0,0.006062315,0.146623432,0.147469336,301,710,1270088,nova,da84cb078e9820c7c9d64bdccd904e1b257ab96e,0,0,No bug: needs tests + better log ,"Bug #1270088 in OpenStack Compute (nova): ""disk/api.py: resize2fs needs tests + better log""","In disk/api.py the method resize2fs does not have a test.
Also, the method firstly use e2fsck to check if the filesystem is correct. if the program failed no information was logged and the actual algorithm try to do the resize anyway. Same with e2fsck, if resize2fs failed not information are logged.
We need to add tests for this function and log every error returned.","disk/api.py: resize2fs fails silently + adds tests

+ The method disk.api.resize2fs fails silently and should
  writes some information in the log.
+ Adds missing tests to disk.api.resize2fs

Change-Id: I4edd88fb85a73c75628958e3f2ea668c0d3cad89
Closes-Bug: #1270088"
1590,da9597aed0186e68dbf1c7304b30e49f8e6a54ff,1402611863,,1.0,9,17,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,869,1303,1317075,cinder,da9597aed0186e68dbf1c7304b30e49f8e6a54ff,1,0,"“the behavior described in this bug is specific to F20 (for now).
“","Bug #1317075 in Cinder: ""Cannot attach volume after volume snapshot create/delete when using lvm>=2.02.99""","lvm backend, with tgtadm ISCSI helper.
1. nova boot test2 --image cirros-0.3.2-x86_64-uec --flavor 42 # wait for 'active'
2. cinder create 1 #  wait for 'available'
3. cinder snapshot-create <vol_id> # wait for create
4. cinder snapshot-delete <snap_id>  # wait for delete
5. nova volume-attach test2 <vol_id> /dev/vdc
6. cinder list # says it is in 'attaching' status for long time (>10s)
7. cinder list #  the volume in 'available' status
The  cinder snapshot-delete <vol_id> causes the volume looses it (a)ctive lvm flag.
 LV                                          VG            Attr       LSize Pool Origin Data%  Move Log Cpy%Sync Convert
volume-c86347f9-3f42-4824-aee1-ae4aa33a2cf9 stack-volumes -wi------- 1.00g
This command has effect also to  the original volume, even if it used on the snapshot:
$ lvchange -y -an  stack-volumes/_snapshot-1b5cfb86-be8e-4ad0-89f5-ecc4e64a5f5d","Retry lvremove with ignore_suspended_devices

A lvremove -f might leave behind suspended devices
when it is racing with udev or other processes
still accessing any of the device files. The previous
solution of using lvchange -an on the LV had the
side-effect of deactivating origin LVs alongway in
the thick volume case, which was undesired.

It turns out retrying the deactivation twice and
ignoring the suspended devices on the second iteration
avoids the hang of all LVM operations after an initial
failure.

Change-Id: I0d6fb74084d049ea184e68f2dcc4e74f400b7dbd
Closes-Bug: #1317075
Related-Bug: #1270192"
1591,da9891a30b3a3eac39458d829abdd5bc50cf876e,1403151553,,1.0,29,3,2,2,1,0.811278124,False,,,,,True,,,,,,,,,,,,,,,,,980,1419,1330856,nova,da9891a30b3a3eac39458d829abdd5bc50cf876e,1,1,,"Bug #1330856 in OpenStack Compute (nova): ""Confusing fault reasion when the flavors disk size was too small""","Fedora-x86_64-20-20140407-sda has 2 GiB virtual size.
$ nova boot fed_1G_2  --image Fedora-x86_64-20-20140407-sda --flavor 1 --key-name mykey
$ nova show fed_1G_2
+--------------------------------------+------------------------------------------------------------------------------------------+
| Property                             | Value                                                                                    |
+--------------------------------------+------------------------------------------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                                                   |
| OS-EXT-AZ:availability_zone          | nova                                                                                     |
| OS-EXT-STS:power_state               | 0                                                                                        |
| OS-EXT-STS:task_state                | -                                                                                        |
| OS-EXT-STS:vm_state                  | error                                                                                    |
| OS-SRV-USG:launched_at               | -                                                                                        |
| OS-SRV-USG:terminated_at             | -                                                                                        |
| accessIPv4                           |                                                                                          |
| accessIPv6                           |                                                                                          |
| config_drive                         |                                                                                          |
| created                              | 2014-06-17T07:35:43Z                                                                     |
| fault                                | {""message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""} |
| flavor                               | m1.tiny (1)                                                                              |
| hostId                               | a904a292f4eb7f6735bef786c4a240a0b9240a6bc4f002519cb0e2b7                                 |
| id                                   | 3c908a54-9682-40ad-8f12-a5bf64066660                                                     |
| image                                | Fedora-x86_64-20-20140407-sda (085610a8-77ae-4bc8-9a28-3bcc1020e06e)                     |
| key_name                             | mykey                                                                                    |
| metadata                             | {}                                                                                       |
| name                                 | fed_1G_2                                                                                 |
| os-extended-volumes:volumes_attached | []                                                                                       |
| private network                      | 10.1.0.5                                                                                 |
| security_groups                      | default                                                                                  |
| status                               | ERROR                                                                                    |
| tenant_id                            | 1d26ad7003cf47e5b0107313be4832c3                                                         |
| updated                              | 2014-06-17T07:35:44Z                                                                     |
| user_id                              | bf52e56b9ca14648b391c5b6d490a0c1                                                         |
+--------------------------------------+------------------------------------------------------------------------------------------+
$ # nova flavor-list
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| ID  | Name      | Memory_MB | Disk | Ephemeral | Swap_MB | VCPUs | RXTX_Factor | Is_Public |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
| 1   | m1.tiny   | 512       | 1    | 0         |         | 1     | 1.0         | True      |
| 2   | m1.small  | 2048      | 20   | 0         |         | 1     | 1.0         | True      |
| 3   | m1.medium | 4096      | 40   | 0         |         | 2     | 1.0         | True      |
| 4   | m1.large  | 8192      | 80   | 0         |         | 4     | 1.0         | True      |
| 42  | m1.nano   | 64        | 0    | 0         |         | 1     | 1.0         | True      |
| 451 | m1.heat   | 1024      | 0    | 0         |         | 2     | 1.0         | True      |
| 5   | m1.xlarge | 16384     | 160  | 0         |         | 8     | 1.0         | True      |
| 84  | m1.micro  | 128       | 0    | 0         |         | 1     | 1.0         | True      |
+-----+-----------+-----------+------+-----------+---------+-------+-------------+-----------+
Many images requires minimum 2,5,10 Gib as minimum disk size, the 1 used by m1.tiny is frequently not enough.
It might be increased to 10 or the original 0 should be restored.
This bug is about why I see 'message"": ""No valid host was found. "", ""code"": 500, ""created"": ""2014-06-17T07:35:44Z""'
when ""Flavor's disk is too small for requested image'"" was raised on the n-cpu side.
It is confusing, 'No valid host was found' type of messages sounds like there is no  n-cpu running, or all of them full.
instance: f62b56da-d1fa-4dc2-ae37-42b8fde3d3a5] Instance failed to spawn
 Traceback (most recent call last):
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 2064, in _build_resources
     yield resources
   File ""/opt/stack/new/nova/nova/compute/manager.py"", line 1966, in _build_and_run_instance
     block_device_info=block_device_info)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2233, in spawn
     admin_pass=admin_password)
   File ""/opt/stack/new/nova/nova/virt/libvirt/driver.py"", line 2607, in _create_image
     project_id=instance['project_id'])
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 182, in cache
     *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 374, in create_image
     prepare_template(target=base, max_size=size, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/openstack/common/lockutils.py"", line 249, in inner
     return f(*args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/imagebackend.py"", line 172, in fetch_func_sync
     fetch_func(target=target, *args, **kwargs)
   File ""/opt/stack/new/nova/nova/virt/libvirt/utils.py"", line 658, in fetch_image
     max_size=max_size)
   File ""/opt/stack/new/nova/nova/virt/images.py"", line 110, in fetch_to_raw
     raise exception.FlavorDiskTooSmall()
 FlavorDiskTooSmall: Flavor's disk is too small for requested image.","Catch image and flavor exceptions in _build_and_run_instance

If a user selects a flavor with a disk size that is smaller than
the requested image, a FlavorDiskTooSmall exception is thrown.
However, this exception is not specifically caught in
_build_and_run_instance, but is caught as generic exception,
causing a RescheduledException to be thrown. This forces the
instance to be rescheduled a few times before finally throwing
a NoValidHost exception. It causes confusion because the actual
cause of it is that the disk size is smaller than the requested
image. The following patch properly catches exceptions thrown by
_check_requested_image in _build_and_run_instance.

Change-Id: Ic8b435ba7e8721aa8ca54f29c1ad808833a7608d
Closes-Bug: #1330856"
1592,daecbdfdebaf89163f6eaf6b72a4cde2322f1b08,1384340096,,1.0,48,69,3,2,1,0.629299672,True,4.0,656031.0,57.0,13.0,False,6.0,7191465.667,24.0,5.0,170.0,700.0,764.0,169.0,653.0,717.0,111.0,310.0,340.0,0.207792208,0.576994434,0.632653061,1756,1250767,1250767,neutron,daecbdfdebaf89163f6eaf6b72a4cde2322f1b08,0,0,"Refactoring, it is not a bug “Move Loadbalancer Noop driver to the unit tests”","Bug #1250767 in neutron: ""LBaaS: Move Noop driver to unit tests""","As discussed in lbaas design session at the summit, it's better to move noop driver to unit tests folder to clearly indicate that this driver is not for production and should not be listed in providers.
This should avoid possible confusion for Neutron deployers.","Move Loadbalancer Noop driver to the unit tests

Move Loadbalancer Noop driver to the corresponding folder in
unit tests to clearly indicate that it is an utility driver
that only suitable for unit testing.

Closes-Bug: #1250767
Change-Id: Ide2ebf212193f2cf5da9cf7a7f068fd918a55ada"
1593,daedfffd95b375f00deacb43b6de5e02ddde6df4,1395172466,,1.0,2,5,2,2,1,0.985228136,True,4.0,621355.0,68.0,21.0,False,27.0,1948370.0,37.0,3.0,0.0,3014.0,3014.0,0.0,2520.0,2520.0,0.0,2906.0,2906.0,0.000131027,0.380896226,0.380896226,498,917,1286297,nova,daedfffd95b375f00deacb43b6de5e02ddde6df4,1,1,There is a bug. The decision is to delete some code,"Bug #1286297 in OpenStack Compute (nova): ""Adding current project as flavor access is throwing an error""","Steps to reproduce
1. Create a new flavor, setting and add the current project as flavor access
2. The flavor is created but an error is displayed saying ""Unable to set flavor access for project.....""
The error is thrown because Horizon creates the flavor and add the access after that. The problem is that once a private flavor is created, nova adds the current project within the flavor accesses so when Horizon tries to add the access, nova throws an ""Access already exist for this flavor"" exception","Do not add current tenant to private flavor access

Avoid adding the current tenant to the flavor access list when
a private flavor is created. In ordeir to add tenants to the
flavor access list we should use the add_tenant api.

Tempest has to be updated accordingly:
https://review.openstack.org/81551

Documentation has to be updated as well:
https://review.openstack.org/82175

Partially (just for the V2 API rather than V2 and V3)
reverts commit 6ba248635b70860a44e486e7739efa4cc6612ce6

Fixes unittest which was added in the original commit so it checks
for the behaviour we have now rather than the behaviour after the
backwards incompatible change which is being reverted.

Change-Id: I731081b6df0d96df1bc1763d214d28c62bbbb51c
Closes-Bug: #1286297"
1594,daf278c0c8b6cc917b84f514fec65758adf17028,1413486512,,1.0,59,3,5,4,1,0.880580248,False,,,,,True,,,,,,,,,,,,,,,,,1074,1523,1343200,nova,daf278c0c8b6cc917b84f514fec65758adf17028,0,0,Feature. “We should add notification for server group operations.”,"Bug #1343200 in OpenStack Compute (nova): ""Add notifications when operating server groups""","Currently, there is no notifications when operating server groups, such as create/delete/update etc. This caused 3rd party cannot know the operation result on time.
We should add notification for server group operations.","Add notification for server group operations

Currently, there is no notification when create/update/delete
server groups, this caused some 3rd party client cannot know
when those operations are finished.

The fix was adding notifications for those operations.

DocImpact
The wiki page https://wiki.openstack.org/wiki/SystemUsageData
needs to be updated once this patch was merged.

Change-Id: Ia1f4a6752668ddb73ebb8faf525b58a28d183754
Closes-Bug: #1343200"
1595,dafe048f2f36d221242036c159271d7544ad39e3,1411460330,,1.0,7,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1344,1812,1372454,cinder,dafe048f2f36d221242036c159271d7544ad39e3,0,0,Feature “we need to to check the value of the configuration item”,"Bug #1372454 in Cinder: ""check the value of the configuration item eqlx_cli_max_retries""","we need to to check the value of the configuration item glance_num_retries in the code in order to ensure the ""eqlx_cli_max_retries "" equal or bigger than 0","check the configuration eqlx_cli_max_retries

we need to to check the value of the configuration item eqlx_cli_max_retries
in the code in order to ensure the ""eqlx_cli_max_retries"" is equal to or
greater than 0

DocImpact: The 'retries' is not a configured number of attempts
Change-Id: If9fadda83a855b4bbda6129d3b3a64d296eb2b54
Closes-Bug: #1372454"
1596,dafe6598c70e74781053acf68a8d5a7cea064a0e,1395088790,,1.0,2,2,1,1,1,0.0,True,1.0,175893.0,30.0,2.0,False,1.0,417142.0,12.0,2.0,49.0,992.0,997.0,49.0,819.0,824.0,41.0,365.0,370.0,0.042639594,0.371573604,0.376649746,638,1064,1293799,neutron,dafe6598c70e74781053acf68a8d5a7cea064a0e,1,1,Bug. Change order,"Bug #1293799 in neutron: ""BigSwitch consistency watchdog launched too soon""","The BigSwitch consistency watchdog is launched too soon during initialization.
It's launched before the server pool has added the servers so by the time the greenthread initializes, the pool is not fully populated.","BigSwitch: Watchdog thread start after servers

Start the watchdog thread after the servers have
been initialized.

Closes-Bug: #1293799
Change-Id: Ic7ae1f55bd7faed6edde84d9d0b52fc8fe7a1fc1"
1597,db0831ec96a6806035c51d94ade1e51eafb42162,1386149376,,1.0,42,5,4,4,1,0.981945244,True,5.0,5181791.0,31.0,12.0,False,184.0,4399994.0,939.0,7.0,4.0,3172.0,3174.0,4.0,2419.0,2421.0,2.0,2924.0,2925.0,0.000442674,0.431606906,0.431754464,1695,1240374,1240374,nova,db0831ec96a6806035c51d94ade1e51eafb42162,1,1, ,"Bug #1240374 in OpenStack Compute (nova): ""some times availability-zone not take effect after it created""","1. create a availability-zone: second_zone, add ns17 to the aggregate
[root@ns11 nova]# nova availability-zone-list
+----------------------------------+----------------------------------------+
| Name                             | Status                                 |
+----------------------------------+----------------------------------------+
| first_zone                       | available                              |
| |- ns11.sce.cn.ibm.com           |                                        |
| | |- nova-compute                | enabled :-) 2013-09-25T09:51:54.352341 |
| second_zone                      | available                              |
| |- ns17-osee.cn.ibm.com          |                                        |
| | |- nova-compute                | enabled :-) 2013-09-25T09:51:58.283034 |
+----------------------------------+----------------------------------------+
2. nova boot with the zone:
[root@ns11 ˜]# nova boot --image d7c64f67-3de7-4aa0-88fb-1292ff520404 --flavor 1 --availability-zone second_zone:ns17-osee.cn.ibm.com test_zone17_2
3.  [root@ns11 nova]# nova list
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| ID                                   | Name            | Status  | Task State  | Power State | Networks               |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
| 6f3ec942-6c8e-46fa-ad7e-d2c900921f65 | test_zone17     | ACTIVE  | None        | Running     | flat_net01=172.10.0.16 |
| 7273973d-a9e1-4917-ad59-c54ece9b0765 | test_zone17_1   | ACTIVE  | None        | Running     | flat_net01=172.10.0.18 |
| 27f71b26-b775-4516-ba2a-13d9e91e96dd | test_zone17_2   | ACTIVE  | None        | Running     | flat_net01=172.10.0.19 |
| 6350a439-2dd3-4b14-b99e-219fad27111b | zhikun_testvm   | ACTIVE  | None        | Running     | flat_net01=172.10.0.13 |
+--------------------------------------+-----------------+---------+-------------+-------------+------------------------+
4.  but the zone of the vm is: nova
[root@ns11 nova]# nova show 27f71b26-b775-4516-ba2a-13d9e91e96dd
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| status                               | ACTIVE                                                   |
| flat_net01 network                   | 172.10.0.19                                              |
| updated                              | 2013-09-25T09:50:23Z                                     |
| OS-EXT-STS:task_state                | None                                                     |
| OS-EXT-SRV-ATTR:host                 | ns17-osee.cn.ibm.com                                     |
| key_name                             | None                                                     |
| image                                | cirros_kvm (d7c64f67-3de7-4aa0-88fb-1292ff520404)        |
| accessIPv6                           |                                                          |
| progress                             | 0                                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-AZ:availability_zone          | nova                                                     |
| config_drive                         |                                                          |
+--------------------------------------+----------------------------------------------------------+
[root@ns11 nova]# date
Wed Sep 25 18:05:26 CST 2013
5.  I did not configure default_zone in nova.conf
6.  It is very strange that this problem could not always been recreated. usually it happens when we create a new AZ, and then use the AZ boot instance. Seems wait for some time, may be one night or sooner, the AZ will become to correct one. I'm confused.
# Expected  results
The AZ should take effect any time if we did not set default AZ in nova.conf","Fixing availability-zone not take effect error

when add/remove a host to one aggregate or update aggregate
metadata incluing availability_zone, ""OS-EXT-AZ:availability_zone""
property of some instances in the host can not show correctly.

The cause is that when getting availability_zone of one instance,
it will try to get the value from cache first, but unfortunately
the cache does not update or reset in time, and it will keep one
hour if we do not change it.

This patch will add update or reset after adding/removing a host
to one aggregate or updating aggregate metadata including
availability_zone.

Change-Id: I5dd07f876471b5faf8fb1016e25a861124b7cb6f
Closes-bug: #1240374"
1598,db0daa3455f114e90cefbe02e205ca1d327fc45c,1381934307,,1.0,56,17,2,2,1,0.98900768,True,2.0,512681.0,14.0,7.0,False,127.0,101751.0,362.0,5.0,11.0,1354.0,1357.0,11.0,1324.0,1327.0,11.0,1321.0,1324.0,0.001891253,0.208353034,0.208825847,1698,1240542,1240542,nova,db0daa3455f114e90cefbe02e205ca1d327fc45c,1,1, ,"Bug #1240542 in OpenStack Compute (nova): ""libvirt: the None value for the 'os_type' property is passed to glance  when doing a snapshot""","This change: https://review.openstack.org/42877 introduced the ability to pass os_type property to glance when we are doing a snapshot, but the change doesn't manage correctly the None value for this property.
The result of that is that we have a porperty called 'os_type': None associated with the snapshotted image.
That is a problem as this property is used when we create the backing file for the ephemeral disks.
From nova/virt/libvirt/driver/py:
 # Lookup the filesystem type if required
 os_type_with_default = instance['os_type']
 if not os_type_with_default:
     os_type_with_default = 'default'
and then we create the ephemeral file name using this rule:
fname = ""ephemeral_%s_%s"" % (ephemeral_gb, os_type_with_default)
Now consider an instance with a flavor with an ephemeral disk of 50 G and without the os_type defined, the resulting backing file will be:
""ephemeral_50_default""
At this point if we create a snapshot from that instance and we boot from it we will have another backing file created and called:
""ephemeral_50_None""
That is bad for at least two reasons:
1 performance issue: we need to create a new backing file which is not actually necessary
2 resources leaking: those backing files consume disk capacity on the host","Manage None value for the 'os_type' property

The I2bab617f6c1e8866650ca7dc5178b640738c9ae6 introduced a way to pass the
'os_type' property to glance when we are doing a snapshot in the libvirt
driver. Currently the code doesn't manage properly the None value for this
property and so None value is passed to glance which creates an image with
a property called 'os_type' with value 'None'.
As results of that all instances booted from a snapshot with this property will
have None value in the os_type field.
That field is used to create file name for backing images for ephemeral disks.
Now consider this scenario:
we boot an instance from an image without the os_type property defined and
with an ephemeral disk of 50 G
the resulting backing file will be called ephemeral_50_default.
At this point we take a snapshot from that instance and we boot a new instance
using this snapshot, and because of this bug we will have a new backing file
called ephemeral_50_None.
That is bad because is leaking resources in the host.

Please note that I did a small refactor of the code to make my change testable.

Change-Id: I40bca2732ee2c911344e9d361612007da1c7c3cf
Closes-bug: #1240542"
1599,db3088b888e9560df93ff608b1e45a28fac14b6f,1380324161,,1.0,10,1,2,2,1,0.845350937,True,2.0,435086.0,21.0,8.0,False,15.0,905677.0,30.0,2.0,9.0,1019.0,1024.0,9.0,907.0,912.0,8.0,881.0,885.0,0.008571429,0.84,0.843809524,1614,1232287,1232287,cinder,db3088b888e9560df93ff608b1e45a28fac14b6f,1,1,“that bootable attribute is missing from:”,"Bug #1232287 in Cinder: ""v2 api does not show bootable attr on ""cinder  list""""","I switched to use v2 api and noticed ""cinder list"" does not return value of bootable flag on volumes.
v2 api does show bootable.
I believe the error is that bootable attribute is missing from:
cinder.api.v2.views.volume.ViewBuilder.detail","v2 api - return bootable attr value on volume list

Add bootable to the list of values returned by display_list.  This
was returned in v1 api, and is still in v2 header, but was missed
in the new implementation.

Closes-Bug: #1232287

Change-Id: If7460b1c8ab4af417117c4bf6cfdccc5fcf21f46"
1600,db5e370b0d68c3e71626c99941fe487059b3cf88,1404131708,,2.0,34,10,5,3,1,0.845207711,False,,,,,True,,,,,,,,,,,,,,,,,1379,1849,1377346,neutron,db5e370b0d68c3e71626c99941fe487059b3cf88,0,0,“ML2: Invalid unit test case”,"Bug #1377346 in neutron: ""ML2: Invalid unit test case""","In test_create_network_multiprovider() , an invalid comparison is used ...
tz = network['network'][mpnet.SEGMENTS]
for tz in data['network'][mpnet.SEGMENTS]:  <=== tz from previous statement is lost
   for field in [pnet.NETWORK_TYPE, pnet.PHYSICAL_NETWORK,
                         pnet.SEGMENTATION_ID]:
        self.assertEqual(tz.get(field), tz.get(field))  <===== this is always true","Schema enhancement to support MultiSegment Network

Description:
Currently, there is nothing in the schema that ensures segments
for a network are returned in the same order they were specified
when the network was created, or even in a deterministic order.

Solution:
We need to add another field named 'segment_index' in
'ml2_network_segment' table containing a numeric position index.
With segment_index field we can retrieve the segments in the
order in which user created.

This patch set also fixes ML2 invalid unit test case in
test_create_network_multiprovider().

Closes-Bug: #1224978
Closes-Bug: #1377346

Change-Id: I560c34c6fe1c5425469ccdf9b8b4905c123d496d"
1601,db5e370b0d68c3e71626c99941fe487059b3cf88,1404131708,,2.0,34,10,5,3,1,0.845207711,False,,,,,True,,,,,,,,,,,,,,,,,1554,1224978,1224978,neutron,db5e370b0d68c3e71626c99941fe487059b3cf88,0,0,"“With introduction of multi-segment support in ml2 plugin, agent misconfiguration could happen under exceptionnal circumstances:” Feature or bug3","Bug #1224978 in neutron: ""port binding on multi segment networks could lead to agent misconfiguration""","With introduction of multi-segment support in ml2 plugin, agent misconfiguration could happen under exceptionnal circumstances:
Supposing a multi-segment provider network is created with different network types (suppose a flat and a vlan segment), and two ports are bound on an agent supporting the associated physical network.
Portbinding validation will occur on network's segments list returned by db.get_network_segments funtion . As this function may not always returns segments in the same order, one port may be bound to the flat segment while the other will be bound to vlan one.
In that case, ports wouldn't be properly plugged in agent, as they'd recieve two contradictory segment details for the same network.
OVS agent would probably bound the two ports within the same segment as they both would use the same LocalVLANMapping
LB agent would probably add two uplink interfaces under the same qbr[net-id] bridge","Schema enhancement to support MultiSegment Network

Description:
Currently, there is nothing in the schema that ensures segments
for a network are returned in the same order they were specified
when the network was created, or even in a deterministic order.

Solution:
We need to add another field named 'segment_index' in
'ml2_network_segment' table containing a numeric position index.
With segment_index field we can retrieve the segments in the
order in which user created.

This patch set also fixes ML2 invalid unit test case in
test_create_network_multiprovider().

Closes-Bug: #1224978
Closes-Bug: #1377346

Change-Id: I560c34c6fe1c5425469ccdf9b8b4905c123d496d"
1602,db5ea5c705757ca20a5789839f96518877c91820,1392606248,0.0,1.0,65,6,5,3,1,0.864577972,True,8.0,2846986.0,142.0,15.0,False,2.0,912226.4,5.0,3.0,14.0,1425.0,1427.0,14.0,1243.0,1245.0,13.0,627.0,628.0,0.017177914,0.770552147,0.771779141,129,528,1257225,neutron,db5ea5c705757ca20a5789839f96518877c91820,1,0,Bug in a duplicate name. Evolution of the project?,"Bug #1257225 in neutron: ""duplicate name of LBaaS obj is not allowed on vShield Edge""","Duplicate name of LBaaS obj(pool/vip/app_profile) is not allowed on vShield Edge, so here need a naming convention to ensure name uniqueness on the edge side.","Fix duplicate name of NVP LBaaS objs not allowed on vShield Edge

Duplicate name of LBaaS objs such as pool/vip/app_profile is not
allowed on vShield Edge, so here a name convention is needed to ensure
name uniqueness on the edge side.
Closes-Bug: #1257225

Change-Id: I953610d0389a78aa01f378c2ff4931d8c74413ea"
1603,db68dc8670fc8943dfc142ebb0f3f1f405c04e4b,1381354705,,1.0,9,9,2,2,1,0.764204507,True,3.0,383195.0,22.0,14.0,False,40.0,343615.5,95.0,4.0,156.0,3659.0,3709.0,156.0,3056.0,3106.0,142.0,2604.0,2645.0,0.022839802,0.416067721,0.422616195,1661,1237622,1237622,nova,db68dc8670fc8943dfc142ebb0f3f1f405c04e4b,1,1,Refactoring “is not actually implementing get_diagnostics”,"Bug #1237622 in OpenStack Compute (nova): ""vmware driver is not actually implementing get_diagnostics""","According to the hypervisor support matrix the vmware driver doesn't support the diagnostics API:
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
But the code suggests otherwise:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L268
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1263
There is an unused get_diagnostics method in vmwareapi.vmops though:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L1284
i'm guessing that was stubbed out before the get_info method was added and someone connected the dots to the vmwareapi driver code to use it.
We should remove the unused get_diagnostics method.
Note that there aren't actually really any unit tests associated with the vmwareapi diagnostics API support though so we should add that in when we remove the unused method.","Fix vmwareapi driver get_diagnostics calls

The vmware driver is calling vmops.get_info for both the get_info and
get_diagnostics APIs which is wrong since they should be returning
different information.  Given the hypervisor support matrix doesn't list
the vmware driver as supporting the nova diagnostics API, it seems this
was just an oversight.

This patch changes the driver code to call the vmops.get_diagnostics
method which raises NotImplementedError.  It also fixes the tests to
actually ensure covers get_diagnostics for both the ESX driver and the
VC driver.

Closes-Bug: #1237622

Change-Id: I0c9015f1f7d5dd219548196de479e87e878ea0fd"
1604,dba196702b43f90928a644bdafaf42c97b71267f,1394137336,,1.0,49,14,5,3,1,0.78618727,True,6.0,1517955.0,122.0,19.0,False,6.0,340545.6,12.0,5.0,491.0,1230.0,1460.0,400.0,1062.0,1231.0,234.0,709.0,770.0,0.258241758,0.78021978,0.847252747,515,935,1287432,neutron,dba196702b43f90928a644bdafaf42c97b71267f,0,0,No bug. ‘ should be renamed’,"Bug #1287432 in neutron: ""nec plugin: quantum_id column in ID mapping tables shoudl be renamed""","ID mapping tables in NEC plugin have columns named ""quantum_id"". Following renaming to Neutron, they should be renamed.","NEC plugin: Rename quantum_id column to neutron_id

ID mapping tables in NEC plugin had columns named quantum_id.
This commit renames them to neutron following project renaming.

Closes-Bug: #1287432
Change-Id: I0669defba7189d5b8259365d88d67db51a28c764"
1605,dba898226ec26ab49cecd24e9d6c255cf6153cd1,1394548831,,2.0,27,26,2,2,1,0.999743186,True,8.0,1250896.0,74.0,23.0,False,160.0,102051.0,498.0,6.0,7.0,2890.0,2894.0,7.0,2301.0,2305.0,4.0,2525.0,2526.0,0.000661288,0.334082793,0.334215051,579,1000,1290487,nova,dba898226ec26ab49cecd24e9d6c255cf6153cd1,0,0,potentially causing threading issues,"Bug #1290487 in OpenStack Compute (nova): ""Libvirt native thread used for ""forbidden"" purposes""","In the nova.virt.libvirt.driver.LibvirtDriver. _get_new_connection method  two different libvirt event handlers are registered, one for lifecycle events (_event_lifecycle_callback) and one for connection events (_close_callback).  These callbacks are called by a native thread that is continually calling libvirt.virEventRunDefaultImpl() in the _native_thread method; the latter method's Docstring contains the following note:
        This is a native thread which runs the default
        libvirt event loop implementation. This processes
        any incoming async events from libvirtd and queues
        them for later dispatch. This thread is only
        permitted to use libvirt python APIs, and the
        driver.queue_event method. In particular any use
        of logging is forbidden, since it will confuse
        eventlet's greenthread integration
while this rule is adhered to by the _event_lifecycle_callback method, it is violated by _close_callback (the other callback) because it calls the _set_host_enabled method which, among other things, writes to the log.
The _close_callback method needs to be altered so that it does not execute any logic that may interfere with eventlet's greenthread integration.","Change libvirt close callback to use green thread

The native thread that calls the close callback is being
used to update a host's status, potentially causing threading
issues, since it involves operations that should only be
performed by green threads. This change fixes the issue
by limiting the native thread's interaction to adding data
to a queue and leaving the remaining work for
a green thread.

Closes-Bug: #1290487
Closes-Bug: #1293641

Change-Id: I59c14da15e4655dc4bf21d4de3d509472b146f7a"
1606,dba898226ec26ab49cecd24e9d6c255cf6153cd1,1394548831,,2.0,27,26,2,2,1,0.999743186,True,8.0,1250896.0,74.0,23.0,False,160.0,102051.0,498.0,6.0,7.0,2890.0,2894.0,7.0,2301.0,2305.0,4.0,2525.0,2526.0,0.000661288,0.334082793,0.334215051,635,1061,1293641,nova,dba898226ec26ab49cecd24e9d6c255cf6153cd1,1,1,deadlock,"Bug #1293641 in OpenStack Compute (nova): ""Libvirt's close callback causes deadlocks""","Libvirt's close callback causes deadlocks
Unlike libvirt's lifecycle event callback which is triggered every time an event occurs, the close callback is only triggered when an attempt is made to use a connection that has been closed.  In that case, the sequence of events is usually as follows:
LibvirtDriver._get_connection acquires _wrapped_conn_lock
LibvirtDriver._get_connection calls _test_connection
LibvirtDriver._test_connection calls libvirt.getLibVersion
libvirt.getLibVersion triggers LibvirtDriver._close_callback (because the connection is closed and this method is the registered handler)
LibvirtDriver._close_callback attempts to acquire _wrapped_conn_lock
_get_connection cannot release the lock because it is waiting for _close_callback to return and the latter cannot complete until it has acquired the lock.
Making the handling of the close callback asynchronous (like the lifecycle event handling) won't work, because by the time the lock is released, the connection object that was passed into the callback will no longer be equal to LibvirtDriver._wrapped_conn.  Even if the connection object is ignored, the instance will have already been disabled via the _get_new_connection method's existing error-handling logic.
The best solution would appear to be to simply not register a close callback.  The only case where it might provide some benefit is when a connection is closed after _get_connection has returned a reference to it.  The benefit of disabling the instance a little earlier in such marginal cases is arguably outweighed by the complexity of a thread-safe implementation, especially when the difficulty of testing such an implementation (to ensure it is indeed thread safe) is taken into consideration.
Note that having _close_callback use _wrapped_conn_lock.acquire(False) instead of ""with _wrapped_conn_lock"" by itself is not a viable solution, because due to connections being opened via tpool.proxy_call, the close callback is called by a native thread, which means it should not be used to perform the various operations (including logging) involved in disabling the instance.","Change libvirt close callback to use green thread

The native thread that calls the close callback is being
used to update a host's status, potentially causing threading
issues, since it involves operations that should only be
performed by green threads. This change fixes the issue
by limiting the native thread's interaction to adding data
to a queue and leaving the remaining work for
a green thread.

Closes-Bug: #1290487
Closes-Bug: #1293641

Change-Id: I59c14da15e4655dc4bf21d4de3d509472b146f7a"
1607,dbab8843ef33e0bb6745ae3ad5951fd86446c1a3,1408725187,,1.0,3,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1087,1537,1346466,cinder,dbab8843ef33e0bb6745ae3ad5951fd86446c1a3,1,1,,"Bug #1346466 in Cinder: ""Olso messaging API error with publish_errors set to true""","I observed that when you have publish_error enabled and and the notification driver set as below:
notification_driver = cinder.openstack.common.notifier.rpc_notifier
publish_errors = true
You will get the following error when a purposely causing a error in cinder(for example, create volume from image and set the volume size something small that the image would not fit in)
2014-07-21 11:46:51.527 ERROR oslo.messaging.rpc.dispatcher [req-e3015168-bf95-4cea-af97-b13ed9edc141 231fdb65b4134c9b864767e851ad72db 5304a70aac9540a5b445cb6a6f62c917] Exception during message handling: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 337, in create_volume
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     _run_flow()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 330, in _run_flow
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     flow_engine.run()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 89, in run
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     for _state in self.run_iter():
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self._change_state(states.FAILURE)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/openstack/common/excutils.py"", line 82, in __exit__
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/engine.py"", line 120, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     for state in runner.run_iter(timeout=timeout):
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/runner.py"", line 130, in run_iter
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     misc.Failure.reraise_if_any(failures)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 788, in reraise_if_any
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     failures[0].reraise()
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/utils/misc.py"", line 795, in reraise
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     six.reraise(*self._exc_info)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/taskflow/engines/action_engine/executor.py"", line 48, in _revert_task
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     result = task.revert(**kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/flows/manager/create_volume.py"", line 193, in revert
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     LOG.error(_(""Volume %s: create failed""), volume_id)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1449, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.logger.error(msg, *args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self._log(ERROR, msg, args, **kwargs)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.callHandlers(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     hdlr.handle(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     self.emit(record)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher   File Traceback (most recent call last):
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 346, in fire_timers
    timer()
  File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/timer.py"", line 56, in __call__
    cb(*args, **kw)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 197, in main
    self._resolve_links()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 212, in _resolve_links
    f(self, *ca, **ckw)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/_executors/impl_eventlet.py"", line 47, in complete
    thread.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
    return self._exit_event.wait()
  File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 120, in wait
    current.throw(*self._exc)
  File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
    result = function(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 129, in <lambda>
    yield lambda: self._dispatch_and_reply(incoming)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 143, in _dispatch_and_reply
    exc_info=exc_info)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1178, in error
    self._log(ERROR, msg, args, **kwargs)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1271, in _log
    self.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1281, in handle
    self.callHandlers(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 1321, in callHandlers
    hdlr.handle(record)
  File ""/usr/lib/python2.7/logging/__init__.py"", line 749, in handle
    self.emit(record)
  File ""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
    dict(error=record.msg))
TypeError: info() takes exactly 4 arguments (3 given)""/opt/stack/cinder/cinder/openstack/common/log_handler.py"", line 29, in emit
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher     dict(error=record.msg))
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher TypeError: info() takes exactly 4 arguments (3 given)
2014-07-21 11:46:51.527 TRACE oslo.messaging.rpc.dispatcher
I see that in /opt/stack/cinder/cinder/openstack/common/log_handler.py that that the notifier.info(...) code(line #28) is not passing in the context as the first parameter as expected in the API  thus the takes 4 or 3 given error above.
rpc.get_notifier('error.publisher').info('error_notification',  dict(error=record.msg))","Pass an empty context to the notifier

The notifier API expects a context as a first parameter. During the port
to oslo.messaging, a bug was introduced here and the first argument in
the notifier call was removed. This patch puts it back.

Change-Id: Id2f47757ee4af8f8c39a8a1e6b11dcad163f5b7b
Closes-bug: #1346466"
1608,dbb7ef845f5684673b320d0914d4f9cea12fef65,1401361656,0.0,1.0,46,5,5,2,1,0.746949225,False,,,,,True,,,,,,,,,,,,,,,,,1415,1100697,1100697,nova,dbb7ef845f5684673b320d0914d4f9cea12fef65,1,0,“Nova doesn't enable pae setting for Xen or KVM guest in its libvirt driver. Windows(Win7 in my enviroment)” A change in the Environment,"Bug #1100697 in OpenStack Compute (nova): ""libvirt should enable pae setting for Xen or KVM guest""","Currently, nova doesn't enable pae setting for Xen or KVM guest in its libvirt driver. Windows(Win7 in my enviroment) guests would not boot successful in such case. This patch adds pae setting in libvirt driver for Xen or KVM guest, which would fix this problem.","libvirt: set pae for Xen PVM and HVM

The Physical Address Extension (PAE) enables paging to produce physical
addresses with more that 32 bit (2^32 bytes, 4 GBytes), namely it
extends the physical address space to 64 GBytes (2^36 bytes).

When spawning a 32 bit image in Xen PVM or HVM, PAE should be enabled in the
hypervisor so as to ensure that it is able to run properly. According to
the Xen documentation it can be enabled safely: ""In general you should
leave this enabled and allow the guest Operating System to choose whether
or not to use PAE. "" [1]

[1] http://xenbits.xen.org/docs/unstable/man/xl.cfg.5.html#pae_boolean

Closes-Bug: #1100697
Change-Id: If8d8ebc38efd969d96a1953446d72e72691734b3"
1609,dbc46b99cc85402c087bb214c120bce2c65dfea9,1408446350,,1.0,5,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1241,1700,1362233,nova,dbc46b99cc85402c087bb214c120bce2c65dfea9,1,1,,"Bug #1362233 in OpenStack Compute (nova): ""instance_create() DB API method implicitly creates additional DB transactions""","In DB API code we have a notion of 'public' and 'private' methods. The former are conceptually executed within a *single* DB transaction and the latter can either create a new transaction or participate in the existing one. The whole point is to be able to roll back the results of DB API methods easily and be able to retry method calls on connection failures. We had a bp (https://blueprints.launchpad.net/nova/+spec/db-session-cleanup) in which all DB API have been re-factored to maintain these properties.
instance_create() is one of the methods that currently violates the rules of 'public' DB API methods and creates a concurrent transaction implicitly.","Add _security_group_ensure_default() DBAPI method

According to the convention we use for DB API methods, every public
method should have a private counterpart. The difference between
those two is that the latter accepts an optional 'session' argument,
which makes it possible for the method to participate in the existing
database transaction.

security_group_ensure_default() was called inside instance_create()
and thus implicitly created a new concurrent db transaction, which
is now fixed by making a call to a private counterpart and passing
the session instance.

Closes-Bug: #1362233

Change-Id: Ifa1b6430328fde1ab0984f726bd8392bbd3f7ee5"
1610,dbdb938fc08452ea80379652ee101b6ebe006e3f,1393796627,,1.0,18,1,3,3,1,0.835055451,True,6.0,3378844.0,49.0,14.0,False,78.0,370178.6667,232.0,3.0,11.0,1539.0,1547.0,9.0,1474.0,1481.0,6.0,1505.0,1511.0,0.000934579,0.201068091,0.201869159,1559,1226171,1226171,nova,dbdb938fc08452ea80379652ee101b6ebe006e3f,1,0,“Update user_id length to match Keystone schema “,"Bug #1226171 in OpenStack Compute (nova): ""When using per-domain-identity backend, user_ids could collide""","When using the per-domain-identity backend usernames could end up colliding when multiple LDAP backends are used since we extract very limited information from the DN.
Example
cn=example user, dc=example1,dc=com
cn=example user, dc=example2,dc=com
Would net the same ""user_id"" of ""example user""
This can also affect groups in the same manner.","Update user_id length to match Keystone schema in volume_usage_cache

Update the maximum length to match the Keystone schema for maximum
user_id length in volume_usage_cache. In the case that Keystone were
to leverage the full varchar 64, there would potentially be data loss
and/or collision of user_ids across records.

This change originates from the conversation about increasing the
length of user_ids in Keystone:

http://lists.openstack.org/pipermail/openstack-dev/2014-February/028125.html

Even with the length not changing, it is important to ensure there is no
loss of data/resolution when relying on user_id to identify users uniquely.

Closes-Bug: #1226171
Change-Id: I05a5644a29d6e2432311c2ee5331970d5e8b0683"
1611,dc02810c5dc64ca48a163645588ea6a60367227a,1391703417,,1.0,2,2,1,1,1,0.0,True,1.0,486073.0,10.0,5.0,False,62.0,563000.0,129.0,2.0,0.0,815.0,815.0,0.0,750.0,750.0,0.0,785.0,785.0,0.00071736,0.56384505,0.56384505,374,787,1277155,cinder,dc02810c5dc64ca48a163645588ea6a60367227a,0,0,tests,"Bug #1277155 in Cinder: ""LVM migrate volume tests are not testing correct code""","cinder/tests/test_volume.py
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_driver
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_host
The location_info is incorrect, it needs to be 5 sections separated by colons,  which means that the following condition does not get tested
691         if (dest_type != 'LVMVolumeDriver' or dest_hostname != self.hostname):
692             return false_ret","Fix LVM migrate_volume tests

In cinder/tests/test_volume.py the following tests fail to correctly
test the functionality.

LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_driver
LVMISCSIVolumeDriverTestCase.test_lvm_migrate_volume_diff_host

The location_info is incorrect, it needs to be 5 sections separated by
colons otherwise they fail due to bad location info.

Change-Id: Ie7ceaba6b35c0aedb47cae0db5c60cc489e94c49
Closes-Bug: #1277155"
1612,dc4655a5ae5a2ad576b68f3bc7d5044e395ee49e,1391039633,,1.0,1,1,1,1,1,0.0,True,1.0,42344.0,5.0,2.0,False,15.0,1218259.0,52.0,2.0,184.0,674.0,794.0,177.0,595.0,712.0,174.0,634.0,745.0,0.127644055,0.463165573,0.544128373,340,751,1274334,cinder,dc4655a5ae5a2ad576b68f3bc7d5044e395ee49e,1,1,typo in code,"Bug #1274334 in Cinder: ""3PAR manually created host detach KeyError: 'WWN' hp_3par_common.py""","how to duplicate
Manually creating host in 3PAR IMC with wwn from host
create volume
Attach volume
detach volume
2014-01-29 15:09:47.582 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got semaphore ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:191
2014-01-29 15:09:47.583 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Attempting to grab file lock ""3par"" for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/lockutils.py:202
2014-01-29 15:09:47.584 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Got file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/common/loc
kutils.py:232
2014-01-29 15:09:47.584 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Connecting to 3PAR from (pid=6822) client_login /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:178
2014-01-29 15:09:48.021 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5
657274455fcad] Disconnect from 3PAR from (pid=6822) client_logout /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:189
2014-01-29 15:09:48.022 DEBUG cinder.openstack.common.lockutils [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455f
cad] Released file lock ""3par"" at /opt/stack/data/cinder/cinder-3par for method ""terminate_connection""... from (pid=6822) inner /opt/stack/cinder/cinder/openstack/commo
n/lockutils.py:239
2014-01-29 15:09:48.022 ERROR cinder.openstack.common.rpc.amqp [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455fc
ad] Exception during message handling
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/utils.py"", line 821, in wrapper
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     self.driver.terminate_connection(volume_ref, connector, force=force)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     wwn=connector['wwpns'])
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connec
tion
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     hostname = self._get_3par_hostname_from_wwn_iqn(wwn, iqn)
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostna
me_from_wwn_iqn
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp     if wwn == fc['WWN']:
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp KeyError: 'WWN'
2014-01-29 15:09:48.022 TRACE cinder.openstack.common.rpc.amqp
2014-01-29 15:09:48.024 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] Returning exception 'WWN' to caller
2014-01-29 15:09:48.025 ERROR cinder.openstack.common.rpc.common [req-f29fe230-471a-4821-8849-c4e895d8a204 77447ba13f024c8db5f0b94617e890c4 e6e6663adee94511ac5657274455
fcad] ['Traceback (most recent call last):\n', '  File ""/opt/stack/cinder/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data\n    **args)\n', '  File ""/op
t/stack/cinder/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', '  File ""/opt/stack/cinder/ci
nder/utils.py"", line 821, in wrapper\n    return func(self, *args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 660, in terminate_connection
\n    self.driver.terminate_connection(volume_ref, connector, force=force)\n', '  File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner\n
 retval = f(*args, **kwargs)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 226, in terminate_connection\n    wwn=connector[\'wwpns\']
)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1024, in terminate_connection\n    hostname = self._get_3par_hostname_from_wwn_iq
n(wwn, iqn)\n', '  File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1010, in _get_3par_hostname_from_wwn_iqn\n    if wwn == fc[\'WWN\']:\n'
, ""KeyError: 'WWN'\n""]","Fixes incorrect key in dictionary

This is a corner case for the 3PAR driver.
The dictionary has a key of 'wwn', not 'WWN'.

Change-Id: Id16a54f14d195a7297ab8db8fc8e2642955338e6
Closes-Bug: #1274334"
1613,dc48ac1a9c02c236157347f715190a2b1107ec70,1377909566,1.0,1.0,38,14,5,3,1,0.779317282,True,4.0,509935.0,26.0,11.0,False,9.0,636793.8,12.0,4.0,3.0,710.0,710.0,3.0,686.0,686.0,3.0,219.0,219.0,0.015267176,0.839694656,0.839694656,1496,1218588,1218588,neutron,dc48ac1a9c02c236157347f715190a2b1107ec70,1,1, ,"Bug #1218588 in neutron: ""Cisco N1K plugin: Network creation and Port creation dont go through when no profile id is specified.""","Plugin: Cisco plugin with N1KV.
n1kv:profile_id is mandatory and hence the network and port creation dont go through if no n1kv:profile_id parameter is passed.
This raises a network binding/ port binding error.
2013-03-15 12:45:47    DEBUG [routes.middleware] Match dict: {'action': u'index', 'controller': wsgify(quantum.api.v2.resource.resource, RequestClass=<class 'quantum.api.v2.resource.Request'>), 'format': u'json'}
2013-03-15 12:45:47    ERROR [quantum.api.v2.resource] index failed
Traceback (most recent call last):
  File ""/opt/stack/quantum/quantum/api/v2/resource.py"", line 95, in resource
    result = method(request=request, **args)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 198, in index
    return self._items(request, True)
  File ""/opt/stack/quantum/quantum/api/v2/base.py"", line 168, in _items
    obj_list = obj_getter(request.context, **kwargs)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 427, in get_networks
    self._extend_network_dict_provider(context, net)
  File ""/opt/stack/quantum/quantum/plugins/cisco/n1kv/n1k_quantum_plugin.py"", line 159, in _extend_network_dict_provider
    network[provider.NETWORK_TYPE] = binding.network_type
AttributeError: 'NoneType' object has no attribute 'network_type'
2013-03-15 12:45:47    DEBUG [eventlet.wsgi.server] 10.0.2.15 - - [15/Mar/2013 12:45:47] ""GET //v2.0/networks.json?router%3Aexternal=True HTTP/1.1"" 500 215 0.052251","Allow default network and policy profiles

Allow for default network/policy profile to be used if
no network/policy profile is specified during network/port creation
in the Cisco N1KV plugin.

Change-Id: I6120abb5abb9a869eb7310453cf27dd8f72bfd1d
Closes-Bug: #1218588"
1614,dc658273e7cfe72e50dad40203db8bb0d9bb2188,1407263795,,2.0,77,6,3,3,1,0.619624568,False,,,,,True,,,,,,,,,,,,,,,,,1127,1582,1350485,neutron,dc658273e7cfe72e50dad40203db8bb0d9bb2188,1,1,,"Bug #1350485 in neutron: ""Fix L2Pop mech driver for dvr to handle DVR interface ports""","L2Pop mech driver does not apply L2Pop rules to DVR router interfaces correctly (this might have been caused by competing changes for blueprint ofagent-l2pop)
The patch for this bug will enable DVR Router interface ports to be handled correctly in order to address the network island problem.
Network island is a situation where VMs belonging to two different networks are hosted on two different compute nodes.  The compute nodes themselves won't have VMs on the other network except for the one they are hosting.
This fix will enable DVR east-west traffic passthrough for cases where router and its interfaces are added after VMs have already been spawned/active on such routed networks.","Fix to enable L2pop to serve DVR

This change fixes the information used by the L2pop
driver to populate l2pop rules that enables DVR to
route packets across compute servers that have
tenant VMs that belong to different networks.
It also fixes the case where VMs were not able to
obtain IP Addresses when such VMs are on DVR
hosted subnets.

Change-Id: Ib630e57c186da60eb15f9ffa6b1b0bfa74f48caa
Closes-Bug: #1350485
Closes-Bug: #1352857"
1615,dc6b07d4a37ef0db9906187611fec8e8753803cd,1405405366,,1.0,36,1,2,2,1,0.909022156,False,,,,,True,,,,,,,,,,,,,,,,,1029,1473,1336207,neutron,dc6b07d4a37ef0db9906187611fec8e8753803cd,1,1,“CVE=2014-3555),"Bug #1336207 in neutron: ""[OSSA 2014-025] There is no quota for allowed address pair (CVE-2014-3555)""","Hi all,
There is no quota for allowed address pair, user can create unlimited allowed address pair, in the backend, there will be at least 1 iptables rule for one allowed address pair.  I tested if we use the attachment script to add about 10,000 allowed address pair. It will cost 30 sec to reflesh iptables rules in kernel...  I think that bad man can use this api to attack compute nodes. This will make the compute nodes crash or very slow only if we add enough allowed address pair rules...
Thanks.
Liping Mao","no quota for allowed address pair

There is no quota for allowed address pair. User can create unlimited
allowed address pairs. I add quota for allowed address pairs.

Change-Id: I2efb0c0f527f1fb22c4d4b07f6d280863f565648
Closes-Bug: #1336207"
1616,dc716bd0ce77b56f4aabe54d6633b7f3bf9b0a5d,1393423770,1.0,1.0,39,7,2,2,1,0.978070971,True,1.0,1139763.0,18.0,7.0,False,55.0,313235.0,87.0,3.0,58.0,2770.0,2794.0,57.0,2277.0,2301.0,53.0,2663.0,2682.0,0.007243461,0.357344064,0.359892689,484,902,1285209,nova,dc716bd0ce77b56f4aabe54d6633b7f3bf9b0a5d,1,1,They dont umount…. ,"Bug #1285209 in OpenStack Compute (nova): ""[libvirt] nfs and glusterfs volume drivers don't unmount their shares when detaching""","When attaching volumes from NFS or GlusterFS backends, nova mounts the share in a temporary directory, this is reused when attaching another volume from the same share so there is no need to mount it several times.
On the other hand, when disconnecting those volumes nova doesn't even try to unmount the share which may remain mounted and unused there. To clean up after detach, nova could at least try to umount the shares.","Unmount the NFS and GlusterFS shares on detach

When attaching an NFS or GlusterFS volume, nova mounts the share so it
can be reused by other instances but it is not unmounted when detaching.

This patch adds the disconnect_volume method to both drivers, which will
try to unmount the share. If a volume in that share is still being used
by another instance, the umount command will fail with the error 'target
is busy' and we will ignore it. Any other error while unmounting will be
properly logged. In any case the detach will fail because nova hasn't
been able to unmount the share.

Closes-Bug: #1285209
Change-Id: I79289524541a88ad359b3465719055cd402b4e3d"
1617,dc8de426066969a3f0624fdc2a7b29371a2d55bf,1390945131,1.0,1.0,301,45,6,4,1,0.406119257,True,18.0,188096.0,131.0,46.0,False,213.0,99489.0,704.0,3.0,452.0,2896.0,3108.0,425.0,2434.0,2627.0,430.0,2780.0,2973.0,0.059969389,0.386948657,0.413802699,1516,1221190,1221190,nova,dc8de426066969a3f0624fdc2a7b29371a2d55bf,1,1, ,"Bug #1221190 in OpenStack Compute (nova): ""[0SSA 2014-009] Image format not enforced when using rescue (CVE-2014-0134)""","Rescuing an instance seems to guess the image format at some point. This allows reading files from the compute host via the qcow2 backing file.
Requirements:
- instances spawned using libvirt
- use_cow_images = False in the config
To reproduce:
1. Create a qcow2 file backed by the path you want to read from the compute host. (qemu-img create -f qcow2 -b /path/to/the/file $((1024*1024)) evil.qcow2)
2. Spawn an instance, scp the file into it.
3. Overwrite the disk inside the instance (dd if=evil.qcow2 of=/dev/vda)
4. Shutdown the instance.
5. Rescue the instance
6. While in rescue mode, login and read /dev/vdb - beginning should be read from the qcow backing file
Libvirt description of the rescued instance will contain the entry for the second disk with attribute type=""qcow2"", even though it should be ""raw"" - same as the original instance.
Mitigating factors:
- files have to be readable by libvirt/kvm
- apparmor/selinux will limit the number of accessible files
- only full blocks of the file are visible in the rescued instance, so short files will not be available at all and long files are going to be truncated
Possible targets:
- private snapshots with known uuids, or instances of other tenants are a good target for this attack","Persist image format to a file, to prevent attacks based on changing it

The attack is based on creating a raw image that looks like a qcow2
image, and taking advantage of the code that used 'qemu-img info' to
autodetect the image format.

Now we store the image format to a 'disk.info' file, for Qcow2 and Raw
images, and only autodetect for images that have never been written to
that file.

SecurityImpact

Co-authored-by: Nikola Dipanov <ndipanov@redhat.com>
Closes-bug: #1221190
Change-Id: I2016efdb3f49a44ec4d677ac596eacc97871f30a"
1618,dca3c8323cf8cf12aa8ce4ba21f647ce631e8153,1410294024,,1.0,56,25,5,2,1,0.893063789,False,,,,,True,,,,,,,,,,,,,,,,,1128,1583,1350504,cinder,dca3c8323cf8cf12aa8ce4ba21f647ce631e8153,1,1,CVE,"Bug #1350504 in Cinder: ""[OSSA 2014-033] GlusterFS driver uses unsafe qcow2 format detection (CVE-2014-3641)""","Concern about this was raised by Duncan Thomas.
The GlusterFS Cinder driver uses ""qemu-img info"" to guess at whether a volume file is a raw image or a qcow2 image.
This is unsafe because if a user writes a qcow2 header into a volume, Cinder will interpret it as a qcow2-formatted image.   It is believed this can lead to data being extracted from files on the Cinder volume host by writing a qcow2 header with a backing file pointer referencing a path to a file, and then cloning the volume.  (Other similar paths may exist.)
To fix this, Cinder needs to track the file format of any file being processed this way and use ""qemu-img convert -f <source_format>"" when performing operations like volume clone, which disables qemu-img's auto format detection.
This seems to affect the GlusterFS driver, but it is possible that other attack vectors exist.  The convert_image() method in cinder/image/image_utils.py is used in assorted places and does not specify a source format, so other uses of it will need to be examined for safety.
Fixing this in the GlusterFS driver is not simple: since volume/snapshot qcow2 chains are manipulated by Nova as well as Cinder, we will need to have Nova pass information back to Cinder when an operation such as volume_snapshot_delete is performed, indicating the resulting format of any files modified.
Since the above is a large effort, it may be possible to mitigate this in the short-term by having Cinder enforce some rules about whether a backing file pointer is valid before performing an operation on the file.  For the GlusterFS driver that would be: must start with 'volume-<x>' and not contain '/', since our valid usage of this only points to another file named volume-<id>.<id> and does not use paths.
This attack hasn't yet been demonstrated to work, but this is a commonly known problem when processing qcow2 files.","Refuse invalid qcow2 backing files

Don't allow qcow2 files that are pointing to backing files outside of:

volume-<id>
volume-<id>.<snap-id>
volume-<id>.tmp-snap-<snap-id>

(optionally prefixed with /mnt/path)

Closes-Bug: #1350504

Change-Id: Ic89cffc93940b7b119cfcde3362f304c9f2875df"
1619,dcd3fb9da5ba64b5ae56a97c147f125bb7e33c4c,1409312650,,1.0,0,9,2,1,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,1250,1709,1363058,neutron,dcd3fb9da5ba64b5ae56a97c147f125bb7e33c4c,0,0,Bug in test,"Bug #1363058 in neutron: ""cfg.CONF.state_path set to wrong value in tests""","cfg.CONF.state_path is set to a random temporary directory in neutron.tests.base:BaseTestCase.setUp. This value was then over written in neutron.tests.unit.__init__. Tests that need to read or otherwise use cfg.CONF.state_path were getting the directory from which the tests were running and not the temporary directory specially created for the current test run.
Note that the usage of state_path to set lock_path, dhcp state path and the likes was working as expected, and was not affected by this bug.","Fix state_path in tests

cfg.CONF.state_path is set to a random temporary directory
in neutron.tests.base:BaseTestCase.setUp. This value was then
over written in neutron.tests.unit.__init__. Tests that need
to read or pass cfg.CONF.state_path were getting the directory
from which the tests were running and not the temporary directory
specially created for the current test run. Note that the usage
of state_path to set lock_path, dhcp state path and the likes
was working as expected, and was not affected by this bug.

Closes-Bug: #1363058
Change-Id: Ib45f663fadaf0f3b4a79a0db4128822187b61ecc"
1620,dcec1b84f4d3462fc408f4152f4c89b8df79d629,1393514452,1.0,1.0,64,4,2,2,1,0.959686894,True,3.0,1911402.0,28.0,5.0,False,1.0,8173271.0,1.0,1.0,45.0,11.0,54.0,43.0,11.0,52.0,45.0,11.0,54.0,0.031039136,0.008097166,0.037112011,819,1247,1310659,cinder,dcec1b84f4d3462fc408f4152f4c89b8df79d629,1,1,,"Bug #1310659 in Cinder: ""NetApp Eseries fails to attach lun if lun is already mapped""","In some situations it may be possible for the lun on an eseries controller to be mapped, but for cinder to not know about it.  In this situation, if you then attempt certain operations in cinder (such as upload-to-image or create volume from image), cinder will attempt to attach the lun to the cinder node.  This will fail because the eseries controller returns back an error that the lun is already mapped.
2014-04-18 12:30:04.424 DEBUG cinder.volume.drivers.netapp.eseries.client [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Invoking rest with method: POST, path: /storage-syste
ms/{system-id}/volume-mappings, data: {'mappableObjectId': u'0200000060080E500023BB340000C7C65350FAAF', 'targetId': u'8400000060080E500023C734003024D55350F8AA', 'lun': 1}, use_system: True, timeout: None, verify: False, kwargs: {}. from
 (pid=30195) _invoke /opt/stack/cinder/cinder/volume/drivers/netapp/eseries/client.py:123
2014-04-18 12:30:04.426 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Setting read timeout to None from (pid=30195) _make_request /usr/lib/pytho
n2.7/dist-packages/urllib3/connectionpool.py:375
2014-04-18 12:30:04.809 DEBUG urllib3.connectionpool [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] ""POST /devmgr/v2/storage-systems/f79b215b-b502-43b7-800c-9b6a08c7086b/volu
me-mappings HTTP/1.1"" 422 None from (pid=30195) _make_request /usr/lib/python2.7/dist-packages/urllib3/connectionpool.py:415
2014-04-18 12:30:04.812 ERROR cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.812 DEBUG cinder.volume.driver [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Cleaning up failed connect initialization. from (pid=30195) _attach_volume /
opt/stack/cinder/cinder/volume/driver.py:399
2014-04-18 12:30:04.862 ERROR oslo.messaging.rpc.dispatcher [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Exception during message handling: Bad or unexpected response from
the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 719, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     payload['message'] = unicode(error)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 713, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     image_meta)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/driver.py"", line 355, in copy_volume_to_image
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     attach_info = self._attach_volume(context, volume, properties)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   File ""/opt/stack/cinder/cinder/volume/driver.py"", line 406, in _attach_volume
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher     raise exception.VolumeBackendAPIException(data=err_msg)
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher VolumeBackendAPIException: Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""retcode"" : ""105"",
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher   ""codeType"" : ""symbol""
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher }.
2014-04-18 12:30:04.862 TRACE oslo.messaging.rpc.dispatcher
2014-04-18 12:30:04.865 ERROR oslo.messaging._drivers.common [req-fca76af9-20d8-407a-9641-416409264944 020beefaca554b97b895fb11fd176177 d1155ac0ccaf4f879b8817151d6a3ee8] Returning exception Bad or unexpected response from the storage volume backend API: Unable to fetch connection information from backend: Response error - {
  ""errorMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""localizedMessage"" : ""The operation cannot complete because the volume you are trying to map is already accessible by a host group or host in this partition."",
  ""retcode"" : ""105"",
  ""codeType"" : ""symbol""
}. to caller","NetApp fix attach fail for already mapped volume

This patch fixes the error raised during mapping of a volume
to the host during attach operation if the volume is already
mapped to the host.

Change-Id: I4f711e7ac18eea0dfddab65fd85a3601fe967a88
Closes-bug: #1310659"
1621,dced1df98e385ebb0615d3789f67d95add49c637,1381360761,0.0,1.0,10,2,1,1,1,0.0,True,4.0,2869186.0,35.0,17.0,False,20.0,6056.0,53.0,7.0,158.0,3200.0,3258.0,158.0,2818.0,2876.0,144.0,2269.0,2319.0,0.023151844,0.362446112,0.370429507,1662,1237679,1237679,nova,dced1df98e385ebb0615d3789f67d95add49c637,1,1,Test files,"Bug #1237679 in OpenStack Compute (nova): ""Fix useless vmware pause/unpause test methods""","The vmwareapi test class that tests the VMwareESXDriver has a test_pause and test_unpause method that simply passes:
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L632
https://github.com/openstack/nova/blob/master/nova/tests/virt/vmwareapi/test_vmwareapi.py#L635
Those APIs aren't supported by the VMwareESXDriver so they should actually test that the code raises NotImplementedError.
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L957
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vmops.py#L961
I think the test class was doing a pass for the esx driver because the test class for the vcdriver extends the esx driver test class, but the method isn't any different for the vcdriver test class so it should just assert it raises NotImplementedError either way.","Make the vmware pause/unpause unit tests actually test something

The pause/unpause tests for vmware were just passing so this patch
actually makes them test the pause/unpause methods for the
VMwareESXDriver and VMwareVCDriver classes.

Closes-Bug: #1237679

Change-Id: Idf7c6e6f9542b9ab4dd2ab710597f1a198889dfa"
1622,dd2ccd12adfb6c7d4aa500ba4120f767beeed35c,1397673085,,1.0,1,1,1,1,1,0.0,True,7.0,1160035.0,104.0,15.0,False,9.0,1143078.0,8.0,5.0,785.0,2088.0,2088.0,654.0,1727.0,1727.0,285.0,991.0,991.0,0.2495637,0.865619546,0.865619546,804,1232,1308675,neutron,dd2ccd12adfb6c7d4aa500ba4120f767beeed35c,1,1,,"Bug #1308675 in neutron: ""netaddr raises ValueError in 0.7.10 and AddrFormatError in 0.7.11""",netaddr raises ValueError in 0.7.10 and AddrFormatError in 0.7.11,"netaddr<=0.7.10 raises ValueError instead of AddrFormatError

This patch ensures that ValueError is also caught in addition to
AddrFormatError as in netaddr>=0.7.11 AddrFormatError is raised and
in netaddr<=0.7.10 ValueError is raised.

Change-Id: I595c90e42129a2d365f3860e3042e826bd031365
Closes-bug: #1308675"
1623,dd3f7cddd380205c26e5e2b9e2002d773eab9047,1394645065,1.0,1.0,2,1,1,1,1,0.0,True,1.0,560654.0,15.0,3.0,False,19.0,1420969.0,51.0,3.0,700.0,3492.0,3860.0,632.0,2723.0,3046.0,688.0,3367.0,3727.0,0.090885108,0.444268566,0.491755705,586,1008,1290975,nova,dd3f7cddd380205c26e5e2b9e2002d773eab9047,0,0,New style,"Bug #1290975 in OpenStack Compute (nova): ""cells AttributeError with compute api methods using new object access style""","The nova-cells service looks up instances locally before passing them to the local compute api, and only converts them to objects if the compute api method is explicitly listed in the run_compute_api method.  There is in fact a FIXME around this process, but it appears to not have been addressed yet :)
2014-03-10 17:27:59.881 30193 ERROR nova.cells.messaging [req-3e27c8c0-6b3c-482d-bb9b-d638933ec949 10226892 5915610] Error processing message locally: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging Traceback (most recent call last):
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 211, in _process_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     resp_value = self.msg_runner._process_message_locally(self)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 1290, in _process_message_locally
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return fn(message, **message.method_kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/cells/messaging.py"", line 706, in run_compute_api_method
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return fn(message.ctxt, *args, **method_info['method_kwargs'])
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 199, in wrapped
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return func(self, context, target, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 189, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return function(self, context, instance, *args, **kwargs)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 170, in inner
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     return f(self, context, instance, *args, **kw)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging   File ""/opt/rackstack/615.0/nova/lib/python2.6/site-packages/nova/compute/api.py"", line 2988, in update_instance_metadata
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging     orig = dict(instance.metadata)
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging AttributeError: 'dict' object has no attribute 'metadata'
2014-03-10 17:27:59.881 30193 TRACE nova.cells.messaging","Cells: Pass instance objects to update/delete_instance_metadata

When nova-cells processes a 'run_compute_api_method' message it pulls
the instance from the local db and calls the appropriate compute api
method with a sqlalchemy model.  Since the update_instance_metadata and
delete_instance_metadata methods in the compute api require objects
these methods are added to the list of methods that will receive an
object.

Change-Id: Iaa5ba6878f0162e2fab8dd3e4b87ea8dd3d527f6
Closes-bug: #1290975"
1624,dd5c73450da908d95724e20eb3be09bc936cb551,1408080000,,1.0,21,1,2,2,1,0.902393283,False,,,,,True,,,,,,,,,,,,,,,,,1199,1657,1357198,neutron,dd5c73450da908d95724e20eb3be09bc936cb551,1,1,“l2pop can send us entry removal without the corresponding addition.”,"Bug #1357198 in neutron: ""ofagent: del_arp_table_entry crash""","del_arp_table_entry  crashes when it gets unknown ip.
l2pop sometimes send us fdb removal without the corresponding fdb add.","ofagent: Ignore unknown l2pop entry removals

l2pop can send us entry removal without the corresponding addition.
Gracefully ignore it instead of crashing.

Closes-Bug: #1357198
Change-Id: I5a0cc44ba62faf15d6fe3730a9532a3826647820"
1625,dd6765f751f9c6f36c027ba7c945c919ecf825e2,1385168707,,1.0,31,7,2,2,1,0.297472249,True,4.0,977209.0,39.0,16.0,False,163.0,1075583.5,810.0,4.0,127.0,2579.0,2623.0,127.0,2333.0,2377.0,124.0,1678.0,1719.0,0.018757503,0.25195078,0.258103241,70,398,1248802,nova,dd6765f751f9c6f36c027ba7c945c919ecf825e2,1,1,Bug in the sequence of the commands,"Bug #1248802 in OpenStack Compute (nova): ""shelve/unshelve notification may be out of order""","when shelve a vm and CONF.shelved_offload_time == 0,the vm would be offloaded right now.
referring to nova/compute/manager.py:def shelve_instance(self, context, instance, image_id):
        if CONF.shelved_offload_time == 0:
            instance.task_state = task_states.SHELVING_OFFLOADING
        instance.power_state = current_power_state
        instance.save(expected_task_state=[
                task_states.SHELVING,
                task_states.SHELVING_IMAGE_UPLOADING])
        if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)
        self._notify_about_instance_usage(context, instance, 'shelve.end')
thus,notification may occurs like:
compute.instance.shelve.start
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
compute.instance.shelve.end
in fact,order should be like:
compute.instance.shelve.start
compute.instance.shelve.end
compute.instance.shelve_offload.start
compute.instance.shelve_offload.end
I suggest that ""self._notify_about_instance_usage(context, instance, 'shelve.end')"" should be moved before
 "" if CONF.shelved_offload_time == 0:
            self.shelve_offload_instance(context, instance)""","Adjust the order of notification for shelve instance

When shelve instance, if CONF.shelved_offload_time == 0, then the
notification order for shelve_instance() is as following:
1) compute.instance.shelve.start
2) compute.instance.shelve_offload.start
3) compute.instance.shelve_offload.end
4) compute.instance.shelve.end

This order is not correct, as actually before shelve_offload.start,
the instance was already reached shelve.end, so we should adjust the
order of notification for shelve instance as following:
1) compute.instance.shelve.start
2) compute.instance.shelve.end
3) compute.instance.shelve_offload.start
4) compute.instance.shelve_offload.end

Change-Id: I6e075b3f29e1c3f641ab1e459e1fbe3b05869a9a
Closes-Bug: #1248802"
1626,dd6fb1246ff2789bd78b772b45e1fcac21eda67a,1403036049,,1.0,32,7,2,2,1,0.988110837,False,,,,,True,,,,,,,,,,,,,,,,,979,1418,1330503,nova,dd6fb1246ff2789bd78b772b45e1fcac21eda67a,1,1,,"Bug #1330503 in OpenStack Compute (nova): ""Restarting destination compute manager during resize migration can cause instance data loss""","During compute manager startup init_host is called. One of the functions there is to delete instance data that doesn't belong to this host i.e. _destroy_evacuated_instances. But this function only checks if the local instance belongs to the host or not. It doesn't check the task_state or vm_state.
If at this time a resize migration is taking place and the destination compute manager is restarted it might destroy the resizing instance. Alternatively, if the resize has completed (vm_state = RESIZED) but has not been confirmed/reverted, then a restart of the source compute manager might destroy the original instance.
A similar bug concerning just the migrating state is outlined here: https://bugs.launchpad.net/nova/+bug/1319797 and a fix is proposed here: https://review.openstack.org/#/c/93903
It was intended to have that fix deal with resize migrating instances as well as those just in the migrating state but as pointed out in a review comment this solution will work for migrating but a fix for resize would require further changes so I have raised this bug to highlight that.","Keep resizing&resized instances when compute init

During compute manager startup init_host is called. One of the
functions there is to delete instance data that doesn't belong
to this host i.e. _destroy_evacuated_instances.
But this function only checks if the local instance belongs to
the host or not. It doesn't check the task_state or vm_state.

In Resize function, user may want to revert or confirm the resize
operations so the instance on source and dest compute node should
be kept. so for RESIZE_MIGRATING, RESIZE_MIGRATED task states and
RESIZED vm state instances, they should be kept in compute node
when the compute restart. This patch adds check for the task
state and vm state before delete the instances.

Closes-Bug: #1330503

Change-Id: I723fa4a8823019391ea83aa189096531032adab1"
1627,dd71021347f782608575d551f2329715c00546ce,1385562206,,1.0,36,2,2,2,1,0.485460761,True,3.0,966277.0,23.0,7.0,False,21.0,574263.0,32.0,4.0,47.0,924.0,968.0,37.0,777.0,812.0,4.0,351.0,352.0,0.008605852,0.605851979,0.60757315,98,497,1255871,neutron,dd71021347f782608575d551f2329715c00546ce,0,0,It should… Add a feature I think,"Bug #1255871 in neutron: ""l3_agent: ri.internal_ports.append/removed should be called only if internal_network_added/removed succeed.""","How to reproduce:
  See the test cases in the following patch.","l3_agent: make process_router more robust

If internal_network_added/removed fails, _sync_routers_task will call
process_router to do fault recovery. Because the port is already
added/removed to/from ri.internal_ports, internal_network_added or
internal_network_removed will not be called again.

The patch fix this issue by calling ri.internal_ports.append/removed
only if internal_network_added/removed succeed. Without the patch,
the added testcases would fail.

Change-Id: I2d2e004caa670c1624257c1d7ccc900438b42d08
Co-Authored-By: Hirofumi Ichihara <ichihara.hirofumi@lab.ntt.co.jp>
Closes-Bug: #1255871"
1628,dd9536ac6e6df0e1ae6754e580b99cfbfd05eb77,1386567673,1.0,1.0,122,3,4,2,1,0.817127297,True,12.0,2141292.0,64.0,24.0,False,11.0,1945423.25,61.0,5.0,0.0,1228.0,1228.0,0.0,1075.0,1075.0,0.0,1068.0,1068.0,0.000811688,0.867694805,0.867694805,145,547,1258033,cinder,dd9536ac6e6df0e1ae6754e580b99cfbfd05eb77,0,0,Evolution: should use optional key value pairs for adding metadata,"Bug #1258033 in Cinder: ""Use optional key-value pair metadata to track OpenStack volume on the 3PAR storage array""","Currently, when a volume is created on the 3PAR, metadata is added onto “comment” section onto the 3PAR which can be modified by users directly from management console. Hence for any new driver related tasks, to avoid manipulation from user, the 3PAR driver should use optional key value pairs for adding metadata. Comments section would be mainly used for informational purposes. All metadata keys should start with a custom prefix for ease of use.","Add additional metadata as key-value pairs in 3PAR

Track status of openstack volumes on 3PAR through
additional metadata added as key-value pairs. During volume attach
and detach, corresponding instance metadata is updated onto the
cinder volumes.

Change-Id: Iea8d2f26555e6be60001bf73755cae42446afec6
Closes-Bug: #1258033"
1629,dda6c89202b015628f74101839301424001a6c63,1397098190,0.0,1.0,25,18,7,6,1,0.809390875,True,2.0,1116308.0,44.0,7.0,False,36.0,105061.0,82.0,6.0,233.0,1473.0,1522.0,230.0,1262.0,1310.0,197.0,912.0,939.0,0.176,0.811555556,0.835555556,772,1199,1305377,neutron,dda6c89202b015628f74101839301424001a6c63,1,1,,"Bug #1305377 in neutron: ""Correct flake8 E711 and E712 violations""","Enable flake8 checking for
E711 comparison to False should be 'if cond is False:' or 'if not cond:'
E712 comparison to True should be 'if cond is True:' or 'if cond:'","Enable flake8 E711 and E712 checking

E711 comparison to False should be 'if cond is False:' or 'if not cond:'
     comparison to None should be 'if cond is None:' or 'if not cond:'
E712 comparison to True should be 'if cond is True:' or 'if cond:'

Most violations were in DB queries. Replace as follows:
  False -> sqlalchemy.sql.false()
  None  -> sqlalchemy.sql.null()
  True  -> sqlalchemy.sql.true()

Change-Id: Iff54747b70f504d5466cfdc6e2ec4d7a0f9ddb7c
Closes-bug: #1305377"
1630,ddcad011db507cde66b6b1e655d5ffc91ab8880f,1409089123,,1.0,7,8,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1236,1695,1361738,cinder,ddcad011db507cde66b6b1e655d5ffc91ab8880f,1,1,"“And in turn to this Cinder patch that introduced the regression:
https://review.openstack.org/#/c/76471/7/cinder/volume/manager.py""","Bug #1361738 in Cinder: ""Live migration disconnects attached volumes on target""","Live migration disconnects attached volumes on target. The action itself does not fail and the instance appears online on the target host, but the underlying iSCSi connection results closed on the destination host, resulting in failures in the guest when accessing the volume(s).
The issue can be traced to this call to the following self.volume_api.terminate_connection(...) in the manager:
https://github.com/openstack/nova/blob/c594340359e3d148b6c349c388f10327ac7b8bfa/nova/compute/manager.py#L4868
And in turn to this Cinder patch that introduced the regression:
https://review.openstack.org/#/c/76471/7/cinder/volume/manager.py
Verified on Hyper-V using Devstack with both LVM and Windows Cinder drivers.","Fixes terminate_connection live migration issue

Reverts the changes to cinder/volume/manager.py added in
commit b868ae707f9ecbe254101e21d9d7ffa0b05b17d1 as calling
remove_export in terminate_connection causes Nova live
migration to fail when volumes are attached.

Change-Id: I38f3db336db22bb096a8855bcbbfeba16e517b80
Closes-Bug: #1361738"
1631,ddd92b229daa31f6d8f6683986e135ebd36829c5,1407849061,,1.0,13,9,2,2,1,0.266764988,True,6.0,4192106.0,54.0,16.0,False,161.0,18918.0,486.0,1.0,148.0,1251.0,1266.0,147.0,1247.0,1262.0,145.0,1241.0,1254.0,0.01820903,0.154901472,0.156522824,697,1123,1298981,nova,ddd92b229daa31f6d8f6683986e135ebd36829c5,0,0,Add an option. Evolution,"Bug #1298981 in OpenStack Compute (nova): ""Skip resizing disk if the parameter resize_instance is False""","On the libvirt driver the driver.finish_migration method is called with an extra
parameter 'resize_instance'. It should be used to know if it is
necessary or not to resize the disks.","libvirt: skip disk resize when resize_instance is False

The method finish_migration is called with an extra
parameter 'resize_instance'. It should be used to know
if it is necessary or not to resize the disks and so
call the method 'disk_resize'.

Change-Id: Ia5a9ad7994fa9abab7cf47d36468c46d97065991
Closes-Bug: #1298981"
1632,de05c2994212e83eba3cedf2544cdc549d9f8184,1398193206,,1.0,41,16,6,6,1,0.809338426,True,1.0,2457801.0,15.0,6.0,False,172.0,916473.5,712.0,2.0,30.0,1461.0,1471.0,30.0,1232.0,1242.0,24.0,1163.0,1167.0,0.003186337,0.14835585,0.148865664,787,1214,1307088,nova,de05c2994212e83eba3cedf2544cdc549d9f8184,1,1,,"Bug #1307088 in OpenStack Compute (nova): ""can't attach a read only volume to an instance""","Description of problem:
An attachment of a read only volume to an instance failed. The openstack was installed as AIO, Cinder was configured with Netapp back end. The following error from the nova compute log:
014-04-13 11:28:17.838 25176 ERROR oslo.messaging.rpc.dispatcher [-] Exception during message handling: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f7
0834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 360, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 88, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     payload)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/exception.py"", line 71, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return f(self, context, *args, **kw)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 244, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     pass
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 230, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 272, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     e, sys.exc_info())
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 259, in decorated_function
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return function(self, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3876, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     bdm.destroy(context)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3873, in attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._attach_volume(context, instance, driver_bdm)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3894, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     self.volume_api.unreserve_volume(context, bdm.volume_id)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     six.reraise(self.type_, self.value, self.tb)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/compute/manager.py"", line 3886, in _attach_volume
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     do_check_attach=False, do_driver_attach=True)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 44, in wrapped
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     ret_val = method(obj, context, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/virt/block_device.py"", line 251, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     instance['uuid'], self['mount_device'])
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 174, in wrapper
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     res = method(self, ctx, volume_id, *args, **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/nova/volume/cinder.py"", line 263, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     mountpoint)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 266, in attach
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     'mode': mode})
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/v1/volumes.py"", line 250, in _action
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self.api.client.post(url, body=body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 210, in post
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     return self._cs_request(url, 'POST', **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 174, in _cs_request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     **kwargs)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/site-packages/cinderclient/client.py"", line 157, in request
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher     raise exceptions.from_response(resp, body)
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher InvalidInput: Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31.
2014-04-13 11:28:17.838 25176 TRACE oslo.messaging.rpc.dispatcher
2014-04-13 11:28:17.858 25176 ERROR oslo.messaging._drivers.common [-] Returning exception Invalid input received: Invalid attaching mode 'rw' for volume 3f5828e1-77b2-4302-9cdf-486f70834c31. to caller
Version-Release number of selected component (if applicable):
openstack-nova-conductor-2014.1-0.13.b3.el7.noarch
openstack-swift-object-1.12.0-1.el7.noarch
openstack-glance-2014.1-0.4.b3.el7.noarch
openstack-packstack-puppet-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-cert-2014.1-0.13.b3.el7.noarch
python-django-openstack-auth-1.1.4-1.el7.noarch
openstack-swift-1.12.0-1.el7.noarch
openstack-keystone-2014.1-0.4.b3.el7.noarch
openstack-utils-2013.2-3.p1.el7.noarch
openstack-nova-api-2014.1-0.13.b3.el7.noarch
openstack-nova-compute-2014.1-0.13.b3.el7.noarch
openstack-nova-novncproxy-2014.1-0.13.b3.el7.noarch
openstack-dashboard-2014.1-0.5.b3.el7.noarch
openstack-swift-account-1.12.0-1.el7.noarch
openstack-swift-proxy-1.12.0-1.el7.noarch
openstack-puppet-modules-2014.1-5.3.el7.noarch
openstack-cinder-2014.1-0.6.b3.el7.noarch
openstack-nova-common-2014.1-0.13.b3.el7.noarch
openstack-nova-console-2014.1-0.13.b3.el7.noarch
openstack-nova-network-2014.1-0.13.b3.el7.noarch
openstack-swift-container-1.12.0-1.el7.noarch
openstack-packstack-2014.1.1-0.7.dev1018.el7.noarch
openstack-nova-scheduler-2014.1-0.13.b3.el7.noarch
openstack-swift-plugin-swift3-1.7-3.el7.noarch
How reproducible:
100%
Steps to Reproduce:
1. Create a volume from a qcow2 image.
2. Attach the volume to an instance and make changes.
3. Detach the volume from the instance.
4. Set the volume as read only.
5. Attach the volume to an instance.
Actual results:
the attachment fails.
Expected results:
the attachment should succeed, but the volume cannot be changed.","Set the volume access mode during volume attach

cinderclient supports an optional keyword argument named 'mode'
for volumes.attach(), which allows you to set the access mode of
a volume at attach time. The keyword argument is not passed in
by nova when it attaches a read-only volume, making the volume
access mode default to 'rw' instead of the desired 'ro'. This
patch finds the access mode of the volume and passes it to
cinderclient, so the correct access mode is used at attach
time.

Change-Id: Ib62f454292e60114f96debbad96c4a356dc32a21
Closes-Bug: #1307088"
1633,de15e0b9c51cf9124de41258c1e3d774de215213,1381102687,2.0,1.0,45,7,8,4,1,0.900967841,True,2.0,137280.0,17.0,4.0,False,20.0,1464329.0,26.0,4.0,970.0,1286.0,1451.0,915.0,1170.0,1324.0,342.0,349.0,370.0,0.807058824,0.823529412,0.872941176,1432,1189671,1189671,neutron,de15e0b9c51cf9124de41258c1e3d774de215213,0,0,“Can we change the default to quantum.db.quota_db.DbQuotaDriver please3”,"Bug #1189671 in neutron: ""default quota driver not suitable for production""","The default quota driver is conf file driven, which isn't particularly useful in non-trivial clouds.
Can we change the default to  quantum.db.quota_db.DbQuotaDriver please?","Enable Quota DB driver by default

Closes-Bug: #1189671

Quota driver is now loaded in lazy mode, i.e. the driver is loaded
the first time the driver is accessed. This is to make unit tests
work. Some unit tests like extension test cases need to use Config
Quota driver (previous default) but QuotaEngine is initialized
when quota.py is imported. Thus the unit tests had no chance to
specify quota_driver.

Change-Id: I9e20961d5a6322361e3c0284b3c2a7ca86755c70"
1634,de403598f608172b1cf41ca4ec0d880642e215d5,1396953671,,1.0,13,7,2,2,1,0.934068055,True,4.0,1911239.0,74.0,9.0,False,6.0,1873610.0,8.0,1.0,40.0,206.0,223.0,40.0,206.0,223.0,40.0,206.0,223.0,0.036837376,0.185983827,0.201257862,760,1187,1304326,neutron,de403598f608172b1cf41ca4ec0d880642e215d5,1,1,,"Bug #1304326 in neutron: ""neutron-db-manage offline mode requires connection URL""","When using offline migration mode with neturon-db-manage, we need to pass config file containing connection string to database. For offline migration is sufficient to know database engine. This and used plugins could be passed directly from command line.","add engine parameter for offline migrations

Offline migration required config file containing connection string.
In that case only engine from URL was used. With this patch engine can be
passed from command line (or config file) along with plugins which sql
script will be generated accordingly.

DocImpact

Change-Id: Ib667a71960b833fd981f97fe7d6b1856084ef5c8
Closes-Bug: #1304326"
1635,de4158861000dedc204314c545cc5682aa38f5f1,1384438347,0.0,1.0,64,19,13,12,1,0.787191313,True,12.0,5970192.0,101.0,40.0,False,245.0,55408.84615,1335.0,7.0,114.0,4098.0,4129.0,114.0,3357.0,3388.0,111.0,3005.0,3033.0,0.017003188,0.456353423,0.46060422,1706,1241337,1241337,nova,de4158861000dedc204314c545cc5682aa38f5f1,1,1,,"Bug #1241337 in OpenStack Compute (nova): ""VM do not resume if attach an volume when suspended""","1) nova suspend vm2
2) nova attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
3) nova volume-attach vm2 6ac2e985-9586-438f-a027-bc9591fa5b43 /dev/sdb
4) nova resume vm2
VM failed to resume and nova compute report the following errors.
2013-10-18 12:16:33.175 ERROR nova.openstack.common.rpc.amqp [req-a8b196e3-dbd5-45f4-814e-56a715b07fdf admin admin] Exception during message handling
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 354, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 295, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3314, in resume_instance
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1969, in resume
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     block_device_info)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 3206, in _create_domain_and_network
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     {'connection_info': jsonutils.dumps(connection_info)})
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 420, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     context, bdm_id, values)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/api.py"", line 170, in block_device_mapping_update
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     context, values, create=False)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/conductor/rpcapi.py"", line 244, in block_device_mapping_update_or_create
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     values=values, create=create)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/rpcclient.py"", line 85, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return self._invoke(self.proxy.call, ctxt, method, **kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/rpcclient.py"", line 63, in _invoke
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return cast_or_call(ctxt, msg, **self.kwargs)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/proxy.py"", line 126, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     result = rpc.call(context, real_topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/__init__.py"", line 139, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/impl_kombu.py"", line 816, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     rpc_amqp.get_connection_pool(conf, Connection))
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 572, in call
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     rv = multicall(conf, context, topic, msg, timeout, connection_pool)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 558, in multicall
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     pack_context(msg, context)
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 308, in pack_context
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp     for (key, value) in context.to_dict().iteritems()])
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'to_dict'
2013-10-18 12:16:33.175 TRACE nova.openstack.common.rpc.amqp","Add context as parameter for resume

Now for KVM when resume an instance with block storage, nova
compute will throw exception and failed to resume the VM.

The root cause is that when resume a VM with block storage,
libvirt driver needs to call conductor via rpcapi to update
block device, but the function of resume() do not have context,
this will cause RPC api failed.

Change-Id: I712777ed1d893a2b6463d30c407b0a677e37b602
Closes-Bug: #1241337"
1636,de4392314f9b0def8fab65679ec5668aec98fbee,1381887562,,1.0,3,2,1,1,1,0.0,True,2.0,2460346.0,12.0,9.0,False,21.0,1537.0,63.0,7.0,1031.0,1754.0,1762.0,912.0,1512.0,1520.0,882.0,1064.0,1067.0,0.807129799,0.973491773,0.976234004,1693,1240299,1240299,cinder,de4392314f9b0def8fab65679ec5668aec98fbee,1,1, ,"Bug #1240299 in Cinder: ""don't secure delete thin provisioned volumes or snapshots""","Performing the secure delete operation on volumes and snapshots writes zeros (or other patterns) across the entire surface of an LVM volume on delete calls.  This is pointless with thin provisioning, and in fact results in defeating the purpose of thin provisioning as it requires actual allocation of all of the blocks.
Add a check before calling lvm.clear_volume and skip this step if thin provisioning is configured.","Don't zero out thin provisioned LV's on delete

Thin provisioned LV's don't need secure delete to protect
from data leakage.  Also, zeroing these out kinda defeats
the purpose of using thing provisioning.

This patch add a check for the lvm type and if it's thin simply
returns from the lvm.clear_volume() method.

Change-Id: Ie6764209018152565295291efc6fbba553698ae6
Closes-Bug: #1240299"
1637,de4b5a2bba57cbd5294fb6f9527d64b8eeddf901,1388497903,1.0,1.0,62,1,2,2,1,0.702466551,True,2.0,91488.0,16.0,4.0,False,3.0,2717962.5,6.0,2.0,224.0,1919.0,2025.0,208.0,1669.0,1760.0,157.0,292.0,361.0,0.239393939,0.443939394,0.548484848,234,638,1265081,neutron,de4b5a2bba57cbd5294fb6f9527d64b8eeddf901,1,1,Fix integrity,"Bug #1265081 in neutron: ""nicira: db integrity error during port deletion""","This is a stacktrace experienced during a test on trunk:
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Matched DELETE /ports/a42a9719-8416-4c44-a4f3-b3861648281f __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:100
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Route path: '/ports/{id}{.format}', defaults: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:102
2013-12-30 13:42:39.606 29118 DEBUG routes.middleware [-] Match dict: {'action': u'delete', 'controller': <wsgify at 72315472 wrapping <function resource at 0x44f2500>>, 'id': u'a42a9719-8416-4c44-a4f3-b3861648281f', 'format': None} __call__ /usr/lib/python2.7/dist-packages/routes/middleware.py:103
2013-12-30 13:42:39.606 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Sending port.delete.start on notifications.info notify /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:598
2013-12-30 13:42:39.607 29118 DEBUG neutron.openstack.common.rpc.amqp [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] UNIQUE_ID is 3f460839b3144eaa911c04de8725eb06. _add_unique_id /opt/stack/neutron/neutron/openstack/common/rpc/amqp.py:339
2013-12-30 13:42:39.610 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Acquired connection https://192.168.1.8:443. 9 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Issuing - request POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.611 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.647 29118 DEBUG neutron.plugins.nicira.nvplib [req-d8b115ac-38c3-4690-b670-7c81a9d9ca15 663434b6088b4984991d07a858d6f6bc 4a9f94ca2c674047b8e86fae8eefc9a4] Looking for port with q_port_id tag 'a42a9719-8416-4c44-a4f3-b3861648281f' on: '6d795954-a836-47f2-b2e3-2438e0da7f80' get_port_by_neutron_tag /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:743
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Acquired connection https://192.168.1.8:443. 8 connection(s) available. acquire_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:134
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Issuing - request GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:99
2013-12-30 13:42:39.648 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Setting X-Nvp-Wait-For-Config-Generation request header: '96230' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:124
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', u'_context_tenant_name': None, u'_context_project_name': None, u'_context_read_deleted': u'no', u'args': {u'agent_state': {u'agent_state': {u'topic': u'dhcp_agent', u'binary': u'neutron-dhcp-agent', u'host': u'cidevstack', u'agent_type': u'DHCP agent', u'configurations': {u'subnets': 3, u'use_namespaces': True, u'dhcp_lease_duration': 86400, u'dhcp_driver': u'neutron.agent.linux.dhcp.Dnsmasq', u'networks': 3, u'ports': 5}}}, u'time': u'2013-12-30T21:42:39.665454'}, u'_context_tenant': None, u'method': u'report_state', u'_unique_id': u'49ca1db3e75247e594c8c4005b4b262a', u'_context_timestamp': u'2013-12-30 21:42:39.665074', u'_context_is_admin': True, u'version': u'1.0', u'_context_project_id': None, u'_context_tenant_id': None, u'_context_user': None, u'_context_user_id': None, u'namespace': None, u'_context_user_name': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.670 29118 DEBUG neutron.openstack.common.rpc.amqp [-] unpacked context: {'project_name': None, 'user_id': None, 'roles': [u'admin'], 'tenant_id': None, 'request_id': u'req-0f858449-8d08-4c71-af5c-3fa7b4470f20', 'is_admin': True, 'tenant': None, 'timestamp': u'2013-12-30 21:42:39.665074', 'tenant_name': None, 'project_id': None, 'user_name': None, 'read_deleted': u'no', 'user': None} _safe_log /opt/stack/neutron/neutron/openstack/common/rpc/common.py:276
2013-12-30 13:42:39.671 29118 DEBUG neutron.context [req-0f858449-8d08-4c71-af5c-3fa7b4470f20 None None] Arguments dropped when creating context: {'project_name': None, 'tenant': None} __init__ /opt/stack/neutron/neutron/context.py:84
2013-12-30 13:42:39.681 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3681] Completed request 'POST https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 (0.07 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.request [-] Reading X-Nvp-config-Generation response header: '96231' _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:146
2013-12-30 13:42:39.682 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3681] Released connection https://192.168.1.8:443. 9 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.request [-] [3682] Completed request 'GET https://192.168.1.8:443//ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 (0.04 seconds) _issue_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request.py:141
2013-12-30 13:42:39.684 29118 DEBUG neutron.plugins.nicira.api_client.client [-] [3682] Released connection https://192.168.1.8:443. 10 connection(s) available. release_connection /opt/stack/neutron/neutron/plugins/nicira/api_client/client.py:189
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3681] Completed request 'POST /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport': 201 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.685 29118 DEBUG neutron.plugins.nicira.api_client.request_eventlet [-] [3682] Completed request 'GET /ws.v1/lswitch/6d795954-a836-47f2-b2e3-2438e0da7f80/lport?fields=uuid&tag_scope=q_port_id&tag=a42a9719-8416-4c44-a4f3-b3861648281f': 200 _handle_request /opt/stack/neutron/neutron/plugins/nicira/api_client/request_eventlet.py:156
2013-12-30 13:42:39.686 29118 DEBUG neutron.plugins.nicira.nvplib [-] Created logical port d612afd7-339d-4f25-97d2-e6f6c966551e on logical switch 6d795954-a836-47f2-b2e3-2438e0da7f80 create_lport /opt/stack/neutron/neutron/plugins/nicira/nvplib.py:856
2013-12-30 13:42:39.696 29118 ERROR neutron.api.v2.resource [-] delete failed
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource Traceback (most recent call last):
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/resource.py"", line 84, in resource
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/api/v2/base.py"", line 438, in delete
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     obj_deleter(request.context, id, **kwargs)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 1342, in delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     port_delete_func(context, neutron_db_port)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 498, in _nvp_delete_port
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     port_data)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/NeutronPlugin.py"", line 750, in _nvp_get_port_id
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     nvp_port['uuid'])
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/plugins/nicira/dbexts/nicira_db.py"", line 54, in add_neutron_nvp_port_mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     return mapping
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 456, in __exit__
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self.commit()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 368, in commit
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self._prepare_impl()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/usr/local/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 347, in _prepare_impl
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     self.session.flush()
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 541, in _wrap
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     _raise_if_duplicate_entry_error(e, get_engine().name)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource   File ""/opt/stack/neutron/neutron/openstack/common/db/sqlalchemy/session.py"", line 492, in _raise_if_duplicate_entry_error
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource     raise exception.DBDuplicateEntry(columns, integrity_error)
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource DBDuplicateEntry: (IntegrityError) (1062, ""Duplicate entry 'a42a9719-8416-4c44-a4f3-b3861648281f' for key 'PRIMARY'"") 'INSERT INTO quantum_nvp_port_mapping (quantum_id, nvp_id) VALUES (%s, %s)' ('a42a9719-8416-4c44-a4f3-b3861648281f', 'd612afd7-339d-4f25-97d2-e6f6c966551e')
2013-12-30 13:42:39.696 29118 TRACE neutron.api.v2.resource
2013-12-30 13:42:39.700 29118 INFO neutron.wsgi [-] 127.0.0.1 - - [30/Dec/2013 13:42:39] ""DELETE //v2.0/ports/a42a9719-8416-4c44-a4f3-b3861648281f HTTP/1.1"" 500 249 0.097381
It is unclear what the root cause is at the moment.","nicira: fix db integrity error during port deletion

Due to the fact that plugin port operations are not
transactional (as they involve non synchronized DB
and Controller operations), concurrent (interleaved)
port requests may accidentally cause the insertion
of the mapping entry (neutron-port-id, nvp-port-id)
more than once. In case this occurs, it's safe to
expect the failure and continue the normal process
of the operation being requested.

Closes-bug: #1265081

Change-Id: Ifcf5b453fa08145df844c2de3cbb08bf2f4baa59"
1638,de58feb796d63dd4eea1fd49e66102686fcf4037,1401352545,,1.0,59,34,2,2,1,0.385285119,False,,,,,True,,,,,,,,,,,,,,,,,933,1369,1324400,nova,de58feb796d63dd4eea1fd49e66102686fcf4037,1,0,"“Since nova.virt.driver:LibvirtDriver.get_guest_config prepends instance root_device_name with 'dev' prefix, root_device_name may not coincide with device_name in block device mapping structure.”","Bug #1324400 in OpenStack Compute (nova): ""Invalid EC2 instance  type for a volume backed instance""","Since nova.virt.driver:LibvirtDriver.get_guest_config prepends instance root_device_name with 'dev' prefix, root_device_name may not coincide with device_name in block device mapping structure.
In this case describe instances operation reports wrong instance type: instance-store instead of ebs.
Environment: DevStack
Steps to reproduce:
1 Create volume backed instance passing vda as root device name
$ cinder create --image-id xxx 1
$ nova boot --flavor m1.nano --block-device-mapping vda=yyy:::1 inst
Note. I used cirros ami image.
2 Describe instances
$ euca-describe-instances
Look on instace type. It must be ebs, but it is instance-store in the output.
Note. If euca-describe-instance crashes on ebs instnce, apply https://review.openstack.org/#/c/95580/","Fix EC2 instance type for a volume backed instance.

EC2 reports 'instance-store' type for a volume backed instance
which was booted with short root device name (no '/dev/' prefix).

This fix make instance type detection independent of addition
'dev' prefix to the instance root device name.

Change-Id: I6c2be09eaa91341e401bbc128b3efc79c60e345e
Closes-Bug: #1324400"
1639,de5fa0dd327b1a81cb980d7553001dca244ae7a2,1406532040,,1.0,5,2,2,2,1,0.863120569,False,,,,,True,,,,,,,,,,,,,,,,,1117,1572,1349898,neutron,de5fa0dd327b1a81cb980d7553001dca244ae7a2,1,1,“port_bound forgets to normalize port name.”,"Bug #1349898 in neutron: ""ofagent: port_bound forgets to normalize port name""","port_bound forgets to normalize port name.
it causes port_unbound fail to find Port for non ""tap"" prefixed ports.","ofagent: Add a missing normalized_port_name

Otherwise port_unbound can not find Port for non ""tap"" prefixed ports.

Closes-Bug: #1349898
Change-Id: I9ac7cc1e35205bddc64e7865c45e801df8cd6b33"
1640,de648bb7bfa9985ab67c0754c25d35cd71f6eb81,1379597749,,1.0,0,1,1,1,1,0.0,True,1.0,5241.0,4.0,1.0,False,2.0,1254174.0,2.0,1.0,130.0,1644.0,1720.0,130.0,1429.0,1505.0,121.0,1553.0,1620.0,0.020075695,0.255718282,0.266743459,1578,1227655,1227655,nova,de648bb7bfa9985ab67c0754c25d35cd71f6eb81,0,0, Bug in test,"Bug #1227655 in OpenStack Compute (nova): ""nova.tests.objects.test_quotas fails H233 hacking check""","I hit this locally, not sure how it got through the gate:
pep8 runtests: commands[0] | flake8
./nova/tests/objects/test_quotas.py:49:9: H233  Python 3.x incompatible use of print operator","Remove print statement from test_quotas that fails H233 check

The nova.tests.objects.test_quotas module had a print statement which is
causing a hacking H233 check failure. It was probably just left there
from debugging the code while it was under development, so this patch
simply removes it.

Closes-Bug: #1227655

Change-Id: I25296d7f4a9d2242bfe3ed2e00df6d840a7b19cd"
1641,de92d65f2981b826e145e82ab2734c19bf1dffa3,1400210525,,1.0,16,1,2,2,1,0.873981048,True,2.0,486132.0,20.0,5.0,False,17.0,674152.0,29.0,1.0,373.0,1221.0,1565.0,251.0,1220.0,1443.0,327.0,1213.0,1512.0,0.041056453,0.151958944,0.189385405,884,1320,1319180,nova,de92d65f2981b826e145e82ab2734c19bf1dffa3,1,1,“I am not sure that this is a bug. The default value is None.”,"Bug #1319180 in OpenStack Compute (nova): ""force_config_drive cannot be set to False from nova.conf""","force_config_drive is not cast to boolean, so the only way to disable this option is not to have it in nova.conf. If it is set to 'false' it is treated as a string and evaluates to true in configdrive.required_by()","Test force_config_drive as a boolean as last resort

The force_config_drive help clearly states that only possible option is
'always' but folks are setting it to 'True'/'False' etc and expecting
it to work. So let's enable that as well. In the worst case scenario
mentioned in the bug, it was set to 'False' and since the check for
force_config_drive took any string as true, we were enabling the flag
to true. With the change in this review, we force a conversion of the
string to true. Additional check for the flag to be set as 'always' is
treated as true as well. The test case checks for all the variations

Closes-Bug: #1319180
DocImpact

Change-Id: Ifa6348fa7dfd9de063eb13d30e3ddf94fa530b57"
1642,debbf0ee49dcb78a92afaed24a60d81ba441430f,1410738601,,1.0,23,37,4,2,1,0.825359395,False,,,,,True,,,,,,,,,,,,,,,,,1313,1779,1369355,cinder,debbf0ee49dcb78a92afaed24a60d81ba441430f,1,1,,"Bug #1369355 in Cinder: ""Getting iscsi_ip_address from config file""","In the current version (2.0) of the VMAX driver, we retrieve iSCSI IP addresses dynamically from SMI-S.  However, we ran into situations where we can't get them reliably during testing.  The fix is to get this information from cinder.conf, just like in version 1.0.","Getting iscsi_ip_address from cinder.conf

In the current version (2.0) of the VMAX driver, we retrieve iSCSI IP
addresses dynamically from SMI-S. However, we ran into situations where
we can't get them reliably during testing. The fix is to get this
information from cinder.conf, just like in version 1.0.

Change-Id: If474f44e99cc45592bacfdec9e1eb2fa48c41e68
Closes-Bug: #1369355"
1643,df03a9b9b7025a0e13d9387a74c304875dec0918,1394100826,,1.0,18,1,2,2,1,0.485460761,True,5.0,1081991.0,36.0,14.0,False,23.0,164025.0,57.0,3.0,6.0,858.0,862.0,6.0,772.0,776.0,2.0,818.0,818.0,0.001998668,0.545636243,0.545636243,541,962,1288645,cinder,df03a9b9b7025a0e13d9387a74c304875dec0918,1,1,keyerror,"Bug #1288645 in Cinder: "" KeyError: 'license_compression_enclosures' when using IBM Storwize/SVC""","when using iSCSI protocol to conenct IBM v7000, starting cinder-volume service maybe occur KeyError.
log:
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] CN-18C9F33 Error encountered during initialization of driver: StorwizeSVCDriver
2014-03-06 01:48:26.409 25160 ERROR cinder.volume.manager [req-ac4f3744-1826-4456-a726-1d07ba5cb37e None] 'license_compression_enclosures'
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager Traceback (most recent call last):
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 207, in init_host
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     self.driver.do_setup(ctxt)
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 153, in do_setup
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     self._helpers.compression_enabled()
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/ibm/storwize_svc/helpers.py"", line 54, in compression_enabled
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager     if resp[key] != '0':
2014-03-06 01:48:26.409 25160 TRACE cinder.volume.manager KeyError: 'license_compression_enclosures'
it should lack of  license_compression_enclosures when compression is not enabled for the system. so need add checking in the code.","resolve KeyError for IBM Storwize/SVC driver

When using iSCSI protocol to conenct IBM v7000, and compression
is not enabled for the system, KeyError may occur because of the response
item's lack of 'license_compression_enclosures'. So add a check to
resolve it.

Change-Id: Ie53631ea5b047650897313ad5614f6e1df5377f2
Closes-Bug: #1288645"
1644,df315513efe62667220562dd2edb5401b15ab2ba,1391562746,,1.0,31,2,2,2,1,0.745517843,True,1.0,90722.0,29.0,3.0,False,7.0,2574552.0,11.0,3.0,4.0,2108.0,2111.0,4.0,1819.0,1822.0,4.0,282.0,285.0,0.00660502,0.373844122,0.377807133,273,678,1267291,neutron,df315513efe62667220562dd2edb5401b15ab2ba,0,0,This possibly causes,"Bug #1267291 in neutron: ""DB lock wait timeout is possible for metaplugin""","There are some places that a target plugin's method is called in
'with context.session.begin(subtransaction=True)'.
This causes 'lock wait timeout' error potentially.
--- error is like:
OperationalError: (OperationalError) (1205, 'Lock wait timeout exceeded; try restarting transaction') ...snip
---
For example say ml2 is target plugin. Ml2's mechanism drivers separates precommit and postcommit
method so that 'lock wait timeout' error does not occur.
But it is meaningless if ml2 is used under metaplugin.","Call target plugin out of DB transaction in the Metaplugin

Previously, there are cases that a target plugin is called
within a DB transaction in the metaplugin. This possibly causes
""DB lock timeout"" error since a target plugin may take a long
time (ex. communicate to a controller).

Change-Id: Ie2971bfae1fecc97086b58cd943c321feb0aba04
Closes-Bug: #1267291"
1645,dfb1a090334d705425e739e8174a9c1c07009433,1392066568,,1.0,65,33,4,2,1,0.917119653,True,11.0,942836.0,63.0,15.0,False,15.0,912306.0,71.0,6.0,197.0,810.0,883.0,189.0,760.0,830.0,187.0,775.0,842.0,0.133617626,0.551528074,0.599147122,390,803,1279137,cinder,dfb1a090334d705425e739e8174a9c1c07009433,1,1,Bug in tests that didn’t show a bug in code,"Bug #1279137 in Cinder: ""3par drivers can't delete a volume being copied""","When running the driver certification tests for Icehouse, we found a bug with the 3PAR FC/iSCSI drivers.
The tempest test calls cinder to create a clone of a volume, and then immediately calls delete on the clone.  The 3PAR won't allow you to do that unless you stop the copy first.   You get a very generic exception, that isn't helpful.","Update 3PAR drivers to pass cert test

This patch updates both the HP 3PAR Fibre Channel
and HP 3PAR iSCSI block storage drivers so that they
can pass the driver certification tests.  We also updated
the unit tests to use mock instead of mox.

These versions of the drivers require the new 3.0.0 hp3parclient,
which can be downloaded from the pypi repository:
https://pypi.python.org/pypi/hp3parclient
The new hp3parclient requires the 3.1.3 firmware on the HP 3PAR.

The driver certification results:
Related-Bug: #1278575
Related-Bug: #1278577

Closes-Bug: #1279137

Change-Id: I72e0a76c865e54c58e41cc4409555d9042e30267
DocImpact: Document new driver requirements."
1646,e011130104950c854bc9e139e24422b753653063,1390318450,,1.0,9,9,1,1,1,0.0,True,4.0,395858.0,14.0,6.0,False,1.0,717154.0,2.0,1.0,15.0,1115.0,1123.0,15.0,937.0,945.0,8.0,972.0,974.0,0.006661732,0.720207254,0.721687639,31,343,1244609,cinder,e011130104950c854bc9e139e24422b753653063,1,1,Typo in message,"Bug #1244609 in Cinder: ""error message on wrong volume size is different between clone from volume and clone from image""","I am working with gluster configured as cinder backend.
if I try to clone a volume from image when the new volume size is smaller than the image we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --image-id 0924d3a4-d163-4ae5-8f57-fb9f1912ee26 --display-name dafna_new
ERROR: Invalid input received: Image minDisk size 20 is larger than the volume size 10.
if I try to clone a volume from a volume when the new volume is smaller than the volume we clone from we get the following error:
[root@cougar06 ~(keystone_admin)]# cinder create 10 --source-volid c3b6cb41-d78d-420d-b64c-d5f3782f7772 --display-name new_clone
ERROR: Invalid input received: Clones currently disallowed when 10 < 20. They must be >= original volume size.","Updated error messages for volume clone

Updated error messages for volume clone from image,
snapshot and volume to be consistent across.
Updated error messages to indicate the unit (GB).

Change-Id: I9f51ad0c9ce40864a838fca06583d7d4283e61d8
Closes-Bug: #1244609"
1647,e0228eb7e739956978f002f58edf35bc4d698022,1400620772,,1.0,12,5,2,2,1,0.522559375,False,,,,,True,,,,,,,,,,,,,,,,,906,1342,1321872,nova,e0228eb7e739956978f002f58edf35bc4d698022,1,1,,"Bug #1321872 in OpenStack Compute (nova): ""Resize/migrate stopped instance fails with Neutron event timeout""","I originally thought this was bug 1306342 but that's a different issue, that was when not having neutron configured properly for calling back to nova and nova would timeout on spawn waiting for a notification from neutron that networking was setup.
This is a different issue where resize/migrate fails if you started from a stopped instance and using neutron.  In this case, the _create_domain_and_network method in the libvirt driver passes in power_on=False since the instance was stopped before the resize/migration.  The virtual interface isn't plugged in that case so we're waiting on a neutron event that's not going to happen, and we hit the eventlet timeout which then tries to destroy the non-running domain, and that fails with a libvirtError telling you that the domain isn't running in the first place.
The fix is to check the power_on flag in _create_domain_and_network and if it's False, don't wait for neutron events, same as if vifs_already_plugged=False is passed in.","Do not wait for neutron event if not powering on libvirt domain

When resizing/migrating a stopped instance, the power_on flag is False
in _create_domain_and_network so virtual interfaces are not plugged and
we are waiting for an event callback from Neutron that won't happen.
This leads to an eventlet timeout and subsequent attempt to destroy the
non-running domain, which results in a libvirtError.

This change just adds the power_on flag to the list of conditions for
determining if we should wait for an event callback from Neutron.

Closes-Bug: #1321872

Change-Id: Id87ed125db8b1e807b7fa0d99180d0820edbed3e"
1648,e0552af51ce1246e9c938720009c89ccfa008f2f,1408755288,,1.0,10,3,2,2,1,0.779349837,False,,,,,True,,,,,,,,,,,,,,,,,1232,1691,1361487,nova,e0552af51ce1246e9c938720009c89ccfa008f2f,0,0,Feature. “Allow backup operation in paused and suspend state”,"Bug #1361487 in OpenStack Compute (nova): ""backup operation cannot be done in pause and suspend state""","jichen@cloudcontroller:~$ nova backup jitest3 jiback1 daily 2
ERROR (Conflict): Cannot 'createBackup' while instance is in vm_state paused (HTTP 409) (Request-ID: req-7554dea8-92aa-480c-a1f4-e3d7e479c6b3)
jichen@cloudcontroller:~$ nova list
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| ID                                   | Name    | Status | Task State | Power State | Networks           |
+--------------------------------------+---------+--------+------------+-------------+--------------------+
| cb7c6742-7b7a-44de-ad5a-8570ee520f9e | jitest1 | ACTIVE | -          | Running     | private=10.0.0.2   |
| 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 | jitest3 | PAUSED | -          | Paused      | private=10.0.0.200 |
+--------------------------------------+---------+--------+------------+-------------+--------------------+
jichen@cloudcontroller:~$ nova image-create  --show jitest3 test3image1
+-------------------------------------+--------------------------------------+
| Property                            | Value                                |
+-------------------------------------+--------------------------------------+
| OS-EXT-IMG-SIZE:size                | 0                                    |
| created                             | 2014-08-26T04:06:41Z                 |
| id                                  | 96a5284c-5feb-4231-8b01-9a522a7c5aab |
| metadata base_image_ref             | 94e061fb-e628-4deb-901c-9d44c059ecd9 |
| metadata clean_attempts             | 2                                    |
| metadata image_type                 | snapshot                             |
| metadata instance_type_ephemeral_gb | 0                                    |
| metadata instance_type_flavorid     | 1                                    |
| metadata instance_type_id           | 2                                    |
| metadata instance_type_memory_mb    | 512                                  |
| metadata instance_type_name         | m1.tiny                              |
| metadata instance_type_root_gb      | 1                                    |
| metadata instance_type_rxtx_factor  | 1.0                                  |
| metadata instance_type_swap         | 0                                    |
| metadata instance_type_vcpus        | 1                                    |
| metadata instance_uuid              | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| metadata kernel_id                  | 20be8b63-5a84-4440-a0bd-8f69898d5965 |
| metadata ramdisk_id                 | 07f6f85f-c1dc-4790-98b5-14ab86f21b59 |
| metadata user_id                    | 256dc6db4b5c45ae90fee8132cbaad7c     |
| minDisk                             | 1                                    |
| minRam                              | 0                                    |
| name                                | test3image1                          |
| progress                            | 25                                   |
| server                              | 702d1d2b-f72d-4759-8f13-9ffbcc0ca934 |
| status                              | SAVING                               |
| updated                             | 2014-08-26T04:06:41Z                 |
+-------------------------------------+--------------------------------------+","Allow backup operation in paused and suspend state

Snapshot operation allows paused and suspend instance to be
executed, this also should be applied to backup operation.

Change-Id: Ie7bfd4962f0f149c423f71dbc528a2736baeef97
Closes-Bug: #1361487"
1649,e066158b5235a3879fe90fa3bd813fc3363c01f5,1385591580,,1.0,99,56,11,5,1,0.523104575,True,7.0,1734661.0,39.0,18.0,False,84.0,373172.8182,294.0,2.0,1.0,379.0,380.0,1.0,340.0,341.0,0.0,346.0,346.0,0.000834028,0.28940784,0.28940784,55,383,1247998,cinder,e066158b5235a3879fe90fa3bd813fc3363c01f5,1,1,There is a problem,"Bug #1247998 in Cinder: ""Create volume from image does not convert with ceph""","When creating a volume from image using the RBD backend for cinder, the image will be copied directly and not converted even if the image is not raw (e.g. qcow2). This causes mounting and booting to fail as the volume is not raw.
The problem, I think, is in clone_image in the RBD volume driver, which doesn't check the type of the source image and doesn't convert it.  It is called from CreateVolumeFromSpecTask#_create_from_image. Implementations of clone_image for other drivers seem to do the conversion correctly.","Do not clone non-raw images in rbd backend

RBD backend only supports booting from images in raw format. A volume
that was cloned from an image in any other format is not bootable. The
RBD driver will consider non-raw images to be uncloneable to trigger
automatic conversion to raw format.

Includes conversion of the corresponding unit test to use mock (instead
of mox) and expanded comments and error messages based on change #58893
by Edward Hope-Morley.

Change-Id: I5725d2f7576bc1b3e9b874ba944ad17d33a6e2cb
Closes-Bug: #1246219
Closes-Bug: #1247998"
1650,e0844f257cd170d01255aa3f8bfd48ff50731f16,1397366996,,1.0,57,1,4,4,1,0.78625483,False,,,,,True,,,,,,,,,,,,,,,,,616,1041,1292309,nova,e0844f257cd170d01255aa3f8bfd48ff50731f16,0,0,Catch a new exception,"Bug #1292309 in OpenStack Compute (nova): ""Exception handle in be better when create a flavor""","If I create a flavor but failed to operate on db layer , a InstanceTypeCreateFailed exception will be raised but
nova-api didn't handle it well, it will report
[root@controller ~]# nova flavor-create test1 111 512 1 1
ERROR: The server has either erred or is incapable of performing the requested operation. (HTTP 500) (Request-ID: req-74050da0-6f78-408e-8a1b-97c2a03e597d)
give more accurate reason will be better","Handle a flavor create failed better

Catch FlavorCreateFailed exceptions which when flavor creation fails.
This type of flavor creation failure is caused by an internal nova problem.
Return an internal server error (500) to the user so they know
to contact their cloud provider.

Closes-Bug: #1292309

Change-Id: I268c84dae290b0170448d789509af781555df9b0"
1651,e088c73b185073943d9b18b7ede0375c096ee8b3,1393345801,1.0,1.0,7,3,2,2,1,0.970950594,True,3.0,29846.0,19.0,9.0,False,16.0,63353.0,71.0,4.0,84.0,1550.0,1552.0,75.0,1321.0,1323.0,78.0,1384.0,1385.0,0.053741497,0.942176871,0.942857143,474,889,1284368,cinder,e088c73b185073943d9b18b7ede0375c096ee8b3,1,1,Returning the wrong value,"Bug #1284368 in Cinder: ""Extending volume is in GB yet 3PAR is expecting MB""","Extending volume diff is in GB yet 3PAR is expecting MB
Extending Volume osv-iGH8frZFSDuht.TTE90Erw from 3 to 7, by 4 GB. from (pid=60905) extend_volume /opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py:246
Cinder is correct
cinder extend 7f688411-9618-460e-a0f6-eb5dc333bc1a 7
Results in incorrect virtual volume size for thin provisioned and extended volume","3PAR: Fix extend volume GiB to MiB

Extend volume was sending GiB instead of MiB to the 3PAR
array. This patch is now sending the growth size in MiBs
instead of GiBs.

Change-Id: Ib4903315a32e090e760883cc80c50f3698fe015f
Closes-Bug: #1284368"
1652,e0f69d69293f0ffba22a6540f483f05baa48cd6f,1392034790,,1.0,99,52,5,3,1,0.848360888,True,5.0,3656794.0,92.0,23.0,False,16.0,148852.8,27.0,5.0,4.0,880.0,880.0,4.0,792.0,792.0,4.0,351.0,351.0,0.006468305,0.455368693,0.455368693,611,1036,1292173,neutron,e0f69d69293f0ffba22a6540f483f05baa48cd6f,0,0,Remove list events,"Bug #1292173 in neutron: ""Remove list events API from Cisco N1kv neutron""","Earlier Cisco was using the list events api to poll policies from VSM.
It was inefficient and caused delay in processing.
So, now Cisco switched to list profiles to poll policies from VSM.","Remove List events API from Cisco N1kv Neutron

Earlier Cisco N1kv plugin was using list events api to
poll policies from VSM.
It was inefficient and caused delay in processing.
So, now Cisco N1kv plugin  switched to list profiles to
poll policies from VSM.

Change-Id: Ia734735361dc3eaee8e276ada0c80045eaa9ef96
Closes-Bug: #1292173"
1653,e10f406a653f3037109e2d7f59bed65bd54300a8,1395429434,,1.0,32,34,3,3,1,0.62823064,True,1.0,22614.0,22.0,4.0,False,14.0,212917.0,37.0,3.0,518.0,1613.0,1843.0,420.0,1379.0,1550.0,261.0,789.0,867.0,0.256360078,0.772994129,0.849315068,643,1069,1294166,neutron,e10f406a653f3037109e2d7f59bed65bd54300a8,1,1,,"Bug #1294166 in neutron: ""nec plugin fails with XML test_list_ports_binding_ext_attr tempest tests""","commit 1ec6e18007691c92fc27235c677d11b0fe1c1f6b in tempest adds a validation for neutron port binding extension, but the check is too strict. binding:profile attribute in port binding extension is designed to allow plugin specific field but the check assumes binding:profile attribute implemented in ML2 plugin. This leads to test failures with neutron plugins which has different keys in binding:profile like NEC plugin or Mellanox plugin (though mellanox plugin does not test the related test now).
The failed test is test_list_ports_binding_ext_attr. It only affects XML.
  tempest.api.network.test_ports.PortsAdminExtendedAttrsIpV6TestXML test_list_ports_binding_ext_attr
  tempest.api.network.test_ports.PortsAdminExtendedAttrsTestXML test_list_ports_binding_ext_attr
Tempest test should allow plugin-specifc binding:profile attribute. If not, it should provide an options to disable these tests.","NEC plugin: Remove a colon from binding:profile key due to XML problem

In XML specification, if a tag name contains a colon, a part before
a colon is interpreted as namespace. If key names in binding:profile
dict contains a colon, Neutron XML serializer sends them as-is.
A colon in key names should be used only if XML namespace are used.
NEC plugin uses a colon in key names and it leads to failures to XML
deserialization. To avoid this problem, this commit removes a colon
and prefix n key names in port binding:profile so that client XML
deserializer decodes XML response successfully.

Change-Id: Ie32a2417bbce03bfc6e8f7907c0b4090fbc9e7b6
Closes-Bug: #1294166"
1654,e13d19cab384a9f5f8a00436ad39118f342af32c,1394967678,,1.0,3,3,3,3,1,1.0,True,6.0,2660046.0,102.0,32.0,False,15.0,1161057.333,18.0,11.0,39.0,1806.0,1806.0,39.0,1527.0,1527.0,39.0,874.0,874.0,0.040983607,0.896516393,0.896516393,629,1055,1293083,neutron,e13d19cab384a9f5f8a00436ad39118f342af32c,1,1,report interval leading to overload,"Bug #1293083 in neutron: ""report_interval too frequent; Causing load on service, failing high CPU usage operations""","report_interval is how often an agent sends out a heartbeat to the service. The Neutron service responds to these 'report_state' RPC messages by updating the agent's heartbeat DB record. The last heartbeat is then compared to the configured agent_down_time to determine if the agent is up or down. The agent's status is used when scheduling networks on DHCP and L3 agents.
The defaults are 4 seconds for report_interval and 9 for agent_down_time.
On a setup with 18 agents (15 layer 2, L3, DHCP, metadata) sitting on 16 nodes, and a Neutron service sitting on a dedicated powerful machine, the service was idle with 20% CPU usage. Changing the report_interval to 28 seconds and agent_down_time to 60 seconds changed the CPU usage to 1%, and allowed bulk operations on a larger scale. (In this case: Creating 30 instances at the same time with 60 ports). With the original values the operation failed (The instances did not get IP addresses), and with the new values we were able to boot 60 instances successfully. Side note: This flow will work better once the Nova-Neutron race is resolved, but that's orthogonal to this proposal.","Change report_interval from 4 to 30, agent_down_time from 9 to 75

report_interval is how often an agent sends out a heartbeat to the
service. The Neutron service responds to these 'report_state' RPC
messages by updating the agent's heartbeat DB record.
The last heartbeat is then compared to the configured
agent_down_time to determine if the agent is up or down.
The agent's status is used when scheduling networks on DHCP
and L3 agents.

In the spirit of sane defaults suited for production, these values
should be bumped to reduce the load on the Neutron service
dramatically, freeing up CPU time to perform intensive operations.

DocImpact
Closes-Bug: #1293083
Change-Id: I77bcf8f66f74ba55513c989caead1f96c92b9832"
1655,e145ef798d2c80ea48be2826f4c648e72958f5d1,1395267568,,1.0,2,1,1,1,1,0.0,True,1.0,75650.0,7.0,2.0,False,1.0,3495524.0,1.0,3.0,1201.0,2216.0,2216.0,950.0,1711.0,1711.0,1056.0,1215.0,1215.0,0.687703318,0.791151594,0.791151594,654,1080,1294724,cinder,e145ef798d2c80ea48be2826f4c648e72958f5d1,1,1,RPC mechanism was forgoten.,"Bug #1294724 in Cinder: ""c-api and c-vol emmit tons of oslo.messaging ERRORs on successful test runs""","On a successful test run cinder vol and api services are emmitting tons of oslo.messaging errors in their consoles of the form:
2014-03-18 23:16:34.316 27093 ERROR oslo.messaging.notify._impl_messaging [req-ecce1edc-8383-4fe9-ad11-704ef9deb1e8 fee2f4dc1ff648f0a37e2ef527d9fc20 39971f7616a447e5ad26dbe3ac202a32 - - -] Could not send notification to notifications. ... <payload>
This can be seen in a live log at: http://logs.openstack.org/95/80895/3/check/check-tempest-dsvm-full/bf79bbb/console.html#_2014-03-18_23_20_29_508
This ends up being hugely scary for deployers, and it's lots of ERRORs in logs, even when things are working perfectly normally.","Add RequestContextSerializer for rpc notifications

RequestContext should be serialized when sent via oslo.messaging. The
serializer is correctly used for the general RPC mechanism, but has been
forgotten in the notifier. This patch fixes that.

Thanks goes to eharney for noticing this and pointing it out!!!

Same bug existed in Nova (1275771)

Change-Id: I99a571ef74bbcd13bf801313eea145ea69f821e1
Closes-Bug: #1294724"
1656,e15ce7735e492f9eb0914efb621211e315ea40d9,1409128385,,1.0,18,18,2,2,1,0.503258335,False,,,,,True,,,,,,,,,,,,,,,,,907,1343,1322025,nova,e15ce7735e492f9eb0914efb621211e315ea40d9,1,1,,"Bug #1322025 in OpenStack Compute (nova): ""[EC2] Terminateinstance returns incorrect current state name""","TerminateInstance returns the currentstate name and previousstate name are same.
In the below sample response elements show the currnentstate name and previoustate name as ""running"".
Ideally the currentstate name should be ""terminated"".
==
<TerminateInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2013-10-15/"">
  <requestId>req-c15f5c7d-2551-4a08-b8b8-255462a09592</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>16</code>
        <name>running</name>
      </currentState>
      <previousState>
        <code>16</code>
        <name>running</name>
      </previousState>
    </item>
  </instancesSet>
</TerminateInstancesResponse>
==","Fix the current state name as 'shutting-down'

Currently, the TerminateInstance returns the currentstate name
as previous state name. Ideally the TerminateInstance return
element's currentstate named as ""shutting-down"".

Addressed the issue and below is the terminateinstance response with fix:
==
<TerminateInstancesResponse xmlns=""http://ec2.amazonaws.com/doc/2013-10-15/"">
  <requestId>req-c15f5c7d-2551-4a08-b8b8-255462a09592</requestId>
  <instancesSet>
    <item>
      <instanceId>i-00000001</instanceId>
      <currentState>
        <code>32</code>
        <name>shutting-down</name>
      </currentState>
      <previousState>
        <code>16</code>
        <name>running</name>
      </previousState>
    </item>
  </instancesSet>
</TerminateInstancesResponse>
==

And also updated the testcases in nova/tests/api/ec2/test_cloud.py
appropriately.

Closes-bug: #1322025

Change-Id: I790e7d86ef8fd9f4fc1612bae2788e0ae7ef9719"
1657,e19a2ae0b37a5ec74f77d3cdd8e639f2dc580ccf,1382691687,2.0,1.0,78,137,4,4,1,0.965345948,True,3.0,3024241.0,24.0,9.0,False,7.0,814900.0,8.0,4.0,4.0,829.0,832.0,4.0,764.0,767.0,3.0,229.0,231.0,0.008064516,0.463709677,0.467741935,1729,1245797,1245797,neutron,e19a2ae0b37a5ec74f77d3cdd8e639f2dc580ccf,0,0,Feature3 “MidonetInterfaceDriver should use the mm-ctl script to bind ports.”,"Bug #1245797 in neutron: ""MidonetInterfaceDriver does not use mm-ctl to bind ports""",MidonetInterfaceDriver should use the mm-ctl script to bind ports. Currently it uses Midonet API calls. This bug is to change MidonetInterfaceDriver to correctly use the mm-ctl script.,"Move MidonetInterfaceDriver and use mm-ctl

* Change the plug method in MidonetInterfaceDriver to use mm-ctl
* Move MidonetInterfaceDriver to interface.py
* adapt interface driver midonet unit tests to mm-ctl

Change-Id: Ib6cfbc212b793fa939cad17017c0b2b8b0a5b7fb
Closes-Bug: #1245797"
1658,e19e2b812031802dd46f2f29b0a0508b9f7ac470,1390749627,,1.0,2,5,1,1,1,0.0,True,1.0,690946.0,20.0,6.0,False,31.0,216838.0,100.0,2.0,157.0,721.0,760.0,157.0,551.0,590.0,146.0,564.0,592.0,0.130550622,0.501776199,0.526642984,241,645,1265448,glance,e19e2b812031802dd46f2f29b0a0508b9f7ac470,1,0,Evolution bug,"Bug #1265448 in Glance: ""exception.NotFound is parsed wrong at the API level""","When user updates an image with location but introduced a typo, it's expected to run into location not found exception. However, because we don't distinguish the image not found and location not found, so Glance will say the image is not found based on current design.
Failed to find image 41a997ac-e647-4b5f-b158-8e6e13875f88 to update
See https://github.com/openstack/glance/blob/master/glance/api/v2/images.py#L124 and https://github.com/openstack/glance/blob/master/glance/store/filesystem.py#L156 Though I think the issue is also existed in other backends.","Don't rewrite the NotFound error message

When user updates an image with a nonexisted location, it's
expected to run into location not found exception. However,
because the error message is rewritten in v2 image update,
so user will get below wrong message:
Failed to find image <image-id> to update

This fix will remove the rewritten message, given the NotFound
message has been set at: glance/db/sqlalchemy/api.py#L159

Closes-Bug: #1265448

Change-Id: I4e5fbbe40ce0fd13e828a40a424d4fcdc66a236b"
1659,e1f8664c9fa83f77f5bb763ffcc3157905ed954c,1409734753,1.0,1.0,14,1,2,2,1,0.996791632,False,,,,,True,,,,,,,,,,,,,,,,,1220,1679,1359138,nova,e1f8664c9fa83f77f5bb763ffcc3157905ed954c,1,1,,"Bug #1359138 in OpenStack Compute (nova): ""[OSSA 2014-037] vmware: deletion VM in resize state will cause VM-orig get leak (CVE-2014-8333)""","For vmware vcenter driver,  resize a VM, during resizing , at the same time, delete the vm, the VM-orig  can not be deleted in ESXi host. So makes VM leaks.","VMWare: Fix VM leak when deletion of VM during resizing

During the VM resizing, before VM arrive RESIZED state, driver
migrate_disk_and_power_off will initially rename orginal vm
'uuid' to be 'uuid-orig' and clone a new vm with 'uuid' name.
When deletion VM is triggered at this time window, it wouldn't
be able to delete the VM uuid-orig in VCenter and so cause VM leak.
As VM task state will be set to 'deleting' and it can not be used to
determine the resize migrating/migrated state, this fix will
attempt to delete orig VM within destroy phase.

Change-Id: I7598afbf0dc3c527471af34224003d28e64daaff
Closes-Bug: #1359138"
1660,e1fb64a7a6c675d909fe76757ae6671e51de9a51,1405704358,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1081,1531,1344178,swift,e1fb64a7a6c675d909fe76757ae6671e51de9a51,1,1,"“Pass disk usage options when using —all""","Bug #1344178 in OpenStack Object Storage (swift): ""Disk usage options are not applied when using --all with swift-recon""","It is currently possible to have swift-recon report on disk usage by two means:
swift-recon -d OR
swift-recon --all (which includes -d)
Disk usage provides two options:
--human-readable to provide usage in MB/GB/TB++ rather than bytes
--top <integer> to list devices with the most usage
These options are currently usable when using -d but not --all.
This is because the options are not passed to disk_usage when using all:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L930
Versus when using -d:
https://github.com/openstack/swift/blob/d317888a7eae276ae9dddf26a9030d01f6ba00fe/swift/cli/recon.py#L965","Pass disk usage options when using --all

This commit allows the arguments --human-readable and --top to be
used with --all to customize the output of the disk usage portion.

There are no unit tests attached to this commit, I have created and
referenced bug #1344200 which shows there is a larger bug to address
to create several missing tests.

Change-Id: I46b8359533989efc3067971acaafec1d0cbc2b9f
Closes-Bug: #1344178
Related-Bug: #1344200"
1661,e1fdd8f9b1536c75e36dd74416dd3aad1495e203,1386622490,2.0,1.0,92,133,3,3,1,0.664821554,True,8.0,6071302.0,76.0,25.0,False,143.0,27479.0,434.0,4.0,12.0,1734.0,1737.0,12.0,1310.0,1313.0,12.0,1518.0,1521.0,0.001906158,0.222727273,0.223167155,1768,1251803,1251803,nova,e1fdd8f9b1536c75e36dd74416dd3aad1495e203,1,1, ,"Bug #1251803 in OpenStack Compute (nova): ""Disabled Reason column becomes empty after update host is disabled""","liugya@liugya-ubuntu:~$ nova host-update --status disable liugya-ubuntu
liugya@liugya-ubuntu:~$ nova service-list
/usr/lib/python2.7/dist-packages/gobject/constants.py:24: Warning: g_boxed_type_register_static: assertion `g_type_from_name (name) == 0' failed
  import gobject._gobject
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| Binary           | Host          | Zone     | Status   | State | Updated_at                 | Disabled Reason |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+
| nova-conductor   | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:33.000000 | None            |
| nova-compute     | liugya-ubuntu | nova     | disabled | up    | 2013-11-16T03:06:29.000000 |                 | <<<<<<< Reason is empty now
| nova-cert        | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:34.000000 | None            |
| nova-network     | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:26.000000 | None            |
| nova-scheduler   | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:31.000000 | None            |
| nova-consoleauth | liugya-ubuntu | internal | enabled  | up    | 2013-11-16T03:06:32.000000 | None            |
+------------------+---------------+----------+----------+-------+----------------------------+-----------------+","libvirt: making set_host_enabled to be a private methods

Currently _set_host_enabled() is being used only for internal libvirt
driver purposes, which doesn't correlate to set_host_enabled in the
compute API.

_set_host_enabled in the driver is disabling and re-enabling
the compute service automatically and shouldn't be exposed to the users.

Also, adding disable_reason as an optional variable and enabled variable
to be boolean only.

Change-Id: I5fd13ef352fcbb4a1c96764a9e473cdaf7b128da
Closes-Bug: #1251803"
1662,e2163abce55bba83f531b4a0c9286b7b329602e9,1384256983,,1.0,1,1,1,1,1,0.0,True,2.0,857039.0,16.0,11.0,False,1.0,10240991.0,1.0,9.0,26.0,2086.0,2103.0,26.0,1901.0,1918.0,8.0,481.0,481.0,0.016917293,0.906015038,0.906015038,1749,1250405,1250405,neutron,e2163abce55bba83f531b4a0c9286b7b329602e9,1,1,“there is mistake in DateTime”,"Bug #1250405 in neutron: ""Mistake in sqlalchemy DateTime type usage""","In migration f9263d6df56_remove_dhcp_lease there is mistake in DateTime type usage for mysql, used sa.DATETIME() instead of sa.DateTime()
   File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/f9263d6df56_remove_dhcp_lease.py"", line 46, in downgrade
    nullable=True))
  File ""<string>"", line 7, in add_column
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 363, in add_column
    schema=schema
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 127, in add_column
    self._exec(base.AddColumn(table_name, column, schema=schema))
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/ddl/impl.py"", line 76, in _exec
    conn.execute(construct, *multiparams, **params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1449, in execute
    params)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1542, in _execute_ddl
    compiled
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1698, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/base.py"", line 1691, in _execute_context
    context)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/engine/default.py"", line 331, in do_execute
    cursor.execute(statement, parameters)
ProgrammingError: (ProgrammingError) type ""datetime"" does not exist
LINE 1: ALTER TABLE ipallocations ADD COLUMN expiration DATETIME
                                                        ^
 'ALTER TABLE ipallocations ADD COLUMN expiration DATETIME' {}","Fix sqlalchemy DateTime type usage

In migration f9263d6df56_remove_dhcp_lease there is mistake in
DateTime type usage for mysql, used sa.DATETIME() instead of
sa.DateTime()

Closes-Bug: #1250405

Change-Id: I41fcd3babc64e3307895a92f03c7417577fa7b1d"
1663,e251535eaa7c6a564eddb9218de177c9bea656bb,1401435153,,1.0,10,5,2,2,1,0.996791632,False,,,,,True,,,,,,,,,,,,,,,,,540,961,1288609,nova,e251535eaa7c6a564eddb9218de177c9bea656bb,1,1,wrong,"Bug #1288609 in OpenStack Compute (nova): ""nova-manage creates network with wrong vlanid""","I faced a bug in latest nova network:
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.0.0/24  --vlan 500
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.1.0/24  --vlan 501
[root@host awasilyev]# /usr/bin/nova-manage network create novanetwork 172.26.2.0/24  --vlan 502
[root@host awasilyev]# nova-manage network list
id   	IPv4              	IPv6           	start address  	DNS1           	DNS2           	VlanID         	project        	uuid
64   	172.26.0.0/24     	None           	172.26.0.3     	8.8.4.4        	None           	500            	None           	225c8cbf-89bb-4171-b405-0047012a7803
65   	172.26.1.0/24     	None           	172.26.1.3     	8.8.4.4        	None           	502            	None           	d461b285-d9c6-4a8c-ae39-5a657bb5926a
66   	172.26.2.0/24     	None           	172.26.2.3     	8.8.4.4        	None           	504            	None           	4c5a5d5b-24c8-4833-8bd0-6dcca11acb68
I try to create 3 networks, specifying exact vlan number for each network. But nova-manage creates networks using wrong vlan id's.
My previous openstack install (it was 3-4 monthes ago) does not have this bug.","Fix the wrong dest of 'vlan' option and add new 'vlan_start' option

* Allow to specify vlan tag when create nova network, the dest of vlan
option should be 'vlan' instead of 'vlan_start'
* Provide new option 'vlan_start' when create nova network
* test case is updated.

DocImpact
Closes-Bug: #1288609
Change-Id: I8dd858e4cf17d8f689613554d1fbc3e56f220886"
1664,e299349b3be240d799822daedfa2730fd96839d3,1403275282,1.0,1.0,10,11,7,6,1,0.894400471,False,,,,,True,,,,,,,,,,,,,,,,,995,1436,1332571,neutron,e299349b3be240d799822daedfa2730fd96839d3,0,0,Feature? “They should be replaced by a unique constant equals to 15 to ensure consistency”,"Bug #1332571 in neutron: ""Multiple constants define linux interface maximum length""","Multiple constants define linux interface maximum length (15):
* neutron.agent.linux.utils: DEVICE_NAME_LEN in get_interface_mac (=15)
* neutron.agent.linux.ip_lib: VETH_MAX_NAME_LENGTH (=15)
* neutron.plugins.common.constants: MAX_DEV_NAME_LEN (=16 incorrect value)
They should be replaced by a unique constant equals to 15 to ensure consistency.","Merge multiple constants defining linux interface maximum length

DEVICE_NAME_MAX_LEN constant in neutron.common.constants replaces
and correct multiple constants defining linux interface maximum
length.

Closes-Bug: #1332571
Change-Id: I63f760a21e17dcd57b3685b1e71c913d2722e097"
1665,e2efa57ab2fb03b92f80abbf612a257c28475140,1382446269,0.0,1.0,7,3,2,2,1,0.881290899,True,5.0,13125867.0,82.0,26.0,False,28.0,365718.0,69.0,5.0,1353.0,2644.0,2647.0,1225.0,2404.0,2407.0,527.0,1515.0,1516.0,0.082409864,0.236616201,0.23677228,1438,1195139,1195139,nova,e2efa57ab2fb03b92f80abbf612a257c28475140,1,0,Change in the environment “This fails is some DBs like postgresql. Needs a fix in the VMware driver”,"Bug #1195139 in OpenStack Compute (nova): ""vmware: error trying to store hypervisor_version as string in using postgresql""","Hi,
I am trying to use VMware hypervisor ESXi 5.0.0 as a compute resource in OpenStack 2013.1.1. The driver being used is ""•vmwareapi.VMwareVCDriver"". The vCenter is contains a single cluster (two nodes) running ESXi version 5.0.
In the compute node, I am seeing the below error
2013-06-26 17:45:27.532 10253 AUDIT nova.compute.resource_tracker [-] Free ram (MB): 146933
2013-06-26 17:45:27.532 10253 AUDIT nova.compute.resource_tracker [-] Free disk (GB): 55808
2013-06-26 17:45:27.533 10253 AUDIT nova.compute.resource_tracker [-] Free VCPUS: 24
2013-06-26 17:45:27.533 10253 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-06-26 17:45:27.534 10253 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is ece63342e4254910be13ee92b948ace6 multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-06-26 17:45:27.534 10253 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is c85fb461be22468b846368a6b8608ac2. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-06-26 17:45:27.566 10253 DEBUG nova.openstack.common.rpc.amqp [-] Making synchronous call on conductor ... multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:583
2013-06-26 17:45:27.567 10253 DEBUG nova.openstack.common.rpc.amqp [-] MSG_ID is 3f5d065d1c8a4989b4d36415d45abe8b multicall /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:586
2013-06-26 17:45:27.567 10253 DEBUG nova.openstack.common.rpc.amqp [-] UNIQUE_ID is 1421bebfd9744271be63dcef74551c93. _add_unique_id /usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py:337
2013-06-26 17:45:27.597 10253 CRITICAL nova [-] Remote error: DBError (DataError) invalid input syntax for integer: ""5.0.0""
LINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, 'VMware ESXi', '5.0.0', '...
                                                             ^
 'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id' {'local_gb': 55808, 'vcpus_used': 0, 'deleted': 0, 'hypervisor_type': u'VMware ESXi', 'created_at': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), 'local_gb_used': 0, 'updated_at': None, 'hypervisor_hostname': u'10.100.10.42', 'memory_mb': 147445, 'current_workload': 0, 'vcpus': 24, 'free_ram_mb': 146933, 'running_vms': 0, 'free_disk_gb': 55808, 'service_id': 7, 'hypervisor_version': u'5.0.0', 'disk_available_least': None, 'deleted_at': None, 'cpu_info': u'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}', 'memory_mb_used': 512}
[u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 430, in _process_data\n    rval = self.proxy.dispatch(ctxt, version, method, **args)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 133, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 350, in compute_node_create\n    result = self.db.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/api.py"", line 192, in compute_node_create\n    return IMPL.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 96, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 498, in compute_node_create\n    compute_node_ref.save()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 54, in save\n    session.flush()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 437, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (DataError) invalid input syntax for integer: ""5.0.0""\nLINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, \'VMware ESXi\', \'5.0.0\', \'...\n                                                             ^\n \'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id\' {\'local_gb\': 55808, \'vcpus_used\': 0, \'deleted\': 0, \'hypervisor_type\': u\'VMware ESXi\', \'created_at\': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), \'local_gb_used\': 0, \'updated_at\': None, \'hypervisor_hostname\': u\'10.100.10.42\', \'memory_mb\': 147445, \'current_workload\': 0, \'vcpus\': 24, \'free_ram_mb\': 146933, \'running_vms\': 0, \'free_disk_gb\': 55808, \'service_id\': 7, \'hypervisor_version\': u\'5.0.0\', \'disk_available_least\': None, \'deleted_at\': None, \'cpu_info\': u\'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}\', \'memory_mb_used\': 512}\n'].
2013-06-26 17:45:27.597 10253 TRACE nova Traceback (most recent call last):
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/bin/nova-compute"", line 85, in <module>
2013-06-26 17:45:27.597 10253 TRACE nova     service.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 689, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     _launcher.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 209, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     super(ServiceLauncher, self).wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 179, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     service.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 168, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     return self._exit_event.wait()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
2013-06-26 17:45:27.597 10253 TRACE nova     return hubs.get_hub().switch()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
2013-06-26 17:45:27.597 10253 TRACE nova     return self.greenlet.switch()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/eventlet/greenthread.py"", line 194, in main
2013-06-26 17:45:27.597 10253 TRACE nova     result = function(*args, **kwargs)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 147, in run_server
2013-06-26 17:45:27.597 10253 TRACE nova     server.start()
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/service.py"", line 446, in start
2013-06-26 17:45:27.597 10253 TRACE nova     self.manager.pre_start_hook(rpc_connection=self.conn)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 612, in pre_start_hook
2013-06-26 17:45:27.597 10253 TRACE nova     self.update_available_resource(nova.context.get_admin_context())
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/manager.py"", line 3877, in update_available_resource
2013-06-26 17:45:27.597 10253 TRACE nova     rt.update_available_resource(context)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/lockutils.py"", line 242, in inner
2013-06-26 17:45:27.597 10253 TRACE nova     retval = f(*args, **kwargs)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 272, in update_available_resource
2013-06-26 17:45:27.597 10253 TRACE nova     self._sync_compute_node(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 293, in _sync_compute_node
2013-06-26 17:45:27.597 10253 TRACE nova     self._create(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 293, in _sync_compute_node
2013-06-26 17:45:27.597 10253 TRACE nova     self._create(context, resources)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/compute/resource_tracker.py"", line 307, in _create
2013-06-26 17:45:27.597 10253 TRACE nova     values)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/conductor/api.py"", line 617, in compute_node_create
2013-06-26 17:45:27.597 10253 TRACE nova     return self.conductor_rpcapi.compute_node_create(context, values)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 349, in compute_node_create
2013-06-26 17:45:27.597 10253 TRACE nova     return self.call(context, msg, version='1.33')
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/proxy.py"", line 80, in call
2013-06-26 17:45:27.597 10253 TRACE nova     return rpc.call(context, self._get_topic(topic), msg, timeout)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/__init__.py"", line 140, in call
2013-06-26 17:45:27.597 10253 TRACE nova     return _get_impl().call(CONF, context, topic, msg, timeout)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/impl_kombu.py"", line 798, in call
2013-06-26 17:45:27.597 10253 TRACE nova     rpc_amqp.get_connection_pool(conf, Connection))
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 612, in call
2013-06-26 17:45:27.597 10253 TRACE nova     rv = list(rv)
2013-06-26 17:45:27.597 10253 TRACE nova   File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 561, in __iter__
2013-06-26 17:45:27.597 10253 TRACE nova     raise result
2013-06-26 17:45:27.597 10253 TRACE nova RemoteError: Remote error: DBError (DataError) invalid input syntax for integer: ""5.0.0""
2013-06-26 17:45:27.597 10253 TRACE nova LINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, 'VMware ESXi', '5.0.0', '...
2013-06-26 17:45:27.597 10253 TRACE nova                                                              ^
2013-06-26 17:45:27.597 10253 TRACE nova  'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id' {'local_gb': 55808, 'vcpus_used': 0, 'deleted': 0, 'hypervisor_type': u'VMware ESXi', 'created_at': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), 'local_gb_used': 0, 'updated_at': None, 'hypervisor_hostname': u'10.100.10.42', 'memory_mb': 147445, 'current_workload': 0, 'vcpus': 24, 'free_ram_mb': 146933, 'running_vms': 0, 'free_disk_gb': 55808, 'service_id': 7, 'hypervisor_version': u'5.0.0', 'disk_available_least': None, 'deleted_at': None, 'cpu_info': u'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}', 'memory_mb_used': 512}
2013-06-26 17:45:27.597 10253 TRACE nova [u'Traceback (most recent call last):\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/amqp.py"", line 430, in _process_data\n    rval = self.proxy.dispatch(ctxt, version, method, **args)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/rpc/dispatcher.py"", line 133, in dispatch\n    return getattr(proxyobj, method)(ctxt, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/conductor/manager.py"", line 350, in compute_node_create\n    result = self.db.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/api.py"", line 192, in compute_node_create\n    return IMPL.compute_node_create(context, values)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 96, in wrapper\n    return f(*args, **kwargs)\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/db/sqlalchemy/api.py"", line 498, in compute_node_create\n    compute_node_ref.save()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 54, in save\n    session.flush()\n', u'  File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/session.py"", line 437, in _wrap\n    raise exception.DBError(e)\n', u'DBError: (DataError) invalid input syntax for integer: ""5.0.0""\nLINE 1: ..., 7, 24, 147445, 55808, 0, 512, 0, \'VMware ESXi\', \'5.0.0\', \'...\n                                                             ^\n \'INSERT INTO compute_nodes (created_at, updated_at, deleted_at, deleted, service_id, vcpus, memory_mb, local_gb, vcpus_used, memory_mb_used, local_gb_used, hypervisor_type, hypervisor_version, hypervisor_hostname, free_ram_mb, free_disk_gb, current_workload, running_vms, cpu_info, disk_available_least) VALUES (%(created_at)s, %(updated_at)s, %(deleted_at)s, %(deleted)s, %(service_id)s, %(vcpus)s, %(memory_mb)s, %(local_gb)s, %(vcpus_used)s, %(memory_mb_used)s, %(local_gb_used)s, %(hypervisor_type)s, %(hypervisor_version)s, %(hypervisor_hostname)s, %(free_ram_mb)s, %(free_disk_gb)s, %(current_workload)s, %(running_vms)s, %(cpu_info)s, %(disk_available_least)s) RETURNING compute_nodes.id\' {\'local_gb\': 55808, \'vcpus_used\': 0, \'deleted\': 0, \'hypervisor_type\': u\'VMware ESXi\', \'created_at\': datetime.datetime(2013, 6, 26, 12, 15, 34, 804731), \'local_gb_used\': 0, \'updated_at\': None, \'hypervisor_hostname\': u\'10.100.10.42\', \'memory_mb\': 147445, \'current_workload\': 0, \'vcpus\': 24, \'free_ram_mb\': 146933, \'running_vms\': 0, \'free_disk_gb\': 55808, \'service_id\': 7, \'hypervisor_version\': u\'5.0.0\', \'disk_available_least\': None, \'deleted_at\': None, \'cpu_info\': u\'{""model"": ""Intel(R) Xeon(R) CPU           L5640  @ 2.27GHz"", ""vendor"": ""HP"", ""topology"": {""cores"": 12, ""threads"": 24, ""sockets"": 2}}\', \'memory_mb_used\': 512}\n'].
2013-06-26 17:45:27.597 10253 TRACE nova
Is this a bug with OpenStack?
Thank you,
Pragadees","VMware: enable driver to work with postgres database

The hypervisor version is expected to be an integer and not
a string.

Change-Id: Ifac9ca3480191413e3a02f0dae94e452707ae82f
Closes-Bug: #1195139"
1666,e31b9e78374bcaeae573cce2915897ceb870ffc3,1393273741,,1.0,21,7,2,2,1,0.940285959,True,1.0,37669.0,7.0,3.0,False,16.0,190693.0,69.0,3.0,7.0,599.0,600.0,7.0,585.0,586.0,7.0,567.0,568.0,0.005460751,0.387713311,0.388395904,454,869,1283233,cinder,e31b9e78374bcaeae573cce2915897ceb870ffc3,1,1,,"Bug #1283233 in Cinder: ""3par: Delete (missing) snapshot stuck in error deleting""","Steps to reproduce:
1. Create a snapshot.
2. Manually delete it from 3PAR outside of cider
3. Delete the snapshot from cinder.
Result: Snapshot stuck in error deleting state
Expected Result: Snapshot removed from cinder.","3PAR: Delete missing snapshot stuck in error_del

If the snapshot is deleted outside of cinder, attempting
to delete it again results in the snapshot state stuck in
error_deleting.

This is because the current code raises a NotFound exception,
but the base cinder code does not catch this and update its db.

Instead, log a warning, and simply tell the base cinder code that it was
successfully deleted.

Change-Id: I5c5d24bc113e320346ff4f1f4f62fffdf7cddfc4
Closes-Bug: #1283233"
1667,e33e1f3e816c64e55d23d32366f22a79bdb45b86,1381222308,,1.0,4,4,8,8,1,0.666666667,True,2.0,45885.0,10.0,5.0,False,92.0,855120.0,237.0,4.0,1284.0,1606.0,2730.0,1162.0,1271.0,2275.0,468.0,1389.0,1724.0,0.075232595,0.222970805,0.276708373,1650,1236744,1236744,nova,e33e1f3e816c64e55d23d32366f22a79bdb45b86,0,0,“Ensure that this is in the correct section”,"Bug #1236744 in OpenStack Compute (nova): ""netaddr is a third party import""",Ensure that this is in the correct section (following the HACKING rules),"Ensure that the netaddr import is in the 3rd party section

There were cases when the netaddr import was not in the correct place.

Change-Id: I7715fcfa8cade6a78a2a58dffdc9cdca13b7d59c
Closes-Bug: #1236744"
1668,e3ac20fca7494122a4c0ed2edc00377aa92cb49d,1387488085,1.0,1.0,16,1,3,2,1,0.870229098,True,4.0,3572260.0,21.0,9.0,False,141.0,81454.66667,433.0,5.0,198.0,2697.0,2873.0,193.0,2302.0,2473.0,10.0,1737.0,1742.0,0.001591665,0.251483143,0.252206627,175,578,1260123,nova,e3ac20fca7494122a4c0ed2edc00377aa92cb49d,1,1,Infinite loop,"Bug #1260123 in OpenStack Compute (nova): ""libvirt wait_for_block_job_info can infinitely loop""","Callers of wait_for_block_job_info may loop infinitely if the job doesn't exist, since libvirt returns an empty dict which this function interprets as cur=0 and end=0 => return True.
I think it should do:
if not any(status):
    return False
Affects online deletion of Cinder GlusterFS snapshots, and possibly other callers of this (live_snapshot).
See http://libvirt.org/git/?p=libvirt-python.git;a=commit;h=f8bc3a9ccc
(Encountered issue on Fedora 19 w/ virt-preview repo, libvirt 1.1.3.)","libvirt: Fix infinite loop waiting for block job

Libvirt may return an empty dict when a block job has completed.
In this case, _wait_for_block_job will return True, causing
its callers to loop.

Affects volume_snapshot_delete, swap_volume, and live_snapshot.

(See http://libvirt.org/git/?p=libvirt.git;a=commit;h=0f9e67bf )

Closes-Bug: #1260123
Change-Id: Iab81299f0ce32a14e46aee0fd389ed29f7e503b8"
1669,e3e0401672c8745092d3f0d99686b2ca94c1eb58,1392914340,,1.0,3,3,2,2,1,0.918295834,True,3.0,7856733.0,58.0,20.0,False,15.0,3063978.5,23.0,8.0,17.0,1405.0,1407.0,15.0,1195.0,1196.0,13.0,715.0,715.0,0.016587678,0.848341232,0.848341232,445,860,1282662,neutron,e3e0401672c8745092d3f0d99686b2ca94c1eb58,1,1,,"Bug #1282662 in neutron: ""l2-population with linuxbridge  : recreating a VM with the same ip fails""","I run in multi node setup with ML2, L2-population and Linuxbridge MD, and vxlan TypeDriver.
I create a VM with ip 10.0.0.105 and mac 00:00:00:55:55:55
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001 lladdr 00:00:00:55:55:55 PERMANENT
I delete it.
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001  FAILED
I recreate a VM with the same ip/mac :
neighbouring table on network node :
# ip neigh show
10.0.0.105 dev vxlan-1001  FAILED
in q-agt.log, we can see that the ""ip neigh add"" command fails.","l2-population/lb/vxlan : ip neigh add command failed

we were using ip neigh add command which must be replaced by
ip neigh replace, to avoid error when creating a VM with an ip
previously used by a deleted VM

Change-Id: I2405096d5925ae37efd5f8abcc02b99cf0c9f5d3
Closes-Bug: #1282662"
1670,e3fbdfb3aa2251bb57e53b856e6c5c2d89a23150,1410526843,,1.0,23,7,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1321,1789,1369973,nova,e3fbdfb3aa2251bb57e53b856e6c5c2d89a23150,0,0,Bug in test,"Bug #1369973 in OpenStack Compute (nova): ""libvirt test cases fail due to lockutils error when run via testtools""","When running libvirt tests via testtools.run, there are a number of cases which fail due to lockutils setup
$ .venv/bin/python -m testtools.run nova.tests.virt.libvirt.test_driver
..snip...
======================================================================
ERROR: nova.tests.virt.libvirt.test_driver.IptablesFirewallTestCase.test_multinic_iptables
----------------------------------------------------------------------
pythonlogging:'': {{{INFO [nova.network.driver] Loading network driver 'nova.network.linux_net'}}}
Traceback (most recent call last):
  File ""nova/tests/virt/libvirt/test_driver.py"", line 10182, in test_multinic_iptables
    self.fw.prepare_instance_filter(instance_ref, network_info)
  File ""nova/virt/firewall.py"", line 184, in prepare_instance_filter
    self.refresh_provider_fw_rules()
  File ""nova/virt/firewall.py"", line 474, in refresh_provider_fw_rules
    self._do_refresh_provider_fw_rules()
  File ""nova/openstack/common/lockutils.py"", line 267, in inner
    with lock(name, lock_file_prefix, external, lock_path):
  File ""/usr/lib64/python2.7/contextlib.py"", line 17, in __enter__
    return self.gen.next()
  File ""nova/openstack/common/lockutils.py"", line 231, in lock
    ext_lock = external_lock(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 180, in external_lock
    lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)
  File ""nova/openstack/common/lockutils.py"", line 171, in _get_lock_path
    raise cfg.RequiredOptError('lock_path')
RequiredOptError: value required for option: lock_path
The tox.ini / run_tests.sh work around this problem by using ""-m nova.openstack.common.lockutils"" but this is somewhat tedious to remember to add. A simple mock addition to the tests in question can avoid the issue in the first place.","libvirt: avoid need for lockutils setup running test cases

Some of the firewall/driver test cases rely on lockutils being
setup, so running tests directly fails:

 $ .venv/bin/python -m testtools.run nova.tests.virt.libvirt
 ...snip...
    Traceback (most recent call last):
      File ""nova/tests/virt/libvirt/test_driver.py"", line 10182, in test_multinic_iptables
        self.fw.prepare_instance_filter(instance_ref, network_info)
      File ""nova/virt/firewall.py"", line 184, in prepare_instance_filter
        self.refresh_provider_fw_rules()
      File ""nova/virt/firewall.py"", line 474, in refresh_provider_fw_rules
        self._do_refresh_provider_fw_rules()
      File ""nova/openstack/common/lockutils.py"", line 267, in inner
        with lock(name, lock_file_prefix, external, lock_path):
      File ""/usr/lib64/python2.7/contextlib.py"", line 17, in __enter__
        return self.gen.next()
      File ""nova/openstack/common/lockutils.py"", line 231, in lock
        ext_lock = external_lock(name, lock_file_prefix, lock_path)
      File ""nova/openstack/common/lockutils.py"", line 180, in external_lock
        lock_file_path = _get_lock_path(name, lock_file_prefix, lock_path)
      File ""nova/openstack/common/lockutils.py"", line 171, in _get_lock_path
        raise cfg.RequiredOptError('lock_path')
    RequiredOptError: value required for option: lock_path

The first workaround is to launch using (which run_tests.sh does)

 $ python -m nova.openstack.common.lockutils

but it is nicer to developers if we just avoid the need for using
external lock files during the test suite execution entirely. This
can be done by mocking the 'external_lock' method to use a transient
semaphore instead of external lock file on disk.

Closes-bug: #1369973
Change-Id: I3dcdf6353986ff35c7237644f467ee7e98423f76"
1671,e416a5420f45391c180f84098f4d2c02ecf857b9,1406035344,,1.0,2,3,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1091,1542,1346900,neutron,e416a5420f45391c180f84098f4d2c02ecf857b9,1,1,,"Bug #1346900 in neutron: ""Table tz_network_bindings uses nullable on primary keys""",Primary key cannot be null therefore setting NULL on column that is primary key is wrong and then generates wrong migration script using autogenerate.,"Replace nullable from primary keys in tz_network_bindings with default

Columns vlan_id and phy_uuid were both nullable and primary keys.
Primary keys cannot be nulls. This situation caused problems in
autogenerating scripts creating always migration that set these columns
to nullable.

Instead of having primary keys nullable this patch creates a default
values.

Change-Id: I04642f6d6ad232997a593bff8ca44f9a7a803ffd
Closes-bug: #1346900"
1672,e498373b26d3fa05274429bcf958ea86eb4e3af2,1392746381,0.0,1.0,2,1,1,1,1,0.0,True,5.0,1311307.0,65.0,33.0,False,47.0,98598.0,109.0,10.0,1.0,3242.0,3242.0,1.0,2697.0,2697.0,1.0,2163.0,2163.0,0.00027115,0.293383948,0.293383948,339,749,1274294,nova,e498373b26d3fa05274429bcf958ea86eb4e3af2,1,1,Fix help msg,"Bug #1274294 in OpenStack Compute (nova): ""vmware section of nova.conf describes host_ip as url but is not""","In nova.conf, the host_ip of the vmware section describes it as being a URL.  But the expected input is a hostname or IP address, not a URL.  A URL is composed of a scheme, two colons, etc.  See http://en.wikipedia.org/wiki/Uniform_resource_locator
https://github.com/openstack/nova/blob/master/etc/nova/nova.conf.sample#L3265
[vmware]
#
# Options defined in nova.virt.vmwareapi.driver
#
# URL for connection to VMware ESX/VC host. (string value)
#host_ip=<None>
# Username for connection to VMware ESX/VC host. (string
# value)
#host_username=<None>
# Password for connection to VMware ESX/VC host. (string
# value)
#host_password=<None>
Also here:
https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/driver.py#L49","Fixing host_ip configuration help message

The vCenter/ESXi client can only accept a hostname or IP address
and not a URL.  Fix the help message to accurately describe what
kind of address is accepted.

Change-Id: Ie90247195181189d22e727a855145313397550ca
Closes-Bug: #1274294"
1673,e4b0d8e9446ad3ad3a514b9cf65722f6ed95888d,1384795550,1.0,1.0,16,5,2,2,1,0.998363673,True,5.0,1296166.0,52.0,19.0,False,137.0,265284.0,396.0,4.0,34.0,594.0,627.0,34.0,550.0,583.0,12.0,562.0,574.0,0.001964637,0.085083875,0.086897386,1775,1252409,1252409,nova,e4b0d8e9446ad3ad3a514b9cf65722f6ed95888d,1,1, ,"Bug #1252409 in OpenStack Compute (nova): ""use of threading in libvirt driver causes deadlock when connecting to libvirtd""","the libvirt driver was running into this issue https://bitbucket.org/eventlet/eventlet/issue/137/use-of-threading-locks-causes-deadlock when trying to connect to the libvirtd daemon.
the driver has logging messages during the _connect method which was called within a native thread. The logging code takes out a eventlet lock and mixing OS native threads and eventlet locks causes deadlocks due to the above eventlet bug. The resulting deadlock was enough to hang the nova-compute process.","Use of logging in native thread causes deadlock connecting to libvirtd

When connecting to libvirtd daemon we were calling the _connect
method in a native thread. The use of logging in this method hit this
bug in eventlet https://bitbucket.org/eventlet/eventlet/issue/137/
causing nova-compute to hang.

Closes-Bug: #1252409

Change-Id: I0d22c3ed1b3288aeb1afae130f11a4f38dc6440a"
1674,e4bde7adda4e885fe847e0bacc4d3fc84547e723,1382308472,,1.0,68,5,1,1,1,0.0,True,3.0,796109.0,13.0,11.0,False,13.0,77917.0,27.0,7.0,30.0,1263.0,1264.0,30.0,1128.0,1129.0,30.0,993.0,994.0,0.027978339,0.897111913,0.89801444,1677,1238967,1238967,cinder,e4bde7adda4e885fe847e0bacc4d3fc84547e723,1,1, ,"Bug #1238967 in Cinder: ""Broken NetAppDirect7modeISCSIDriver reports incorrect free capacity, scheduler won't balance between multiple Netapp backends""","The driver is broken, reporting incorrectly the backends free capacities, as an infinite resource.
This causes two main problems:
- the scheduler always chooses the same netapp backend, and isn´t making any balance between the different backends.
- If the first storage backend has no real free capacity, the driver chooses it anyway, therefore after 3 schedule attemps, the volume creation stops with an error.
Is it possible to fix the driver, in order to report the real backends free capacity, so the scheduler balances the volume creation?
We can attach some log if is necessary.
Thanks in advance.
Agustin.","NetApp fix for 7mode iscsi volume stats

This fixes the bug for reporting correct
capacity information in case of 7mode
iscsi drivers.

Change-Id: Ie41009eca866830173809211d58470025be847e3
Closes-Bug: #1238967"
1675,e517c884eff3c2bb1255dc2bf8e52fd655b1ea1a,1410879218,,1.0,16,8,2,2,1,0.543564443,False,,,,,True,,,,,,,,,,,,,,,,,1324,1792,1370068,nova,e517c884eff3c2bb1255dc2bf8e52fd655b1ea1a,1,1,,"Bug #1370068 in OpenStack Compute (nova): ""Numa filter fails to get instance_properties""",NUMATopologyFilter tries to get  instance_properties from filter_properties. But in fact instance_properties are in another dictionary (request_spec)  that is embedded in  filter_properties.,"Get instance_properties from request_spec

In NUMATopologyFilter instance_properties should
be retrived from request_spec which is embedded in
filter_properties and not from filter_properties
directly

Closes-bug: #1370068

Change-Id: If8f9464c6eb5451d0e21ddd14b072be3d2946c50"
1676,e52b5e8e98dd640c69d009a3d5546a479e394d81,1376798321,,1.0,7,11,1,1,1,0.0,True,2.0,355738.0,26.0,12.0,False,3.0,1511650.0,3.0,4.0,5.0,896.0,897.0,5.0,835.0,836.0,5.0,183.0,184.0,0.028985507,0.888888889,0.893719807,1473,1213541,1213541,neutron,e52b5e8e98dd640c69d009a3d5546a479e394d81,1,1,Typos and 1 LINE of code: LOG = logging.getLogger(__name_₎,"Bug #1213541 in neutron: ""typos in neutron.api.extensions and hacking check""","In neutron/api/extensions.py, there are some typos, no needed blank lines and docstring needed to be improved (according to HACKING.rst)","Fix typos and code style check

In neutron/api/extensions.py, there are some typos, no needed blank
lines and docstrings can be improved (according to HACKING.rst).

Closes-Bug: #1213541

Change-Id: I01087d133e1b2a7e69f0bcc2ae359a3a2042b1dc"
1677,e575fde4cbe9b23cfc103e2de2f4052dd5874b02,1398339285,,1.0,5,5,1,1,1,0.0,True,1.0,10456.0,18.0,6.0,False,1.0,1174993.0,1.0,6.0,52.0,942.0,961.0,51.0,907.0,925.0,34.0,693.0,694.0,0.029787234,0.590638298,0.591489362,830,1259,1312124,neutron,e575fde4cbe9b23cfc103e2de2f4052dd5874b02,1,1,“In downgrade of 4eca4a84f08a_remove_ml2_cisco_cred_db migartion there is a mistake”,"Bug #1312124 in neutron: ""Downgrade doesn't work in 4eca4a84f08a_remove_ml2_cisco_cred_db migration""","In downgrade for 4eca4a84f08a_remove_ml2_cisco_cred_db there is a mistake in usage SQLAlchemy String type. Used sa.string instead of sa.String
akamyshnikova@akamyshnikova:/opt/stack/neutron$ neutron-db-manage --config-file /etc/neutron/neutron.conf --config-file /etc/neutron/plugins/ml2/ml2_conf.ini downgrade -10
INFO  [alembic.migration] Context impl MySQLImpl.
INFO  [alembic.migration] Will assume non-transactional DDL.
INFO  [alembic.migration] Running downgrade 1dde83e0359e -> 26a933acf533, add_index_psql_cisco
INFO  [alembic.migration] Running downgrade 26a933acf533 -> 30231c78a878, add_index_psql_packetfilter
INFO  [alembic.migration] Running downgrade 30231c78a878 -> 168ce7333432, add_index_psql_metering
INFO  [alembic.migration] Running downgrade 168ce7333432 -> 6be312499f9, add_index_psql_fwaas
INFO  [alembic.migration] Running downgrade 6be312499f9 -> d06e871c0d5, set_not_null_vlan_id_cisco
INFO  [alembic.migration] Running downgrade d06e871c0d5 -> 4eca4a84f08a, set_admin_state_up_not_null_ml2
INFO  [alembic.migration] Running downgrade 4eca4a84f08a -> 33c3db036fe4, Remove ML2 Cisco Credentials DB
Traceback (most recent call last):
  File ""/usr/local/bin/neutron-db-manage"", line 10, in <module>
    sys.exit(main())
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 167, in main
    CONF.command.func(config, CONF.command.name)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 81, in do_upgrade_downgrade
    do_alembic_command(config, cmd, revision, sql=CONF.command.sql)
  File ""/opt/stack/neutron/neutron/db/migration/cli.py"", line 59, in do_alembic_command
    getattr(alembic_command, cmd)(config, *args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/command.py"", line 150, in downgrade
    script.run_env()
  File ""/usr/local/lib/python2.7/dist-packages/alembic/script.py"", line 199, in run_env
    util.load_python_file(self.dir, 'env.py')
  File ""/usr/local/lib/python2.7/dist-packages/alembic/util.py"", line 205, in load_python_file
    module = load_module_py(module_id, path)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/compat.py"", line 58, in load_module_py
    mod = imp.load_source(module_id, path, fp)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 103, in <module>
    run_migrations_online()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/env.py"", line 87, in run_migrations_online
    options=build_options())
  File ""<string>"", line 7, in run_migrations
  File ""/usr/local/lib/python2.7/dist-packages/alembic/environment.py"", line 681, in run_migrations
    self.get_context().run_migrations(**kw)
  File ""/usr/local/lib/python2.7/dist-packages/alembic/migration.py"", line 225, in run_migrations
    change(**kw)
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/4eca4a84f08a_remove_ml2_cisco_cred_db.py"", line 53, in downgrade
    sa.Column('credential_id', sa.string(length=255), nullable=True),
AttributeError: 'module' object has no attribute 'string'","Fix incorrect usage of sa.String() type

In downgrade of 4eca4a84f08a_remove_ml2_cisco_cred_db migartion
there is a mistake in usage SQLAlchemy String type.
Used sa.string() instead of sa.String()

Change-Id: I521dd63ca2b48e902ca2cb17c45a3fe996b060e7
Closes-bug: #1312124"
1678,e588ee5b470bdca25abc634db3144d2aa9b84554,1401212910,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,926,1362,1323769,neutron,e588ee5b470bdca25abc634db3144d2aa9b84554,1,0,“NEC plugin: Bump L3RPC callback version to 1.1”,"Bug #1323769 in neutron: ""nec plugin: AttributeError: No such RPC function 'update_floatingip_statuses'""","In nec plugin with l3-agent (icehouse) AttributeError: No such RPC function 'update_floatingip_statuses' occurs.
update_floatingip_statuses was implemented in Icehouse and RPC callback version related to L3RpcCallbackMixin was bumped to 1.1, but the version of L3RpcCallback in NEC plugin was not bumped to 1.1 yet.
update_floatingip_statues RPC call from l3-agent expects RPC version 1.1.","NEC plugin: Bump L3RPC callback version to 1.1

update_floatingip_statuses RPC call implemented in Icehouse expects
RPC version 1.1 and RPC version of L3RpcCallback of other plugins
was bumped to 1.1, but the version of L3RpcCallback in NEC plugin
was not bumped to 1.1 yet.

Change-Id: I0073a3de9ea0cd59d43c821f9bdaea89bd125a65
Closes-Bug: #1323769"
1679,e5cd9d90062d778b7725de0c76d44d6077e8e237,1394504699,,1.0,54,2,3,2,1,0.833540402,True,7.0,6276231.0,118.0,32.0,False,84.0,917847.0,248.0,6.0,14.0,2409.0,2414.0,14.0,1822.0,1827.0,9.0,2282.0,2284.0,0.001323101,0.302064038,0.302328658,417,832,1280572,nova,e5cd9d90062d778b7725de0c76d44d6077e8e237,1,1,bad return status,"Bug #1280572 in OpenStack Compute (nova): ""[EC2] Attaching volume returns volume is 'detached' in response""","While attaching the volume, the API response is that the volume state is detached, while it should be 'attaching' as per EC2's API docs.
http://docs.aws.amazon.com/AWSEC2/latest/APIReference/ApiReference-query-AttachVolume.html","[EC2]Correct the return status of attaching volume

The EC2 API return volume's status as 'detached' in response when
attaching a volume.
It should be 'attaching' according to EC2's API docs, so we should convert
the volume_status or volume attach_status to ec2 volume attachment status.
The situation is similar in detaching volume.

Change-Id: Ifacf0ffcab49394def948d428c4c4c13d08152f0
Closes-bug: #1280572"
1680,e5e76fffbddddf8e6cd8e0b0b1173d4cca53fafe,1401321616,,1.0,133,24,3,3,1,0.917317929,True,2.0,665072.0,26.0,1.0,True,,,,,,,,,,,,,,,,,828,1257,1311960,glance,e5e76fffbddddf8e6cd8e0b0b1173d4cca53fafe,1,1,“ The current implementation can lead to session timeouts”,"Bug #1311960 in Glance: ""Handle session timeout in the VMware store""","The current implementation can potentially lead to a timeout when the invoke_api is not triggered (for example PUT/GET: direct HTTP access to the datastore).
We need to recreate the session responses and retry when getting 401 HTTP responses.","Handle session timeout in the VMware store

The current implementation can lead to session timeouts. This
will occur when accessing the datastore directly through HTTP.
This patch provides a retry mechanism by recreating the session
when getting a 401 status code.

Change-Id: I54cc9e30c9bc374a2cf82dec4beb9b06594835f8
Closes-Bug: #1311960"
1681,e62b70562dd8d2d94b526f3f7a52ef4aff9a3f40,1390732767,,1.0,38,36,2,2,1,0.821812731,True,8.0,1713601.0,45.0,24.0,False,20.0,339108.0,49.0,2.0,0.0,755.0,755.0,0.0,739.0,739.0,0.0,722.0,722.0,0.000733676,0.530447542,0.530447542,304,713,1270204,cinder,e62b70562dd8d2d94b526f3f7a52ef4aff9a3f40,1,1,Bug with spaces,"Bug #1270204 in Cinder: ""storwize driver doesn't handle spaces in server names""","If there's a space in the server's name, the storwize driver's SSH command will get caught by the SSH injection check in cinder.","Allow spaces in host names in the storwize driver

Storwize/SVC naming rules allow host objects to have spaces in their
names. The SSH injection filter will currently reject such names, and so
we surround the names with quotes. Please note that this doesn't allow
to have an arbitrary character in there except a space (anything else
will be cleaned up by the driver or rejected by the SSH injection filter
up the call chain).

Change-Id: If9aec40fe34293031f08d759dd930d73ead456f5
Closes-Bug: #1270204"
1682,e62e5f3763934bef452d07dafc29ee3709bc2213,1391880499,,1.0,23,2,2,2,1,0.721928095,True,4.0,144655.0,37.0,7.0,False,21.0,666462.5,36.0,4.0,54.0,1341.0,1366.0,46.0,1136.0,1153.0,31.0,622.0,625.0,0.041720991,0.812255541,0.816166884,378,791,1277914,neutron,e62e5f3763934bef452d07dafc29ee3709bc2213,1,1,Raise a exception that does not exist,"Bug #1277914 in neutron: ""ML2 plugin cannot raise NoResultFound exception""","The ML2 plugin cannot raise NoResultFound exception because it does not use the correct sqlalchemy library.
'from sqlalchemy import exc as ...'  instead of 'from sqlalchemy.orm import exc as ...'","ML2 plugin cannot raise NoResultFound exception

The ML2 plugin cannot raise NoResultFound exception because it does not
use the correct sqlalchemy library:
'from sqlalchemy import exc as ...' instead of 'from sqlalchemy.orm
import exc as ...'

Closes-Bug: #1277914
Change-Id: If3819adc62b9254f0c08eea6dcfcf5f06288e20e"
1683,e65bf1febb66cb2771834f4bbb11c2e61579ad7d,1410660209,,1.0,6,6,3,2,1,0.920619836,False,,,,,True,,,,,,,,,,,,,,,,,1312,1778,1369151,nova,e65bf1febb66cb2771834f4bbb11c2e61579ad7d,1,0,"Chnage requirements, specifications. API change. “This patch updates the relevate baremetal/ironic host state
    classes to do likewise.”","Bug #1369151 in OpenStack Compute (nova): ""TypeError: consume_from_instance() takes exactly 2 arguments""","As of today we are seeing the following scheduler errors when trying to schedule Ironic instances:
Sep 13 16:42:48 ubuntu nova-scheduler: a/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 147, in select_destinations\n    filter_properties)\n', '  File ""/opt/stack/venvs/nova/local/lib/python2.7/site-packages/nova/scheduler/filter_scheduler.py"", line 300, in _schedule\n    chosen_host.obj.consume_from_instance(context, instance_properties)\n', 'TypeError: consume_from_instance() takes exactly 2 arguments (3 given)\n']","Correct baremetal/ironic consume_from_instance...

In 2bc74e66d90b6fae3c5ca8f79f99e75169da644f we recently changed
the HostState.consume_from_instance method parameters to include a
context. This patch updates the relevate baremetal/ironic host state
classes to do likewise.

This fixes a critical TypeError that occurs when trying to use
Ironic or Nova Baremetal otherwise.

Change-Id: I46eebce19603cb4c4a934b1647ff6433d75799ad
Closes-bug: #1369151"
1684,e686131fc4b8724328f0922067569120c90eb261,1395699421,,1.0,40,10,2,2,1,1.0,True,4.0,3551860.0,68.0,21.0,False,209.0,43637.0,1166.0,4.0,117.0,1111.0,1128.0,117.0,1094.0,1111.0,117.0,1094.0,1111.0,0.015370587,0.142633841,0.144848248,673,1099,1296519,nova,e686131fc4b8724328f0922067569120c90eb261,0,0,implementation: should handle ,"Bug #1296519 in OpenStack Compute (nova): ""finish migration should handle exception to revert instance info""","when instance resize, it will call finish_migration at last to create new instance and destroy old instance
if driver layer has problem in create new instance, the instance will be set to 'ERROR' state
we are able to use  reset-state --active  to reset the instance and use it
but the instance information is set to new instance and not reverted to old one","Restore instance flavor info when driver finish_migration fails

when instance resize, it will call finish_migration at last to create
new instance and destroy old instance. If driver layer has problem in
create new instance, the instance will be set to 'ERROR' state.

we are able to use reset-state --active to reset the instance and use it
but the instance information is set to new flavor
and not reverted to old one

Change-Id: I5961260f50e7893c7bf03e329e06edfd4f295640
Closes-Bug: #1296519"
1685,e6ac12041ebe85fe9d46ab9346ae0bc9ef0a3599,1391263380,,1.0,1,1,1,1,1,0.0,True,1.0,2338231.0,21.0,7.0,False,33.0,473092.0,81.0,7.0,328.0,4132.0,4230.0,328.0,3386.0,3484.0,311.0,3028.0,3117.0,0.043147559,0.418890886,0.431199004,347,758,1274718,nova,e6ac12041ebe85fe9d46ab9346ae0bc9ef0a3599,0,0,tests,"Bug #1274718 in OpenStack Compute (nova): ""_test_postgresql_opportunistically calls get_mysql_connection_info""","Not sure if this is intentional or an oversight, but nova.tests.db.test_migrations.BaseWalkMigrationTestCase._test_postgresql_opportunistically calls get_mysql_connection_info even though there is a get_pgsql_connection_info method that is slightly different and used elsewhere.
Obviously not breaking anything, but should probably be cleaned up.","Call get_pgsql_connection_info from _test_postgresql_opportunistically

The _test_postgresql_opportunistically method was calling the method to
parse the mysql connection URL which is wrong when there is one
specifically for postgresql, so call the right method.

This wasn't causing issues obviously but is something that needs to be
cleaned up regardless.

Closes-Bug: #1274718

Change-Id: Ic572e612c7ce31d854dc7258aa9c8b89a197260e"
1686,e6c03e15e8caa957716eabfee047240622ac3bf5,1396378516,1.0,1.0,209,8,6,3,1,0.788126168,True,12.0,3449492.0,45.0,4.0,True,,,,,,,,,,,,,,,,,715,1141,1300906,cinder,e6c03e15e8caa957716eabfee047240622ac3bf5,1,0,"""Windows does not support using dynamic vhds as iSCSI”","Bug #1300906 in Cinder: ""Cinder fails to create volumes from images on Windows""","After the volume is created, qemu-img is supposed to copy the downloaded image to the volume. This fails as the disk is enabled and is not accessible. To fix this, the flow of creating a volume from an image must be changed when using Windows. So instead of creating the volume before copying the image to it, it would be better to skip the volume creation and let the driver create the disk based on the downloaded image and then import it as an iSCSI disk.
Windows does not support using dynamic vhds as iSCSI disks and as qemu-img cannot create fixed vhd images, there must be an intermediate conversion before importing the disk.
Trace: http://paste.openstack.org/show/74765/","Fixes cinder volume from image on Windows

When creating a volume from an image, after the volume is created,
qemu-img is supposed to copy the downloaded image to the volume.
This fails as the disk is enabled and is not accessible.

Also, Windows does not support using dynamic vhds as iSCSI disks
and as qemu-img cannot create fixed vhd images, there must be an
intermediate conversion before importing the disk.

In order to be able to modify an iSCSI disk, it must be disabled
first.

Closes-Bug: #1300906

Change-Id: I726b23287c5b710227288d6215538f01dfb0f979"
1687,e6ea26ea1272f9407879b822b3913b4308e3f2f8,1393486743,,1.0,67,7,2,2,1,0.974024864,False,,,,,True,,,,,,,,,,,,,,,,,1271,1733,1365881,cinder,e6ea26ea1272f9407879b822b3913b4308e3f2f8,0,0,Feature.” This patch optimizes the data path”,"Bug #1365881 in Cinder: ""E-Series driver does not determine preferred path/controller""","The E-Series driver does not determine the preferred data path to access a newly created lun.
This can cause problems with data access and the iscsi path to the device that is handed to a vm or that is used when copying an image to the device should always use the preferred path. E-Series arrays do something called an implicit transfer when a host accesses data from a non-preferred path for more than 5 minutes. At 5 minutes the array will transfer ownership of the volume to the path over which IO is currently flowing. If this issue is not resolved the current behavior of the driver is to establish every IO path on the first iSCSI target that is returned. In this scenario the controller A will quickly become heavily loaded and the controller B will have no load because of the implicit transfers from B --> A To get better throughput this issued needs to be fixed.","NetApp fix for controller preferred path

This patch optimizes the data path by choosing
the preferred path to access the volume by selecting
the controller owning the volume.

Closes-Bug: #1365881

Change-Id: I7069111dfd22d4fca92615b5730854f6d696ec13"
1688,e73072e76c2aedfc52cd2de480b2f41b60769acc,1388686683,,1.0,26,1,4,2,1,0.945530556,True,4.0,610110.0,29.0,10.0,False,16.0,1732358.75,82.0,3.0,984.0,1026.0,1810.0,887.0,1009.0,1699.0,965.0,997.0,1763.0,0.138892883,0.143493889,0.253630482,247,651,1265607,nova,e73072e76c2aedfc52cd2de480b2f41b60769acc,1,1,,"Bug #1265607 in OpenStack Compute (nova): ""Instance.refresh() sends new info_cache objects""",If an older node does an Instance.refresh() it will fail because conductor will overwrite the info_cache field with a new InstanceInfoCache object. This happens during the LifecycleEvent handler in nova-compute.,"Prevent Instance.refresh() from returning a new info cache

If an older node does a refresh() on an instance, conductor will
return a new copy of the InstanceInfoCache, which the client
will balk at. Instead, add a refresh() method to info_cache and
call it from Instance.refresh().

Closes-bug: #1265607
Change-Id: I6a4787a23cd851cf56958c61f8b0559a79847ca0"
1689,e7319bcebdc225a4b7f784acebbf2cd00f3a3e3b,1382988564,,1.0,31,8,2,2,1,0.73206669,True,13.0,1334243.0,39.0,9.0,False,9.0,1042148.0,22.0,3.0,3.0,525.0,526.0,3.0,509.0,510.0,3.0,482.0,483.0,0.003555556,0.429333333,0.430222222,1733,1246079,1246079,cinder,e7319bcebdc225a4b7f784acebbf2cd00f3a3e3b,1,1, ,"Bug #1246079 in Cinder: ""Nexenta volume driver leaves snapshot uremoved""",After deletion of a volume which was created by cloning Nexenta backend still contains snapshot used for cloning.,"Nexenta: Remove snapshot after volume-clone deletion.

Detect if volume was created via cloning and remove a snapshot from the
appliance if so. Return 'provider_location' to update the volume DB record
after creating a volume via cloning.

Closes-Bug: #1246079
Change-Id: I6c946d34871119600d7197614f5be22fa926e5dc"
1690,e74b85f854e248919299fd875066903eba0a37c9,1408988020,1.0,1.0,35,5,2,2,1,0.543564443,False,,,,,True,,,,,,,,,,,,,,,,,1192,1650,1356734,neutron,e74b85f854e248919299fd875066903eba0a37c9,1,1,“The recently added external_gateway method does not check the agent mode or host binding before adding an external gateway to a particular node/agent.”,"Bug #1356734 in neutron: ""SNAT name space is being hosted on all nodes though agent_mode is set as dvr""","SNAT name space is being hosted on all nodes though agent_mode is set as dvr
Here are the steps followed.
1. one service node with agent_mode=snat
2. Two compute nodes with agent_mode=dvr
3. Create an external network. Subnet within it.
4.  Create a private network & subnet(sub1) within it.
5 .Create a Distributed Router(DVR)
6. Set the  external network as gateway to the DVR.
7. Add the ptivate subnet  (sub1) as interface to the DVR.
8. Since the service node has the agent_mode as DVR-snat, the snat namespace will sit on this service node.  But the snat namespace is hosted on all 3 nodes(service node with agent_mode=dvr_snat & Compute nodes with agent_mode=dvr)
Expected behavior:
snat name space should sit only on the node with agent_mode=dvr_snat.","ext-gw update on dvr router improperly handled by l3-agent

The recently added external_gateway method does not check
the agent mode or host binding before adding an external
gateway to a particular node/agent.  This fix adds the
required checks in the update path.

Change-Id: Idbb76eea1faec1c5d6b7615d23d74760cc327bb2
Closes-bug: #1356734"
1691,e74ceb68e1642d773f2ad0c655e6354f7a7fb362,1383198968,,1.0,1,1,1,1,1,0.0,True,4.0,2999702.0,29.0,24.0,False,8.0,6076105.0,21.0,16.0,0.0,4097.0,4097.0,0.0,3528.0,3528.0,0.0,3015.0,3015.0,0.000153988,0.464428703,0.464428703,41,369,1246592,nova,e74ceb68e1642d773f2ad0c655e6354f7a7fb362,1,1,typo error,"Bug #1246592 in OpenStack Compute (nova): ""Nova live migration failed due to OLE error""","When migrate vm on hyperV, command fails with the following error:
2013-10-25 03:35:40.299 12396 ERROR nova.openstack.common.rpc.amqp [req-b542e0fd-74f5-4e53-889c-48a3b44e2887 3a75a18c8b60480d9369b25ab06519b3 0d44e4afd3d448c6acf0089df2dc7658] Exception during message handling
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\amqp.py"", line 461, in _process_data
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\openstack\common\rpc\dispatcher.py"", line 172, in dispatch
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 90, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\exception.py"", line 73, in wrapped
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\compute\manager.py"", line 4103, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\driver.py"", line 118, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     block_migration, migrate_data)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 44, in wrapper
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return function(self, *args, **kwds)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 76, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     recover_method(context, instance_ref, dest, block_migration)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationops.py"", line 69, in live_migration
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     dest)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 231, in live_migrate_vm
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     disk_paths = self._get_physical_disk_paths(vm_name)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\livemigrationutils.py"", line 114, in _get_physical_disk_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     ide_paths = self._vmutils.get_controller_volume_paths(ide_ctrl_path)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\vmutils.py"", line 553, in get_controller_volume_paths
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     ""parent"": controller_path})
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 1009, in query
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return [ _wmi_object (obj, instance_of, fields) for obj in self._raw_query(wql) ]
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\win32com\client\util.py"", line 84, in next
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp     return _get_good_object_(self._iter_.next(), resultCLSID = self.resultCLSID)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp com_error: (-2147217385, 'OLE error 0x80041017', None, None)
2013-10-25 03:35:40.299 12396 TRACE nova.openstack.common.rpc.amqp","Fix OLE error for HyperV

When run live-migration on HyperV, an OLE error
will occur. This is because we use wrong WQL
sentence when we try to query table
'Msvm_ResourceAllocationSettingData'.

This commit fixes the wrong WQL sentence.

Closes-Bug: #1246592

Change-Id: Iabb4096002a5be1fb930b077fbf5ef734893c258"
1692,e7c1bdf146c0492a7f122399b099f0fee8b6d125,1393902645,0.0,1.0,29,7,2,2,1,0.918295834,True,2.0,125990.0,9.0,3.0,False,21.0,534154.0,31.0,2.0,62.0,773.0,780.0,62.0,571.0,578.0,56.0,615.0,618.0,0.048264183,0.521591871,0.524132091,517,938,1287495,glance,e7c1bdf146c0492a7f122399b099f0fee8b6d125,0,0,No bug. should check,"Bug #1287495 in Glance: ""VMware parse_uri method should check for the scheme of the uri provided""","Need to add a check in parse_uri(...) [1] to make sure that a URI with the vsphere scheme is provided. Throw a BadStoreUri otherwise.
[1] https://github.com/openstack/glance/blob/master/glance/store/vmware_datastore.py#L151","Store URI must start with the expected  URI scheme

Add a check to make sure the store location URI starts
with the expected scheme.
Also add some more tests for the location.

Change-Id: I4ef360b3444d29de209a6e90a5ff29bd74f21155
Closes-Bug: #1287495"
1693,e7ca2541b13d9e755f443b866708865ac22a8bd1,1397338056,,2.0,6,7,2,2,1,0.619382195,True,2.0,42022.0,37.0,3.0,False,16.0,215633.0,21.0,3.0,72.0,2295.0,2315.0,72.0,1930.0,1950.0,63.0,441.0,460.0,0.056387665,0.389427313,0.406167401,785,1212,1307025,neutron,e7ca2541b13d9e755f443b866708865ac22a8bd1,0,0,Bug in test,"Bug #1307025 in neutron: ""Patch in cisco unit test not being correctly cleared by stopall""","A recent patch removed the explicit patch stops from most of the unit tests to let the default mock.patch.stopall handle the cleanup[1]. However, there appears to be cases where stopall loses the reference to a patch and doesn't stop it correctly. In this particular case, the nexus test patch to sys.modules is not being stopped correctly so it's causing unit test failures in the gate[2].
1. https://github.com/openstack/neutron/commit/6546ba570367aba7209ec36544ec4cf742e0bd63
2. http://logs.openstack.org/10/86810/5/check/gate-neutron-python27/89dfa5c/testr_results.html.gz","Fix dangling patches in Cisco and Midonet tests

Cisco Nexus Tests:
Explicitly stops the patch to sys.modules immediately
after use to fix sporadic failures caused by the patch
not being handled correctly by mock.patch.stopall.

Midonet Interface Test:
Removes the double-patch of the 'device_exists' method
in ip_lib.

Closes-Bug: #1307025
Closes-Bug: #1307038
Change-Id: Ie22505bb8e406a61e40819d60c76d229916a6e01"
1694,e7ca2541b13d9e755f443b866708865ac22a8bd1,1397338056,,2.0,6,7,2,2,1,0.619382195,True,2.0,42022.0,37.0,3.0,False,16.0,215633.0,21.0,3.0,72.0,2295.0,2315.0,72.0,1930.0,1950.0,63.0,441.0,460.0,0.056387665,0.389427313,0.406167401,786,1213,1307038,neutron,e7ca2541b13d9e755f443b866708865ac22a8bd1,0,0,Bug in test,"Bug #1307038 in neutron: ""linux IP lib tests conflicting patches causing sporadic UT failures""","The following failure is happening sporadically when running the unit tests:
ft1.11275: neutron.tests.unit.test_linux_ip_lib.TestDeviceExists.test_device_exists_StringException: Empty attachments:
  pythonlogging:''
  pythonlogging:'neutron.api.extensions'
  stderr
  stdout
Traceback (most recent call last):
  File ""neutron/tests/unit/test_linux_ip_lib.py"", line 785, in test_device_exists
    _execute.assert_called_once_with('o', 'link', ('show', 'eth0'))
  File ""/home/jenkins/workspace/gate-neutron-python27/.tox/py27/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 0 times.","Fix dangling patches in Cisco and Midonet tests

Cisco Nexus Tests:
Explicitly stops the patch to sys.modules immediately
after use to fix sporadic failures caused by the patch
not being handled correctly by mock.patch.stopall.

Midonet Interface Test:
Removes the double-patch of the 'device_exists' method
in ip_lib.

Closes-Bug: #1307025
Closes-Bug: #1307038
Change-Id: Ie22505bb8e406a61e40819d60c76d229916a6e01"
1695,e7f0b56d74fbfbb08a3b7a0d2da4cefb6fe2aa67,1410535904,0.0,1.0,39,6,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1300,1766,1368152,neutron,e7f0b56d74fbfbb08a3b7a0d2da4cefb6fe2aa67,1,1,,"Bug #1368152 in neutron: ""l3 agent dies after MessagingTimeout error""","l3 agent dies after MessagingTimeout error, attached is the trace.
I am not sure about the root cause of the MessagingTimeout either but that seems a different problem, I think agent should survive the timeout and retry maybe.","Retry getting the list of service plugins

On systems that start both neutron-server and neutron-l3-agent together,
there is a chance that the first call to neutron will timeout. Retry upto
4 more times to avoid the l3 agent exiting on startup.

This should make the l3 agent a little more robust on startup but still
not ideal, ideally it wouldn't exit and retry periodically.

Change-Id: I2171a164f3f77bccd89895d73c1c8d67f7190488
Closes-Bug: #1353953
Closes-Bug: #1368152
Closes-Bug: #1368795"
1696,e80121a2c5ccac42f857b0628ea6f340eda1ca3a,1380280281,,1.0,27,38,2,2,1,0.923338889,True,3.0,1569021.0,22.0,7.0,False,6.0,1252710.5,8.0,4.0,52.0,1416.0,1447.0,52.0,1260.0,1291.0,52.0,1356.0,1387.0,0.008633328,0.221045773,0.226095455,1544,1224014,1224014,nova,e80121a2c5ccac42f857b0628ea6f340eda1ca3a,1,1, ,"Bug #1224014 in OpenStack Compute (nova): ""live_migrate task ignores extra_specs""","The new live_migrate task in the conductor does not pass extra_specs from the flavor through to the filters - thus giving an incorrect result.
This showed up when using the TrustedFilter which depends on extra_specs (set by nova.scheduler.utils.build_request_spec) - however nova.conductor.tasks.live_migrate.LiveMigrationTask._get_candidate_destination does not use this build_request_spec and builds it's own - which missed this extra_specs value.
Marked as a security vulnerability as it means that the use of live migration will bypass filters intended to provide a secure environment such as TrustedFilter.","Make LiveMigrateTask use build_request_spec()

Some filters like TrustedFilter makes use of extra_specs. Currently,
when live-migration uses the scheduler to select a host, it constructs
a request_spec that has no extra_specs in it.

By making use of the existing helper method build_request_spec(), the
request to the scheduler will include extra_specs.

Change-Id: I5bc6c6418653c256a42da7b0a343086ec9863da1
Closes-Bug: #1224014"
1697,e8273cdb3f4a838aad78f298658f7685bc90d02f,1406238771,,1.0,1,9,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,1858,1341752,1341752,Cinder,e8273cdb3f4a838aad78f298658f7685bc90d02f,0,0,"I think the error is on the test, the source code of the software is not buggy","Bug #1341752 in Cinder: ""No handlers could be found for logger""","When I do run_tests.sh in cinder I get the following error at the end of the run.
vagrant@devstack:/opt/stack/cinder$  ./run_tests.sh
 [SNIP]
Slowest 10 tests took 11.54 secs:
cinder.tests.api.contrib.test_volume_actions.VolumeRetypeActionsTest
    test_update_readonly_flag                                             1.12
cinder.tests.test_storwize_svc.StorwizeSVCDriverTestCase
    test_storwize_svc_host_maps                                           1.39
    test_storwize_svc_multi_host_maps                                     1.11
    test_storwize_svc_retype_need_copy                                    1.09
    test_storwize_svc_retype_only_change_iogrp                            1.09
    test_storwize_svc_snapshots                                           1.13
    test_storwize_svc_volume_migrate                                      1.13
    test_storwize_svc_volume_params                                       1.15
    test_storwize_terminate_connection                                    1.10
    test_storwize_vdisk_copy_ops                                          1.23
Ran 3083 tests in 148.093s
OK
No handlers could be found for logger ""cinder.volume.drivers.san.hp.hp_lefthand_rest_proxy""
vagrant@devstack:/opt/stack/cinder$","Fix no handlers could be found issue

This patch fixes the error message during
unit tests for 2 issues.

1) test_migrations tries to LOG and exception before
setUp is called because of the method decorator.

2) the hp_lefthand_rest_proxy tries to LOG an exception
for a missing import.

Change-Id: I11d2c9320c0a216b5795207488b906795d27b1e0
Closes-Bug: #1341752"
1698,e842fff31fd77289507d89387a62eee288aea6bc,1381536393,,1.0,8,7,2,2,1,0.918295834,True,1.0,2649543.0,15.0,8.0,False,39.0,398931.0,90.0,3.0,807.0,1991.0,2601.0,711.0,1751.0,2279.0,749.0,1886.0,2448.0,0.119255844,0.300047702,0.389410081,1678,1239009,1239009,nova,e842fff31fd77289507d89387a62eee288aea6bc,0,0,feature. “Make scheduler disk_filter take swap into account”,"Bug #1239009 in OpenStack Compute (nova): ""scheduler filter disk_filter doesn't take swap into account""","from nova/scheduler/filters/disk_filter.py
    def host_passes(self, host_state, filter_properties):
        """"""Filter based on disk usage.""""""
        instance_type = filter_properties.get('instance_type')
        requested_disk = 1024 * (instance_type['root_gb'] +
                                 instance_type['ephemeral_gb'])
This should take into account swap, which is stored in MB, as well.","Make scheduler disk_filter take swap into account

The disk_filter, which makes sure a node has enough disk space for an
instance doesn't take the swap partition into account, but swap takes up
disk space so it should.

Change-Id: I0779dcf0f1cd60017b5b49ca38c0a15b23d802d7
Closes-Bug: #1239009"
1699,e8988fe72eaddb38282356fe92076fd9cd2897b1,1392862333,,1.0,1,1,1,1,1,0.0,True,3.0,1932806.0,78.0,4.0,False,1.0,109730.0,1.0,2.0,668.0,831.0,831.0,583.0,726.0,726.0,204.0,320.0,320.0,0.244630072,0.383054893,0.383054893,437,852,1282354,neutron,e8988fe72eaddb38282356fe92076fd9cd2897b1,0,0,No bug. ‘should’,"Bug #1282354 in neutron: ""NSX: nicira_models should import model_base directly""",NSX: nicira_models should import model_base directly,"NSX: nicira_models should import model_base directly

Change-Id: I49bdf72434384899d12aa6c528f8d31125a3b34e
Closes-bug: #1282354"
1700,e8b9a11fd728d287a1c64aa025ebba009bf5025e,1403162368,,1.0,9,9,2,2,1,0.99107606,False,,,,,True,,,,,,,,,,,,,,,,,989,1428,1332041,neutron,e8b9a11fd728d287a1c64aa025ebba009bf5025e,1,1,,"Bug #1332041 in neutron: ""type_tunnel.TunnelRpcCallbackMixin  __init__ method incompatible with other mixins""","The init method in type_tunnel.TunnelRpcCallbackMixin expects extra parameters[1] not passed to the rest of the RPC mixins so it requires a custom __init__ method in the ML2 plugin[2].
1. https://github.com/openstack/neutron/blob/caf60442247ef0a13595db2691733f3b7589d24f/neutron/plugins/ml2/rpc.py#L54
2. https://github.com/openstack/neutron/blob/0755e7b379232285e434d827eeb854260a1db595/neutron/plugins/ml2/drivers/type_tunnel.py#L90","Remove __init__ method from TunnelCallback mixin

Removed an __init__ method from a mixin class that
made mixing with other classes fragile and inflexible.
This replaces it with an explicit setup method.

This allows the ML2 RPCCallbacks class to correctly
inherit the common RpcCallback class.

Closes-Bug: #1332041
Change-Id: I36cb7dcf57147468f252d61f846b2b28dd77c8ff"
1701,e8c86fc82df85a87ee7ddec0db633567924a4369,1386240620,,1.0,0,3,1,1,1,0.0,True,1.0,147798.0,10.0,5.0,False,9.0,4230540.0,7.0,4.0,1534.0,2198.0,3390.0,1347.0,1853.0,2861.0,686.0,2092.0,2447.0,0.101208014,0.308338244,0.360636417,149,551,1258103,nova,e8c86fc82df85a87ee7ddec0db633567924a4369,0,0,Unused code,"Bug #1258103 in OpenStack Compute (nova): ""Unused imports in libvirt fake utilities""",Removed unused imports and assignments,"libvirt: remove unused imports from fake libvirt utils

Remove code that is not used.

Change-Id: I3943ac2c68a149e27b47fb2604c05e3d2e5dc2e2
Closes-bug: #1258103"
1702,e8e2392321e0fd3e2b8a0345b725f2e8df854a34,1404472346,,1.0,3,1,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1050,1495,1339855,neutron,e8e2392321e0fd3e2b8a0345b725f2e8df854a34,1,1,,"Bug #1339855 in neutron: ""Raise NotImplementedError instead of NotImplemented""","NotImplementedError is the name of the exception (https://docs.python.org/2/library/exceptions.html).
NotImplemented is the name of a constant (https://docs.python.org/2/library/constants.html).
It makes no sense to raise a constant. The exception should be raised instead.","Raise NotImplementedError instead of NotImplemented

NotImplementedError is the name of the exception
(https://docs.python.org/2/library/exceptions.html).

NotImplemented is the name of a constant
(https://docs.python.org/2/library/constants.html).

It makes no sense to raise a constant. The exception should
be raised instead.

Change-Id: I4969e26eb7b46f008ea3c8bd0093490c425f7069
Closes-Bug: #1339855"
1703,e8fac5eda3309a19ee2e1b36883a4e30476644a9,1392393093,,1.0,148,14,2,2,1,0.860778118,True,5.0,966786.0,36.0,12.0,False,9.0,4446869.0,29.0,4.0,41.0,1320.0,1328.0,40.0,1110.0,1118.0,41.0,1167.0,1175.0,0.029370629,0.816783217,0.822377622,520,941,1287643,cinder,e8fac5eda3309a19ee2e1b36883a4e30476644a9,1,1,Fix conversion methods,"Bug #1287643 in Cinder: ""NaElement translate_struct does not work for non unique tags""",Method translate_struct of NaElement does not work properly in case of unique tags.,"NetApp api fix structure conversion methods

This fixes the issue in api element creation
when data structures are supplied to be converted
into NaElements. It could not handle multiple
keys with same name and also not converted different
types properly into NaElement. This change fixes the issue.

Change-Id: I84f1ee97a73bbad88bbb64d4a9ac4e1b29cad741
Closes-bug: #1287643"
1704,e90d6d6c833db51964037afd1c4aa1f521ee380a,1411028693,,1.0,37,18,2,2,1,0.912115631,False,,,,,True,,,,,,,,,,,,,,,,,1333,1801,1370914,neutron,e90d6d6c833db51964037afd1c4aa1f521ee380a,1,1,,"Bug #1370914 in neutron: ""When two ovs ports contain same external_ids:face-id field, ovs agent might fail finding correct port.""","As the title says, if there are 2 different ovs ports with the same external_ids:iface-id field (which is the port_id), when at least one of them is managed by the ovs agent, it might fail finding the correct one if they are not connected to the same bridge.
Steps to reproduce:
1. Create a router with an internal port to some Neutron network
2. Find the port in 'ovs-vsctl show'
3. Use the following command to find the port_id in ovs: sudo ovs-vsctl  --columns=external_ids list Interface <port_name>
4. Use the following commands to create a new port with the same field in a new bridge:
 sudo ovs-vsctl add-br a
 sudo ip link add dummy12312312 type dummy
 sudo ovs-vsctl add-port br-a dummy12312312
 sudo ovs-vsctl set Interface dummy12312312 external_ids:iface-id=""<port_id>"" # port_id was obtained in point 3.
5. Restart the ovs agent.
At this point the ovs agent's log should show ""Port: dummy12312312 is on br-a, not on br-int"".
Expected result: ovs agent should know to iterate though the options and find the correct port in the correct bridge.","Iterate over same port_id if more than one exists

In certain cases where multiple ports can have the same
external_ids:iface_id property, the ovs agent will arbitrarily choose
one and ignore the rest. In case the chosen port isn't on the
integration bridge the ovs agent is managing, an error is returned to
the calling function. This is faulty since one of the other ports may
belong to the correct bridge and it should be chosen instead.

This is interesting for future L3 HA integration tests, where 2
different instances of l3 agents are needed to run on the same machine.
In this case, both agents will register ports which have the same
iface_id property, but obviously only one of the ports is relevant for
each agent.

Closes-bug: #1370914
Related-bug: #1374947
Change-Id: I05d70417e0f78ae51a9ce377fc93a3f12046b313"
1705,e936cac67612015e1123b4539a60ec1aa1b1ff82,1381467277,,1.0,11,1,2,2,1,0.650022422,True,4.0,3981406.0,32.0,14.0,False,14.0,619584.0,20.0,6.0,13.0,2330.0,2343.0,12.0,2057.0,2070.0,0.0,2171.0,2171.0,0.000159261,0.345914955,0.345914955,1664,1237802,1237802,nova,e936cac67612015e1123b4539a60ec1aa1b1ff82,1,1, ,"Bug #1237802 in OpenStack Compute (nova): ""IPMI power manager hangs when the password is empty.""","While creating Baremetal Node by `nova baremetal-node-create' with --pm_password '' [1] and starting a deployment, IPMI power manager hangs with showing prompt ""Password:"" in nova-compute process. IPMI power manager creates an empty file and specifies it as the password file in the ipmitool command line, but ipmitool ignores that file [2].
This is an uncommon case that an administrator set password empty, but this is not a low importance bug due to it stops the thread.
I think we can avoid this bug by writing '\0' into the password file [3], since ipmitool checks return value of fgets() is not NULL which means the file is not start with EOF and no error had occurred.
[1] e.g.:
    $ nova baremetal-node-create --pm_address 192.0.2.200 --pm_user admin --pm_password '' service-host 1 1000 10000 00:11:22:33:44:55
[2] In ipmitool manpage:
    -f <password_file>
        Specifies a file containing the remote server password. If this option is absent, or if password_file is empty, the password will default to NULL.
[3] I checked that ipmitool works with a file containing '\0';
    # touch a
    # ipmitool -I lanplus -H 192.0.2.94 -f a -U administrator power status
    Unable to read password from file a
    Unable to read password from file a
    Password:                       <-- Enter
    Chassis Power is off
    # echo -ne '\0' > b
    # ipmitool -I lanplus -H 192.0.2.94 -f b -U administrator power status
    Chassis Power is off","Fix power manager hangs while executing ipmitool

In nova/virt/baremetal, IPMI power manager uses ipmitool with '-F' option to
specify a file containing the remote server password.  When the passowrd was
empty, ipmitool ignores that file and asks user to set password in stdin.
This makes the thread stop.

This patch changes power manager to write '\0' to the password file in the
case of empty password, to force the password on impitool.

Closes-bug: #1237802

Change-Id: I44f56129a8ce2af4dda8dea35eb9c60d9518d54b"
1706,e9468d77a41ada6d77021c32c5df4351d368ea72,1385125711,,1.0,6,6,1,1,1,0.0,True,2.0,175828.0,9.0,4.0,False,3.0,16376550.0,3.0,3.0,3.0,681.0,682.0,3.0,530.0,531.0,0.0,477.0,477.0,0.000979432,0.468168462,0.468168462,1797,1254046,1254046,glance,e9468d77a41ada6d77021c32c5df4351d368ea72,1,0,“Original change to Nova: If4dd973acc23921dbc2bc69bb76225deb2802dad”,"Bug #1254046 in Glance: ""openstack.common.local module is out of date""","local has a broken TLS symbol - strong_store, fixed in oslo some time ago in Ib544be1485823f6c619312fdee5a04031f48bbb4. All direct and indirect (lockutils and rpc) usages of strong_store might be potentially affected.
Original change to Nova: https://review.openstack.org/#/c/57509/","Sync openstack.common.local from oslo

`local` has a broken TLS symbol - strong_store, fixed in oslo some time ago in
Ib544be1485823f6c619312fdee5a04031f48bbb4. All direct and indirect
(lockutils and rpc) usages of strong_store might be potentially affected.
Original change to Nova: If4dd973acc23921dbc2bc69bb76225deb2802dad

Closes-Bug: #1254046
Change-Id: I934149092cacaaa78009565b6d8a6ba348c2dbe4"
1707,e95571a6e350338a1567234bc934f70f83e69d86,1397612789,,1.0,39,14,1,1,1,0.0,True,8.0,525673.0,117.0,18.0,False,2.0,10369778.0,2.0,3.0,7.0,1337.0,1338.0,7.0,1125.0,1126.0,7.0,798.0,799.0,0.007005254,0.699649737,0.700525394,460,875,1283930,neutron,e95571a6e350338a1567234bc934f70f83e69d86,0,0,Improvement. Not a bug,"Bug #1283930 in neutron: ""    Make metaplugin be used with a router service plugin""","https://review.openstack.org/65034
commit c04785e0ced18ebab6bada1d3961c1394c541a69
Author: Itsuro Oda <email address hidden>
Date:   Mon Jan 6 15:03:14 2014 +0900
    Make metaplugin be used with a router service plugin
    ""l3_plugin_list"" configuration parameter of the metaplugin is permitted
    blank now.
    If ""l3_plugin_list"" is blank, router extension and extensions which extend
    the router extension don't be included in ""supported-extension-aliases"" of
    the metaplugin.
    This makes the metaplugin be able to be used with a router service plugin.
    Note that if ""l3_plugin_list"" is not blank, a router service plugin must
    not be specified, otherwise the error of the bug report still occurs.
    This patch removes some router extension related meaningless codes also.
    (e.g.  external-net extension belongs to L2 functionality and be handled
     by core plugins properly.)
    Closes-bug: 1266347
    DocImpact
    Change-Id: I0454bc0a4bd7eda5dad18b0538fb7baebe0b9f91","Make help texts more descriptive in Metaplugin

This patch makes help texts of configuration parameters of
metaplugin.ini more descriptive. Comments in metaplugin.ini
are also improved.

Change-Id: I1fac888ecc223a7199cc9c9aeb603b7493f22171
Closes-Bug: #1283930"
1708,e9a5463bf71cb2f2e7185dea3ed8ee8b973a3208,1408544876,,1.0,9,1,2,2,1,0.881290899,False,,,,,True,,,,,,,,,,,,,,,,,1216,1675,1358818,nova,e9a5463bf71cb2f2e7185dea3ed8ee8b973a3208,1,0,"“The API change guidelines”, “In icehouse, we were able to store ints, floats in extra_specs, with I195bd5d45a896e9b26dd81dab1e49c9f939b4805 we forced the value(s) to be just strings.”","Bug #1358818 in OpenStack Compute (nova): ""extra_specs string check breaks backward compatibility""","We've found that while with Icehouse we were able to specify extra_specs values as ints or floats, in Juno the command fails unless we make these values strings by quoting them. This breaks backward compatibility.
compare Icehouse:
curl -k -i -X POST http://127.0.0.1:8774/v2/982607a6a1134514abac252fc25384ad/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":1,""powervm:max_vcpu"":5,""powervm:min_mem"":1024,""powervm:max_mem"":4096,""powervm:availability_priority"":127,""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":128}}'; echo
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 385
X-Compute-Request-Id: req-9132922d-c703-4573-9822-9ca7a6bf7b0d
Date: Thu, 14 Aug 2014 18:25:02 GMT
{""extra_specs"": {""powervm:processor_compatibility"": ""default"", ""powervm:max_proc_units"": ""0.5"", ""powervm:shared_weight"": 128, ""powervm:min_mem"": 1024, ""powervm:max_mem"": 4096, ""powervm:uncapped"": ""true"", ""powervm:proc_units"": ""0.2"", ""powervm:dedicated_proc"": ""false"", ""powervm:max_vcpu"": 5, ""powervm:availability_priority"": 127, ""powervm:min_proc_units"": ""0.1"", ""powervm:min_vcpu"": 1}}
to Juno:
curl -k -i -X POST http://127.0.0.1:8774/v2/be2ffade1e0b4bed83619e00482317d1/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":1,""powervm:max_vcpu"":5,""powervm:min_mem"":1024,""powervm:max_mem"":4096,""powervm:availability_priority"":127,""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":128}}'; echo
HTTP/1.1 400 Bad Request
Content-Length: 88
Content-Type: application/json; charset=UTF-8
Date: Thu, 14 Aug 2014 18:25:46 GMT
{""badRequest"": {""message"": ""extra_specs value is not a string or unicode"", ""code"": 400}}
if I modify the data sent so that everything is a string, it will work for Juno:
curl -k -i -X POST http://127.0.0.1:8774/v2/be2ffade1e0b4bed83619e00482317d1/flavors/1/os-extra_specs -H ""X-Auth-Token: *****"" -H ""Accept: application/json"" -H ""Content-Type: application/json"" -d '{""extra_specs"":{""powervm:proc_units"":""0.2"",""powervm:processor_compatibility"":""default"",""powervm:min_proc_units"":""0.1"",""powervm:max_proc_units"":""0.5"",""powervm:min_vcpu"":""1"",""powervm:max_vcpu"":""5"",""powervm:min_mem"":""1024"",""powervm:max_mem"":""4096"",""powervm:availability_priority"":""127"",""powervm:dedicated_proc"":""false"",""powervm:uncapped"":""true"",""powervm:shared_weight"":""128""}}'; echo
HTTP/1.1 200 OK
Content-Type: application/json
Content-Length: 397
Date: Thu, 14 Aug 2014 18:26:27 GMT
{""extra_specs"": {""powervm:processor_compatibility"": ""default"", ""powervm:max_proc_units"": ""0.5"", ""powervm:shared_weight"": ""128"", ""powervm:min_mem"": ""1024"", ""powervm:max_mem"": ""4096"", ""powervm:uncapped"": ""true"", ""powervm:proc_units"": ""0.2"", ""powervm:dedicated_proc"": ""false"", ""powervm:max_vcpu"": ""5"", ""powervm:availability_priority"": ""127"", ""powervm:min_proc_units"": ""0.1"", ""powervm:min_vcpu"": ""1""}}
The API change guidelines (https://wiki.openstack.org/wiki/APIChangeGuidelines) describe as ""generally not acceptable"": ""A change such that a request which was successful before now results in an error response (unless the success reported previously was hiding an existing error condition)"". That is exactly what this is.","Restore backward compat for int/float in extra_specs

In icehouse, we were able to store ints, floats in
extra_specs, with I195bd5d45a896e9b26dd81dab1e49c9f939b4805 we
forced the value(s) to be just strings. Let's loosen up
this restriction and still enforce a length check.

Closes-Bug: #1358818
Change-Id: I7687d0214e44d4af1b595c6c6c7ce685d4083556"
1709,e9eb1d69bf75c22b0ad4b50e4892a5644c78cf68,1408726893,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1240,1699,1362221,nova,e9eb1d69bf75c22b0ad4b50e4892a5644c78cf68,1,1,,"Bug #1362221 in OpenStack Compute (nova): ""VMs fail to start when Ceph is used as a backend for ephemeral drives""","VMs' drives placement in Ceph option has been chosen (libvirt.images_types == 'rbd').
When user creates a flavor and specifies:
   - root drive size >0
   - ephemeral drive size >0 (important)
and tries to boot a VM, he gets ""no valid host was found"" in the scheduler log:
Error from last host: node-3.int.host.com (node node-3.int.host.com): [u'Traceback (most recent call last):\n', u'
 File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1305, in _build_instance\n set_access_ip=set_access_ip)\n', u' File ""/usr/l
ib/python2.6/site-packages/nova/compute/manager.py"", line 393, in decorated_function\n return function(self, context, *args, **kwargs)\n', u' File
 ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1717, in _spawn\n LOG.exception(_(\'Instance failed to spawn\'), instance=instanc
e)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.py"", line 68, in __exit__\n six.reraise(self.type_, self.value, se
lf.tb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 1714, in _spawn\n block_device_info)\n', u' File ""/usr/lib/py
thon2.6/site-packages/nova/virt/libvirt/driver.py"", line 2259, in spawn\n admin_pass=admin_password)\n', u' File ""/usr/lib/python2.6/site-packages
/nova/virt/libvirt/driver.py"", line 2648, in _create_image\n ephemeral_size=ephemeral_gb)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/
libvirt/imagebackend.py"", line 186, in cache\n *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebackend.py"",
line 587, in create_image\n prepare_template(target=base, max_size=size, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/opens
tack/common/lockutils.py"", line 249, in inner\n return f(*args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebac
kend.py"", line 176, in fetch_func_sync\n fetch_func(target=target, *args, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/virt/libvir
t/driver.py"", line 2458, in _create_ephemeral\n disk.mkfs(os_type, fs_label, target, run_as_root=is_block_dev)\n', u' File ""/usr/lib/python2.6/sit
e-packages/nova/virt/disk/api.py"", line 117, in mkfs\n utils.mkfs(default_fs, target, fs_label, run_as_root=run_as_root)\n', u' File ""/usr/lib/pyt
hon2.6/site-packages/nova/utils.py"", line 856, in mkfs\n execute(*args, run_as_root=run_as_root)\n', u' File ""/usr/lib/python2.6/site-packages/nov
a/utils.py"", line 165, in execute\n return processutils.execute(*cmd, **kwargs)\n', u' File ""/usr/lib/python2.6/site-packages/nova/openstack/commo
n/processutils.py"", line 193, in execute\n cmd=\' \'.join(cmd))\n', u""ProcessExecutionError: Unexpected error while running command.\nCommand: sudo
 nova-rootwrap /etc/nova/rootwrap.conf mkfs -t ext3 -F -L ephemeral0 /var/lib/nova/instances/_base/ephemeral_1_default\nExit code: 1\nStdout: ''\nStde
rr: 'mke2fs 1.41.12 (17-May-2010)\\nmkfs.ext3: No such file or directory while trying to determine filesystem size\\n'\n""]","Fix instance boot when Ceph is used for ephemeral storage

is_block_dev attribute was mistakenly set to True for RBD images
(we don't actually map RBD images to block devices in the *host*
system, but only in *guests*, so as far as the host system is
concerned an RBD image must not be treated like a system block
device). This led to a situation when mkfs was called for a non
existing file and failed when trying to create an ephemeral fs.

Closes-Bug: #1362221

Change-Id: I54c0d117a53bb61f278b2e137bd29a595548f5a1"
1710,ea19fb10c5e09ff5df383607654ab9dc2791ec21,1408298454,,1.0,8,12,3,2,1,0.817345422,False,,,,,True,,,,,,,,,,,,,,,,,1207,1665,1357972,nova,ea19fb10c5e09ff5df383607654ab9dc2791ec21,0,0,Bug in test,"Bug #1357972 in OpenStack Compute (nova): ""boot from volume fails on Hyper-V if boot device is not vda""","The Tempest test ""tempest.scenario.test_volume_boot_pattern.TestVolumeBootPatternV2.test_volume_boot_pattern"" fails on Hyper-V.
The cause is related to the fact that the root device is ""sda"" and not ""vda"".","Fixes Hyper-V boot from volume root device issue

Checks that instances are booting from volume by checking
the correct root device obtained from the block device info.

Change-Id: I94941dfe96c1596c8e9b91df3d3d19d33ae7fe92
Co-Authored-By: Zsolt Dudas <zdudas@cloudbasesolutions.com>
Closes-Bug: #1357972"
1711,ea1aa7794585c7cca3118ece282e08cfb760218b,1397827960,,1.0,35,32,7,5,1,0.938997173,True,10.0,1739628.0,181.0,36.0,False,32.0,2041066.143,41.0,11.0,47.0,1895.0,1900.0,47.0,1556.0,1561.0,47.0,1028.0,1033.0,0.041558442,0.890909091,0.895238095,589,1011,1291032,neutron,ea1aa7794585c7cca3118ece282e08cfb760218b,0,0,More pythonic,"Bug #1291032 in neutron: ""Correct H301 and H302 violations""",There are many violations of this rule.  Originally it was intended to only fix H302 but flake8 will not check for H302 unless H301 is enabled as well.  So must fix them both at the same time.,"Fix H302 violations

H302 violation is reported by flake8 when importing separated objects from
modules instead of importing the whole module.
e.g.   from package.module import function
       function()
is changed to
       from package import module
       module.function()

Change-Id: Ifbf31b52316d3cade40743752a49ce700f384a21
Closes-Bug: #1291032"
1712,ea32e07f36781c2c0c7a0aa28817cc2df23df32d,1411030187,,1.0,32,4,3,2,1,0.615977319,False,,,,,True,,,,,,,,,,,,,,,,,1320,1788,1369815,cinder,ea32e07f36781c2c0c7a0aa28817cc2df23df32d,1,1,,"Bug #1369815 in Cinder: ""Retype a ""None"" type volume to a ""replication"" type will always be failed""","Test Step:
1. Create a type-1,  backend IBM storwize SVC, without replication .
2. Create a type-2, backend IBM storwize SVC, with replication = TRUE.
3. Create a volume without type information:
command :　cinder create 1
check the volume status, the information of the volume :
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |     None    |  false   |                                      |
The volume type is none.
4. Do Retype to enable replication for the volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will always be failed, check the c-vol.log, it was found :
2014-09-16 09:16:01.664 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mVolume ffd8f99a-9e17-4943-ac56-eee2e060a86a: driver error when trying to retype, falling back to generic mechanism.^[[00m
2014-09-16 09:16:01.665 ^[[01;31mERROR cinder.volume.manager [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mInvalid volume type: id cannot be None^[[00m
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1410, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    host)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/__init__.py"", line 926, in retype
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    model_update = self.replication.create_replica(ctxt, volume)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/drivers/ibm/storwize_svc/replication.py"", line 73, in create_replica
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    vol_type = volume_types.get_volume_type(ctxt, vol_type)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/volume_types.py"", line 103, in get_volume_type
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m    raise exception.InvalidVolumeType(reason=msg)
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00mInvalidVolumeType: Invalid volume type: id cannot be None
^[[01;31m2014-09-16 09:16:01.665 TRACE cinder.volume.manager ^[[01;35m^[[00m
2014-09-16 09:16:01.737 ^[[01;31mERROR oslo.messaging.rpc.dispatcher [^[[01;36mreq-97d04d55-3d12-4317-bce1-ba61b0edb8d7 ^[[00;36m22e6c0f1cabc4e0ea8a7191e6942500a e83fd5b227c64ca3be40186d35670b3e^[[01;31m] ^[[01;35m^[[01;31mException during message handling: Volume migration failed: Retype requires migration but is not allowed.^[[00m
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mTraceback (most recent call last):
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 134, in _dispatch_and_reply
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    incoming.message))
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 177, in _dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return self._do_dispatch(endpoint, method, ctxt, args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 123, in _do_dispatch
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    result = getattr(endpoint, method)(ctxt, **new_args)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/usr/local/lib/python2.7/dist-packages/osprofiler/profiler.py"", line 105, in wrapper
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    return f(*args, **kwargs)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m  File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1434, in retype
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m    raise exception.VolumeMigrationFailed(reason=msg)
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00mVolumeMigrationFailed: Volume migration failed: Retype requires migration but is not allowed.
^[[01;31m2014-09-16 09:16:01.737 TRACE oslo.messaging.rpc.dispatcher ^[[01;35m^[[00m
It looks the volume type can't be ""none"" if retype to a volume with replication=TRUE.
5. Do retype to try if all the retype are disabled for the ""none"" type volume with command :
cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-1
Since type-1 replication =false, the operation was successful.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |    type-1   |  false   |                                      |
6. Do retype to enable replication again with command:
 cinder retype ffd8f99a-9e17-4943-ac56-eee2e060a86a type-2
It will be success since the current operation is from type-1 to type-2.
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
|                  ID                  |   Status  | Name | Size | Volume Type | Bootable |             Attached to              |
+--------------------------------------+-----------+------+------+-------------+----------+--------------------------------------+
| ffd8f99a-9e17-4943-ac56-eee2e060a86a | available | None |  1   |    type-2   |  false   |                                      |
From the front  test, the retype to enable replication with ""none"" volume type have issues","IBM Storwize:Failed to retype from non-type to replication enable

It used none as volume type to create replication. Storwize Driver
did not check whether it's none.
Change to use new type to create replication.

Closes-bug: #1369815

Change-Id: I78501e1d3558bd6c3e6e1abb0c312cec7d11efd4"
1713,ea56cf255ac956d23eba5bb4f6e7312b944eb5fb,1403532612,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,999,1441,1333252,cinder,ea56cf255ac956d23eba5bb4f6e7312b944eb5fb,1,0,“Update _resize_volume_file() to support appropriate permissions”,"Bug #1333252 in Cinder: ""IBMNAS driver fails to resize volume after setting 600 or 660 permissions to volume""","Based on the discussion of drivers setting inappropriate permissions (https://wiki.openstack.org/wiki/OSSN/OSSN-0014), ibmnas driver inherits  _set_rw_permissions_for_all() from nfs.py which sets 666 to the volumes.
Changes are expected to be made in nfs.py such that it sets 600 or 660 permissions, while doing so ibmnas driver fails performing the operation  _resize_volume_file(). This can be fixed by performing resize operation using root user.","Update _resize_volume_file() to support appropriate permissions

ibmnas driver inherits _set_rw_permissions_for_all() from nfs.py
which sets 666 to the volumes. Changes are expected to be made in
nfs.py such that it sets 600 or 660 permissions, by doing so
ibmnas driver fails performing the operation _resize_volume_file()

Adding run_as_root in resize_volume operation.

Change-Id: Iecaf111f96f93b4e24bd81edd6aa82829113b4bb
Closes-Bug: #1333252"
1714,ea7242a293abe59dd91add5e44e58a2eb6cd99fc,1391684973,,1.0,3,2,2,2,1,0.970950594,True,1.0,396795.0,13.0,4.0,False,198.0,2966.0,1103.0,4.0,391.0,1573.0,1783.0,374.0,1488.0,1683.0,384.0,1525.0,1731.0,0.052870091,0.209557814,0.237846745,373,786,1277054,nova,ea7242a293abe59dd91add5e44e58a2eb6cd99fc,1,1,,"Bug #1277054 in OpenStack Compute (nova): ""Poll rescued instances fails with key error""","After an instance has been in the rescue state for some time, a periodic task triggers to unrescue the instances:
_poll_rescued_instances
File nova/notifications.py info_from_instance
  instance_type = flavors.extract_flavor(instance_ref)
File ""nova/compute/flavors.py"" in extract_flavor
  instance_type[key] = type_fn(sys_meta[type_key])
KeyError: 'instance_type_memory_mb'
This then continues to happen on every run of the periodic task, and starts to fill up the DB with instance faults.","Fix auto instance unrescue after poll period

We were seeing massive amounts on instance faults from failed automatic
unrescue. It was coming from the instance not having system metadata
populated.

While the route cause is a combination of using non-object instances
that were not populated with system metadata. With an object, the system
metadata would lazyload as required, but clearly not with a pure dict.

This fix ensures the non-object instance has system_metadata, so the
extraction of the flavor from the instance, during notifications sent
during unrescue, now succeed.

Closes-Bug: #1277054
Change-Id: I63bdd6c49505557829241869fd3efd2b2754530b"
1715,ea7d4a599224b5d0c7674d03993bbe72c49f0d51,1392299684,,1.0,1,1,1,1,1,0.0,True,3.0,2604843.0,29.0,10.0,False,32.0,612.0,54.0,4.0,103.0,1227.0,1304.0,102.0,1018.0,1095.0,2.0,11.0,12.0,0.046875,0.1875,0.203125,375,788,1277168,cinder,ea7d4a599224b5d0c7674d03993bbe72c49f0d51,1,0,Bug because of evolution of the names etc,"Bug #1277168 in Cinder: ""having oslo.sphinx in namespace package causes issues with devstack""","http://lists.openstack.org/pipermail/openstack-dev/2014-January/023759.html
We've decided to rename oslo.sphinx to oslosphinx. This will require small changes in the doc builds for a lot of the other projects.
The problem seems to be when we pip install -e oslo.config on the system, then pip install oslo.sphinx in a venv. oslo.config is unavailable in the venv, apparently because the namespace package for o.s causes the egg-link for o.c to be ignored.","Switch over to oslosphinx

oslosphinx is the new name of oslo.sphinx

Closes-Bug: #1277168
Change-Id: Ib30a4a5eb3a86ad5c03324a59e1a8ee4d4897a70"
1716,ea91ad2b2317cb88fb14c0cdaf31187eca93ff57,1399563982,,1.0,244,7,6,2,1,0.797747907,True,11.0,803137.0,85.0,23.0,False,40.0,1773446.0,74.0,3.0,14.0,1937.0,1942.0,14.0,1922.0,1927.0,7.0,1435.0,1435.0,0.001006036,0.180583501,0.180583501,1522,1221525,1221525,nova,ea91ad2b2317cb88fb14c0cdaf31187eca93ff57,1,0,"“This is not an issue with most volume backends, but it causes the attach to fail when using Storwize as a Cinder backend.” Compatibility or bug3","Bug #1221525 in OpenStack Compute (nova): ""Cannot attach a volume to a Hyper-V Nova instance with a Storwize v7000 target""","I found we can not attach one volume to one instance(this instance is in hyperV compute node).
Test Env:
Cinder using Storwizedriver, and nova compute node is hyperV env.
When I want to attach one available volume to an instance which in hyper compute node,  after ran ""nova volume-attach"" command, the volume status still is available(available-> attaching->availble), and from hyper compute node, in compute.log. I got the following error message:
2013-09-05 19:54:30.273 2204 AUDIT nova.compute.manager [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] [instance: 1b892ca0-3b5f-4792-a394-e9de661b429a] Attaching volume a053f7f3-8ffd-4392-b278-3791c2cd29bd to /dev/sdb
2013-09-05 19:54:30.305 2204 INFO nova.virt.hyperv.basevolumeutils [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] The ISCSI initiator name can't be found. Choosing the default one
2013-09-05 19:54:30.336 2204 INFO urllib3.connectionpool [-] Starting new HTTP connection (1): 9.123.137.71
2013-09-05 19:54:31.618 2204 ERROR nova.virt.hyperv.volumeops [req-a62e1672-c825-488f-998f-c5c58c64ccde 427700be22794f588b97ff07854e2031 dd8f0667af5c44bab29899fb88adae96] Attach volume failed: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops Traceback (most recent call last):
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 113, in attach_volume
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     self._login_storage_target(connection_info)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeops.py"", line 100, in _login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     target_portal)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\nova\virt\hyperv\volumeutilsv2.py"", line 53, in login_storage_target
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     TargetPortalPortNumber=target_port)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 431, in __call__
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     handle_com_error ()
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops   File ""C:\Program Files (x86)\IBM\SmartCloud Entry\Hyper-V Agent\Python27\lib\site-packages\wmi.py"", line 241, in handle_com_error
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops     raise klass (com_error=err)
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops x_wmi: <x_wmi: Unexpected COM Error (-2147352567, 'Exception occurred.', (0, u'SWbemObjectEx', u'Generic failure ', None, 0, -2147217407), None)>
2013-09-05 19:54:31.618 2204 TRACE nova.virt.hyperv.volumeops","Fixes Hyper-V iSCSI target login method

Hyper-V is trying to log in to portals/targets every time a volume
is attached, even if the portal/target was already connected. This
is not an issue with most volume backends, but it causes the attach
to fail when using Storwize as a Cinder backend.

This patch fixes the issue by checking if the target or portal is
already connected, and simply perform an update in this case,
instead of attempting to create a new connection.

Also, a retry loop is introduced for the case when the iSCSI target
login fails. As in this loop it is checked whether or not the
operation completed successfully, this method does not rely anymore
only on a sleep interval to ensure that the connection to the target
has been made. Even so, on V2 volume utilities, the sleep interval
is still needed to avoid having a new connection attempt before the
last one finished.

The method which gets the according mounted disk must raise an
exception when the device number is -1, which signifies that the
disk has not been mounted properly.

Co-Authored-By: Jay Bryant <jsbryant@us.ibm.com>

Closes-bug: #1221525

Change-Id: I92d2a586704fa3e857f46432bb571c81e86d183b"
1717,eacab523be3d8f2f97809c785dba98e5e2e75c7b,1396619894,,1.0,31,3,5,5,1,0.764676347,True,4.0,3265840.0,24.0,7.0,False,33.0,1468065.4,67.0,4.0,57.0,1670.0,1686.0,57.0,1395.0,1411.0,44.0,1469.0,1472.0,0.028827675,0.941704036,0.943625881,744,1171,1302609,cinder,eacab523be3d8f2f97809c785dba98e5e2e75c7b,1,1,,"Bug #1302609 in Cinder: ""Create empty volume when snapshot id is """" or srv volume is """"""","When call create volume api with image id """", it will create an empty volume.
But when create volume with snapshot id """" or srv volume """", it will return an 404 error.
So we should create an empty volume when call create volume api with snapshot id """" or srv volume """".","Create volume fail when image id is """"

When call create volume api with image id """", it will create an empty
volume.
But when create volume with snapshot id """" or srv volume """", it will
return an 404 error.
So it should create volume fail when call create volume api with image id
"""".

Change-Id: If2c7b061753bbbd172a09d35b343e73e90a1b7b4
Closes-Bug: #1302609"
1718,ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38,1399700491,,1.0,71,2,2,2,1,0.30642473,True,1.0,78863.0,14.0,3.0,False,158.0,300597.0,478.0,2.0,37.0,1580.0,1592.0,37.0,1349.0,1361.0,29.0,1280.0,1284.0,0.003767424,0.160869019,0.161371342,1715,1242366,1242366,nova,ead9cfca93c9b5ca55e3ba269213c98d5e6e1d38,1,1, ,"Bug #1242366 in OpenStack Compute (nova): ""volume attach failed if attach again to an pause to active VM""","Steps are as following:
1) Create one VM
2) Attach volume to the VM
3) pause the VM
4) detach the volume
5) unpause the VM
6) re-attch the VM to same device, nova compute throw exception
2013-10-20 23:21:22.520 DEBUG amqp [-] Channel open from (pid=19728) _open_ok /usr/local/lib/python2.7/dist-packages/amqp-1.0.12-py2.7.egg/amqp/channel.py:420
2013-10-20 23:21:22.520 ERROR nova.openstack.common.rpc.amqp [req-5f0d786e-1273-4611-b0a5-a787754c6bc8 admin admin] Exception during message handling
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 244, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 230, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 272, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 259, in decorated_function
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3649, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     context, instance, mountpoint)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3644, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     mountpoint, instance)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3690, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     connector)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 3680, in _attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     encryption=encryption)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/libvirt/driver.py"", line 1107, in attach_volume
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp     raise exception.DeviceIsBusy(device=disk_dev)
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp DeviceIsBusy: The supplied device (vdb) is busy.
2013-10-20 23:21:22.520 TRACE nova.openstack.common.rpc.amqp
^C2013-10-20 23:21:24.871 INFO nova.openstack.common.service [-] Caught SIGINT, exiting","libvirt: Use VIR_DOMAIN_AFFECT_LIVE for paused instances

When a volume is attached to a paused instance, it does not
appear in the instance's block devices list (i.e. lsblk)
after the instance has resumed. A similar situation
happens when detaching a volume from a paused instance; the
block device continues to exists in the block devices list,
where it should not.  It was found that the volume is only
persisted in the domain's config settings and not its active
settings, since only the VIR_DOMAIN_AFFECT_CONFIG flag was used
on attach and detach volume.  In order to affect the active
settings, the VIR_DOMAIN_AFFECT_LIVE flag has to be added when
attaching and detaching volumes.

Change-Id: I9c90a410a7ecb91f5a4de28acee21fe7da49242c
Closes-Bug: #1242366
Related-Bug: #1299331"
1719,eaf5636544a9b2ae1e87f74d0cdb989f8a41b008,1381628974,,1.0,4,2,1,1,1,0.0,True,5.0,170031.0,31.0,15.0,False,7.0,81682.0,7.0,7.0,440.0,3186.0,3329.0,428.0,2815.0,2958.0,342.0,2103.0,2223.0,0.054453088,0.334021273,0.353071916,1680,1239220,1239220,nova,eaf5636544a9b2ae1e87f74d0cdb989f8a41b008,0,0,Bug in test,"Bug #1239220 in OpenStack Compute (nova): ""boto version checking in test cases is not backwards compatible""","The changes in  commit:
  https://github.com/openstack/nova/commit/7161c62c22ebe609ecaf7e01d2feae473d01495a
Introduced version checking for boto 2.14; however string comparison is not effective for version checking as
'2.9.6' >= '2.14' == true
Resulting in older boto versions (as found in saucy) trying to test with the 2.14 code.","Harden version checking for boto

Simple string comparison is not effective for version checking.
Let's switch to using pkg_resources.parse_version and compare
the tuples. Per review feedback, enabling the code path for
2.13 as well. Originally we had handled this by excluding
2.13 from requirements. With this we should be able to removing
the exclusion for 2.13 from global requirements.

Change-Id: Ie047f7201d8e6a9a9641000d387c4312506f49b1
Closes-Bug: #1239220"
1720,eaf9522c842614dc83d3fd541bfdc1159416d438,1408793201,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1218,1677,1359002,nova,eaf9522c842614dc83d3fd541bfdc1159416d438,1,1,,"Bug #1359002 in OpenStack Compute (nova): ""comments misspelled in type_filter""","the comments of nova.scheduler.filters.type_filter.py:
class TypeAffinityFilter(filters.BaseHostFilter):
...
def host_passes(self, host_state, filter_properties):
        """"""Dynamically limits hosts to one instance type
        Return False if host has any instance types other then the requested
        type. Return True if all instance types match or if host is empty.
        """"""
...
'other then' in the next-to-last line should be 'other than'","fix typo in docstring

Change-Id: Id89ddf8110ed63596f23fcabb2996fd0c0afe87f
Closes-Bug: #1359002"
1721,eafa186b499eb8fd5d37a4c47e99156546e4cab8,1392951387,,1.0,10,10,2,2,1,0.468995594,True,3.0,374035.0,34.0,9.0,False,42.0,325180.0,143.0,3.0,15.0,410.0,412.0,15.0,398.0,400.0,15.0,325.0,327.0,0.013757524,0.280309544,0.282029235,443,858,1282437,glance,eafa186b499eb8fd5d37a4c47e99156546e4cab8,0,0,"Refactor? Indices start in 0, not in 1 according to a rule.","Bug #1282437 in Glance: ""Glance server PATCH operations should use 0-based indices for location entries""","PATCH indices on the Glance server currently use 1-based index for location entries. This goes against the JSON-pointer RFC (rfc6901) which requires array indices be 0-based.
The glance client should also be fixed to use 0-based indexing.","Use 0-based indices for location entries

Glance server currently uses 1-based indices for indexing location
entries. This violates the JSON-pointer RFC (RFC 6901) which states that
indicies should be 0-based. Furthermore, since this deviation is not
mentioned in the docs any application directly calling the REST APIs
would trip over it.

This commit makes Glance use zero-based indexing for location entries.

Note that the ""show_multiple_locations"" config option should be set to
True to expose location related REST operations.

DocImpact: Since this change modifies user-visible Glance behaviour it
should be explicitly documented because the previous behaviour (1-based
indexing), though not explicitly documented, was exposed through the
REST APIs.

Change-Id: I6326455874c381fcb0d8babf9cc4fa8311e219d2
Closes-bug: #1282437"
1722,eb23b345876796f4002cab3632bad1c840289d04,1385482697,1.0,1.0,15,3,4,4,1,0.723308334,True,1.0,185567.0,23.0,8.0,False,20.0,2894417.5,44.0,3.0,527.0,677.0,1024.0,428.0,629.0,888.0,176.0,324.0,396.0,0.309440559,0.568181818,0.694055944,87,485,1255145,neutron,eb23b345876796f4002cab3632bad1c840289d04,1,1,No need to trigger a notification,"Bug #1255145 in neutron: ""address pairs in update request always trigger agent notification""","currently some plugins always notify the agent of a port_update even if the allowed address pairs attribute is specified: https://github.com/openstack/neutron/blob/master/neutron/plugins/ml2/plugin.py#L600
However, if there's no change in allowed address pairs there is no need for triggering a notification
Affected plugins:
ml2
openvswitch
nec","Rebind allowed address pairs only if they changed

This patch ensures allowed address pairs bindings are refreshed
only when they actually change.
This will also avoid sending a notification to the agent if no
change actually occured.

Closes-Bug: #1255145
Partial blueprint neutron-tempest-parallel

Change-Id: Iac2502586a0d215a29194590c16c2e1a064f943b"
1723,eb3db3be60f22a462f30004c55000e26545748cd,1387370055,,1.0,1,1,1,1,1,0.0,True,3.0,675155.0,34.0,8.0,False,4.0,1760237.0,10.0,4.0,179.0,2509.0,2576.0,179.0,2158.0,2225.0,170.0,1590.0,1648.0,0.024804178,0.230780389,0.239193502,192,595,1261621,nova,eb3db3be60f22a462f30004c55000e26545748cd,1,1,,"Bug #1261621 in OpenStack Compute (nova): ""nova api value error is not right""","I was trying to add a json field to DB but forget to dumps the json to string, and nova api report the following error.
2013-12-17 12:37:51.615 TRACE object Traceback (most recent call last):
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/base.py"", line 70, in setter
2013-12-17 12:37:51.615 TRACE object     field.coerce(self, name, value))
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/fields.py"", line 166, in coerce
2013-12-17 12:37:51.615 TRACE object     return self._type.coerce(obj, attr, value)
2013-12-17 12:37:51.615 TRACE object   File ""/opt/stack/nova/nova/objects/fields.py"", line 218, in coerce
2013-12-17 12:37:51.615 TRACE object     raise ValueError(_('A string is required here, not %s'),
2013-12-17 12:37:51.615 TRACE object ValueError: (u'A string is required here, not %s', 'dict') <<<<<<<<<<<
The error should be <string is required here, not dict>","ValueError should use '%' instead of ','

ValueError do not support using ',' to connect log and variables,
we should use '%' instead.

Change-Id: I04c2002a970465d8250e5550f6bf992b48845a0d
Closes-Bug: #1261621"
1724,eb4b29d243150d2f348d163ecc2fb552675891bf,1364237305,1.0,,105,0,3,3,3,0.671223353,True,8.0,2815627.0,41.0,12.0,False,68.0,148132.3333,157.0,2.0,2.0,207.0,207.0,2.0,200.0,200.0,2.0,197.0,197.0,0.005464481,0.360655738,0.360655738,1810,1257295,1257295,Swift,eb4b29d243150d2f348d163ecc2fb552675891bf,0,0,List of known misspellings,"Bug #1257295 in OpenStack Compute (nova): ""openstack is full of misspelled words""","List of known misspellings
http://paste.openstack.org/show/54354
Generated with:
  pip install misspellings
  git ls-files | grep -v locale | misspellings -f -","Add crossdomain.xml middleware

Allows client-side technologies such as Flash, Java and Silverlight running
on web pages served elsewhere to interact with the Swift API.

Bug #1159960

Change-Id: I7d0533a0aaf189ac452abbd983469acb064fdca4"
1725,eb8973dc34901406f4ca8e47956cde5c43ca54c6,1402011196,,1.0,9,3,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,943,1379,1325982,neutron,eb8973dc34901406f4ca8e47956cde5c43ca54c6,1,1,"“Validate expected parameters in add/remove router interfaces
    The add and remove router interface methods check that interface_info
    is not empty but don't check if it contains any of expected parameters:”","Bug #1325982 in neutron: ""l3_db add/remove_router_interface: interface_info validation is incomplete""","L3_NAT_db_mixin  add_router_interface() and remove_router_interface() methods check that interface_info is not empty but don't check that it contains any of expected parameters - port_id or subnet_id:
 if not interface_info:
   msg = _(""Either subnet_id or port_id must be specified"")
   raise n_exc.BadRequest(resource='router', msg=msg)
Expected parameters should be explicitly checked.","Validate expected parameters in add/remove router interfaces

The add and remove router interface methods check that interface_info
is not empty but don't check if it contains any of expected parameters:
port_id and subnet_id
This patch adds a helper method to validate that interface_info contains
at least one of the expected parameters
Include a unit test for the empty port_id and subnet_id case

Closes-Bug: #1325982

Change-Id: Ia370565235a33a847704b972c875d8f1573306c0"
1726,ebc88e6110660c0d543e75140d2e19ccf4796bea,1391219239,,1.0,2,49,1,1,1,0.0,True,1.0,1637897.0,36.0,9.0,False,10.0,691770.0,15.0,6.0,7.0,566.0,567.0,7.0,497.0,498.0,7.0,316.0,317.0,0.010680908,0.423230975,0.424566088,352,763,1275166,neutron,ebc88e6110660c0d543e75140d2e19ccf4796bea,0,0,remove unused code,"Bug #1275166 in neutron: ""Clean up unused RPC calls from Cisco N1kv plugin""",Remove unused RPC calls since n1kv plugin does not communicate with l2 agent.,"Remove unused RPC calls from n1kv plugin code

Change-Id: I75a70786827a5fd6c321b560f1f2ef02a69aa85d
Closes-Bug: #1275166"
1727,ebf2763508535d9b3aaa6753139858b6fdd21f23,1393018181,,1.0,0,9,1,1,1,0.0,True,2.0,95175.0,29.0,3.0,False,9.0,155574.0,14.0,3.0,673.0,976.0,976.0,587.0,868.0,868.0,209.0,436.0,436.0,0.246478873,0.512910798,0.512910798,455,870,1283250,neutron,ebf2763508535d9b3aaa6753139858b6fdd21f23,0,0,tests,"Bug #1283250 in neutron: ""remove pointless test TestN1kvNonDbTest""",remove pointless test TestN1kvNonDbTest,"remove pointless test TestN1kvNonDbTest

Change-Id: Id0cd2ea394276f159c7edfadbdc517a16b24776c
closes-bug: #1283250"
1728,ec442e41d2d243003a42ed60ae862e96142a5cde,1381886025,,1.0,2,1,1,1,1,0.0,True,1.0,13039.0,6.0,1.0,False,21.0,477389.0,62.0,2.0,992.0,1098.0,1098.0,874.0,971.0,971.0,851.0,945.0,945.0,0.779505947,0.865507777,0.865507777,1691,1240287,1240287,cinder,ec442e41d2d243003a42ed60ae862e96142a5cde,1,1, ,"Bug #1240287 in Cinder: ""cinder : extend volume Error""","On the environment where using Cinder LVM driver, extend_volume fails like the following.
# cinder extend 9dd8b44a-6835-40a6-ad9d-a103c25c532d 2
# cinder list
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
|                  ID                  |      Status     |  Display Name  | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending |     test03     |  1   |     None    |  false   |             |
+--------------------------------------+-----------------+----------------+------+-------------+----------+-------------+
2013-10-16 09:00:05.573 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Error extending Volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Traceback (most recent call last):
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/brick/local_dev/lvm.py"", line 469, in extend_volume
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     run_as_root=True)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/utils.py"", line 142, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     return processutils.execute(*cmd, **kwargs)
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm   File ""/opt/openstack/cinder/cinder/openstack/common/processutils.py"", line 173, in execute
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm     cmd=' '.join(cmd))
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm ProcessExecutionError: Unexpected error while running command.
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Command: sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Exit code: 3
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stdout: '  Rounding up size to full physical extent 4.00 MiB\n'
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm Stderr: ""  New size given (1 extents) not larger than existing size (256 extents)\n  Run `lvextend --help' for more information.\n""
2013-10-16 09:00:05.573 19511 TRACE cinder.brick.local_dev.lvm
2013-10-16 09:00:05.577 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] Cmd     :sudo cinder-rootwrap /opt/openstack/cinder/etc/cinder/rootwrap.conf lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdOut  :  Rounding up size to full physical extent 4.00 MiB
2013-10-16 09:00:05.580 19511 ERROR cinder.brick.local_dev.lvm [req-5ef88a15-b50d-4135-ad7c-926ad3cc8e5d e68844e2afb14001a4e322e76c0ded99 469289b44db3489fb635083f2000cf4e] StdErr  :  New size given (1 extents) not larger than existing size (256 extents)
  Run `lvextend --help' for more information.
# cinder help extend
usage: cinder extend <volume> <new-size>
Attempt to extend the size of an existing volume.
Positional arguments:
  <volume>    Name or ID of the volume to extend.
  <new-size>  New size of volume in GB
In cinder cli, the integer of 'new-size' is in GB .
But In LVM command, (lvextend -L 2 cinder-volumes/volume-9dd8b44a-6835-40a6-ad9d-a103c25c532d ) '-L 2' is in MB.
so, If I really want to extend size of volume, 1 GB to 2 GB, I have to try like the following.
# cinder extend 54a7125f-3a8f-48dc-a8b8-8f623c742ecb 2048
# cinder list
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
|                  ID                  |      Status     | Display Name | Size | Volume Type | Bootable | Attached to |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
| 54a7125f-3a8f-48dc-a8b8-8f623c742ecb |    available    |  haha-vol01  | 2048 |     None    |  false   |             |
| 9dd8b44a-6835-40a6-ad9d-a103c25c532d | error_extending |    test03    |  1   |     None    |  false   |             |
+--------------------------------------+-----------------+--------------+------+-------------+----------+-------------+
# lvs
  LV                                          VG             Attr   LSize Origin Snap%  Move Log Copy%  Convert
  volume-54a7125f-3a8f-48dc-a8b8-8f623c742ecb cinder-volumes -wi-ao 2.00g
please check this bug. Thank you.","Fix lvm.extend_volume to pass Gig suffix

The extend function in the lvm driver was not converting
the cinder size value to Gigabytes before passing the call
to the vg module.  The result was that we would attempt to
extend a volume to ""new size in Megabytes"" which of course
is less than the current size since we do a Gigabyte string
conversion on create and everywhere else.

This change makes sure we pass the integer change through
the sizestr method to get the G suffix needed to work properly.

Change-Id: I070962a3aa7038f612e19a93ccaa60cbc13008f6
Closes-Bug: #1240287"
1729,ec979d5b112e2d23eb428cdfaf467d64cfa79499,1389162022,,1.0,1,1,1,1,1,0.0,True,4.0,128923.0,25.0,16.0,False,8.0,1105975.0,8.0,4.0,6.0,734.0,738.0,6.0,551.0,555.0,2.0,585.0,585.0,0.002724796,0.532243415,0.532243415,268,672,1266986,glance,ec979d5b112e2d23eb428cdfaf467d64cfa79499,1,1,typo in a message ,"Bug #1266986 in Glance: ""Fix typo in gridfs store""","Referring commit 61a715e17e8d6e6dbe60d35447c70fba990bd2e9
https://github.com/openstack/glance/blob/master/glance/store/gridfs.py#L99
Fix typo:
msg = (""Missing dependecies: pymongo"")
It should be,
msg = (""Missing dependencies: pymongo"")","Fix typo in gridfs store

Closes-Bug: #1266986

Change-Id: Ic69ef987b785d0db25e2fea2a745f3d00780241a"
1730,eca9069be3d730c788e433edb291123eede07839,1408359058,,1.0,10,2,2,2,1,0.918295834,False,,,,,True,,,,,,,,,,,,,,,,,1208,1666,1358196,neutron,eca9069be3d730c788e433edb291123eede07839,1,1,,"Bug #1358196 in neutron: ""IpNetnsCommand will refuse to work without root_helper""","The network namespace is not mandatory, but that makes
root_wrap non mandatory too, because you could
want to start non-privileged processes outside a
namespace through the same API, covering both
the namespace & non-namespace needs.","Fix IpNetnsCommand to execute without root_wrapper when no netns

IpNetnsCommand accept execution with a network namespace, and
that requires root privileges and a root_wrapper.
IpNetnsCommand does accept no namespace too, in that case, the
root_wrapper doesn't have to be present necessarily, unless the
command we're executing requires root privileges itself.

This patch fixes the check condition on IpNetnsCommand execute
method.

Change-Id: I5ab2f3f1daf4a5a080611dbcd3dbd43292f6766a
Closes-Bug: #1358196"
1731,eccf5e819ef918d566b7efd1e91719550d0fbef9,1410321009,,1.0,6,6,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1289,1753,1367552,neutron,eccf5e819ef918d566b7efd1e91719550d0fbef9,0,0,Bug in test,"Bug #1367552 in neutron: ""Cisco test cases are using _do_side_effect() which has been renamed""","In https://review.openstack.org/78880  NeutronDbPluginV2TestCase._do_side_effect()  was renamed to  _fail_second_call().
But some cisco nexus test cases still use the old name.","Use renamed _fail_second_call() in cisco nexus tests

In commit 5e4b0c6fc6670ea036d801ce53444272bc311929 NeutronDbPluginV2TestCase
_do_side_effect() was renamed to _fail_second_call(), but the nexus test cases
were not updated. The tests still worked because they check for
HTTPInternalServerError which is the end result of the server encountering
""AttributeError: object has no attribute '_do_side_effect'"". So the fault
injection still worked, but not in the way intended.

Note: the monolithic nexus plugin is not updated here since it is being
removed from the tree.

Closes-bug: #1367552

Change-Id: I083285fff5144d8dc35bd5b64faa7e400c68bcff"
1732,ed1071ec1981821275fedf35c5c90ab0a5080c58,1390246083,,1.0,1,1,1,1,1,0.0,True,2.0,1340165.0,26.0,11.0,False,11.0,292022.0,11.0,9.0,123.0,1474.0,1532.0,88.0,1270.0,1302.0,37.0,674.0,676.0,0.052924791,0.940111421,0.942896936,1413,1095346,1095346,neutron,ed1071ec1981821275fedf35c5c90ab0a5080c58,1,1, “recent change to oslo allows the configuration of the interval that ProcessLauncher waits between checks of child exit.” The change didn’t check the current code and caused the bug because the neutron service consumed unnecessary cpu,"Bug #1095346 in neutron: ""Excessive CPU usage in ProcessLauncher()'s wait loop""","See https://review.openstack.org/18689 for some background
We can't use os.wait() to block until a child exited so, instead, we're busy-looping
We should be able to come up with another way of doing this - e.g. using pipes provided to each child to give us a selectable handle we can block on","Minimize the cost of checking for api worker exit

A recent change to oslo allows the configuration of the interval
that ProcessLauncher waits between checks of child exit.  The
default interval of 0.01s resulted in the neutron service consuming
unnecessary cpu cycles checking whether api workers had exited (5%
cpu on idle in a VM).  This patch extends the interval to 1s to
minimize the cost of the checks.

Change-Id: I0407ccb2db65cd3839586faff15e70dbc35f005e
Closes-bug: #1095346"
1733,ed2f3ad04410e1236eba95ef6d9025aa901ddbf3,1410867169,,1.0,80,12,2,2,1,0.828055725,False,,,,,True,,,,,,,,,,,,,,,,,1066,1515,1342046,cinder,ed2f3ad04410e1236eba95ef6d9025aa901ddbf3,1,1,,"Bug #1342046 in Cinder: ""GET /v2/​{tenant_id}​/volumes?all_tenants=0 lists volumes in all tenants""","When a user with admin role tries to list volumes belong to a given tenant, it does not work when all_tenants=0 is specified.
Requesting with all_tenants=0 should get the same response as requesting without all_tenants parameter.
Details:
OpenStack Release Version: IceHouse
Issue a REST API with an admin token (user's role in the specified tenant is admin).
- GET /v2/​{tenant_id}​/volumes
  Lists volumes in the specified tenant.
- GET /v2/​{tenant_id}​/volumes?all_tenants=0
  Lists volumes in all tenants.
- GET /v2/​{tenant_id}​/volumes?all_tenants=1
  Lists volumes in all tenants.","Fix a problem with 'volume list' when 'all_tenants=0'

This change checks the value of 'all_tenants' and returns a list of volumes
for all tenants when 'all_tenants' is 1 (or True). A tenant-specific volume
list is returned when 'all_tenants' is 0 (or False).

An InvalidInput exception is thrown if the 'all_tenants' value is not 0, 1,
True, or False (case insensitive).

Provided new unit tests for the get_all api which check the all_tenants, and
limits parameters.

Fixed pep8 compliance test fails with code being improperly indented, and
changed calls using str() to use six.text_type()

Change-Id: If0b8c343ee7e4e05c9aec89241458dbdf2e4734b
Closes-Bug: #1342046"
1734,ed499b6af1ee8cc3003a991eea2ab4d0fdd461c3,1410340502,,1.0,13,12,2,2,1,0.942683189,False,,,,,True,,,,,,,,,,,,,,,,,1299,1765,1368055,neutron,ed499b6af1ee8cc3003a991eea2ab4d0fdd461c3,1,1,,"Bug #1368055 in neutron: ""NoneType not iterable with None in bulk body""","2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.base [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] Request body: {u'subnets': None}
2014-09-11 01:38:57.752 22667 ERROR neutron.api.v2.resource [req-91733c92-6d15-4fd1-97b6-edcd704280d3 None] create failed
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource Traceback (most recent call last):
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/resource.py"", line 87, in resource
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     result = method(request=request, **args)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 357, in create
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     allow_bulk=self._allow_bulk)
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource   File ""/usr/lib/python2.7/site-packages/neutron/api/v2/base.py"", line 573, in prepare_request_body
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource     bulk_body = [prep_req_body(item) for item in body[collection]]
2014-09-11 01:38:57.752 22667 TRACE neutron.api.v2.resource TypeError: 'NoneType' object is not iterable","Fail on None before iteration attempt

Check for a 'None' value before trying to iterate over it
in the bulk code. Also eliminates an unneccessary anonymous
recursive function in the same bulk handling code.

Closes-Bug: #1368055
Change-Id: Id4aca81e4882a3cdf9c790bdea0b0b515abc9a8c"
1735,ed83265bb794d38f6f66d9e5182a9abc6b82ed53,1384513579,0.0,1.0,43,9,3,2,1,0.728258025,True,5.0,7392617.0,78.0,31.0,False,19.0,1451601.0,79.0,4.0,517.0,758.0,1098.0,420.0,707.0,961.0,166.0,272.0,350.0,0.306422018,0.500917431,0.644036697,1764,1251422,1251422,neutron,ed83265bb794d38f6f66d9e5182a9abc6b82ed53,1,1, ,"Bug #1251422 in neutron: ""deleting security-group fails when neutron and nvp are out of sync""","%neutron security-group-list
+--------------------------------------+-----------------------------+--------------------------------+
| id                                   | name                        | description                    |
+--------------------------------------+-----------------------------+--------------------------------+
| 0163fc21-e0d2-4219-9efc-47c71e102ba8 | default                     | default                        |
| 1023c62b-029f-4faf-9257-97ffc870fffd | default                     | default                        |
| 7c096113-2e5d-4fc3-b3ba-8462ec5e6964 | default                     | default                        |
| b9fc7dc0-c22b-45ee-a811-d2a11b7e864f | security-tempest-1342522857 | description-tempest-1430856972 |
| dfc6bff9-b412-4f47-b38b-6d0079412a4d | default                     | default                        |
+--------------------------------------+-----------------------------
%neutron security-group-delete b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
500-{u'NeutronError': {u'message': u'An unknown exception occurred.', u'type': u'NeutronException', u'detail': u''}}
neutron:/var/log/neutron.log
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6997] Completed request 'POST https://x.x.x.x:443//ws.v1/security-profile': 201 (0.06 seconds)
2013-11-14 05:08:05,037 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774691'
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6997] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,038 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6997] Completed request 'POST /ws.v1/security-profile': 201
2013-11-14 05:08:05,039 (neutron.plugins.nicira.nvplib): DEBUG nvplib create_security_profile Created Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quan
tum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'log
ical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group.create.end on notifications.info
2013-11-14 05:08:05,063 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 3221f6dd937e4657993532db6910d7fc.
2013-11-14 05:08:05,069 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:05,070 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,071 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,072 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""POST /v2.0/security-groups.json HTTP/1.1"" 201 954 0.180143
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,280 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,283 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,284 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,285 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,285 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:05,287 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:05,288 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:05,303 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 200 949 0.023315
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,308 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:04.000000Z into {'tz_sign': None, 'second_fraction': u'000000', 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'04', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,311 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,312 (iso8601.iso8601): DEBUG iso8601 to_int Got u'04' for 'second' with default None
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_get Returning cached token 2df68c733e72f70e2b09ac49a953f98d
2013-11-14 05:08:05,312 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: tempest with project_id : csi-tenant-tempest and roles: csi-tenant-tempest
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Matched POST /security-group-rules.json
2013-11-14 05:08:05,314 (routes.middleware): DEBUG middleware __call__ Route path: '/security-group-rules.:(format)', defaults: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>}
2013-11-14 05:08:05,315 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'create', 'controller': <wsgify at 41022736 wrapping <function resource at 0x270aed8>>, 'format': u'json'}
2013-11-14 05:08:05,316 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.start on notifications.info
2013-11-14 05:08:05,317 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is 66fa31f3dc8d4f2eb9cd7fb561a17f49.
2013-11-14 05:08:05,324 (amqp): DEBUG channel __init__ using channel_id: 1
2013-11-14 05:08:05,325 (amqp): DEBUG channel _open_ok Channel open
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token __call__ Authenticating user token
2013-11-14 05:08:05,383 (keystoneclient.middleware.auth_token): DEBUG auth_token _remove_auth_headers Removing headers from request environment: X-Identity-Status,X-Domain-Id,X-Domain-Name,X-Project-Id,X-Project-Name,X-Project-Domain-Id,X-Project-Domain-Name,X-User-Id,X-User-Name,X-User-Domain-Id,X-User-Domain-Name,X-Roles,X-Service-Catalog,X-User,X-Tenant-Id,X-Tenant-Name,X-Tenant,X-Role
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.client): DEBUG client acquire_connection [6998] Acquired connection https://x.x.x.x:443. 8 connection(s) available.
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Issuing - request PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f
2013-11-14 05:08:05,394 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Setting X-Nvp-Wait-For-Config-Generation request header: '37774691'
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 parse_date Parsed 2013-11-15T05:08:05Z into {'tz_sign': None, 'second_fraction': None, 'hour': u'05', 'tz_hour': None, 'month': u'11', 'timezone': u'Z', 'second': u'05', 'tz_minute': None, 'year': u'2013', 'separator': u'T', 'day': u'15', 'minute': u'08'} with default timezone <iso8601.iso8601.Utc object at 0x1798c50>
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'2013' for 'year' with default None
2013-11-14 05:08:05,400 (iso8601.iso8601): DEBUG iso8601 to_int Got u'11' for 'month' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'15' for 'day' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'hour' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'08' for 'minute' with default None
2013-11-14 05:08:05,401 (iso8601.iso8601): DEBUG iso8601 to_int Got u'05' for 'second' with default None
2013-11-14 05:08:05,401 (keystoneclient.middleware.auth_token): DEBUG auth_token _cache_put Storing 81224fd97e0fca9805338cbbb08a3900 token in memcache
2013-11-14 05:08:05,402 (keystoneclient.middleware.auth_token): DEBUG auth_token _build_user_headers Received request from user: service-user with project_id : csi-tenant-admin and roles: csi-tenant-admin,csi-role-admin
2013-11-14 05:08:05,404 (routes.middleware): DEBUG middleware __call__ No route matched for GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Matched GET /ports.json
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Route path: '/ports{.format}', defaults: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>}
2013-11-14 05:08:05,405 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'index', 'controller': <wsgify at 40952784 wrapping <function resource at 0x270ad70>>, 'format': u'json'}
2013-11-14 05:08:05,423 (neutron.wsgi): INFO log write 10.140.129.30,x.x.x.x - - [14/Nov/2013 05:08:05] ""GET /v2.0/ports.json?network_id=07490058-67bd-4038-9ae2-bcc7aa973b02&device_owner=network%3Adhcp HTTP/1.1"" 200 136 0.039706
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request [6998] Completed request 'PUT https://x.x.x.x:443//ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200 (0.06 seconds)
2013-11-14 05:08:05,456 (neutron.plugins.nicira.api_client.request): DEBUG request _issue_request Reading X-Nvp-config-Generation response header: '37774694'
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.client): DEBUG client release_connection [6998] Released connection https://x.x.x.x:443. 9 connection(s) available.
2013-11-14 05:08:05,457 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [6998] Completed request 'PUT /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 200
2013-11-14 05:08:05,458 (neutron.plugins.nicira.nvplib): DEBUG nvplib update_security_group_rules Updated Security Profile: {u'display_name': u'security-tempest-1342522857', u'_href': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', u'tags': [{u'scope': u'quantum', u'tag': u'2014.1.a230.g5c9c9b9'}, {u'scope': u'os_tid', u'tag': u'csi-tenant-tempest'}], u'logical_port_egress_rules': [{u'ethertype': u'IPv4', u'port_range_max': 22, u'port_range_min': 22, u'protocol': 6}, {u'ethertype': u'IPv4', u'port_range_max': 68, u'port_range_min': 68, u'protocol': 17}], u'_schema': u'/ws.v1/schema/SecurityProfileConfig', u'logical_port_ingress_rules': [{u'ethertype': u'IPv4'}, {u'ethertype': u'IPv6'}], u'uuid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f'}
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp notify Sending security_group_rule.create.end on notifications.info
2013-11-14 05:08:05,490 (neutron.openstack.common.rpc.amqp): DEBUG amqp _add_unique_id UNIQUE_ID is d7b3139142a04511867d57090f7e22cf.
2013-11-14 05:08:05,497 (amqp): DEBUG channel _do_close Closed channel #1
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Matched GET /security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Route path: '/security-groups/:(id).:(format)', defaults: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>}
2013-11-14 05:08:11,993 (routes.middleware): DEBUG middleware __call__ Match dict: {'action': u'show', 'controller': <wsgify at 41021840 wrapping <function resource at 0x270ae60>>, 'id': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'format': u'json'}
2013-11-14 05:08:12,001 (neutron.api.v2.resource): ERROR resource resource show failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 290, in show
    parent_id=parent_id),
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 258, in _item
    obj = obj_getter(request.context, id, **kwargs)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 180, in get_security_group
    context, id), fields)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/db/securitygroups_db.py"", line 194, in _get_security_group
    raise ext_sg.SecurityGroupNotFound(id=id)
SecurityGroupNotFound: Security group b9fc7dc0-c22b-45ee-a811-d2a11b7e864f does not exist
2013-11-14 05:08:12,002 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 05:08:12] ""GET /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 404 277 0.032987
* Delete
2013-11-14 18:50:04,498 (neutron.plugins.nicira.api_client.request_eventlet): DEBUG request_eventlet _handle_request [8422] Completed request 'DELETE /ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f': 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Received error code: 404
2013-11-14 18:50:04,498 (NVPApiHelper): ERROR NvpApiClient request Server Error Message: Security Profile 'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f' not registered.
2013-11-14 18:50:04,498 (neutron.plugins.nicira.nvplib): ERROR nvplib delete_security_profile Error. Unknown exception: An unknown exception occurred.. locals=[{'path': u'/ws.v1/security-profile/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'e': NotFound(u'An unknown exception occurred.',), 'spid': u'b9fc7dc0-c22b-45ee-a811-d2a11b7e864f', 'cluster': <neutron.plugins.nicira.nvp_cluster.NVPCluster object at 0x2d96f10>}]
2013-11-14 18:50:04,499 (neutron.api.v2.resource): ERROR resource resource delete failed
Traceback (most recent call last):
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/resource.py"", line 84, in resource
    result = method(request=request, **args)
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/api/v2/base.py"", line 432, in delete
    obj_deleter(request.context, id, **kwargs)
    security_group['id'])
  File ""/usr/local/csi/share/csi-neutron.venv/lib/python2.6/site-packages/neutron/plugins/nicira/nvplib.py"", line 1148, in delete_security_profile
    raise exception.NeutronException()
NeutronException: An unknown exception occurred.
2013-11-14 18:50:04,500 (neutron.wsgi): INFO log write x.x.x.x,x.x.x.x - - [14/Nov/2013 18:50:04] ""DELETE /v2.0/security-groups/b9fc7dc0-c22b-45ee-a811-d2a11b7e864f.json HTTP/1.1"" 500 248 0.071508","NVP plugin:fix delete sec group when backend is out of sync

If a security group does not exist on the NVP backend, an error
should not be raised on deletion of the security group.

This patch changes the plugin behavior by deleting the record
from the database and just logging that the security group
was not found on the NVP backend.

Closes-Bug: #1251422

Change-Id: Ib8adf7a830ff336655fd83ad4118cde641adf284"
1736,ed8b4706f554e5ae5d57f6017b1fd762216dad97,1377846174,,1.0,19,7,2,2,1,0.961236605,True,6.0,1869675.0,34.0,9.0,False,118.0,33863.0,333.0,5.0,37.0,2331.0,2350.0,36.0,2023.0,2041.0,35.0,2158.0,2175.0,0.006119327,0.366989631,0.369879313,1530,1222663,1222663,nova,ed8b4706f554e5ae5d57f6017b1fd762216dad97,0,0,"Feature, “Catch more accuracy exception for _lookup_by_name”","Bug #1222663 in OpenStack Compute (nova): ""libvirt - should catch more accuracy exception for _lookup_by_name""","When an instance is not found by libvirtd, _lookup_by_name in libvirt
driver raises InstanceNotFound.
It's better to catch this exception instead of NotFound who is the father
of InstanceNotFound.","Catch more accuracy exception for _lookup_by_name

When an instance is not found by libvirtd, _lookup_by_name in libvirt
driver raises InstanceNotFound. This patch catchs this exception instead
of NotFound who is the father of InstanceNotFound.

Closes-Bug: #1222663
Change-Id: I1b4ce8f6c44eef29617a42151152cda2159a0088"
1737,edc66467fc0dcf729990d8e7baed254f8ded9bfa,1383770364,,1.0,5,3,1,1,1,0.0,True,1.0,1628530.0,14.0,6.0,False,61.0,1082131.0,141.0,4.0,3.0,2420.0,2422.0,3.0,2120.0,2122.0,1.0,2281.0,2281.0,0.000306185,0.349357012,0.349357012,68,396,1248699,nova,edc66467fc0dcf729990d8e7baed254f8ded9bfa,1,1,"It says Ubuntu, but I think is more general with the use of the command","Bug #1248699 in OpenStack Compute (nova): ""Fix checking if an ip interface exist when machine hostname is not configured in Ubuntu""","In Ubuntu when you don't specify the hostname of the machine in /etc/hosts , and you try to run sudo <whatever> you get this message shown in your terminal:
sudo: unable to resolve host ....
But this shouldn't be causing nova compute to fail, but it does because of this function (nova/network/linux_net.py):
def device_exists(device):
    (_out, err) = _execute('ip', 'link', 'show', 'dev', device,
                                             check_exit_code=False, run_as_root=True)
    return not err
As you can see this function instead of checking the return code of the command ""ip link show"", it try to check whether anything was displayed to stderr or no,  and guess what ? the message ""sudo: unable to resolve host ...."" is sent to stderr !!
I will go as far as to tell (And maybe i am wrong) that there is only one way and one preferable way to check wether a command succeed or failed is by checking the return code of this latter, and if there is some special case where a command is returning wrong return code, than it should be treated as it is, i.e. special case. Remember ""Special cases aren't special enough to break the rules.""","Check return code of command instead of checking stderr

Command ""ip link show"" return a no 0 return code when passing
a no existing ip interface so we should use the return code
instead of checking stderr because stderr can contain messages
from the system even though the command succeeded.

Change-Id: Ic43e72dab3c5b6e266240b7ff7309ca3b44f8ed5
Closes-Bug: #1248699"
1738,edc93383f7cd9d055d655a8e345a3efa81b7de5b,1389859111,0.0,1.0,22,14,3,3,1,0.820042596,True,1.0,457904.0,9.0,3.0,False,22.0,1621544.0,38.0,2.0,13.0,1241.0,1247.0,13.0,1070.0,1076.0,6.0,1090.0,1090.0,0.005255255,0.819069069,0.819069069,201,604,1261849,cinder,edc93383f7cd9d055d655a8e345a3efa81b7de5b,1,0,Wrong href because evolution,"Bug #1261849 in Cinder: ""version list shows wrong hrefs""","If you hit the cinder endpoint directly, you get the version list. The generated hrefs all point to /v1.
cory@cfsyn28:~/devstack$ curl localhost:8776 | python -m json.tool
{
    ""versions"": [
        {
            ""id"": ""v1.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-01-04T11:33:21Z""
        },
        {
            ""id"": ""v2.0"",
            ""links"": [
                {
                    ""href"": ""http://localhost:8776/v1/"",
                    ""rel"": ""self""
                }
            ],
            ""status"": ""CURRENT"",
            ""updated"": ""2012-11-21T11:33:21Z""
        }
    ]
}","Updates for version list to show correct references

Updated views to consider version while creating href.
Updated fakes to consider version while creating fake href.
Updated some test cases in v2 api which were using v1 as
expected output

Change-Id: Ib05180041309ef75c3718924969bd2354d6d8b3a
Closes-Bug: #1261849"
1739,edcc7dcbcbfa50a002767d437ea9cf344d1d7a37,1381902963,,1.0,57,1,2,2,1,0.216396932,True,5.0,1125042.0,25.0,8.0,False,35.0,943.0,79.0,4.0,240.0,1667.0,1784.0,237.0,1343.0,1458.0,152.0,1452.0,1481.0,0.024132492,0.229179811,0.233753943,1694,1240351,1240351,nova,edcc7dcbcbfa50a002767d437ea9cf344d1d7a37,1,1, ,"Bug #1240351 in OpenStack Compute (nova): ""v3 server controller load update extension point without checking right require function""","server's controller load the update extension point as below:
       # Look for implmentation of extension point of server update
        self.update_extension_manager = \
            stevedore.enabled.EnabledExtensionManager(
                namespace=self.EXTENSION_UPDATE_NAMESPACE,
                check_func=_check_load_extension('server_resize'),
                invoke_on_load=True,
                invoke_kwds={""extension_info"": self.extension_info},
                propagate_map_exceptions=True)
        if not list(self.update_extension_manager):
But it's checking function with wrong params, it should be _check_load_extension('server_update').","Correct update extension point's check_func for v3 server's controller

When loading update extension point, it should check the extension has
'server_update' function. But current code check with wrong function
name 'server_resize'. And this patch adds testcase for servers extension
point.

Change-Id: I2bee7ff306b3337d24047e471fac9485fec9a6f7
Closes-bug: #1240351"
1740,ee26de1ed22fc67591a7b5465132f78af1f1c0f4,1378362023,,1.0,4,4,3,3,1,0.94639463,True,7.0,50817.0,43.0,20.0,False,8.0,598829.6667,10.0,4.0,38.0,1008.0,1046.0,38.0,968.0,1006.0,0.0,252.0,252.0,0.003508772,0.887719298,0.887719298,1515,1221036,1221036,neutron,ee26de1ed22fc67591a7b5465132f78af1f1c0f4,1,1, ,"Bug #1221036 in neutron: ""conversion type missing in log message""","target codes: (https://github.com/openstack/neutron/blob/master/neutron/plugins/mlnx/agent/eswitch_neutron_agent.py)
  def provision_network(self, port_id, port_mac,
                          network_id, network_type,
                          physical_network, segmentation_id):
        LOG.info(_(""Provisioning network %s""), network_id)
        if network_type == constants.TYPE_VLAN:
            LOG.debug(_(""creating VLAN Network""))
        elif network_type == constants.TYPE_IB:
            LOG.debug(_(""creating IB Network""))
        else:
            LOG.error(_(""Unknown network type %(network_type) ""      <======== miss a conversion type here like 's'
                        ""for network %(network_id)""),                                          <======== miss a conversion type here like 's'
                      {'network_type': network_type,
                       'network_id': network_id})
            return
        data = {
            'physical_network': physical_network,
            'network_type': network_type,
            'ports': [],
            'vlan_id': segmentation_id}
        self.network_map[network_id] = data","fix conversion type missing

Conversion type is missing in some places which would cause some
unexcepted error. By using 'grep -rn ""%(\w\+)\W""', we could find
all cases of '%(variable_a)' and fix them.

Change-Id: I05cbaac73976c70be8428bf5a2d0017ea7059cb3
Closes-Bug: #1221036"
1741,ee6299620ec1ce5c5ff22e0888e00a70b840a708,1409981873,,1.0,4,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1328,1796,1370308,cinder,ee6299620ec1ce5c5ff22e0888e00a70b840a708,0,0,Bug in test,"Bug #1370308 in Cinder: ""wrong test for test_create_missing_specs_name in test_qos_specs_manage.py""","Since the test is `test_create_missing_specs_name` [1], we should define the body like `body = {'qos_specs': {'a': 'b'}}`. If we use body={'foo': {'a': 'b'}}, the code even do not reach [2], and it is actually raised by a not valid body [3].
[1]: https://github.com/openstack/cinder/blob/master/cinder/tests/api/contrib/test_qos_specs_manage.py#L326
[2]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L131
[3]: https://github.com/openstack/cinder/blob/master/cinder/api/contrib/qos_specs_manage.py#L127","Use right body for test_create_missing_specs_name

Since the test is `test_create_missing_specs_name`, we should define
the the body like `body = {'qos_specs': {'a': 'b'}}`. And add a test
to cover the invalid body case.

Change-Id: Ib1cf9a8b5bfddc0091e28e3765a36c75044642f3
Closes-bug: #1370308"
1742,ee9fe2458f813b5d367bc7263c7a30a3af46aa2b,1404219799,1.0,1.0,10,6,3,2,1,0.838779468,False,,,,,True,,,,,,,,,,,,,,,,,1030,1474,1336251,neutron,ee9fe2458f813b5d367bc7263c7a30a3af46aa2b,1,1,“Remove db lock and add missing contexts”,"Bug #1336251 in neutron: ""Big Switch: Consistency hash table lock causing deadlocks""","The new consistency hash table seems to be causing deadlocks.
http://openstack-ci-gw.bigswitch.com/logs/refs-changes-13-92013-4/BSN_PLUGIN/logs/screen/screen-q-svc.log.gz","BSN: Remove db lock and add missing contexts

Adds context tracking decorators that were missing
from router interface methods. Without them, new
sessions were being created instead of using the
existing context which was causing transaction
issues.

Modifies the servermanager to store context references
as weakrefs so if multiple functions are called before
the rest functions are called, the first one doesn't steal
the only context reference with a pop() call.

Removes a DB lock for update in the server manager that occured
during rest calls that was triggering deadlocks due to the
file lock synchronization for the rest calls.

Closes-Bug: #1336251
Change-Id: Iad3d61e2c23832b3ad760a999fbab7feaa13f805"
1743,eeaf046c18d08403a0fefd778d4eb8401db0d357,1394364843,,1.0,1,1,1,1,1,0.0,True,1.0,901460.0,32.0,12.0,False,20.0,2736761.0,40.0,5.0,17.0,4551.0,4560.0,17.0,3615.0,3624.0,8.0,3517.0,3517.0,0.001193001,0.466330859,0.466330859,569,990,1289993,nova,eeaf046c18d08403a0fefd778d4eb8401db0d357,0,0,remove code,"Bug #1289993 in OpenStack Compute (nova): ""dhcp_options_enabled used in comment""","https://review.openstack.org/#/c/58089/ removes check CONF.dhcp_options_enabled from nova but it is still used in the help string in nova/virt/baremetal/pxe.py:
    cfg.StrOpt('pxe_bootfile_name',
               help='This gets passed to Neutron as the bootfile dhcp '
               'parameter when the dhcp_options_enabled is set.',
               default='pxelinux.0'),","Remove mention of removed dhcp_options_enabled

Commit 3292103d80676fc789615465759de966dba4e9f7 removed
dhcp_options_enabled, remove it from the help string as well.

Change-Id: I1f89e0b809c5a47b02a11f2c013a8cbb6d0afa4a
Closes-Bug: #1289993"
1744,eeb97eb16c1f56cecca83700ab28a594ed27ab1f,1392738697,,1.0,10,4,2,2,1,0.985228136,True,1.0,1109115.0,15.0,4.0,False,9.0,545505.0,15.0,4.0,193.0,3696.0,3736.0,191.0,2953.0,2993.0,184.0,3466.0,3497.0,0.025084746,0.470101695,0.474305085,423,838,1281440,nova,eeb97eb16c1f56cecca83700ab28a594ed27ab1f,0,0,No bug. Should handle boolean string parameters,"Bug #1281440 in OpenStack Compute (nova): ""should handle boolean string parameters through create multiple servers API""","If specifying false string (""False"") as ""return_reservation_id"" parameter of create multiple servers API, Nova considers it as True.
On the other hand, nova can consider false string as false in the case of ""on_shared_storage"" parameter of evacuate API.
That behavior seems API inconsistency.","Add boolean convertor to ""create multiple servers"" API

""create multiple servers"" API contains ""return_reservation_id"" as an
attribute, and the attribute should be boolean. There are the other
boolean attributes such as ""on_shared_storage"" of evacuate API, they
can be passed with either boolean or string(e.g. ""True""). For the
consistency of whole Nova APIs, this patch adds boolean convertors to
multiple_create API.

Change-Id: Id6f458a2ced869a31d5d25d661309973132e9360
Closes-Bug: #1281440"
1745,eece55ceb687c425de1066851c9601221f1ef2b7,1379677717,,1.0,166,60,7,3,1,0.594218857,True,20.0,9896916.0,132.0,42.0,False,2.0,1032401.429,2.0,5.0,17.0,642.0,659.0,17.0,598.0,615.0,0.0,143.0,143.0,0.00273224,0.393442623,0.393442623,1412,1065531,1065531,neutron,eece55ceb687c425de1066851c9601221f1ef2b7,1,1, ,"Bug #1065531 in neutron: ""lockutils - remove lock dir creation and cleanup""","See https://review.openstack.org/14139
This:
                    if not local_lock_path:
                        cleanup_dir = True
                        local_lock_path = tempfile.mkdtemp()
                    if not os.path.exists(local_lock_path):
                        cleanup_dir = True
                        ensure_tree(local_lock_path)
                    ...
                    finally:
                        # NOTE(vish): This removes the tempdir if we needed
                        #             to create one. This is used to cleanup
                        #             the locks left behind by unit tests.
                        if cleanup_dir:
                            shutil.rmtree(local_lock_path)
Why are we deleting the lock dir here? Does that even work? i.e. what if someone concurrently tries to take the lock, re-creates the dir and lock a new file?","Update lockutils and fixture in openstack.common

lockutils: included commits:
  8b2b0b7 Use hacking import_exceptions for gettextutils._
  6d0a6c3 Correct invalid docstrings
  12bcdb7 Remove vim header
  79e6bc6 fix lockutils.lock() to make it thread-safe
  ace5120 Add main() to lockutils that creates temp dir for locks
  537d8e2 Allow lockutils to get lock_path conf from envvar
  371fa42 Move LockFixture into a fixtures module
  d498c42 Fix to properly log when we release a semaphore
  29d387c Add LockFixture to lockutils
  3e3ac0c Modify lockutils.py due to dispose of eventlet
  90b6a65 Fix locking bug
  27d4b41 Move synchronized body to a first-class function
  15c17fb Make lock_file_prefix optional
  1a2df89 Enable H302 hacking check

fixture: created, included commits:
  45658e2 Fix violations of H302:import only modules
  12bcdb7 Remove vim header
  3970d46 Fix typos in oslo
  371fa42 Move LockFixture into a fixtures module
  f4a4855 Consolidate the use of stubs
  6111131 Make openstack.common.fixture.config Py3 compliant
  3906979 Add a fixture for dealing with config
  d332cca Add a fixture for dealing with mock patching.
  1bc3ecf Start adding reusable test fixtures.

Also tox.ini was corrected to let lockutils work in tests.

This change is needed for work on bp: db-sync-models-with-migrations

Closes-Bug: #1065531

Change-Id: I139f30b4767ff2c9d1f01ee728823859c09b3859"
1746,eef97cdf4bb7f426d7feb394ef54510db8b1656b,1411741909,,1.0,2,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1360,1829,1374458,nova,eef97cdf4bb7f426d7feb394ef54510db8b1656b,1,1,“There is an error in the n-cpu log”,"Bug #1374458 in OpenStack Compute (nova): ""test_encrypted_cinder_volumes_luks fails to detach encrypted volume""","http://logs.openstack.org/98/124198/3/check/check-grenade-dsvm-icehouse/c89f18f/console.html#_2014-09-26_03_38_56_940
2014-09-26 03:38:57.259 |     Traceback (most recent call last):
2014-09-26 03:38:57.259 |       File ""tempest/scenario/manager.py"", line 142, in delete_wrapper
2014-09-26 03:38:57.259 |         delete_thing(*args, **kwargs)
2014-09-26 03:38:57.259 |       File ""tempest/services/volume/json/volumes_client.py"", line 108, in delete_volume
2014-09-26 03:38:57.259 |         resp, body = self.delete(""volumes/%s"" % str(volume_id))
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.259 |         return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.259 |         resp, resp_body)
2014-09-26 03:38:57.259 |       File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.259 |         raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.259 |     BadRequest: Bad request
2014-09-26 03:38:57.260 |     Details: {u'message': u'Invalid volume: Volume status must be available or error, but current status is: in-use', u'code': 400}
2014-09-26 03:38:57.260 |     }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.260 |     traceback-2: {{{
2014-09-26 03:38:57.260 |     Traceback (most recent call last):
2014-09-26 03:38:57.260 |       File ""tempest/common/rest_client.py"", line 561, in wait_for_resource_deletion
2014-09-26 03:38:57.260 |         raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.260 |     TimeoutException: Request timed out
2014-09-26 03:38:57.260 |     Details: (TestEncryptedCinderVolumes:_run_cleanups) Failed to delete resource 704461b6-3421-4959-8113-a011e6410ede within the required time (196 s).
2014-09-26 03:38:57.260 |     }}}
2014-09-26 03:38:57.260 |
2014-09-26 03:38:57.261 |     traceback-3: {{{
2014-09-26 03:38:57.261 |     Traceback (most recent call last):
2014-09-26 03:38:57.261 |       File ""tempest/services/volume/json/admin/volume_types_client.py"", line 97, in delete_volume_type
2014-09-26 03:38:57.261 |         resp, body = self.delete(""types/%s"" % str(volume_id))
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 225, in delete
2014-09-26 03:38:57.261 |         return self.request('DELETE', url, extra_headers, headers, body)
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 435, in request
2014-09-26 03:38:57.261 |         resp, resp_body)
2014-09-26 03:38:57.261 |       File ""tempest/common/rest_client.py"", line 484, in _error_checker
2014-09-26 03:38:57.261 |         raise exceptions.BadRequest(resp_body)
2014-09-26 03:38:57.261 |     BadRequest: Bad request
2014-09-26 03:38:57.261 |     Details: {u'message': u'Target volume type is still in use.', u'code': 400}
2014-09-26 03:38:57.262 |     }}}
2014-09-26 03:38:57.262 |
2014-09-26 03:38:57.262 |     Traceback (most recent call last):
2014-09-26 03:38:57.262 |       File ""tempest/test.py"", line 142, in wrapper
2014-09-26 03:38:57.262 |         return f(self, *func_args, **func_kwargs)
2014-09-26 03:38:57.262 |       File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 56, in test_encrypted_cinder_volumes_luks
2014-09-26 03:38:57.262 |         self.attach_detach_volume()
2014-09-26 03:38:57.262 |       File ""tempest/scenario/test_encrypted_cinder_volumes.py"", line 49, in attach_detach_volume
2014-09-26 03:38:57.262 |         self.nova_volume_detach()
2014-09-26 03:38:57.262 |       File ""tempest/scenario/manager.py"", line 439, in nova_volume_detach
2014-09-26 03:38:57.262 |         'available')
2014-09-26 03:38:57.262 |       File ""tempest/services/volume/json/volumes_client.py"", line 181, in wait_for_volume_status
2014-09-26 03:38:57.263 |         raise exceptions.TimeoutException(message)
2014-09-26 03:38:57.263 |     TimeoutException: Request timed out
2014-09-26 03:38:57.263 |     Details: Volume 704461b6-3421-4959-8113-a011e6410ede failed to reach available status within the required time (196 s).
http://logstash.openstack.org/#eyJzZWFyY2giOiJtZXNzYWdlOlwiRGV0YWlsczogKFRlc3RFbmNyeXB0ZWRDaW5kZXJWb2x1bWVzOl9ydW5fY2xlYW51cHMpIEZhaWxlZCB0byBkZWxldGUgcmVzb3VyY2VcIiBBTkQgbWVzc2FnZTpcIndpdGhpbiB0aGUgcmVxdWlyZWQgdGltZVwiIEFORCB0YWdzOlwiY29uc29sZVwiIiwiZmllbGRzIjpbXSwib2Zmc2V0IjowLCJ0aW1lZnJhbWUiOiI2MDQ4MDAiLCJncmFwaG1vZGUiOiJjb3VudCIsInRpbWUiOnsidXNlcl9pbnRlcnZhbCI6MH0sInN0YW1wIjoxNDExNzM4OTc0MTMwfQ==
130 hits in 7 days, check and gate, all failures.","Retry on closing of luks encrypted volume in case device is busy

We're seeing races in the gate on detach of an encrypted volume where
the device is busy so the detach fails, which causes the delete of the
volume on the cinder side to fail because the volume is still in use and
then Tempest times out waiting for the volume to be deleted.

This simply adds a retry to close the encryption device.  I use
attempts=3 since that's what we use for lvremove.

Closes-Bug: #1374458

Change-Id: Ia0667da7dee247ba8c3338b296244b2a71d1045d"
1747,ef3fbe99af498a40555b44f9ef0fcf6b88eb5d30,1380615050,1.0,1.0,7,3,2,2,1,0.970950594,True,5.0,830969.0,42.0,19.0,False,35.0,394800.0,137.0,7.0,1699.0,1476.0,2820.0,1470.0,1284.0,2430.0,1602.0,1203.0,2465.0,0.260185035,0.195422821,0.400259698,1623,1233563,1233563,nova,ef3fbe99af498a40555b44f9ef0fcf6b88eb5d30,1,1, ,"Bug #1233563 in OpenStack Compute (nova): ""unshelve feature does not work""","When unshelving a shelved server, the server cannot be changed to 'Active' forever:
$ nova list
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| ID                                   | Name | Status            | Task State | Power State | Networks         |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
| 919234d1-4a3d-4c26-bddd-77d004f7d41e | vm01 | SHELVED_OFFLOADED | unshelving | Shutdown    | private=10.0.0.3 |
+--------------------------------------+------+-------------------+------------+-------------+------------------+
and nova-scheduler outputs the following error messages:
2013-10-01 17:22:55.456 ERROR nova.openstack.common.rpc.amqp [req-51700174-fc3a-48f4-8962-8f54d6f30164 admin demo] Exception during message handling
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 439, in inner
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     return catch_client_exception(exceptions, func, *args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/common.py"", line 420, in catch_client_exception
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     return func(*args, **kwargs)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/manager.py"", line 298, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     filter_properties)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 144, in select_destinations
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 288, in _schedule
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp     scheduler_hints = filter_properties.get('scheduler_hints') or {}
2013-10-01 17:22:55.456 TRACE nova.openstack.common.rpc.amqp AttributeError: 'list' object has no attribute 'get'","Fix filter_properties of unshelve API

When unshelving a shelved server, nova-scheduler fails because nova-conductor
of current unshelve feature passes an empty list as filter_properties, and
nova-scheduler handles it as a dict.

To fix the problem, this patch changes the filter_properties to an empty dict.

Closes-Bug: #1233563

Change-Id: I2c7a2a743a8142af0be0a182067f3295980aa997"
1748,ef42f7c3f982092be6b5199bd9c2ade69e0446f3,1386657515,,1.0,4,4,2,2,1,1.0,True,1.0,13100.0,5.0,2.0,False,22.0,63438.0,31.0,2.0,328.0,744.0,977.0,257.0,606.0,793.0,53.0,251.0,293.0,0.087520259,0.408427877,0.47649919,168,570,1259431,neutron,ef42f7c3f982092be6b5199bd9c2ade69e0446f3,0,0,Variable for agent instance should be named as 'agent' instead of plugin,"Bug #1259431 in neutron: ""plugin variable name should be agent""","plugin = LinuxBridgeNeutronAgentRPC(interface_mappings,    ------> agent =
                                        polling_interval,
                                        root_helper)
    LOG.info(_(""Agent initialized successfully, now running... ""))
    plugin.daemon_loop()    -----> agent.daemon_loop()","change variable name from plugin into agent

Variable for agent instance should be named as 'agent'
instead of plugin.

Change-Id: I9bb2245049c6d6084284c2311a4c6aa2ad880bc1
Closes-Bug: #1259431"
1749,ef7e17e66591bb93eeeff8d58a8e5c96bf2eca89,1395326598,,1.0,2,1,1,1,1,0.0,True,1.0,130420.0,19.0,4.0,False,2.0,3036233.0,2.0,4.0,251.0,1627.0,1699.0,245.0,1238.0,1308.0,190.0,742.0,782.0,0.188735178,0.734189723,0.773715415,657,1083,1295187,neutron,ef7e17e66591bb93eeeff8d58a8e5c96bf2eca89,1,1,typo in code,"Bug #1295187 in neutron: ""Fix typo in lbaas agent exception message""",Fix typo in lbaas agent exception message,"Fix typo in lbaas agent exception message

Change-Id: Iec2a5e02d40f73e4576ed853577a6a9f85e49e67
Closes-Bug: #1295187"
1750,efd4ff9c2584de9acfbae432f889e222f5058ea1,1392818328,,1.0,0,3,1,1,1,0.0,True,1.0,95717.0,6.0,2.0,False,8.0,2959217.0,6.0,2.0,12.0,823.0,834.0,11.0,795.0,805.0,1.0,788.0,788.0,0.001384083,0.546020761,0.546020761,434,849,1282098,cinder,efd4ff9c2584de9acfbae432f889e222f5058ea1,0,0,unused function,"Bug #1282098 in Cinder: ""Remove unused function: stub_out_rate_limiting""","This function never used.
grep -r ""stub_out_rate_limiting"" *
cinder/tests/api/fakes.py:def stub_out_rate_limiting(stubs):","Remove unused function

Romove unused function stub_out_rate_limiting in
cinder/tests/api/fakes.py

Change-Id: Ib6fea599eacb140108ca4293cdd089b010a8b979
Closes-Bug: #1282098"
1751,efdca8eeaffe6e63d57243147658f4691da3d89e,1381256882,,1.0,35,11,5,2,1,0.962011801,True,6.0,4879096.0,36.0,22.0,False,32.0,593808.0,67.0,6.0,3.0,2735.0,2737.0,3.0,2444.0,2446.0,1.0,1856.0,1856.0,0.000320359,0.297453148,0.297453148,1630,1234759,1234759,nova,efdca8eeaffe6e63d57243147658f4691da3d89e,1,1, ,"Bug #1234759 in OpenStack Compute (nova): ""Hyper-V fails to spawn snapshots""","Creating a snapshot of an instance and then trying to boot from it will result the following Hyper-V exception: ""HyperVException: WMI job failed with status 10"". Here is the trace: http://paste.openstack.org/show/47904/ .
The ideea is that Hyper-V fails to expand the image, as it gets the request to resize it to it's actual size, which leads to an error.","Fixes Hyper-V snapshot spawning issue

In Hyper-V, snapshoting an instance and then spawning a new
one using the resulted snapshot image with the same flavor,
will result an Hyper-V Exception.

Hyper-V will get the request to resize the base image to it's
actual size, leading to an error.

The reason is that the logic of the resize_vhd method has been
changed, as it is meant to resize images to the flavor size minus
the image metadata size, thus it's maximum internal size. For this
reason, the way to decide whether the root image should be resized
or not must also be changed.

So, instead of comparing the root image and base image sizes,
we should compare their maximum internal sizes.

Closes-Bug: #1234759

Change-Id: I83d2df76b726d705cedc52a8309a2be7037d6a79"
1752,efe1002459145c43577b53ba03a228ecf9a78434,1410297079,,1.0,2,2,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,1284,1747,1367342,nova,efe1002459145c43577b53ba03a228ecf9a78434,1,1,,"Bug #1367342 in OpenStack Compute (nova): ""call to _set_instance_error_state is incorrect in do_build_and_run_instance""","nova/compute/manager.py  in do_build_and_run_instance
Under  except exception.RescheduledException as e:
...
self._set_instance_error_state(context, instance.uuid)
This should be passing instance only not instance.uuid","Pass instance to set_instance_error_state vs. uuid

Quick fix to patch one case in which instance.uuid was still being
passed to _set_instance_error_state when retries are disabled in
rescheduling cases.  Also updated the respective test case.

Change-Id: I24a682b926bc570d7e932ffedc664a95d41fda85
Closes-Bug: #1367342"
1753,effa12e8711ff6e23aa2d4b40f4e9f248e141915,1391088671,0.0,1.0,26,17,4,2,1,0.857566747,True,25.0,2870567.0,168.0,44.0,False,215.0,177508.0,1279.0,4.0,319.0,3218.0,3346.0,306.0,2797.0,2914.0,315.0,2313.0,2437.0,0.043803715,0.320765179,0.337953978,1710,1241615,1241615,nova,effa12e8711ff6e23aa2d4b40f4e9f248e141915,1,0,“Which will return the old style (legacy) bdm format if called without legacy=True. “,"Bug #1241615 in OpenStack Compute (nova): ""rebuild with volume attached leaves instance without the volume and in an inconsistent state""","I created a backup to an instance which had no volume attached.
attached a volume -> rebuild the instance from the backup.
It appears as though the volume is not attached anymore after the rebuild, but if we try to attach it to the same device we get an error that a device is already attached:
2013-10-18 16:54:36.632 2478 DEBUG qpid.messaging [-] RETR[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""dd2a85b63c56498c8f2835f9b96e9bb9""
, ""failure"": null, ""_msg_id"": ""7ce524cebbb34aecab9d608a48103a1c"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) _get /usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py:654
2013-10-18 16:54:36.633 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: MessageFlow(destination='0', unit=0, value=1L, id=serial(5206)) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.634 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: SessionCompleted(commands=[0-5199]) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:54:36.639 2478 ERROR nova.openstack.common.rpc.amqp [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Exception during message handling
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3624, in reserve_block_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return do_reserve()
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3613, in do_reserve
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     context, instance, bdms, device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 135, in get_device_name_for_instance
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     mappings['root'], device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 217, in get_next_device_name
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp     raise exception.DevicePathInUse(path=device)
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp DevicePathInUse: The supplied device path (/dev/vdc) is in use.
2013-10-18 16:54:36.639 2478 TRACE nova.openstack.common.rpc.amqp
2013-10-18 16:54:36.641 2478 ERROR nova.openstack.common.rpc.common [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Returning exception The supplied device path (/dev/vdc) is i
n use. to caller
2013-10-18 16:54:36.642 2478 ERROR nova.openstack.common.rpc.common [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] ['Traceback (most recent call last):\n', '  File ""/usr/lib/p
ython2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data\n    **args)\n', '  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch\n    result = getattr(proxyo
bj, method)(ctxt, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped\n    payload)\n', '  File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped\n    return f(self, con
text, *args, **kw)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function\n    pass\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function\
n    return function(self, context, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function\n    e, sys.exc_info())\n', '  File ""/usr/lib/python2.6/site-packages/nova/compu
te/manager.py"", line 258, in decorated_function\n    return function(self, context, *args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3624, in reserve_block_device_name\n    return do_reserve()
\n', '  File ""/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py"", line 246, in inner\n    return f(*args, **kwargs)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3613, in do_reserve\n    c
ontext, instance, bdms, device)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/utils.py"", line 135, in get_device_name_for_instance\n    mappings[\'root\'], device)\n', '  File ""/usr/lib/python2.6/site-packages/nova/compute/u
tils.py"", line 217, in get_next_device_name\n    raise exception.DevicePathInUse(path=device)\n', 'DevicePathInUse: The supplied device path (/dev/vdc) is in use.\n']
2013-10-18 16:54:36.642 2478 DEBUG nova.openstack.common.rpc.amqp [req-4a884bb4-5ba6-403d-8be7-df1eeebc1324 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] UNIQUE_ID is 5d08dde3f40e4186b50e28a9dad5385b. _add_unique_id
/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py:341
If I try to attach the same volume to the same instance on a different device it seems to be working, however, when we try to detach it we get the following error:
2013-10-18 16:58:11.288 2478 DEBUG qpid.messaging [-] RCVD[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""20a035f66eac46f7b44e8420cd5a2d14""
, ""failure"": null, ""_msg_id"": ""27d7abfd907146c4a0e052cf067a4d69"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) do_message_transfer /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:1295
2013-10-18 16:58:11.289 2478 DEBUG qpid.messaging [-] RETR[2fca830]: Message(properties={'x-amqp-0-10.routing-key': u'reply_70fb16e321724b38b3d3face4e83f363'}, content={u'oslo.message': u'{""_unique_id"": ""20a035f66eac46f7b44e8420cd5a2d14""
, ""failure"": null, ""_msg_id"": ""27d7abfd907146c4a0e052cf067a4d69"", ""result"": null, ""ending"": true}', u'oslo.version': u'2.0'}) _get /usr/lib/python2.6/site-packages/qpid/messaging/endpoints.py:654
2013-10-18 16:58:11.289 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: MessageFlow(destination='0', unit=0, value=1L, id=serial(5308)) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:58:11.290 2478 DEBUG qpid.messaging.io.ops [-] SENT[2fa6cb0]: SessionCompleted(commands=[0-5301]) write_op /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:686
2013-10-18 16:58:11.296 2478 ERROR nova.openstack.common.rpc.amqp [req-62e56d8a-9dc1-447c-acef-2b2dafdd5079 bbce236d5aac4d1dbc086a8835ed0ebc d09f3bf0f9224affa92ab97010b37270] Exception during message handling
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     **args)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 90, in wrapped
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     payload)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/exception.py"", line 73, in wrapped
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 243, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     pass
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 229, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 271, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     e, sys.exc_info())
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 258, in decorated_function
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     return function(self, context, *args, **kwargs)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3760, in detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     self._detach_volume(context, instance, bdm)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3732, in _detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     self.volume_api.roll_detaching(context, volume_id)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/compute/manager.py"", line 3725, in _detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     encryption=encryption)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py"", line 1205, in detach_volume
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp     raise exception.DiskNotFound(location=disk_dev)
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp DiskNotFound: No disk at vdc
2013-10-18 16:58:11.296 2478 TRACE nova.openstack.common.rpc.amqp
2013-10-18 16:58:11.298 2478 DEBUG qpid.messaging.io.raw [-] SENT[2fa6cb0]: '\x0f\x01\x00\x19\x00\x01\x00\x00\x00\x00\x00\x00\x04\n\x01\x00\x07\x00\x010\x00\x00\x00\x00\x01\x0f\x00\x00\x1a\x00\x00\x00\x00\x00\x00\x00\x00\x02\n\x01\x00\x0
0\x08\x00\x00\x00\x00\x00\x00\x14\xb5' writeable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:480
2013-10-18 16:58:14.265 2478 DEBUG qpid.messaging.io.raw [-] READ[2fa6cb0]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2013-10-18 16:58:14.266 2478 DEBUG qpid.messaging.io.ops [-] RCVD[2fa6cb0]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2013-10-18 16:58:14.291 2478 DEBUG qpid.messaging.io.raw [-] READ[2fca998]: '\x0f\x00\x00\x10\x00\x00\x00\x00\x00\x00\x00\x00\x01\n\x00\x00' readable /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:416
2013-10-18 16:58:14.291 2478 DEBUG qpid.messaging.io.ops [-] RCVD[2fca998]: ConnectionHeartbeat() write /usr/lib/python2.6/site-packages/qpid/messaging/driver.py:654
2013-10-18 16:58:15.019 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._poll_volume_usage run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.024 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager._instance_usage_audit run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.024 2478 DEBUG nova.openstack.common.periodic_task [-] Running periodic task ComputeManager.update_available_resource run_periodic_tasks /usr/lib/python2.6/site-packages/nova/openstack/common/periodic_task.py:176
2013-10-18 16:58:15.025 2478 DEBUG nova.openstack.common.lockutils [-] Got semaphore ""compute_resources"" lock /usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py:166
2013-10-18 16:58:15.025 2478 DEBUG nova.openstack.common.lockutils [-] Got semaphore / lock ""update_available_resource"" inner /usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py:245
2013-10-18 16:58:15.025 2478 AUDIT nova.compute.resource_tracker [-] Auditing locally available compute resources","Move rebuild to BDM objects

This patch makes the rebuild code path use new-world block device
mapping objects.

Closes-bug: #1241615
Part of blueprint: clean-up-legacy-block-device-mapping
Part of blueprint: icehouse-objects

Change-Id: Id09dba6560b6c0624f9b7fbfa893360a2a3d41b6"
1754,f024b5f9fa0c6b772c597298792b94d01c075384,1385618836,0.0,1.0,44,2,3,2,1,0.959249884,True,10.0,7455522.0,89.0,30.0,False,30.0,457692.3333,73.0,4.0,1.0,4089.0,4089.0,1.0,3410.0,3410.0,1.0,3118.0,3118.0,0.000297133,0.463378398,0.463378398,120,519,1257038,nova,f024b5f9fa0c6b772c597298792b94d01c075384,0,0,A better solution should be,"Bug #1257038 in OpenStack Compute (nova): ""VMware: instance names can be edited, breaks nova-driver lookup""","Currently the VMware Nova Driver relies on the VM name in vCenter/ESX to match the UUID in Nova. The name can be easily edited by vCenter administrators and break Nova administration of VMs. A better solution should be found allowing the Nova Compute Driver for vSphere to look up VMs by a less volatile and publicly visible mechanism.
EDIT:
A fix would make the link between vSphere and Nova more solid and involve using a vSphere metadata value that cannot be easily edited. Currently the UUID is stored as an extra config metadata property inside vSphere (associated with the instance's virtual-machine) and
this value is not easy to accidentally change. That would make the link much more robust.","VMware: fix instance lookup against vSphere

Modifies the logic to search the vsphere vm for a given
instance. The change now makes use of the config.extraConfig
property to search for the instance.

Change-Id: Id11f44b9792f196ad627879d082ad829869efcdf
Closes-Bug: #1257038"
1755,f035efd043aa75d662d2a26a963358db59b75628,1388133221,1.0,1.0,57,40,8,7,1,0.899744354,True,5.0,1454721.0,20.0,10.0,False,73.0,62365.0,316.0,2.0,1.0,685.0,685.0,1.0,524.0,524.0,1.0,530.0,530.0,0.001834862,0.487155963,0.487155963,1753,1250633,1250633,glance,f035efd043aa75d662d2a26a963358db59b75628,1,0, ,"Bug #1250633 in Glance: ""core domain objects depend on config""","The core of the domain model should be as dependency free as possible.
However, since 830f27ba342ca0881e3677ac11c3d818962cba3e (change id Ic52ffb46df9438c247ba063748cadd69b9c90bcd) we have depended on configuration in glance.domain.__init__.
This configuration should instead be depended on in the infrastructure (probably in glance.gateway) and passed in as an initialization parameter to the domain model objects.","Decouple the config dependence on glance domain

Move the configurations which core domain objects depend on to
common/config.py, and add a argument ""task_time_to_live"" of Task's
 __init__ method and TaskFactory's new_task method for delivering
the CONF.task.task_time_to_live value.
For convenience,the argument ""task_time_to_live"" is set a default value 48.

Change-Id: Iffda1ecd25470824c812d66a27fa64ebbaabcf07
Closes-Bug: #1250633"
1756,f045a5dfa66aa46e11d3302ee707f067e86c9b59,1386042719,,1.0,1,1,1,1,1,0.0,True,1.0,3332182.0,14.0,6.0,False,29.0,486.0,47.0,6.0,22.0,4158.0,4175.0,22.0,3458.0,3475.0,9.0,3142.0,3146.0,0.001478415,0.464665878,0.465257244,125,524,1257151,nova,f045a5dfa66aa46e11d3302ee707f067e86c9b59,0,0,Typo in docs,"Bug #1257151 in OpenStack Compute (nova): ""Fix docstring on SnapshotController""","""The Volumes API controller for the OpenStack API.""  is inappropriate for SnapshotController.","Fix docstring on SnapshotController

Change-Id: I66a8b15ef700253bbad54ee3955550aeb2a522b7
Closes-Bug: #1257151"
1757,f05a0c486d8478cdea3808108b172b2487b38bf4,1389951666,,1.0,3,9,2,2,1,0.979868757,True,4.0,2154745.0,30.0,13.0,False,148.0,25554.0,451.0,4.0,54.0,1605.0,1650.0,50.0,1116.0,1158.0,54.0,1387.0,1432.0,0.007755217,0.19571348,0.202058658,1606,1231215,1231215,nova,f05a0c486d8478cdea3808108b172b2487b38bf4,1,0,“we should keep consistency of these two places.”,"Bug #1231215 in OpenStack Compute (nova): ""to_xml should be used in post_live_migration_at_destination method""","in the post_live_migration_at_destination method, the to_xml method is called but the return value never be used, and the comment say that the reason is 'the uuid is not included in to_xml() result'.
but I think the uuid is already  included in to_xml() result now, so we may refactor post_live_migration_at_destination method and change it to use the correct way to get the xml of migrated instance.
# In case of block migration, destination does not have
# libvirt.xml
disk_info = blockinfo.get_disk_info(CONF.libvirt_type,
                                    instance)
self.to_xml(instance, network_info, disk_info,
            block_device_info, write_to_disk=True)
# libvirt.xml should be made by to_xml(), but libvirt
# does not accept to_xml() result, since uuid is not
# included in to_xml() result.
dom = self._lookup_by_name(instance[""name""])
self._conn.defineXML(dom.XMLDesc(0))","libvirt: use to_xml() in post_live_migration_at_destination

Currently post_live_migration_at_destination use the dumpped
xml to define a live-migrated instance, and write the libvirt.xml
with to_xml(), we should keep consistency of these two places.

Closes-bug: #1231215

Change-Id: Id98ee3f21fb7a908a318d0f9abec0b740ff9e410"
1758,f0817d3ac11f98c3d518303a50711a6a9c677851,1395797112,,1.0,19,8,2,2,1,0.764204507,True,4.0,692106.0,48.0,17.0,False,18.0,1376303.0,20.0,3.0,1.0,2093.0,2093.0,1.0,1905.0,1905.0,1.0,2050.0,2050.0,0.000260281,0.266918272,0.266918272,675,1101,1296662,nova,f0817d3ac11f98c3d518303a50711a6a9c677851,1,0,"Bug because evolution, wrong exception applied","Bug #1296662 in OpenStack Compute (nova): ""NotFound should be insteaded of InstanceNotFound ""","we can see the exception is InstanceNotFound  from: https://github.com/openstack/nova/blob/master/nova/compute/api.py#L1758,
but, the exception is NotFound in https://github.com/openstack/nova/blob/master/nova/api/openstack/compute/contrib/server_diagnostics.py#L47.
so, the NotFound should be insteaded of InstanceNotFound","Change NotFound to InstanceNotFound in server_diagnostics.py

According to the get() method in nova/compute/api.py,
the index() method in nova/api/openstack/compute/
contrib/server_diagnostics.py should catch the
InstanceNotFound, rather than the NotFound.
This patch just applies to V2 API, V3 API is already
catching InstanceNotFound.

Change-Id: I9e021837b9050dade9af4725a0712f260eac6f7a
Closes-bug: #1296662"
1759,f087a6f77ef1338bb8d10943d2a18712220c3c44,1398572915,1.0,1.0,110,52,3,2,1,0.778700394,True,4.0,765735.0,71.0,17.0,False,216.0,233944.3333,1215.0,5.0,32.0,4455.0,4460.0,32.0,3431.0,3436.0,26.0,3192.0,3192.0,0.003426831,0.405254474,0.405254474,774,1201,1305423,nova,f087a6f77ef1338bb8d10943d2a18712220c3c44,1,1,,"Bug #1305423 in OpenStack Compute (nova): ""nova libvirt re-write broken with mulitiple ephemeral disks""","Seem to be experiencing a bug with libvirt.xml device formatting when --ephemeral flag is used after initial booth and then use of nova stop/start or nova reboot --hard.  We are using following libvirt options in nova.conf for storage:
libvirt_images_type=lvm
libvirt_images_volume_group=vglocal
When normally using nova boot with a flavor that has ephemeral defined it create two LVM volumes appropriatly ex.
instance-0000077e_disk
instance-0000077e_disk.local
The instance libvirt.xml contains disk devices entry as follows:
<devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-0000077e_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
If we use ""nova boot --flavor 757c75fa-0b6d-4d4f-a128-27813009bff4 --image caa978e0-acae-4205-a4a4-2cf159c166fd --nic net-id=44f2fb0b-0a7a-475c-8fff-54cd4b37958b --ephemeral size=1 --ephemeral size=1 localdisk-1"" the LVM disks for ephemeral goes through enumeration logic whether there is one or more --ephemeral options
 instance-000007ed_disk
 instance-000007ed_disk.eph0
 instance-000007ed_disk.eph1
The instance libvirt.xml after instance spawn has disk device entries like below and the instances happily boots.
 <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph0""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.eph1""/>
      <target bus=""virtio"" dev=""vdc""/>
    </disk>
If nova stop/start or nova reboot --hard is executed the instance is destroyed and libvirt.xml gets recreated.  At this stage whatever values we passed with --ephemeral are not respected and libvirt.xml revirts to configuration that would have been generated without the use of the --ephemeral option like below where we only have one extra disk and it is not using the enumerated naming.
  <devices>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk""/>
      <target bus=""virtio"" dev=""vda""/>
    </disk>
    <disk type=""block"" device=""disk"">
      <driver name=""qemu"" type=""raw"" cache=""none""/>
      <source dev=""/dev/vgephemeral/instance-000007ed_disk.local""/>
      <target bus=""virtio"" dev=""vdb""/>
    </disk>
This causes instances booting to fail at this stage.  The nova block_device_mapping table has records for all 3 devices.","Update block_device_info to contain swap and ephemeral disks

An ephemeral or swap disk is attached to an instance on boot
as follows: nova boot --flavor FLAVOR --image IMAGE_ID
--swap 512 --ephemeral size=2 INSTANCE.  When a hard reboot is
performed on the instance, nova fails to recreate the
appropriate libvirt XML definition, containing the ephemeral
disk.  This is because the correct block_device_info dict that
is passed to the compute manager's reboot_instance method does
not contain swap or ephemeral disk key values that are necessary
to recreate those disks.  In addition to nova boot, the correct
block_device_info dict is also needed by nova rebuild, reboot,
resize, and migrate to recreate those disks.  This patch updates
_get_instance_volume_block_device_info (renamed
_get_instance_block_device_info) to return the swap and ephemeral
disk key values, in addition to the block_device_mapping it
already returns today.

Change-Id: Iec329d1c12a48ea90ba9d57decd0996fde6544f0
Closes-Bug: #1305423"
1760,f0921a4bbad5946abb6d9679867928a970cf5820,1387568767,,1.0,36,20,2,2,1,0.924133542,True,7.0,7965833.0,84.0,24.0,False,24.0,678818.0,33.0,5.0,64.0,1414.0,1419.0,64.0,1304.0,1309.0,59.0,580.0,584.0,0.092735703,0.897990726,0.904173107,214,617,1263217,neutron,f0921a4bbad5946abb6d9679867928a970cf5820,0,0, Remove unnecessary call,"Bug #1263217 in neutron: ""Unnecessary call to get_dhcp_port from DeviceManager setup""","In the file neutron/agent/linux/dhcp.py, the DeviceManager setup method calls get_device which calls get_dhcp_port.  This results in an RPC call.  But, we already had the port in the setup method.
I discovered this as I was trying to optimize the number of these RPC calls.","Remove unnecessary call to get_dhcp_port from DeviceManager

Change-Id: I35459c352e712ea009589f3050205a4c307b909a
Closes-Bug: #1263217"
1761,f0f87ca56bb76ebad94b6f01e04b475bd9362bdf,1380716016,1.0,1.0,130,42,16,12,1,0.900131107,True,4.0,1131772.0,21.0,10.0,False,30.0,710621.125,46.0,4.0,756.0,433.0,1075.0,717.0,348.0,957.0,302.0,97.0,333.0,0.737226277,0.238442822,0.812652068,1553,1224967,1224967,neutron,f0f87ca56bb76ebad94b6f01e04b475bd9362bdf,1,1,"“First of all, I think there is a bug in nova” We are in neutro, how this can work","Bug #1224967 in neutron: ""port down after live migration""","I'm using live-migration with devstack and ML2 plugin (the same error occurs with the OVS plugin)
https://wiki.openstack.org/wiki/Devstack/LiveMigration
First of all, I think there is a bug in nova :
https://bugs.launchpad.net/nova/+bug/1224960
I've proposed a patch attached to this bug to resolv it quickly.
it seems that there is also something that goes wrong in neutron.
after live-migrating a VM, the port is correctly created on the new host and the dataplane seems correct. But the port is still down in the port table, and the vif-type is ""binding_failed"" in ml2_port_bindings table","change port status only if port is bound to the good host

if host is set in the rpc message update_device_up/down sent by the agent,
the port status will be changed only if the port is bound to the host.

Change-Id: I0e607c734fbebf0b69f83c3bbd3e25a9783672dc
Closes-Bug: #1224967"
1762,f10f67dcc82dc7fda5aaf546b145dad34db82f7b,1378749004,,1.0,5,5,2,2,1,0.881290899,True,3.0,6696709.0,30.0,22.0,False,132.0,109240.0,520.0,13.0,1611.0,3019.0,3020.0,1414.0,2687.0,2688.0,1520.0,2817.0,2817.0,0.254731201,0.471947747,0.471947747,1442,1198796,1198796,nova,f10f67dcc82dc7fda5aaf546b145dad34db82f7b,1,1, “The method nova.api.openstack.compute.contrib.cells_filter_keys() uses the 'Query' object as a dict”,"Bug #1198796 in OpenStack Compute (nova): ""update cell error""","I updated the cell info and the response is as follows:
{
    ""computeFault"": {
        ""message"": ""The server has either erred or is incapable of performing the requested operation."",
        ""code"": 500
    }
}
I found error in nova-cells.log:
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack Traceback (most recent call last):
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/__init__.py"", line 81, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return req.get_response(self.application)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1296, in send
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     application, catch_exc_info=False)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1260, in call_application
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     app_iter = application(self.environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/keystoneclient/middleware/auth_token.py"", line 450, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return self.app(env, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/routes/middleware.py"", line 131, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     response = self.app(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 144, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return resp(environ, start_response)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     resp = self.call_func(req, *args, **self.kwargs)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return self.func(req, *args, **kwargs)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 890, in __call__
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     content_type, body, accept)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 942, in _process_stack
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     action_result = self.dispatch(meth, request, action_args)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/wsgi.py"", line 1022, in dispatch
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return method(req=request, **action_args)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 257, in update
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return dict(cell=_scrub_cell(cell))
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 118, in _scrub_cell
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     cell_info = _filter_keys(cell, keys)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack   File ""/usr/lib/python2.7/dist-packages/nova/api/openstack/compute/contrib/cells.py"", line 110, in _filter_keys
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack     return dict((k, v) for k, v in item.iteritems() if k in keys)
2013-07-07 23:55:20.777 31477 TRACE nova.api.openstack AttributeError: 'Query' object has no attribute 'iteritems'
I viewed the code and found that:
@require_admin_context
def cell_update(context, cell_name, values):
    session = get_session()
    with session.begin():
        cell = _cell_get_by_name_query(context, cell_name, session=session)
        cell.update(values)
    return cell
The method nova.db.sqlalchemy.api.cell_update() returns a 'Query' object.
def _filter_keys(item, keys):
    """"""
    Filters all model attributes except for keys
    item is a dict
    """"""
    return dict((k, v) for k, v in item.iteritems() if k in keys)
The method nova.api.openstack.compute.contrib.cells_filter_keys() uses the 'Query' object as a dict.
This bring on the error.","Don't return query from db API

The cell_update() method of the sqlalchemy db API returned a Query when
it really intended to return a Cell.  Fix this and tweak the db API test
so that it verifies that the result of this method is a Cell.

Change-Id: I7ab7ea19f00ac2e8c2c73afe8a14f331940b4580
Closes-bug: #1198796"
1763,f1b76971ba9f670b0ee916625318d65f18c3c4aa,1392105224,,1.0,19,1,2,2,1,0.721928095,True,8.0,6290221.0,94.0,23.0,False,68.0,1314936.0,215.0,4.0,19.0,4668.0,4676.0,19.0,3724.0,3732.0,16.0,3660.0,3666.0,0.002318287,0.499249966,0.500068185,388,801,1278696,nova,f1b76971ba9f670b0ee916625318d65f18c3c4aa,1,1,wrong msg,"Bug #1278696 in OpenStack Compute (nova): ""'DescribeInstances' in ec2 shows wrong image-message to instance booting from volumes""","'DescribeInstances' in ec2 shows the wrong image message('ami-00000002') to the instance booting from a volume:
n781dba539a84:~ # euca-describe-instances i-0000003a
RESERVATION     r-mdcwadip      c1c092e1c88f4027aeb203d50d63135b
INSTANCE        i-0000003a      ami-00000002            wjvm1   running None (c1c092e1c88f4027aeb203d50d63135b, n781dba57c996)  0               m1.small        2014-02-10T07:00:40.000Z        nova          20.20.16.12                      ebs
BLOCKDEVICE     /dev/vda        vol-0000000d            false
More info can be found here: http://paste.openstack.org/show/64087/
--------------
The codes in ec2utils.py:
def glance_id_to_id(context, glance_id):
    """"""Convert a glance id to an internal (db) id.""""""
    if glance_id is None:
        return
    try:
        return db.s3_image_get_by_uuid(context, glance_id)['id']
    except exception.NotFound:
        return db.s3_image_create(context, glance_id)['id']
------
The image_ref (as glance_id in this function) of the instance booting from a volume, will be """", not None.
The protected codes above can't take efforts.","DescribeInstances in ec2 shows wrong image-message

If one instance boots from a volume,
the value of 'image_id' by 'DescribeInstances' in ec2,
will be a wrong number.

The value of 'image_ref' for this type of instances, is '', not None.
So the original protected codes can't take efforts.

Change-Id: I8c5e1c626f2b7873e7720f3615db1ee1ec6c681a
Closes-Bug: #1278696"
1764,f1d30c9bc4ae626883d827d3b0a0458aa9f70820,1389901453,,1.0,3,2,1,1,1,0.0,True,2.0,648621.0,19.0,6.0,False,41.0,1955229.0,108.0,5.0,182.0,1433.0,1451.0,179.0,1256.0,1274.0,172.0,1271.0,1284.0,0.129394166,0.951383695,0.961106956,295,703,1269633,cinder,f1d30c9bc4ae626883d827d3b0a0458aa9f70820,0,0,missing method,"Bug #1269633 in Cinder: ""FibreChannelDriver is missing accept_transfer""",The base FibreChannelDriver is missing the new accept_transfer method.,"Added missing accept_transfer to FC

The base class FibreChannelDriver was
missing the new accept_transfer method.
This was caught while running the devstack
driver certification tests again the
3PAR FC driver.
This patch adds the missing method to the
base VolumeDriver.

Change-Id: I0aa91a4f11edd03eee7607d15317b89c30732cb7
Closes-Bug: #1269633"
1765,f1d91b2456353b92bda7017422aaa97d6720d7a9,1379938440,1.0,1.0,32,3,4,2,1,0.801217527,True,1.0,43616.0,7.0,3.0,False,5.0,2086757.25,12.0,3.0,118.0,812.0,848.0,118.0,764.0,800.0,77.0,307.0,320.0,0.209115282,0.825737265,0.860589812,1586,1229082,1229082,neutron,f1d91b2456353b92bda7017422aaa97d6720d7a9,1,1,“Recently merged patch - https://review.openstack.org/#/c/42090 - adds an ability to update status of a member according to health statistics”,"Bug #1229082 in neutron: ""LBaaS inactive members are not passed to agent on pool reload""","Recently merged patch - https://review.openstack.org/#/c/42090 - adds an ability to update status of a member according to health statistics coming from lbaas agent so that if a member stops responding it is marked as INACTIVE. Such members however are not passed to the agent on next reload_pool operation (pool update, new member, new monitor, etc) so remain INACTIVE even if corresponding instance is repaired.","LBaaS: include inactive members when retrieving logical config

Closes-Bug: #1229082
Change-Id: I16f452b51a98d912ba8bdf374b4d1e9285d757dc"
1766,f1df39fe7be45c9d789f7b889de35328158079c4,1385373308,,1.0,1,1,1,1,1,0.0,True,2.0,93469.0,10.0,3.0,False,16.0,7160494.0,30.0,3.0,14.0,652.0,657.0,14.0,646.0,651.0,9.0,599.0,599.0,0.0084246,0.50547599,0.50547599,1806,1254635,1254635,cinder,f1df39fe7be45c9d789f7b889de35328158079c4,0,0," Bug in the comments , “Fix docstring”","Bug #1254635 in Cinder: ""Fix docstring for Snapshot model.""","""Represents a block storage device that can be attached to a VM."" as the description of model Snapshot is incorrect.","Fix docstring for Snapshot model

""Represents a block storage device that can be attached to a VM."" as the
description of model Snapshot is incorrect.

Change-Id: I1d7c985869f0829b9305a18a3570928a9da40519
Closes-Bug: #1254635"
1767,f23d081b26071e1b309f49e5a5ab0cdc1a739e9d,1394744783,,1.0,11,5,3,2,1,0.787120357,True,3.0,2881096.0,85.0,23.0,False,14.0,556954.0,19.0,9.0,0.0,1900.0,1900.0,0.0,1613.0,1613.0,0.0,897.0,897.0,0.001043841,0.93736952,0.93736952,609,1034,1292114,neutron,f23d081b26071e1b309f49e5a5ab0cdc1a739e9d,1,1,Mixing cisco plugin when it is working,"Bug #1292114 in neutron: ""cisco plugin missing from few migration files causing missing db tables""","cisco plugin is missing from a few migration files causing missing DB tables. The following files in particular:
versions/folsom_initial.py
versions/176a85fc7d79_add_portbindings_db.py","Include cisco plugin in migration plugins with ovs

Currently we have many migration files with missing cisco
plugin in migrate_plugin when ovs is included.This
causes missing tables when cisco plugin is enabled
and migration is run. This fix should automatically
include the cisco plugin if ovs is detected in the
migrate_plugins.

Change-Id: I4dedfbafe9b431e85255d5427766e22eed09ee5e
Closes-Bug: #1292114"
1768,f274816f073cfe9d0071a99159309fee643dbdb8,1413207949,,1.0,16,1,2,2,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1399,1869,1380624,nova,f274816f073cfe9d0071a99159309fee643dbdb8,1,1,,"Bug #1380624 in OpenStack Compute (nova): ""VMware: booting from a volume does not configure config driver if necessary""",When booting from a volume the config driver will not be mounted (if configured),"VMware: attach config drive if booting from a volume

Ensure that the config drive is configured when booting from a
volume

Change-Id: Ia05021ff076c4546af181287a9ff6ae15ffb4857
Closes-bug: #1380624"
1769,f27e5244adcac9a12c04605fbf5c49a086d5fce5,1402934475,,1.0,6,4,2,2,1,1.0,False,,,,,True,,,,,,,,,,,,,,,,,886,1322,1319361,neutron,f27e5244adcac9a12c04605fbf5c49a086d5fce5,1,1,,"Bug #1319361 in neutron: ""Lbaas: Active_connections and Total_Connection in lbaas always show 0 for lb-pool-stat""","Steps to Reproduce:
1. Create network, subnet
2.Create pool,member vip  and healthmonitor.
neutron lb-pool-list
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| id                                   | name  | provider | lb_method   | protocol | admin_state_up | status |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
| d488b38e-c8f1-4db6-bb5f-8b2500bef0ed | pool1 | haproxy  | ROUND_ROBIN | HTTP     | True           | ACTIVE |
+--------------------------------------+-------+----------+-------------+----------+----------------+--------+
neutron lb-member-list
+--------------------------------------+-----------+---------------+----------------+--------+
| id                                   | address   | protocol_port | admin_state_up | status |
+--------------------------------------+-----------+---------------+----------------+--------+
| 303f3d66-c51c-4c92-b5da-d43440db9fb2 | 10.10.1.5 |            80 | True           | ACTIVE |
| ac58985b-49de-480e-97e5-bff80a538c87 | 10.10.1.4 |            80 | True           | ACTIVE |
+--------------------------------------+-----------+---------------+----------------+--------+
4. Validate the algorithm and try to send request from server to client .
5. Check the lp-pool-stats for the pool created.
Actual Results:
neutron lb-pool-stats pool1
+--------------------+-------+
| Field              | Value |
+--------------------+-------+
| active_connections | 0     |
| bytes_in           | 1440  |
| bytes_out          | 560   |
| total_connections  | 0     |
+--------------------+-------+
Connections in response for this command always show 0
Expected Results: Active connection should show the value when the connection is active and total connenction should show total number of active connections","Fix pool statistics for LBaaS Haproxy driver

Total connections was mapped to none of the counters
of haproxy. Active connections was mapped to active request
counter(qcur)) of haproxy which in low load situations
always remains zero.

Patch maps connections API counters to haproxy session counters
and fixes unit test accordingly.

Change-Id: I0ef4f76a75340232eb11a9b95453769ecd89d13a
Closes-Bug: #1319361"
1770,f2a6e77bc75ea72a374ded4dba82cf2e95c11404,1399930068,,1.0,10,1,2,1,1,0.845350937,True,2.0,604183.0,13.0,2.0,True,,,,,,,,,,,,,,,,,868,1301,1316824,cinder,f2a6e77bc75ea72a374ded4dba82cf2e95c11404,0,0,Bug in test,"Bug #1316824 in Cinder: ""remove hplefthandclient requirement from unit tests""","In order to eliminate the need to have the hplefthandclient in the global-requirements project, we need to remove the hplefthandclient from being imported in all hplefthand driver unit tests in cinder.
It should _at least_ be optional for the tests.","eliminate the need for hplefthandclient in tests

In order to eliminate the need to have the hplefthandclient in the
global-requirements project, we need to remove the hplefthandclient
from being imported in all HP LeftHand driver unit tests in cinder.

Change-Id: I4009ea17b507b1deb2a567dd420f309e15aaf92d
Closes-Bug: #1316824"
1771,f2f58eef93aec45e46a9a2ab06fc8b00a9420350,1381262067,,1.0,51,60,6,3,1,0.621824847,True,5.0,619602.0,35.0,20.0,False,174.0,104448.6667,1011.0,5.0,838.0,3345.0,3791.0,822.0,2815.0,3251.0,819.0,2460.0,2891.0,0.131305044,0.39407526,0.463090472,1613,1232179,1232179,nova,f2f58eef93aec45e46a9a2ab06fc8b00a9420350,1,0,“based on evidence it looks like caller passing information with key metadata (change back in August3)”,"Bug #1232179 in OpenStack Compute (nova): ""Aggregate metadata is not correctly handled by compute""","Hopefully this is not covered under a different bug.  What I'm seeing is that compute does not properly handle metadata in the ""aggregate"" class of API commands.  Specifically, after adding metadata and performing an add-host you get the following stacktrace exception:
2013-09-27 17:48:44.396 DEBUG nova.openstack.common.rpc.amqp [-] received {u'_context_roles': [u'admin'], u'_context_request_id': u'req-643da000-e5ae-482c-ba01-027ee61f5085', u'_context_quota_class': None, u'_context_user_name': u'admin', u'_context_project_name': u'admin', u'_context_service_catalog': [{u'endpoints_links': [], u'endpoints': [{u'adminURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'region': u'RegionOne', u'publicURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'internalURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'id': u'1733df26b558409ca93bb1ffe8851929'}], u'type': u'volume', u'name': u'cinder'}], u'_context_tenant': u'0fa0013cceb4430ba09e88a18d344537', u'_context_auth_token': '<SANITIZED>', u'args': {u'aggregate': {u'name': u'foo', u'availability_zone': u'nova', u'deleted': False, u'created_at': u'2013-09-27T17:47:24.000000', u'updated_at': None, u'hosts': [u'devstack2'], u'deleted_at': None, u'id': 2, u'metadata': {u'hypervisor': u'true', u'availability_zone': u'nova'}}, u'host': u'devstack2', u'slave_info': None}, u'namespace': None, u'_context_instance_lock_checked': False, u'_context_timestamp': u'2013-09-27T17:48:44.352804', u'_context_is_admin': True, u'version': u'2.14', u'_context_project_id': u'0fa0013cceb4430ba09e88a18d344537', u'_context_user': u'bd2e35f5cdfa4272b7f03d601c95a61d', u'_unique_id': u'13c5f2d81c484bbebdd8214574060ab3', u'_context_read_deleted': u'no', u'_context_user_id': u'bd2e35f5cdfa4272b7f03d601c95a61d', u'method': u'add_aggregate_host', u'_context_remote_address': u'172.24.4.10'} from (pid=10210) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
013-09-27 17:48:44.397 DEBUG nova.openstack.common.rpc.amqp [-] unpacked context: {'read_deleted': u'no', 'project_name': u'admin', 'user_id': u'bd2e35f5cdfa4272b7f03d601c95a61d', 'roles': [u'admin'], 'timestamp': u'2013-09-27T17:48:44.352804', 'auth_token': '<SANITIZED>', 'remote_address': u'172.24.4.10', 'quota_class': None, 'is_admin': True, 'user': u'bd2e35f5cdfa4272b7f03d601c95a61d', 'service_catalog': [{u'endpoints': [{u'adminURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'region': u'RegionOne', u'id': u'1733df26b558409ca93bb1ffe8851929', u'internalURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537', u'publicURL': u'http://172.24.4.10:8776/v1/0fa0013cceb4430ba09e88a18d344537'}], u'endpoints_links': [], u'type': u'volume', u'name': u'cinder'}], 'request_id': u'req-643da000-e5ae-482c-ba01-027ee61f5085', 'instance_lock_checked': False, 'project_id': u'0fa0013cceb4430ba09e88a18d344537', 'user_name': u'admin', 'tenant': u'0fa0013cceb4430ba09e88a18d344537'} from (pid=10210) _safe_log /opt/stack/nova/nova/openstack/common/rpc/common.py:277
2013-09-27 17:48:44.400 ERROR nova.openstack.common.rpc.amqp [req-643da000-e5ae-482c-ba01-027ee61f5085 admin admin] Exception during message handling
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 461, in _process_data
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     **args)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     result = getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 90, in wrapped
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     payload)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/exception.py"", line 73, in wrapped
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     return f(self, context, *args, **kw)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/compute/manager.py"", line 4993, in add_aggregate_host
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     slave_info=slave_info)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/xenapi/driver.py"", line 628, in add_to_aggregate
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     return self._pool.add_to_aggregate(context, aggregate, host, **kwargs)
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/virt/xenapi/pool.py"", line 77, in add_to_aggregate
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp     if not pool_states.is_hv_pool(aggregate['metadetails']):
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp KeyError: 'metadetails'
2013-09-27 17:48:44.400 TRACE nova.openstack.common.rpc.amqp
Based on evidence it looks like caller passing information with key metadata (change back in August?) while we are looking for it in the Xen code as metadetails.  The key name needs to change to correspond.
This was produced on a devstack with a commit of ab5a99bbca4d68002c887b5dd7b3741b57f650ee for nova.","Make XenAPI use Aggregate object

This makes the XenAPI driver use the Aggregate object for its work,
and avoids the need to call back through virtapi to conductor
directly. It also allows us to convert the two aggregate-related
compute manager methods fully to new-world objects.

Related to blueprint compute-manager-objects
Related to blueprint virt-objects

Closes-bug: #1232179
Change-Id: Ib38ab0e4d6feefebda37888f150752167474b693"
1772,f31ac7711bacb1408b17759426f3407937ba4d79,1408384200,,1.0,70,3,6,6,1,0.81611108,False,,,,,True,,,,,,,,,,,,,,,,,1210,1668,1358380,nova,f31ac7711bacb1408b17759426f3407937ba4d79,1,0,“This goes way back to grizzly and commit 718a3f057cee0b1163c40fbcbedda29bd2ef9dfe”,"Bug #1358380 in OpenStack Compute (nova): ""rebuild API doesn't handle OnsetFile*LimitExceeded quota errors""","I noticed this while reviewing https://review.openstack.org/#/c/102103/ for bug 1298131, the 3 OnsetFile*LimitExceeded exceptions from nova.compute.api.API._check_injected_file_quota are not handled in the os-compute rebuild APIs (v2 or v3), and I'm not even seeing specific unit testing for those exceptions in the _check_injected_file_quota method.","Add QuotaError handling to servers rebuild API

Commit 718a3f057cee0b1163c40fbcbedda29bd2ef9dfe made
nova.compute.api.API._check_injected_file_quota raise more specific
over-quota exceptions but the rebuild API was never updated to handle
the QuotaError exceptions and translate to a proper HTTP error.

This change does a few things:

1. Makes the specific file path/content limit exceeded exceptions
   extend the more generic OnsetFileLimitExceeded exception.

2. Adds the OverQuota checking to the rebuild APIs with tests.

3. Adds unit tests for the three different exceptions raised from
   _check_injected_file_quota in the compute API since those did not
   exist before.

Closes-Bug: #1358380

Change-Id: I9c72dea6075fcf554abb8e669cf4dd3129176912"
1773,f330eac22ad47901ccd3252ffd73c46d7157644e,1381217305,,1.0,0,13,2,1,1,0.995727452,True,1.0,61150.0,7.0,3.0,False,14.0,2390519.0,26.0,3.0,13.0,1081.0,1086.0,13.0,953.0,958.0,10.0,929.0,931.0,0.010242086,0.865921788,0.867783985,1648,1236723,1236723,cinder,f330eac22ad47901ccd3252ffd73c46d7157644e,0,0,Refactoring “remove unused methods in driver.Scheduler”,"Bug #1236723 in Cinder: ""HostManager doesn't have get_host_list() method""","cinder.scheduler.host_manager.HostManager doesn't have get_host_list()
and get_service_capabilities() method.
But I look like which cinder.scheduler.driver.Scheduler is trying to call them.
Are they not being used? I think they can be deleted.","remove unused methods in driver.Scheduler

cinder.scheduler.host_manager.HostManager doesn't have get_host_list()
and get_service_capabilities(). But we don't catch the AttributeError
by them. Therefore, get_host_list() and get_service_capabilities() in
cinder.scheduler.driver.Scheduler have not been used.

Also, get_host_list() and get_service_capabilities() in
cinder.scheduler.manager.SchedulerManager have not been used.

Change-Id: I402cf068612866649cc8645391eff38fca56f634
Closes-Bug: #1236723"
1774,f33a25a3c40722644c774395b38fd7a7ed0246e1,1402615627,,1.0,8,1,2,2,1,0.503258335,False,,,,,True,,,,,,,,,,,,,,,,,973,1412,1329559,nova,f33a25a3c40722644c774395b38fd7a7ed0246e1,1,1,"“This was intentionally done in
https://review.openstack.org/#/c/58829/ . We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(https://review.openstack.org/#/c/55444/).""","Bug #1329559 in OpenStack Compute (nova): ""Cannot delete an instance that failed a previous delete""","Currently we have a situation where if an instance fails to delete,
instead of having its state reverted, like we do in most places we set
it to error,deleting. This was intentionally done in
https://review.openstack.org/#/c/58829/ . We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(https://review.openstack.org/#/c/55444/). The combination of these two
things means that if an instance fails to delete for some reason a
tenant is unable to delete that instance.
It turns out this is really bad because instances in deleting state
count against quota, so the tenant slowly looses usable quota.
To fix this, allow duplicate delete calls to go through if the instance
is in error state.","Failure during termination should always leave state as error()

Currently we have a situation where if an instance fails to delete,
instead of having its state reverted, like we do in most places we set
it to error,deleting. This was intentionally done in
I5fb1bbd56035792f566a6e076edfe7a69df006ef. We also intentionally ignore
duplicate requests to delete an instance if its already being deleted
(I2f97f93bd714e0ea3b6d4fa3ac457ab43eed00e1). The combination of these two
things means that if an instance fails to delete for some reason a
tenant is unable to delete that instance.

It turns out this is really bad because instances in deleting state
count against quota, so the tenant slowly looses usable quota.

To fix this, upon a failed termination set the vm_state to error and
revert the task_state. This is a partial revert of
I55742203bdd071c7df90902868e46c2020f799bd.

Change-Id: I55742203bdd071c7df90902868e46c2020f799bd
Closes-Bug: #1329559"
1775,f35c63df24d8f47927d055729e9ca08c7bc3fe8c,1383998917,,1.0,76,53,4,2,1,0.858771995,True,12.0,9337980.0,97.0,36.0,False,187.0,233474.5,1032.0,2.0,330.0,369.0,369.0,318.0,357.0,357.0,323.0,362.0,362.0,0.049458098,0.055411388,0.055411388,1766,1251602,1251602,nova,f35c63df24d8f47927d055729e9ca08c7bc3fe8c,1,1,“missing exception info”,"Bug #1251602 in OpenStack Compute (nova): ""Some error notifications are missing exception info""","At the moment we are quite inconsistent with some of the error notifications.
Those triggered by the wrappers of compute manager calls are good. However:
* where an instance fault is registered inline, we don't always send an error notification
* where we send an error notification inline, we generally don't send the message and exception information in a consistent way
I am looking to resolve that, so there is more consistency when raising error notifications in the compute manager.","Make error notifications more consistent.

The error notifications sent by wrap_exception are a good source of
information when tracking down errors in the system. It is easy to spot
patterns in the errors by looking at similarities in the exceptions
raised.

This change looks at making the in-line error notifications
follow the same format, so they become equally useful.

Closes-Bug: #1251602
Change-Id: I1cd64a90ecefc64d12af05487eb6e45aa0362d69"
1776,f37905a3c1fd09597898c93a1cbc3050f335cf61,1403562680,,1.0,6,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1685,1239864,1239864,nova,f37905a3c1fd09597898c93a1cbc3050f335cf61,1,1,“Service groups using zookeeper don't work due to apparently improper handling of the zookeeper session creation.”,"Bug #1239864 in OpenStack Compute (nova): ""nova-api fails to query ServiceGroup status from Zookeeper""","I am running with the ZooKeeper servicegroup driver on CentOS 6.4 (Python 2.6) with the RDO distro of Grizzly.
All nova services are successfully connecting to ZooKeeper, which I've verified using zkCli.
However, when I run `nova service-list` I get an HTTP 500 error from nova-api.  The nova-api log (/var/log/nova/api.log) shows:
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/api.py""\
, line 93, in service_is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     return self._driver.is_up(member)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 116, in is_up
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     all_members = self.get_all(group_id)
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack   File ""/usr/lib/python2.6/site-packages/nova/servicegroup/drivers\
/zk.py"", line 141, in get_all
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack     raise exception.ServiceGroupUnavailable(driver=""ZooKeeperDrive\
r"")
2013-10-14 16:33:15.110 6748 TRACE nova.api.openstack ServiceGroupUnavailable: The service from servicegroup driver ZooK\
eeperDriver is temporarily unavailable.
The problem seems to be around evzookeeper (using version 0.4.0).
To isolate the problem, I added some evzookeeper.ZKSession synchronous get() calls to test the roundtrip communication to ZooKeeper.  When I do a `self._session.get(CONF.zookeeper.sg_prefix)` in the zk.py ZooKeeperDriver __init__() method it works fine.  The logs show that this is immediately before the wsgi server starts up.
When I do the get() operation from within the ZooKeeperDriver get_all() method, the web request hangs indefinitely.  However, if I recreate the evzookeeper.ZKSession within the get_all() method (after the wsgi server has started) the nova-api request is successful.
diff --git a/nova/servicegroup/drivers/zk.py b/nova/servicegroup/drivers/zk.py
index 2a3edae..7de2488 100644
--- a/nova/servicegroup/drivers/zk.py
+++ b/nova/servicegroup/drivers/zk.py
@@ -122,7 +122,14 @@ class ZooKeeperDriver(api.ServiceGroupDriver):
         monitor = self._monitors.get(group_id, None)
         if monitor is None:
             path = ""%s/%s"" % (CONF.zookeeper.sg_prefix, group_id)
-            monitor = membership.MembershipMonitor(self._session, path)
+
+            null = open(os.devnull, ""w"")
+            local_session = evzookeeper.ZKSession(CONF.zookeeper.address,
+                                                  recv_timeout=
+                                                    CONF.zookeeper.recv_timeout,
+                                                  zklog_fd=null)
+
+            monitor = membership.MembershipMonitor(local_session, path)
             self._monitors[group_id] = monitor
             # Note(maoy): When initialized for the first time, it takes a
             # while to retrieve all members from zookeeper. To prevent","Fix service groups with zookeeper

Service groups using zookeeper don't work due to apparently improper
handling of the zookeeper session creation.

No additional unit tests as the mocking these interfaces probably
doesn't actually provide substantial future guaruntees. The correct
long term fix is to enable zk for unit testing upstream.

Change-Id: I21691843cb4936d10a0b82df41aef3afe5bf2519
Closes-bug: #1239864"
1777,f3f46b532c528c31fbcdc0d18b18105c42a0d74c,1393891792,,1.0,66,2,4,4,1,0.912128172,True,5.0,1218006.0,48.0,17.0,False,24.0,2287326.0,40.0,4.0,23.0,2816.0,2822.0,23.0,2368.0,2374.0,18.0,2697.0,2698.0,0.002533333,0.359733333,0.359866667,512,932,1287367,nova,f3f46b532c528c31fbcdc0d18b18105c42a0d74c,1,1,Raising the improper code,"Bug #1287367 in OpenStack Compute (nova): ""The rescue API should handle NotImplementedError""","There are several nova virt drivers that don't implement the rescue API, but the os compute API doesn't handle NotImplementedError, it returns a 400 instead of a 501.
https://wiki.openstack.org/wiki/HypervisorSupportMatrix
The API could be tightened up a bit to return a 501 instead like how the pause admin action is handling NotImplementedError.","Rescue API handle NotImplementedError

There are several nova virt drivers that don't implement the rescue API,
but the os compute API doesn't handle NotImplementedError,
it returns a 400 instead of a 501.

The patch add the proper logic to return a 501 instead.

Change-Id: Ia649c4dadd50985efed631ce8f3e4b212646766e
Closes-Bug: #1287367"
1778,f45c1c52423dcbb2ea7690d56b3edc232d16636e,1394896583,,1.0,133,55,5,2,1,0.862155864,True,3.0,518231.0,46.0,5.0,False,16.0,306173.0,40.0,2.0,503.0,1464.0,1683.0,408.0,1191.0,1356.0,246.0,755.0,833.0,0.25385406,0.776978417,0.857142857,666,1092,1295754,neutron,f45c1c52423dcbb2ea7690d56b3edc232d16636e,1,1,"Bug , cannot delete","Bug #1295754 in neutron: ""nec plugin: Cannot delete ERROR resources which don't exist on OpenFlow controller""","In NEC Plugin, if a resource is in ERROR status and there is no corresponding resource on OpenFlow controller, the resource     cannot be deleted by an API request. Unless critical reasons that resources cannot be deleted, resources should be able to be deleted from API.","nec plugin: allow to delete resource with ERROR status

Previously if a resource is in ERROR status and there is no
corresponding resource on OpenFlow controller, the resource
cannot be deleted through an API request.
This commit rearrange ERROR status check to allow resource
with ERROR status to be deleted.

Closes-Bug: #1295754
Change-Id: I709f5e2066eb5d12ec0f42dff15797acddc2009e"
1779,f4646b74532d3e75a105290e8333bff92eb838ad,1403096607,,1.0,14,9,3,2,1,0.940725014,True,16.0,16109218.0,119.0,27.0,False,74.0,3359494.0,237.0,2.0,243.0,1573.0,1731.0,233.0,1527.0,1676.0,225.0,1556.0,1697.0,0.028242939,0.194576356,0.212196951,1771,1251943,1251943,nova,f4646b74532d3e75a105290e8333bff92eb838ad,1,1, ,"Bug #1251943 in OpenStack Compute (nova): ""xenserver disable/enable host report error""","When XenServer disable/enable host, the api set_host_enabled() need to get service info for the target host, but the rpc call of set_host_enabled() did not transfer host as parameter, this will cause the api call failed.
def set_host_enabled(self, host, enabled):
        """"""Sets the specified host's ability to accept new instances.""""""
        # Since capabilities are gone, use service table to disable a node
        # in scheduler
        status = {'disabled': not enabled,
                'disabled_reason': 'set by xenapi host_state'
                }
        cntxt = context.get_admin_context()
        service = self._conductor_api.service_get_by_args(
                cntxt,
                host, <<<<<<<<<
                'nova-compute')
        self._conductor_api.service_update(
                cntxt,
                service,
                status)
        args = {""enabled"": jsonutils.dumps(enabled)}
        response = call_xenhost(self._session, ""set_host_enabled"", args)
        return response.get(""status"", response)
======================================
    def set_host_enabled(self, ctxt, enabled, host):
        cctxt = self.client.prepare(server=host)
        return cctxt.call(ctxt, 'set_host_enabled', enabled=enabled) <<<<<<<<<< No host","XenAPI: disable/enable host will be failed when using XenServer

When disable/enable host with XenServer, the api of set_host_enabled()
in XenServer driver needs to get service info for the target host, but
the rpc call of set_host_enabled() did not transfer target host as
parameter, this will cause the api call failed.

This patch was let the API of set_host_enabled() use CONF.host.

Closes-Bug: #1251943

Change-Id: I2b6427df80ee3188d6f7c4eeb4d2e7be300f0126"
1780,f4b78c7f17e29448ed54b136eeb4ac700b324120,1380278640,2.0,1.0,71,17,3,2,1,0.763995628,True,4.0,1141440.0,26.0,13.0,False,5.0,165167.3333,10.0,3.0,738.0,1237.0,1237.0,702.0,1127.0,1127.0,290.0,321.0,321.0,0.742346939,0.821428571,0.821428571,1608,1231913,1231913,neutron,f4b78c7f17e29448ed54b136eeb4ac700b324120,1,1, ,"Bug #1231913 in neutron: ""Floating IP is not removed when VM is terminated in midonet plugin""","In Midonet plugin, when a VM terminates, floating IP does not get diassociated properly.  This left the floating IP associated with a terminated instance and required you to explicitly diassociate in a separate step.  This also left MidoNet resources dangling around when the VM terminates.","Disassociate floating IPs from port on terminate

Bugfix - floating IPs were left associated after VM
was terminated. Now call disassociate_floatingips
within delete_port as in other networking plugins.

Add L3NatDBIntTestCase suite to cover the
floating IP disassociation case, and fix all failing
tests from that suite.

Change-Id: I856c46631e495d513065fc9e987898408441a21e
Closes-Bug: #1231913"
1781,f4dc301efda13a6a1769b071b1955a4869110b98,1380044966,,1.0,8,10,2,1,1,0.764204507,True,2.0,50214.0,21.0,8.0,False,7.0,84630.0,12.0,3.0,553.0,634.0,1184.0,370.0,614.0,981.0,24.0,591.0,612.0,0.024319066,0.575875486,0.596303502,1595,1229867,1229867,cinder,f4dc301efda13a6a1769b071b1955a4869110b98,1,1,"“This reverts commit d5cd652.""","Bug #1229867 in Cinder: ""TypeError: create_volume() takes at least 6 arguments""","Today... after d5cd6528f361979b073aabd036be0d28dc1c4b95 landed I'm now seeing the following exceptions in /var/log/cinder/scheduler.log:
2013-09-24 17:29:00.771 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 1 seconds
2013-09-24 17:29:01.777 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 2 seconds
2013-09-24 17:29:03.781 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 4 seconds
2013-09-24 17:29:07.787 10117 ERROR cinder.openstack.common.rpc.impl_qpid [req-8753999e-c616-4e58-b626-3f234da3a7a4 None None] Unable to connect to AMQP server: [Errno 111] ECONNREFUSED. Sleeping 8 seconds
2013-09-24 17:31:47.402 10117 ERROR cinder.volume.flows.create_volume [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Failed to schedule_create_volume: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.428 10117 WARNING cinder.taskflow.utils [-] Activating 3 rollbacks due to <cinder.taskflow.utils.FlowFailure object at 0x3102290>.
2013-09-24 17:31:47.429 10117 ERROR cinder.openstack.common.rpc.amqp [req-fe4faf9f-c837-4263-b1f6-ed0e85bc1366 fc85ee7925894313aa61221957a7b6ce 5c97c1ad1ea24177936ea41043b403a5] Exception during message handling
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     **args)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/scheduler/manager.py"", line 94, in create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     flow.run(context)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 105, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(self, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 232, in run
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     run_it(r)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 212, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.rollback(context, cause)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/patterns/linear_flow.py"", line 172, in run_it
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     result = runner(context, *args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/utils.py"", line 260, in __call__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.result = self.task(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 148, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 264, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 199, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 234, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/taskflow/decorators.py"", line 177, in wrapper
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     return f(*args, **kwargs)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1678, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     _error_out_volume(context, db, volume_id, reason=e)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib64/python2.7/contextlib.py"", line 24, in __exit__
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     self.gen.next()
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/volume/flows/create_volume/__init__.py"", line 1663, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     filter_properties)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.7/site-packages/cinder/scheduler/chance.py"", line 81, in schedule_create_volume
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp     image_id=image_id)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp TypeError: create_volume() takes at least 6 arguments (7 given)
2013-09-24 17:31:47.429 10117 TRACE cinder.openstack.common.rpc.amqp","Revert ""Fix volume_rpcapi calls for chance/simple scheds""

This reverts commit d5cd6528f361979b073aabd036be0d28dc1c4b95.

Closes-Bug: #1229867

Change-Id: Iee866ddb08d52642bc36bd6ae82bd0d7283cad8e"
1782,f4fec08e9850dae163f447e72cd1c7f638b2bb10,1403006706,,1.0,62,3,7,2,1,0.832707432,False,,,,,True,,,,,,,,,,,,,,,,,1278,1741,1366506,nova,f4fec08e9850dae163f447e72cd1c7f638b2bb10,1,1,,"Bug #1366506 in OpenStack Compute (nova): ""VMware: administrator is unable to know which VM's belong to openstack""",An administrator can delete VM's running omthe VC without knowing that they belong to OpenStack,"VMware: mark virtual machines as 'belonging' to OpenStack

This patch registers an extension in vCenter that indicates that the
VM's that are run from OpenStack will belong to OpenStack.

If an administrator tries to update the VM via vCenter then they will
get the following message:

""Solution OpenStack manages the selected virtual machine. You should
not modify the virtual machine directly. Use the management console of
the solution to make changes. Do you want to proceed?""

The extension will be called 'org.openstack.compute'. This will make use
of a type called 'instance'.

Closes-bug: #1366506

Change-Id: I1046576c448704841ae8e1800b8390e947b0d457"
1783,f50074c0f65fdeae90dcd4b469fa2ac6f306e4b7,1391607193,0.0,1.0,25,2,2,2,1,0.876716289,True,10.0,1107352.0,65.0,20.0,False,64.0,922170.0,197.0,2.0,0.0,280.0,280.0,0.0,279.0,279.0,0.0,244.0,244.0,0.000875657,0.214535902,0.214535902,363,774,1276142,glance,f50074c0f65fdeae90dcd4b469fa2ac6f306e4b7,1,1,Fix unstable state after image wasnt deleted because an error,"Bug #1276142 in Glance: ""Physical image can not be deleted again if deletion is failed with OSError""","If the deletion of physical image failed with OSError for some reason, it is not possible to delete the image using image-delete api call.
In the longevity test we have encountered this issue, IMO there is some issue with OS and it is throwing ""Forbidden"" exception and somehow its not deleting that file, so in order to reproduce this issue please follow the below mentioned steps:
Steps to reproduce:
1. image creation, upload
    openstack@opencloud1:~$ glance image-create
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | None                                 |
    | container_format | None                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | False                                |
    | deleted_at       | None                                 |
    | disk_format      | None                                 |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | name             | None                                 |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 0                                    |
    | status           | queued                               |
    | updated_at       | 2014-01-29T02:27:07                  |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ glance image-update 675a82ab-4515-451a-932b-6da7f8bce056 --file images/CorePlus4.7.1.qcow2 --disk-format qcow2 --container-format bare
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | a5282e9259f822c782bc7aea8a8870c6     |
    | container_format | bare                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | False                                |
    | deleted_at       | None                                 |
    | disk_format      | qcow2                                |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | name             | None                                 |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 43450368                             |
    | status           | active                               |
    | updated_at       | 2014-01-29T02:27:20                  |
    +------------------+--------------------------------------+
2. write protect the image file
    openstack@opencloud1:~$ sudo chattr +i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
3. delete the image using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
4. Image is logically deleted, but physically remaining
    openstack@opencloud1:~$ glance image-show 675a82ab-4515-451a-932b-6da7f8bce056
    +------------------+--------------------------------------+
    | Property         | Value                                |
    +------------------+--------------------------------------+
    | checksum         | a5282e9259f822c782bc7aea8a8870c6     |
    | container_format | bare                                 |
    | created_at       | 2014-01-29T02:27:07                  |
    | deleted          | True                                 |
    | deleted_at       | 2014-01-29T02:30:49                  |
    | disk_format      | qcow2                                |
    | id               | 675a82ab-4515-451a-932b-6da7f8bce056 |
    | is_public        | False                                |
    | min_disk         | 0                                    |
    | min_ram          | 0                                    |
    | owner            | de4a6631051a4df09bc1b12923244296     |
    | protected        | False                                |
    | size             | 43450368                             |
    | status           | deleted                              |
    | updated_at       | 2014-01-29T02:30:49                  |
    +------------------+--------------------------------------+
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance   43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
5. Remove the write-protect from image file
    openstack@opencloud1:~$ sudo chattr -i /opt/stack/data/glance/images/675a82ab-4515-451a-932b-6da7f8bce056
6. Try to delete the image again using ""glance image-delete""
    openstack@opencloud1:~$ glance image-delete 675a82ab-4515-451a-932b-6da7f8bce056
    Request returned failure status.
    HTTPForbidden (HTTP 403)
7. Image file will not get deleted
    openstack@opencloud1:~$ sudo ls -ls /opt/stack/data/glance/images/
    total 33088
    42432 -rw-r----- 1 glance glance   43450368 Jan 29 11:27 675a82ab-4515-451a-932b-6da7f8bce056
This issue is reproducible only in v1 version. In case of v2 version, it doesn't delete the meta data associated with the image before actually deleting it from the backend store.
In glance, if you enable delayed_delete = True and run glance-scrubber service, glance api will put the image in the queue when the image is deleted by the user and this image will be deleted asynchronously. If it encounters the above reported issue while deleting the image in the glance-scrubber, then this service will simply log error message and keep on retrying deleting the image until it is deleted finally.
Possible solution:
1. When delayed_delete is enabled.
delete_image_metadata method should be moved to the scrubber and called immediately after setting the image status from ""pending_delete"" to ""deleted"".
2. When delayed_delete is not enabled.
delete_image_metadata method should be called after deleting the actual image from the backend.
i.e. after upload_utils.initiate_deletion(req, image['location'], id, CONF.delayed_delete) is called
Also the status of the image should be set back to the previous state in the forbidden exception block.","Delete image metadata after image is deleted

Delete image metadata after the image is deleted successfully from
the backend store.
Set image status to it's original status if the image deletion fails.

Change-Id: I5fee1bace58ff49a8952292250629c1628edbac8
Closes-Bug: #1276142"
1784,f50be86330e81cb9dd61b8a2a635fa1a784b702a,1403897888,,1.0,4,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1020,1463,1335295,cinder,f50be86330e81cb9dd61b8a2a635fa1a784b702a,1,1,"“However, this config option was accidentally dropped in https://review.openstack.org/#/c/67657/20/cinder/service.py

“","Bug #1335295 in Cinder: ""osapi_volume_worker config option was removed""","osapi_volume_worker was introduced when enabling multi-process API support for Cinder.  However, this config option was accidentally dropped in https://review.openstack.org/#/c/67657/20/cinder/service.py","Restore osapi_volume_workers config option

osapi_volume_worker was introduced when enabling multi-process API
support for Cinder. However, this config option was accidentally
dropped in commit 464277220ddb800ac69562649fde62e5ca9358c4
(https://review.openstack.org/#/c/67657/20/cinder/service.py).
As a result, the value in cinder.conf wouldn't be honored.  This
change restore this config option.

Closes-bug: #1335295

Change-Id: Ib2d8532d3e6d0f48da0bc4dd6f6e64bfff8973ea"
1785,f50df8eb3c5bf38abbb0389b85c4b628cffb59f4,1392843198,,1.0,20,4,2,2,1,0.954434003,True,6.0,3410952.0,116.0,25.0,False,31.0,338613.0,50.0,7.0,20.0,803.0,805.0,20.0,723.0,725.0,15.0,473.0,474.0,0.019161677,0.567664671,0.568862275,426,841,1281694,neutron,f50df8eb3c5bf38abbb0389b85c4b628cffb59f4,1,1,,"Bug #1281694 in neutron: ""Delete subnet fails if assoc port has IPs from another subnet""","Perform the following:
- Create a network
- Create two subnets on the network
- Create a port on the network using a fixed IP from one of the subnets
- Delete the other subnet
= = = > FAILURE: Subnet delete fails because SUPPOSEDLY there
are port(s) still associated with that subnet.
Looking at delete_subnet() in neutron/db/db_base_plugin_v2.py,
the check for port(s) still being associated with that _subnet_
is actually checking for port(s) still being associated with
the _network_ (not the subnet), i.e. it's doing a:
    filter_by(network_id=subnet.network_id)
rather than a:
    filter_by(subnet_id=subnet['id'])","Delete subnet fails if assoc port has IPs from another subnet

This change fixes the following failure scenario:
- Create a network
- Create two subnets on the network
- Create a port on the network using one of the subnets
- Delete the other subnet
= = = > FAILURE: Subnet delete fails because supposedly there
is/are port(s) still associated with that subnet.

The problem addressed is that delete_subnet() in
neutron/db/db_base_plugin.py is checking for port(s) still being
associated with the subnet's network, not the subnet itself.

Change-Id: I7adbe18cce412135b2e42dcb7c592e60c1ec5f8f
Closes-Bug: #1281694"
1786,f52a7f181ab8eb7b9e3e780183662fde8755cabd,1390875566,,1.0,29,1,4,4,1,0.954306349,True,4.0,1190030.0,39.0,12.0,False,24.0,85278.0,34.0,4.0,57.0,2165.0,2181.0,57.0,1812.0,1828.0,57.0,2107.0,2123.0,0.008080245,0.293675118,0.295904152,394,807,1279195,nova,f52a7f181ab8eb7b9e3e780183662fde8755cabd,0,0,Catch a new exception,"Bug #1279195 in OpenStack Compute (nova): ""get_spice_console should catch NotImplement exception and handle it""","in v2/v3 API layer
def get_spice_console(self, req, id, body):
didn't catch NotImplement exception and handle it","Catch NotImplementedError in get_spice_console in v2/v3 API

Currently get_spice_console doesn't catch NotImpelementedError
and it will lead to inaccurate information returned back
to API caller. This patch adds the catch exception routine and
returns a HTTPNotImplemented excpetion.

Closes-Bug: #1279195
Change-Id: I16cb0a83c0e1aa9a041dd21af324a39388ca3bb8"
1787,f55e451fb790b976d3fbd3a23dd8688bcc584c77,1398060172,,1.0,23,15,1,1,1,0.0,True,3.0,1400762.0,52.0,13.0,False,8.0,1774865.0,7.0,3.0,8.0,284.0,287.0,8.0,275.0,278.0,8.0,247.0,250.0,0.007758621,0.213793103,0.21637931,835,1264,1312482,neutron,f55e451fb790b976d3fbd3a23dd8688bcc584c77,1,1,,"Bug #1312482 in neutron: ""scalability problem of router routes update""","Updating router routes takes long time when increasing a number of routes.
The critical problem is that it is CPU bound task of neutron-server and neutron-server can not reply for other request.
I show below a measurement example of neutron-server's CPU usage.
Setting routes to a router. (0 to N)
100 routes: 1 sec
1000 routes: 5 sec
10000 routes: 51 sec
I found validation check of parameter  is inefficient. The following example support it too.
No change but just specify same routes to a router. (N to N, DB is not changed)
100 routes: <1 sec
1000 routes: 4 sec
10000 routes: 52 sec
Remove routes from a router. (N to 0)
100 routes: <1 sec
1000 routes:  8 sec
10000 routes: 750 sec
I found handling of record deletion is bad. It takes N**2 order.","Performance improvement of router routes operations

This patch fixes inefficiency of updating extra routes.
* remove the code repeated by every routes in validation check.
* remove searching a record to delete per record.

Note: Unit tests are covered by existing ones. So no unit test added.

Change-Id: Ia335ac21c65148063d330e4533a15a73962c43d8
Closes-bug: #1312482"
1788,f5601393e97baf87daa5b2065cb8783e3fd34214,1394187829,,1.0,1,1,1,1,1,0.0,True,2.0,549361.0,62.0,9.0,False,11.0,686312.0,14.0,4.0,35.0,1164.0,1185.0,35.0,1010.0,1031.0,17.0,684.0,687.0,0.019480519,0.741341991,0.744588745,563,984,1289256,neutron,f5601393e97baf87daa5b2065cb8783e3fd34214,1,1,Bad usage,"Bug #1289256 in neutron: ""Incorrect usage of sqlalchemy type Integer""","In migration folsom_initial in cisco_upgrade function table nexusport_bindings create column vlan_id with incorrect type Integer(255). It causes the following exception:
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 87, in upgrade
    upgrade_cisco()
  File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/folsom_initial.py"", line 455, in upgrade_cisco
    sa.Column('vlan_id', sa.Integer(255)),
TypeError: object() takes no parameters","Fix usage of sqlalchemy type Integer

In migration folsom_initial in cisco_upgrade function table
nexusport_bindings create column vlan_id with incorrect type
Integer(255).

Closes-bug: #1289256

Change-Id: I8ef99526d19b19b51d9284ccab5703de21838ee1"
1789,f56972193be903c67cd54c57bcc2becf6222aced,1396363010,,1.0,23,21,4,2,1,0.843169293,True,2.0,2950837.0,43.0,9.0,False,3.0,1098703.0,9.0,4.0,10.0,1364.0,1367.0,10.0,1179.0,1182.0,7.0,844.0,844.0,0.007518797,0.794172932,0.794172932,703,1129,1299159,neutron,f56972193be903c67cd54c57bcc2becf6222aced,1,1,Fix the rules and add new ones,"Bug #1299159 in neutron: ""Hyper-V agent cannot add ICMP security group rules""","The Hyper-V agent throws exceptions each time it tries to add a security group rule which has ICMP as protocol.
This is caused by the fact that Hyper-V Msvm_EthernetSwitchPortExtendedAclSettingData can accept only TCP and UDP as text as protocols. Any other protocol must be a string of the protocol number.
Source: http://technet.microsoft.com/en-us/library/dn464289.aspx
Agent log:
http://pastebin.com/HwrczsfX","Fixes Hyper-V agent security group ICMP rules

Converts ICMP protocol to the equivalent protocol number.
Adds default ICMP reject rules.
Adds default ANY protocol rules if the rule does not
contain any protocol.

Closes-Bug: #1299159
Change-Id: Iff51a58fdb532eda0fe7a63abf96004ee74bb073"
1790,f591eee475df177fef5f10898b2d95c1a75f5563,1391509018,,1.0,112,17,2,2,1,0.446481347,True,2.0,1816781.0,24.0,8.0,False,47.0,718730.0,99.0,5.0,53.0,1491.0,1539.0,45.0,1246.0,1287.0,19.0,1430.0,1444.0,0.002760524,0.197515528,0.199447895,362,773,1276038,nova,f591eee475df177fef5f10898b2d95c1a75f5563,0,0, Disable IGMP and refactorize tests,"Bug #1276038 in OpenStack Compute (nova): ""Linux bridge create by the OVS hybrid vif driver sends IGMP query""","The Linux bridge (qbr) created by the OVS hybrid vif driver sends usefulness IGMP queries.
By default, Linux bridge activates IGMP snooping onto a switch.
In the case of the Linux bridge was created by the OVS hybrid vif driver, is usefulness.
Because this bridge is only here to be able to apply security groups through netfilter (OVS bridge doesn't use netfilter hooks) and so it contains only 2 interfaces: the VM tap and a side of the veth patch port.
IGMP snooping on a switch of 2 ports is unnecessarily.
Furthermore, the Linux bridge could (I didn't find why and when) send some IGMP queries when the IGMP snooping is activated.
This IGMP queries could unnecessarily pollute the network and the network equipemnts log.","Disable IGMP snooping on hybrid Linux bridge

By default, Linux bridge activates the IGMP snooping on his switch.
It is unnecessarily on Linux bridge created by OVS/VIF hybrid vif
drivers because that switch is limited two ports.

That patch also refactoring OVS and IVS plug and unplug unit tests of the
hybrid drivers.

Closes-Bug: #1276038
Change-Id: Ib8bb1874d7a300fddffdc76ba67851109248bebf"
1791,f5bc15687bcab1e0e4a2163a194c04bd8c5d14d9,1393959894,,1.0,16,3,3,2,1,0.869397881,True,9.0,755494.0,85.0,20.0,False,56.0,373042.0,146.0,4.0,1806.0,1970.0,3177.0,1518.0,1841.0,2801.0,914.0,1465.0,1945.0,0.12183755,0.195206391,0.259121172,522,943,1287844,nova,f5bc15687bcab1e0e4a2163a194c04bd8c5d14d9,0,0,We need to raise a exception,"Bug #1287844 in OpenStack Compute (nova): ""VMware: raise an error for invalid disk_format""",We need to raise an exception for an disk_format that is not supported,"VMware: raise an exception for unsupported disk formats

At the moment 'vmdk' and 'iso' are the only disk formats supported.
If an invalid format is passed with the image then the driver will
throw and exception.

Change-Id: I834856f33a0644bc03b89a2b98e4e188c112e338
Closes-bug: #1287844"
1792,f5caac43ac40e103f3f2156eedb313385dbb6ba8,1382557206,1.0,,1332,0,6,3,2,0.764611373,True,33.0,18034354.0,171.0,32.0,False,0.0,0.0,0.0,3.0,4.0,219.0,221.0,4.0,190.0,192.0,4.0,156.0,158.0,0.006738544,0.211590296,0.214285714,1862,1348869,1348869,Swift,f5caac43ac40e103f3f2156eedb313385dbb6ba8,1,0,“The tempfile.mktemp() function has been deprecated since Python 2.3 due to security issues.”,"Bug #1348869 in OpenStack Object Storage (swift): ""Avoid usage of insecure mktemp() function""","The tempfile.mktemp() function has been deprecated since Python 2.3 due to security issues.  There are more secure alternatives available, such as tempfile.TemporaryFile().  There are more details on this in the Python tempfile documentation.
Swift is using tempfile.mktemp() in a few locations in the profiling middleware:
  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/html_viewer.py
  https://github.com/openstack/swift/blob/master/swift/common/middleware/x_profile/profile_model.py
These should be modified to use a secure method of temporary file creation for security hardening reasons.","Add profiling middleware in Swift

The profile middleware provide a tool to profile Swift
code on the fly and collect statistic data for performance
analysis. An native simple Web UI is also provided to help
query and visualize the data.

Change-Id: I6a1554b2f8dc22e9c8cd20cff6743513eb9acc05
Implements: blueprint profiling-middleware"
1793,f5d33051b6d9b0dd0fe08692758748ed34d50136,1397142079,,1.0,2,1,1,1,1,0.0,True,1.0,968387.0,26.0,6.0,False,8.0,856772.0,14.0,6.0,4.0,1462.0,1462.0,4.0,1279.0,1279.0,4.0,939.0,939.0,0.004432624,0.833333333,0.833333333,776,1203,1305957,neutron,f5d33051b6d9b0dd0fe08692758748ed34d50136,1,0,“This change https://review.openstack.org/#/c/69803/3 added new quota resources.”,"Bug #1305957 in neutron: ""LBaaS extension doesn't register it's resources to quota engine""","LBaaS extension is using the new resource_helper module to register its resources to resource manager.
This change https://review.openstack.org/#/c/69803/3 added new quota resources.
In order those new quota resources to be registered in quotas engine, register_quota=True argument should be passed to the resource_helper.build_resource_info() function.
neutron/extensions/loadbalancer.py line 351
This bug causes modified tempest quotas test from https://review.openstack.org/#/c/60008 to fail
because new lbaas quota resources are not registered","Register LBaaS resources to quotas engine

Adding register_quotas=True argument to
resource_helper.build_resource_info() function
in order to register LBaaS resources to quotas engine

This change is actually a continuation of https://review.openstack.org/#/c/69803
change which was already approved

Change-Id: Ib75027ce4a1bea3d453f57b107d29546ec6743e5
Closes-Bug: #1305957"
1794,f64eacfd27220c180f6afc979087b35aa1385550,1394167871,5.0,1.0,302,73,9,2,1,0.604790797,True,15.0,2643885.0,173.0,9.0,False,11.0,2265286.444,30.0,2.0,37.0,1238.0,1241.0,37.0,1032.0,1035.0,29.0,663.0,666.0,0.03257329,0.720955483,0.724212812,550,971,1289027,neutron,f64eacfd27220c180f6afc979087b35aa1385550,0,0,improve unit test coverage,"Bug #1289027 in neutron: ""BigSwitch: More unit tests required for servermanager""",The server manager component of the BigSwitch plugin needs more unit tests to exercise some functions that are currently mocked out as part of the existing unit tests.,"BigSwitch: Improves server manager UT coverage

Improves the unit test coverage for the Big Switch
server manager module (100%). Also reorganizes the
capabilities test to avoid duplicating a lot of
router tests that are already covered.

Closes-Bug: #1289027
Change-Id: Ib51e8160f8d95686e86430b0a9c9f54deeda0e84"
1795,f668b347ca181229fab4b89fda25f1aadfbfdef1,1408009180,,1.0,34,3,2,2,1,0.753197991,False,,,,,True,,,,,,,,,,,,,,,,,1193,1651,1356794,cinder,f668b347ca181229fab4b89fda25f1aadfbfdef1,1,1,,"Bug #1356794 in Cinder: ""VMware: Volume creation fails when config 'vmware_volume_folder' contains special characters""","Set vmware_volume_folder=/cinder-volumes in cinder.conf
> cinder create 1
fails with following error:
Unable to find suitable datastore for volume of size: 1 GB under host: (obj){
   value = ""host-361""
   _type = ""HostSystem""
 }. More details: Server raised fault: 'The name '/cinder-volumes' already exists","VMware:Unquote folder name for folder exists check

vCenter server escapes special characters in the folder name using URL
encoding and returns back the encoded string while querying. This causes
the check for folder existence to return false. Therefore, folder
creation is reattempted which eventually fails. This patch fixes the
problem by decoding the folder name returned by vCenter before
comparison.

Change-Id: I40aa6f42ea0d85fbfcb40970c55b20e13ea46522
Closes-Bug: #1356794"
1796,f66f9e5805070e5ad2ba4688c6c4571ef1ec395a,1406573455,,1.0,11,2,2,1,1,0.89049164,False,,,,,True,,,,,,,,,,,,,,,,,1034,1478,1336648,cinder,f66f9e5805070e5ad2ba4688c6c4571ef1ec395a,1,1,,"Bug #1336648 in Cinder: ""iSER transport protocol is Broken""","when configuring ISER  ( at /etc/cinder/cinder.conf: volume_driver=cinder.volume.drivers.lvm.LVMISERDriver)
volumes cannot be attached.
this happens also in Icehouse (stable).
i will add more logs soon.","fixing the iSER transport protocol when using LVMISERDriver

ISER capabilities were broken in icehouse when trying to remove
duplication between ISCSIDriver and ISERDriver.
this fix does minimal changes in the code in order to fix
the capability.
the fix is basically re-adding the ISERTgtAdm and re-adding
the support in LVMISERDriver: change target_lun based on
iseradm.

Closes-Bug: #1336648
Change-Id: Ic3c9da9577d09d9199218ea82bda56599527223c"
1797,f676d9215958b1af1105d4ff4671ce6cefd83eb7,1404981403,,1.0,101,34,4,2,1,0.954352609,False,,,,,True,,,,,,,,,,,,,,,,,1063,1510,1341465,neutron,f676d9215958b1af1105d4ff4671ce6cefd83eb7,1,1,Resgression “after recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665”,"Bug #1341465 in neutron: ""ofagent: non tapXXX device handling is broken""","after recently merged commit 9d13ea884bff749b3975acb5eb5630e5aca4a665, non tapXXX device handling is broken.
(the corresponding gerrit review is https://review.openstack.org/#/c/100404/)","ofagent: Handle device name prefixes other than ""tap""

This fixes regressions in commit 9d13ea884bff749b3975acb5eb5630e5aca4a665.

Handle device name prefixes other than ""tap"".
For example, nova hybrid interface driver uses ""qvo"" prefix.

Also, ignore non neutron ports correctly.  For example, veth pairs
used to connect physical bridges.

Closes-Bug: #1341465
Change-Id: I1d71c8a2cf8c2f71f0dbcfb54c9b347e24c03562"
1798,f691ebe03916a78cbf18017d628a28b17f147700,1386594077,,1.0,7,6,3,3,1,0.842088782,True,5.0,213738.0,35.0,11.0,False,24.0,618940.3333,34.0,3.0,327.0,1736.0,1831.0,256.0,1522.0,1603.0,52.0,233.0,264.0,0.086178862,0.380487805,0.430894309,162,564,1259088,neutron,f691ebe03916a78cbf18017d628a28b17f147700,0,0,the dispatch MAYBE dispatch the rpc message to an unready agent,"Bug #1259088 in neutron: ""setup_rpc should be the last thing in __init__ method""","if setup_rpc is too early, the dispatch maybe dispatch the rpc message to an unready agent.  take ovs plugin agent for instance,
after setup_rpc is called, many of the initialization work are still needed to be done. If the message is coming during this time, the instance will  not be fully initialized:
    def __init__(self, integ_br, tun_br, local_ip,
                 bridge_mappings, root_helper,
                 polling_interval, tunnel_types=None,
                 veth_mtu=None, l2_population=False,
                 minimize_polling=False,
                 ovsdb_monitor_respawn_interval=(
                     constants.DEFAULT_OVSDBMON_RESPAWN)):
        '''Constructor.
        :param integ_br: name of the integration bridge.
        :param tun_br: name of the tunnel bridge.
        :param local_ip: local IP address of this hypervisor.
        :param bridge_mappings: mappings from physical network name to bridge.
        :param root_helper: utility to use when running shell cmds.
        :param polling_interval: interval (secs) to poll DB.
        :param tunnel_types: A list of tunnel types to enable support for in
               the agent. If set, will automatically set enable_tunneling to
               True.
        :param veth_mtu: MTU size for veth interfaces.
        :param minimize_polling: Optional, whether to minimize polling by
               monitoring ovsdb for interface changes.
        :param ovsdb_monitor_respawn_interval: Optional, when using polling
               minimization, the number of seconds to wait before respawning
               the ovsdb monitor.
        '''
        self.veth_mtu = veth_mtu
        self.root_helper = root_helper
        self.available_local_vlans = set(xrange(q_const.MIN_VLAN_TAG,
                                                q_const.MAX_VLAN_TAG))
        self.tunnel_types = tunnel_types or []
        self.l2_pop = l2_population
        self.agent_state = {
            'binary': 'neutron-openvswitch-agent',
            'host': cfg.CONF.host,
            'topic': q_const.L2_AGENT_TOPIC,
            'configurations': {'bridge_mappings': bridge_mappings,
                               'tunnel_types': self.tunnel_types,
                               'tunneling_ip': local_ip,
                               'l2_population': self.l2_pop},
            'agent_type': q_const.AGENT_TYPE_OVS,
            'start_flag': True}
        # Keep track of int_br's device count for use by _report_state()
        self.int_br_device_count = 0
        self.int_br = ovs_lib.OVSBridge(integ_br, self.root_helper)
        self.setup_rpc()
        self.setup_integration_br()
        self.setup_physical_bridges(bridge_mappings)
        self.local_vlan_map = {}
        self.tun_br_ofports = {constants.TYPE_GRE: {},
                               constants.TYPE_VXLAN: {}}
        self.polling_interval = polling_interval
        self.minimize_polling = minimize_polling
        self.ovsdb_monitor_respawn_interval = ovsdb_monitor_respawn_interval
        if tunnel_types:
            self.enable_tunneling = True
        else:
            self.enable_tunneling = False
        self.local_ip = local_ip
        self.tunnel_count = 0
        self.vxlan_udp_port = cfg.CONF.AGENT.vxlan_udp_port
        self._check_ovs_version()
        if self.enable_tunneling:
            self.setup_tunnel_br(tun_br)
        # Collect additional bridges to monitor
        self.ancillary_brs = self.setup_ancillary_bridges(integ_br, tun_br)
        # Security group agent supprot
        self.sg_agent = OVSSecurityGroupAgent(self.context,
                                              self.plugin_rpc,
                                              root_helper)
        # Initialize iteration counter
        self.iter_num = 0","move rpc_setup to the last step of __init__

rpc setup should be the last stuff in agent initialization
so that the rcp message handler(the agent instance itself)
can have a fully initialized agent instance.

Change-Id: I58c966e2c8ee92c307b565932e867b5d1ec73b67
Closes-Bug: #1259088"
1799,f6988b3f1ebf56364696ff25448a8018866fd20b,1386953950,,1.0,37,2,2,2,1,0.477071306,True,5.0,2935228.0,34.0,14.0,False,191.0,95897.0,1052.0,2.0,262.0,2599.0,2742.0,251.0,2117.0,2254.0,258.0,2457.0,2597.0,0.037837838,0.359094229,0.379547115,187,590,1260806,nova,f6988b3f1ebf56364696ff25448a8018866fd20b,1,1,,"Bug #1260806 in OpenStack Compute (nova): ""Defaulting device names fails to update the database""","_default_block_device_names method of the compute manager, would call the conductor block_device_mapping_update method with the wrong arguments, causing a TypeError and ultimately the instance to fail.
This bug happens only when using a driver that does not provid it's own implementation of default_device_names_for_instance, (currently only the libvirt driver does this).
Also affects havana since https://review.openstack.org/#/c/40229/","Fix updating device names when defaulting

If the virt driver does not support defaulting device names, the default
behaviour of defaulting them in the manager was calling the conductor
block_device_mapping_update method with the wrong arguments, which
caused a TypeError and ultimately a failure to boot the instance.

This patch fixed the signature and adds a test that exercises it so
that it does not regress.

Closes-bug: #1260806
Change-Id: I6116540c9aca4847afacf80b3d1e0c027c364159"
1800,f6d6c632f620411816979f04b7b3dda28681db18,1410562289,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1251,1710,1363324,nova,f6d6c632f620411816979f04b7b3dda28681db18,1,1,,"Bug #1363324 in OpenStack Compute (nova): ""a bug in quota check""","\nova\db\sqlalchemy\api.py   quota_reserve()
when decided whether to refresh the user_usages[resource], one rule is that if the last refresh was too long time ago, we need refresh user_usages[resource].
 elif max_age and (user_usages[resource].updated_at -
                              timeutils.utcnow()).seconds >= max_age:
using last update time minus current time result in a overflow ,so that the refresh action always be executed,in consideration of the max_age won't be a max number.","correct inverted subtraction in quota check

From a user reported bug: we were doing the subtraction backwards
here, causing int overflow, causing all kinds of invalid. Instead
actually subtract the smaller number from the bigger number.

Change-Id: I7c099df5e31aa25b8be65c570e2e0b8a3e93c277
Closes-Bug: #1363324"
1801,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,1403013855,1.0,1.0,273,8081,78,20,1,0.785135113,False,,,,,True,,,,,,,,,,,,,,,,,1819,1273087,1273087,Glance,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,0,0,"""There is no merge""","Bug #1273087 in Glance: ""Image status stuck at 'saving' when an upload fails using v2 APIs""","Step 1: Create an empty image (using v1 or v2 - doesn't matter).
   glance image-show <IMAGE-ID> will show the image status as ""queued"".
Step 2: Upload an image file for the image, but cancel the upload midway.
   glance image-show <IMAGE-ID> will show the image status as ""saving"" even though the upload has been cancelled.
Expected result: The image status should be ""queued"" instead of ""saving"".","Adopt glance.store library in Glance

This commits removes the old `store` package from glance and adopts the
usage of the new glance.store library. The library was designed to
preserve backwards compatibility as much as possible. In fact, most of
the changes in this patch are related to function args ordering and not
function renames or workflow changes.

Some changes that are worth mentioning:

1. Glance store doesn't rely on a global config object. All config
options must be explicitly registered.

2. All store operations now accepted an optional context. This is a
fallout from the context not being required in the `Store` constructor
anymore.

3. Store drivers are behind a private package called `_drivers` and
they're not suppose to be accessed directly. Instead, functions like
`get_store_from_scheme` should be used.

4. Stores are disabled by default

5. All the store specific options are under the `glance_store` group.

DocImpact:
The old store related configuration options have been moved under the
`glance_store` section. However, the old options will go through a
deprecation path. That is, they'll still be read from the `DEFAULT`
section to give deployers enough time to update their config files.

In k-2, the deprecated options will be completely obsolete.

Closes-bug: #1291848
Implements-blueprint: create-store-package

Change-Id: Iaacc70993ad5da292b93de42bbecda73d53b19fd"
1802,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,1403013855,1.0,1.0,273,8081,78,20,1,0.785135113,False,,,,,True,,,,,,,,,,,,,,,,,1834,1291848,1291848,Glance,f6e7992a68f6ab36f87604fffbe5349f2a12b25e,0,0,"""Tracking the need of removing deprecation warning "", ""This commits removes the old `store` package from glance""","Bug #1291848 in Glance: ""Remove deprecation warning when loading stores""",Tracking the need of removing deprecation warning and the _EXTRA_STORES global from stores/__init__,"Adopt glance.store library in Glance

This commits removes the old `store` package from glance and adopts the
usage of the new glance.store library. The library was designed to
preserve backwards compatibility as much as possible. In fact, most of
the changes in this patch are related to function args ordering and not
function renames or workflow changes.

Some changes that are worth mentioning:

1. Glance store doesn't rely on a global config object. All config
options must be explicitly registered.

2. All store operations now accepted an optional context. This is a
fallout from the context not being required in the `Store` constructor
anymore.

3. Store drivers are behind a private package called `_drivers` and
they're not suppose to be accessed directly. Instead, functions like
`get_store_from_scheme` should be used.

4. Stores are disabled by default

5. All the store specific options are under the `glance_store` group.

DocImpact:
The old store related configuration options have been moved under the
`glance_store` section. However, the old options will go through a
deprecation path. That is, they'll still be read from the `DEFAULT`
section to give deployers enough time to update their config files.

In k-2, the deprecated options will be completely obsolete.

Closes-bug: #1291848
Implements-blueprint: create-store-package

Change-Id: Iaacc70993ad5da292b93de42bbecda73d53b19fd"
1803,f702fe7e30e4021895dac8e7ab243e5192f8182d,1378326702,1.0,1.0,46,7,3,3,1,0.724606886,True,6.0,1154738.0,59.0,36.0,False,57.0,111579.6667,133.0,3.0,8.0,1126.0,1134.0,5.0,1040.0,1045.0,0.0,897.0,897.0,0.00104712,0.940314136,0.940314136,1437,1191812,1191812,cinder,f702fe7e30e4021895dac8e7ab243e5192f8182d,1,1, ,"Bug #1191812 in Cinder: ""volume cloning is failing with an I/O error happening when the backing snapshot is wiped""","cloning a volume using the --source-volid argument is failing with the following in the volume.log file:
""""""
Command: sudo cinder-rootwrap /etc/cinder/rootwrap.conf dd if=/dev/zero of=/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb count=1024 bs=1M conv=fdatasync
Exit code: 1
Stdout: ''
Stderr: ""/bin/dd: fdatasync failed for `/dev/mapper/cinder--volumes-clone--snap--727c84cc--69ee--4b7d--bdb0--832b5086f1bb': Input/output error\n1024+0 records in\n1024+0 records out\n1073741824 bytes (1.1 GB) copied, 19.7612 s, 54.3 MB/s\n""
""""""
NOTE: the actual dd from the origin snapshot into the destination volume is working; it is the cleanup of the origin snapshot which fails","Fix secure delete for thick LVM snapshots

This change modifies the behaviour of the secure delete for thick
LVM snapshots to wipe the underlying COW of the snapshot LV
instead of the snapshot LV itself.

This change is necessary because the snapshot LV does not contain
exactly the same number of writable blocks as the original LV.  The
COW includes header information per COW block that identifies the
device as a COW device as well as the source and destination blocks
for the changed item.  The amount of metadata contained in the COW is
variable based on I/O performed on the snapshot.

This does not change the behavior of secure deletes on thin LVs
or secure deletes on the thick LV snapshot origin.

Closes-Bug: #1191812
Change-Id: I20e02b6c20d5ac539b5b5469e665fc986180f2e9"
1804,f778931252fbee34980e32289e96534eebb89c2b,1387850311,,1.0,4,2,1,1,1,0.0,True,2.0,1253466.0,29.0,15.0,False,5.0,8353358.0,4.0,6.0,369.0,2020.0,2387.0,167.0,1809.0,1975.0,9.0,29.0,38.0,0.037593985,0.112781955,0.146616541,221,624,1263813,neutron,f778931252fbee34980e32289e96534eebb89c2b,1,1, Corrects broken format strings ,"Bug #1263813 in neutron: ""TypeError - not enough args for format string in i18n_cfg.py""","jpipes@uberbox:~/repos/openstack/neutron$ git log --oneline | head -1
84aeb9a Merge ""Imported Translations from Transifex""
Running tox -eALL resulted in:
i18n runtests: commands[0] | python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py
Traceback (most recent call last):
  File ""./tools/check_i18n.py"", line 151, in <module>
    debug):
  File ""./tools/check_i18n.py"", line 112, in check_i18n
    ASTWalker())
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 106, in walk
    walker.preorder(tree, visitor)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 63, in preorder
    self.dispatch(tree, *args) # XXX *args make sense?
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 38, in default
    compiler.visitor.ASTVisitor.default(self, node, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 40, in default
    self.dispatch(child, *args)
  File ""/usr/lib/python2.7/compiler/visitor.py"", line 57, in dispatch
    return meth(node, *args)
  File ""./tools/check_i18n.py"", line 62, in visitConst
    self.lines[node.lineno - 1][:-1], msg),
TypeError: not enough arguments for format string
ERROR: InvocationError: '/home/jpipes/repos/openstack/neutron/.tox/i18n/bin/python ./tools/check_i18n.py ./neutron ./tools/i18n_cfg.py'","Corrects broken format strings in check_i18n.py

In tools/check_i18n.py, the error messages printed out
had format strings that included 5 interpolations, but only
four variables were supplied. This patch simply corrects that
mismatch by including the name of the predicate or checker
that failed.

Closes-Bug: #1263813
Change-Id: I5cacee9ea000f56a9ef45f07367e85f20fa333c7"
1805,f78746062f942b30d73f2575c41dee26b3619f2f,1389264593,,1.0,1,1,1,1,1,0.0,True,2.0,1076280.0,28.0,8.0,False,14.0,528828.0,22.0,2.0,47.0,774.0,793.0,47.0,727.0,746.0,35.0,745.0,752.0,0.027355623,0.566869301,0.57218845,275,680,1267423,cinder,f78746062f942b30d73f2575c41dee26b3619f2f,0,0,unused code,"Bug #1267423 in Cinder: ""Remove unused variable in os-extend api""","Temp variable _val  in _extend  method is  not being used, so remove this '_val' variable.","Remove unused variable in os-extend api

Temp variable _val in _extend method is not being used, so remove this
'_val' variable.

Change-Id: I4a3f5e28d1882698a3efd116044c4fddff86c474
Closes-Bug: #1267423"
1806,f7d99b07f72037b87d5e9f63b48a7c49f7116fe5,1393735545,,1.0,60,6,2,2,2,0.957181439,True,1.0,131763.0,10.0,2.0,False,8.0,110293.0,16.0,2.0,37.0,742.0,754.0,27.0,706.0,715.0,35.0,716.0,727.0,0.024144869,0.480885312,0.488262911,503,922,1286694,cinder,f7d99b07f72037b87d5e9f63b48a7c49f7116fe5,0,0,Feature. Add initiator target map ,"Bug #1286694 in Cinder: ""Add initiator target map which is required by the FC zone manager in EMC SMI-S FC driver""",Add initiator target map in initialize_connection and terminate_connection of the EMC SMI-S FC driver as it is required by the FC zone manager.,"Add initiator target map in EMC SMI-S FC driver.

This patch add the initiator target map in initialize_connection
and terminate_connection as it is required by the FC Zone Manager.

Change-Id: I84729968b8e4e6ee74536fb28d3d921a9baf9f6e
Closes-Bug: #1286694"
1807,f7ef2b09a195f853cecaa28db710d643a8e30f78,1410414996,,1.0,5,5,2,2,1,0.721928095,False,,,,,True,,,,,,,,,,,,,,,,,1298,1764,1368035,nova,f7ef2b09a195f853cecaa28db710d643a8e30f78,0,0,Feature “V3 resource name needs to be changed to work that for V2.1.”,"Bug #1368035 in OpenStack Compute (nova): ""'os-interface' resource name is wrong for Nova V2.1""","'os-interface' resource name needs to be fixed in V3 code base to work for V2.1 API.
V2 and V3 diff for this resource name is below-
V2 - '/servers/os-interface'
V3 - '/servers/os-attach-interfaces'
V3 resource name needs to be changed to work that for V2.1.
This needs to be fixed to make V2.1 backward compatible with V2 APIs","Fix 'os-interface' resource name for Nova V2.1

V2 and V3 diff for this resource name is below-
V2 - '/servers/os-interface'
V3 - '/servers/os-attach-interfaces'

V3 resource name needs to be changed to work that for V2.1

Change-Id: I37bd7cee7e8879b01a8aee739d74c2dd6e033dc4
Closes-Bug: #1368035"
1808,f7f9e6228d8b73be6c1a4ad3fd998d02104a7a3b,1399069847,,1.0,33,2,2,2,1,0.660962335,True,1.0,270006.0,21.0,6.0,False,18.0,349200.0,79.0,4.0,227.0,849.0,936.0,211.0,767.0,848.0,217.0,808.0,888.0,0.135910224,0.50436409,0.554239401,856,1289,1315542,cinder,f7f9e6228d8b73be6c1a4ad3fd998d02104a7a3b,1,1,,"Bug #1315542 in Cinder: ""3PAR initialize_connection fails when no NSP is returned""","When a VLUN is created to attach a 3PAR volume to a host/instance and an NSP isn't used, the NSP isn't returned in the _create_vlun call.   We were always assuming the nsp would be returned from the REST call.","Fixes an issue with 3PAR attach

There was a case that during vlun creation
the location returned back from the 3PAR
didn't include the NSP value.

Change-Id: I6c6341fda65df0e8ce148f3d926990a50471a2db
Closes-Bug: #1315542"
1809,f807023246501b77e792371d98b612b208f5e8e3,1403301910,,1.0,182,4,4,3,1,0.972883772,False,,,,,True,,,,,,,,,,,,,,,,,997,1438,1332719,neutron,f807023246501b77e792371d98b612b208f5e8e3,1,0,"“However it is necessary to support all versions of NOS, hence a run time check of the NOS version should be made and the appropriate template should be used.”","Bug #1332719 in neutron: ""brocade ml2 mechanism does not support VDX/NOS version greater than 4.1.0""","In order to support VDX/NOS version greater than 4.1.0, NETCONF template needs to be enhanced. However it is necessary to support all versions of NOS, hence a run time check of the NOS version should be made and the appropriate template should be used.","Added support for NOS version 4.1.0, 5.0.0 and greater

NETCONF temaplates for NOS version greater than 4.1.0 are slightly
different (argh). An init time check of the NOS version is done
to enable selection of the correct NETCONF templates.

Change-Id: I01e82ad402fbbb25d92a99a3325ca2608dd514cb
Closes-bug: #1332719"
1810,f83b2ef4fc042d8559b1f75dbe949460f2843c7e,1394074960,2.0,1.0,254,321,7,5,1,0.849559063,True,8.0,1963470.0,177.0,22.0,False,20.0,403434.0,37.0,4.0,12.0,1244.0,1247.0,12.0,1058.0,1061.0,12.0,706.0,709.0,0.014380531,0.782079646,0.78539823,534,955,1288407,neutron,f83b2ef4fc042d8559b1f75dbe949460f2843c7e,1,1,,"Bug #1288407 in neutron: ""Fix segment allocation tables in Cisco N1kv plugin""","The segment ranges for VLAN and VXLAN are being populated using an in memory dictionary. The segment allocation table is emptied on deleting any network profile.
This change allows the use of segment range from the network profile table.
By using the network profile UUID as a foreign key in the segment allocations table, tables are cleaned up only for the segments associated with
the deleted network profile via CASCADE, leaving no inconsistencies.
Add more UT along with this.","Fix segment allocation tables in Cisco N1kv plugin

The segment allocation table is emptied on deleting any network profile.
This change allows the use of segment range from the network profile table.
By using the network profile UUID as a foreign key in the segment allocations table,
tables are cleaned up only for the segments associated with
the deleted network profile via CASCADE, leaving no inconsistencies.

Change-Id: I507041fac3768a7b688ddcf28c4d97c618bfe3f9
Closes-Bug: #1288407"
1811,f85f240cabb0a7cc1adb3fa1c3139984e8e30d4c,1395698595,,1.0,1,1,1,1,1,0.0,True,1.0,50311.0,22.0,5.0,False,22.0,458666.0,41.0,4.0,13.0,1686.0,1691.0,13.0,1293.0,1298.0,10.0,798.0,800.0,0.01068999,0.776482021,0.778425656,681,1107,1297145,neutron,f85f240cabb0a7cc1adb3fa1c3139984e8e30d4c,1,0,"deprecated, no BIC","Bug #1297145 in neutron: ""L3 agent uses deprecated root_helper option""","L3 agent uses deprecated root_helper option through self.conf.root_helper in neutron.agent.l3_agent.L3NATAgent._update_routing_table instead of using self.root_helper (result of neutron.agent.common.config.get_root_helper.
It implies if AGENT/root_helper option is configured and root_helper is not configured, extra routes are not pushed in routers.","Replace a usage of the deprecated root_helper option

This change replaces a usage of the deprecated root_helper option
by the usage of get_root_helper result.

Change-Id: Icfc698243784557cbf987a817c13d0b80969e5d3
Closes-Bug: #1297145"
1812,f86d244e45310dbef7f22c6f320fac6897fdce1f,1396383460,,1.0,66,18,3,1,1,0.95049048,True,14.0,1189974.0,228.0,26.0,False,2.0,3806852.667,3.0,3.0,214.0,569.0,694.0,198.0,466.0,578.0,171.0,301.0,393.0,0.161350844,0.283302064,0.369606004,718,1144,1301101,neutron,f86d244e45310dbef7f22c6f320fac6897fdce1f,0,0,Bug in test,"Bug #1301101 in neutron: ""Add functional tests to verify VXLAN support""",Functional tests to verify that a host can support VXLAN are needed. These can also test the logic in ovs_lib and compare the results of the functional test to the ovs_lib result.,"Add functional tests to verify ovs_lib VXLAN detection

This commit adds a functional test to verify host VXLAN support. It compares
the results of this functional test with the logic in ovs_lib to ensure both
report the same values.

Closes-Bug: #1301101

Change-Id: Id91c755d762bea134cc451952d0f13d64576663a"
1813,f89a6bd30b6f99bc39f266ae8d3880380379f8b9,1402300780,,1.0,12,3,2,2,1,0.836640742,False,,,,,True,,,,,,,,,,,,,,,,,977,1416,1330135,neutron,f89a6bd30b6f99bc39f266ae8d3880380379f8b9,1,1,,"Bug #1330135 in neutron: ""Big Switch: consistency watchdog killed by exceptions""","If the consistency watchdog encounters an exception, it is uncaught and it kills the greenthread so it no longer works until the neutron server is restarted.","Big Switch: Catch exceptions in watchdog thread

Catch and log exceptions in the watchdog greenthread to prevent
them from stopping the periodic consistency checks.

Closes-Bug: #1330135
Change-Id: I6834c0fee0429bb72b8c61307be7bdca77f6de9b"
1814,f89d13b141eba66487b3d858cd075a47b2de6016,1398163011,,1.0,48,1,4,2,1,0.741078427,True,5.0,814748.0,37.0,10.0,False,62.0,1244732.75,184.0,2.0,1945.0,1074.0,2662.0,1614.0,1073.0,2331.0,1027.0,1068.0,1739.0,0.131089008,0.136317266,0.221882173,820,1248,1310817,nova,f89d13b141eba66487b3d858cd075a47b2de6016,1,1,,"Bug #1310817 in OpenStack Compute (nova): ""VMware: concurrent access error when deleting snapshot""","The VMware Minesweeper CI is occasionally seeing an error when deleting snapshots. The error is:
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
    incoming.message))
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
    return self._do_dispatch(endpoint, method, ctxt, args)
  File ""/usr/local/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
    result = getattr(endpoint, method)(ctxt, **new_args)
  File ""/opt/stack/nova/nova/exception.py"", line 88, in wrapped
    payload)
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/exception.py"", line 71, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 280, in decorated_function
    pass
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 266, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 309, in decorated_function
    e, sys.exc_info())
  File ""/opt/stack/nova/nova/openstack/common/excutils.py"", line 82, in __exit__
    six.reraise(self.type_, self.value, self.tb)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 296, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2692, in backup_instance
    task_states.IMAGE_BACKUP)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 2758, in _snapshot_instance
    update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 645, in snapshot
    _vmops.snapshot(context, instance, name, update_task_state)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 873, in snapshot
    self._delete_vm_snapshot(instance, vm_ref, snapshot)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vmops.py"", line 780, in _delete_vm_snapshot
    self._session._wait_for_task(delete_snapshot_task)
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 948, in _wait_for_task
    ret_val = done.wait()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/event.py"", line 116, in wait
    return hubs.get_hub().switch()
  File ""/usr/local/lib/python2.7/dist-packages/eventlet/hubs/hub.py"", line 187, in switch
    return self.greenlet.switch()
VMwareDriverException: A general system error occurred: concurrent access
Full logs for an affected run can be found here: http://10.148.255.241/logs/85961/2","VMware: handle case when VM snapshot delete fails

Minesweeper fails on occasion with concurrent access to files.

If a snapshot deletion fails with the backend exception
TaskInProgress then we will retry.

A generic decorator has been added to do the retry operations.

Closes-bug: #1310817

Change-Id: I8ed7a24ccd34aeea49352ac98f34ec2960edbf97"
1815,f8ab9d4366c87724cba2e49e71dc72e2427d5a68,1391749678,1.0,1.0,94,5,5,4,1,0.633865874,True,9.0,2173118.0,152.0,27.0,False,16.0,212419.0,24.0,5.0,1.0,1066.0,1066.0,1.0,917.0,917.0,1.0,377.0,377.0,0.002621232,0.495412844,0.495412844,447,862,1282754,neutron,f8ab9d4366c87724cba2e49e71dc72e2427d5a68,0,0,No bug. ‘Validate multicast ip range in Cisco N1kv’,"Bug #1282754 in neutron: ""Validate multicast ip range in Cisco N1kv Plugin""",Updates network description and validates multicast ip range check,"Validate multicast ip range in Cisco N1kv Plugin

Do the validation of multicast ip range in Cisco N1kv Plugin and send
appropriate parameters to VSM.

Change-Id: Iaa1288d8a6deb2a2f32b263d4556dde8938d75f5
Closes-Bug: #1282754"
1816,f8ac42460dca22f83a59097530f73b282129cc2b,1378649898,,1.0,263,184,3,2,1,0.750277535,True,4.0,604158.0,29.0,12.0,False,18.0,109258.0,32.0,2.0,123.0,951.0,976.0,122.0,861.0,885.0,110.0,816.0,833.0,0.113846154,0.837948718,0.855384615,1545,1224030,1224030,cinder,f8ac42460dca22f83a59097530f73b282129cc2b,1,0,“uses arguments for qemu-img (--backing-chain and --output=json) that are newer than the versions”,"Bug #1224030 in Cinder: ""GlusterFS snapshot code uses new qemu-img arguments""","The GlusterFS snapshot code introduced in Havana uses arguments for qemu-img (--backing-chain and --output=json) that are newer than the versions of qemu-img that Cinder is generally expected to support.
Additionally, some code was added to parse qemu-img output rather than using the well-tested image_utils methods which do this today.
This code should not call qemu-img --backing-chain or --output=json, and should leverage image_utils as much as possible for the work being done with qemu_img.","GlusterFS: Use image_utils for qemu-img calls

Code added for GlusterFS snapshot support in Havana included
new code to parse qemu-img output.  This removes this code
and uses image_utils qemu_img_info instead.

Add test to image_utils for parsing a different style of
qemu-img info output.

Improve GlusterFS driver test coverage with tests for cloning,
create from snapshot, and initialize_connection.

Closes-Bug: #1224030
Change-Id: I8f1811b400c06edb3cd7416c52aa297921841e54"
1817,f8ae852c1a267a15f6b70026ad40d5d219fc0d33,1403867511,,1.0,17,10,6,6,1,0.961368628,False,,,,,True,,,,,,,,,,,,,,,,,1018,1461,1335076,nova,f8ae852c1a267a15f6b70026ad40d5d219fc0d33,1,1,"""This should be a coherent message.”","Bug #1335076 in OpenStack Compute (nova): ""Exception raised by attach interface is problematic""",The exception raised is inappropriate. It just returns the instance object. This should be a coherent message.,"Network: interface attach and detach raised confusing exception

When the aforementioned operations failed they raised an exception
that was not coherent - this is due to the fact that the instance object
was passed to the exception. That would print the whole instance object
which is really confusing to a openstack user. The exceptions should has passed
the instance ID and not the instance.

TrivialFix

Change-Id: If3fa89b17210c1db3540cd813157b51e786e1494
Closes-bug: #1335076"
1818,f91d4ebeac9181ff279158fe89a8d50b34184a89,1413364563,,1.0,7,5,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1405,1875,1381414,nova,f91d4ebeac9181ff279158fe89a8d50b34184a89,0,0,Bug in test,"Bug #1381414 in OpenStack Compute (nova): ""Unit test failure ""AssertionError: Expected to be called once. Called 2 times."" in test_get_port_vnic_info_3""","This looks to be due to tests test_get_port_vnic_info_2 and 3 sharing some code and is easily reproduced by running these two tests alone with no concurrency.
./run_tests.sh --concurrency 1 test_get_port_vnic_info_2 test_get_port_vnic_info_3
The above always results in:
Traceback (most recent call last):
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2615, in test_get_port_vnic_info_3
    self._test_get_port_vnic_info()
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 1201, in patched
    return func(*args, **keywargs)
  File ""/home/hans/nova/nova/tests/network/test_neutronv2.py"", line 2607, in _test_get_port_vnic_info
    fields=['binding:vnic_type', 'network_id'])
  File ""/home/hans/nova/.venv/local/lib/python2.7/site-packages/mock.py"", line 845, in assert_called_once_with
    raise AssertionError(msg)
AssertionError: Expected to be called once. Called 2 times.","Fix unit test failure due to tests sharing mocks

A recent refactor made test_get_port_vnic_info 2 and 3 share code
in a private method _test_get_port_vnic_info(). A mock on this method
is shared between the tests and can cause the following failure:

AssertionError: Expected to be called once. Called 2 times.

This is changed so that the two test methods create separate mocks and
pass to the shared method.

Change-Id: I3902b7b7cf4b4b3fdc2885bf2611a712d008c617
Closes-Bug: #1381414"
1819,f924943f250b52522a9285410d076d3567199c0f,1388025427,0.0,1.0,27,11,4,3,1,0.89467531,True,8.0,639367.0,45.0,14.0,False,75.0,314088.0,248.0,2.0,10.0,227.0,227.0,10.0,226.0,226.0,10.0,194.0,194.0,0.010128913,0.179558011,0.179558011,184,587,1260738,glance,f924943f250b52522a9285410d076d3567199c0f,1,1,Images are created with negative sizes :O,"Bug #1260738 in Glance: ""image is creating with option size= a negative number""","I just try to create an image by giving size =-1
and the image is creating succesfully.
 glance image-create --name cirros --is-public true --container-format bare --disk-format qcow2 --location https://launchpad.net/cirros/trunk/0.3.0/+download/cirros-0.3.0-x86_64-disk --size -1
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | None                                 |
| container_format | bare                                 |
| created_at       | 2013-12-13T13:48:07                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 2da4e4f9-5f1a-4c8d-a67c-272588e2efbc |
| is_public        | True                                 |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | cirros                               |
| owner            | 6a2db75adb964c5b84010fa22b464715     |
| protected        | False                                |
| size             | -1                                   |
| status           | active                               |
| updated_at       | 2013-12-13T13:48:38                  |
+------------------+--------------------------------------+","Disallow negative image sizes

Currently Glance v1 allows creation of images with negative sizes when
the source URL provided by the --location parmeter does not exist at
the time of creation.

There is already a check to verify whether or not an image size can be
converted to an int but the check doesn't verify whether or not the
resulting value is >= 0. This commit supplements that check to
disallow negative values for the size parameter.

Change-Id: I47511f3e34cf5adfdf944747d94285ecbbda4249
Closes-Bug: #1260738"
1820,f92d1300e1aca68b39b927282af038acc68c5013,1411063212,,1.0,21,28,4,2,1,0.407026154,False,,,,,True,,,,,,,,,,,,,,,,,1337,1805,1371160,neutron,f92d1300e1aca68b39b927282af038acc68c5013,1,1,,"Bug #1371160 in neutron: ""HTTP 500 while retrieving metadata by non-existent key""","HTTP 500 error occurs when one tries to get metadata by path constructed from folder name with appended value.
Steps to repro:
1. Launch VM and access its terminal
2. curl http://169.254.169.254/latest/meta-data/instance-id  -- this returns some string, i.e. i-00000001
3. curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001  -- this returns HTTP 500
It's expected that the last call returns meaningful message and not produce trace backs in logs.
Errors:
----------
In VM terminal:
$ curl http://169.254.169.254/latest/meta-data/instance-id/i-00000001
<html>
 <head>
  <title>500 Internal Server Error</title>
 </head>
 <body>
  <h1>500 Internal Server Error</h1>
  Remote metadata server experienced an internal server error.<br /><br />
 </body>
</html>$
In Neutron metadata agent:
2014-09-18 14:44:37.563 ERROR neutron.agent.metadata.agent [-] Unexpected error.
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Traceback (most recent call last):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent   File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 130, in __call__
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent     return webob.exc.HTTPNotFound()
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent   File ""/opt/stack/neutron/neutron/agent/metadata/agent.py"", line 248, in _proxy_request
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent     def _sign_instance_id(self, instance_id):
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent Exception: Unexpected response code: 400
2014-09-18 14:44:37.563 TRACE neutron.agent.metadata.agent
2014-09-18 14:44:37.566 INFO eventlet.wsgi.server [-] 10.0.0.2,<local> - - [18/Sep/2014 14:44:37] ""GET /latest/meta-data/instance-id/i-00000001 HTTP/1.1"" 500 229 0.348877
In Nova API service:
2014-09-18 14:31:19.030 ERROR nova.api.ec2 [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] FaultWrapper: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2 Traceback (most recent call last):
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 87, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2     application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2     app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/ec2/__init__.py"", line 99, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     rv = req.get_response(self.application)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1320, in send
2014-09-18 14:31:19.030 TRACE nova.api.ec2     application, catch_exc_info=False)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/request.py"", line 1284, in call_application
2014-09-18 14:31:19.030 TRACE nova.api.ec2     app_iter = application(self.environ, start_response)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 130, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     resp = self.call_func(req, *args, **self.kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/usr/lib/python2.7/dist-packages/webob/dec.py"", line 195, in call_func
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return self.func(req, *args, **kwargs)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/handler.py"", line 128, in __call__
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = meta_data.lookup(req.path_info)
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 418, in lookup
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = self.get_ec2_item(path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 300, in get_ec2_item
2014-09-18 14:31:19.030 TRACE nova.api.ec2     return find_path_in_tree(data, path_tokens[1:])
2014-09-18 14:31:19.030 TRACE nova.api.ec2   File ""/opt/stack/nova/nova/api/metadata/base.py"", line 565, in find_path_in_tree
2014-09-18 14:31:19.030 TRACE nova.api.ec2     data = data[path_tokens[i]]
2014-09-18 14:31:19.030 TRACE nova.api.ec2 TypeError: string indices must be integers, not unicode
2014-09-18 14:31:19.030 TRACE nova.api.ec2
2014-09-18 14:31:19.032 INFO nova.metadata.wsgi.server [req-5c84e0ae-7d18-4113-a08b-ed068e5333ed None None] 10.0.0.2,172.18.76.77 ""GET /latest/meta-data/placement/availability-zone/nova HTTP/1.1"" status: 400 len: 246 time: 0.5495760","Fix 500 error on retrieving metadata by invalid URI

An invalid URI should return a BadRequest error rather
than making the metadata proxy bomb out.

Closes-bug: #1371160

Change-Id: Ifb495f9e8929062a0c24d090c3e702109a38803a"
1821,f94d671e627dd7b5143422ffe739418fcfb51a70,1410881016,,1.0,42,13,2,2,1,0.980597441,False,,,,,True,,,,,,,,,,,,,,,,,1319,1787,1369750,cinder,f94d671e627dd7b5143422ffe739418fcfb51a70,1,1,,"Bug #1369750 in Cinder: ""ZFSSA iSCSI  driver cannot add multple initiators to a group""","The zfssa driver is configured by giving a list of initiators and an initiator group
zfssa_initiator = iqn1,iqn2
zfssa_initiator_group = group1
The driver creates the initiator group on the zfssa storage appliance and adds the initiators to the group.
But currently the driver creates the initiators but will only add one initiator to the group. So the others will not be able to
do IO with the volume.
The problem seems to be in the zfssarest.py file add_to_initiatorgroup() function which replaces the existing initiators in the group with the new one instead of adding.","ZFSSA iSCSI driver cannot add multple initiators to a group

All initiators defined in zfssa_initiator property would be
added to the group.
Also fixed some typos related to initiators error messages.

Change-Id: Iec6c90702e5aafa153b4a7f1e429974ac450afc0
Closes-Bug: #1369750"
1822,f96cf93e70f8c434499efb903820aa72665a7fd4,1377034138,,,1698,2,17,7,1,0.644999911,True,26.0,1340559.0,127.0,40.0,False,5.0,26930.82353,7.0,4.0,2.0,499.0,500.0,2.0,434.0,435.0,0.0,100.0,100.0,0.004651163,0.469767442,0.469767442,1562,1226229,1226229,Neutron,f96cf93e70f8c434499efb903820aa72665a7fd4,1,1,"“The root cause is when deployment finished, we only update router status to
    active if the status is in pending create.”","Bug #1226229 in neutron: ""Edge service router status in PENDING_CREATE even though it is been created successfully""","When creating a logic router using plugin NvpAdvancedPlugin with service_router=True, the Edge service router status sometimes show PENDING_CREATE even when the Edge is created successfully.
Also, if the plugin is restarted, deleting an logic router does not delete Edge router.","VCNS driver implementation

Implement API/driver interface for configuring vShield Edge Appliance.

Currently implemented functions:
    - Deploy an Edge
    - Destroy an Edge
    - Configuring interfaces
    - Configuring SNAT/DNAT rules
    - Configuring default gateway and static routes
    - Query Edge status
    - Task-based asynchronous model
    - Allow old routes/nat config to be skipped if new updates are coming

Implements: blueprint vcns-driver
Change-Id: I881bde907f4c90de4c919d008b76b8c2a2d0e1fd"
1823,f96cf93e70f8c434499efb903820aa72665a7fd4,1377034138,,,1698,2,17,7,1,0.644999911,True,26.0,1340559.0,127.0,40.0,False,5.0,26930.82353,7.0,4.0,2.0,499.0,500.0,2.0,434.0,435.0,0.0,100.0,100.0,0.004651163,0.469767442,0.469767442,1825,1284549,1284549,Neutron,f96cf93e70f8c434499efb903820aa72665a7fd4,0,0,Bug in tests,"Bug #1284549 in neutron: ""tests: notification driver isn't properly cleaned up""","Some unit tests override notification driver, but doesn't clean it up properly.
So tests can be run against unintended notification driver resulting in error.","VCNS driver implementation

Implement API/driver interface for configuring vShield Edge Appliance.

Currently implemented functions:
    - Deploy an Edge
    - Destroy an Edge
    - Configuring interfaces
    - Configuring SNAT/DNAT rules
    - Configuring default gateway and static routes
    - Query Edge status
    - Task-based asynchronous model
    - Allow old routes/nat config to be skipped if new updates are coming

Implements: blueprint vcns-driver
Change-Id: I881bde907f4c90de4c919d008b76b8c2a2d0e1fd"
1824,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,1407163458,1.0,2.0,36,4,5,3,1,0.932764048,False,,,,,True,,,,,,,,,,,,,,,,,1078,1528,1343854,neutron,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,1,1,“Missing token in Neutron context”,"Bug #1343854 in neutron: ""Missing token in Neutron context but Nova/Cinder context has""","Missing token in Neutron context but Nova/Cinder context has.  Store the toke into the context can make the third party plugin or driver integrate the authentication with KeyStone.
And Phillip Toohill [mailto:<email address hidden>] also ask for that.
Currently, Neutron did not pass the token to the context. But Nova/Cinder did that. It's easy to do that, just 'copy' from Nova/Cinder.
1.  How Nova/Cinder did that
class NovaKeystoneContext(wsgi.Middleware)
///or CinderKeystoneContext for cinder
              auth_token = req.headers.get('X_AUTH_TOKEN',
                                     req.headers.get('X_STORAGE_TOKEN'))
              ctx = context.RequestContext(user_id,
                                     project_id,
                                     user_name=user_name,
                                     project_name=project_name,
                                     roles=roles,
                                     auth_token=auth_token,
                                     remote_address=remote_address,
                                     service_catalog=service_catalog)
2.  Neutron not passed token. Also not good for the third part network infrastructure to integrate the authentication with KeyStone.
class NeutronKeystoneContext(wsgi.Middleware)
.................
##### token not get from the header and not passed to context. Just change here like what Nova/Cinder did.
        context.Context(user_id, tenant_id, roles=roles,
                              user_name=user_name, tenant_name=tenant_name,
                              request_id=req_id)
        req.environ['neutron.context'] = ctx","add auth token to context

As discussed at
http://lists.openstack.org/pipermail/openstack-dev/2014-July/040644.html
SerivceVM project (and other routervm plugins) need auth token in context.
The first user will be l3 routervm plugin.

Closes-Bug: #1343854
Closes-Bug: #1352698
Change-Id: Id5a4c98059894eef33faf19d5ab063780b362f4a"
1825,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,1407163458,1.0,2.0,36,4,5,3,1,0.932764048,False,,,,,True,,,,,,,,,,,,,,,,,1147,1602,1352698,neutron,f9c285fd617403ab89c857e3d8bab0d06d19d3ba,0,0,Feature “it makes sense for neutron context to include auth token.”,"Bug #1352698 in neutron: ""neutron context doesn't include auth token""","Discussed with the thread starting with
http://lists.openstack.org/pipermail/openstack-dev/2014-July/040644.html
Neutron context isn't populated with auth token unlike nova, glance.
Since there are several (potential) users for it. servicevm project, routervm implementation(cisco csr1kv, vyatta vrouter),
it makes sense for neutron context to include auth token.","add auth token to context

As discussed at
http://lists.openstack.org/pipermail/openstack-dev/2014-July/040644.html
SerivceVM project (and other routervm plugins) need auth token in context.
The first user will be l3 routervm plugin.

Closes-Bug: #1343854
Closes-Bug: #1352698
Change-Id: Id5a4c98059894eef33faf19d5ab063780b362f4a"
1826,f9dfb5b25c087b5146e0c6a9956b2402aec6458f,1397292734,,1.0,15,21,6,3,1,0.811841511,True,5.0,2251964.0,84.0,18.0,False,75.0,977597.0,299.0,5.0,91.0,4448.0,4464.0,91.0,3439.0,3455.0,89.0,3192.0,3207.0,0.011536982,0.409306499,0.41122933,795,1222,1307662,nova,f9dfb5b25c087b5146e0c6a9956b2402aec6458f,0,0,refactoring “We should remove this unused argument.”,"Bug #1307662 in OpenStack Compute (nova): ""xenapi: image downloading code requires unnecessary instance argument""","The downloading code in xenserver requires an instance argument.  However in practice this argument is not used for debugging or any other purpose.  Downloading in fact can occur without an instance being involved at all.  We should remove this unused argument.
This is in preparation to separate out the download code and leverage it for precaching blueprint work.  This should be treated as a bug on it's own as it's a code refactor/cleanup work.","XenAPI: Remove unneeded instance argument from image downloading

The Xenapi image downloading code requires an instance argument
when launching a VM.  However, the downloading code doesn't
need the instance information as it doesn't use it for
any processing/debugging/loging.  We can remove this
argument safely.

Change-Id: Id8b531b74e5f0e59638b608bff93fc89b66b889d
Closes-bug: #1307662"
1827,fa0abeb043c2f66c537e43a8ea7181f648c71d33,1408720869,,1.0,4,6,2,2,1,0.468995594,False,,,,,True,,,,,,,,,,,,,,,,,1223,1682,1360104,neutron,fa0abeb043c2f66c537e43a8ea7181f648c71d33,1,1,,"Bug #1360104 in neutron: ""bind_router fails to log proper error message""","auto_schedule_routers can call bind_router within a transaction.  If a DBDuplicateEntry exception happens in bind_router in such a case, the access to chosen_agent.id causes another exception as the outer transaction is already aborted, resulting in a useless debug log.","Fix InvalidRequestError in auto_schedule_routers

This was discussed in review [1], and was deferred until the time was ripe
for the appropriate fix. As suggested and reported, auto_schedule_routers
is too affected by this error.

This patch takes care of the issue, in a similar way.

[1] - https://review.openstack.org/#/c/112740/

Related-bug: #1354072
Closes-bug: #1360104

Change-Id: Ie3cb0c31dfa571c694cd38e19f72ff8503815635"
1828,fa1857184db370ff78b9b7344be5f3a1030b862f,1382999142,,1.0,24,4,4,4,1,0.853984537,True,10.0,3470085.0,66.0,25.0,False,105.0,405318.0,316.0,4.0,530.0,1915.0,2227.0,498.0,1769.0,2067.0,152.0,1066.0,1151.0,0.023687877,0.165195851,0.178355783,1727,1245696,1245696,nova,fa1857184db370ff78b9b7344be5f3a1030b862f,1,0, “Fix response code and improve error message”,"Bug #1245696 in OpenStack Compute (nova): ""Fix response code and improve error message network not found""","When booting an instance if the network_uuid is not of a valid network the following error is returned:
ERROR: The resource could not be found. (HTTP 404) (Request-ID: req-f86b297f-bbec-4ca1-93f9-f495250f1a3f) (This doesn't really tell the user what resource is not found which should be improved).
In addition  http response code should be 400 not 404 to align with other resources i.e security_groups:
ERROR: Unable to find security_group with name 'asdfasdf' (HTTP 400) (Request-ID: req-ff8b528a-50cf-4ca5-9598-b9ed1a69482d)","Fix HTTP response code for network APIs and improve error message

This patch corrects the http response codes for network resources to be
consistant with other resources by returning a 400 instead of a 404
if the desired resource is not found.

This patch makes the following api changes:

v2-api: NetworkNotFound now returns 400 instead of 404 and improved error
        message which network was not found instead of
        'The resource could not be found'.

v3-api: PortNotFound and NetworkNotFound now returns a 400 instead of a 404.

Closes-bug: #1245696
DocImpact

Change-Id: I66eb0c0ab926e0a8d1e2c9cfe1f7fd579ea3da21"
1829,fa2f139e22ed1317f4afe01faaf7ee3943444715,1405103188,,1.0,97,77,4,2,1,0.928640621,True,3.0,2723996.0,29.0,7.0,False,11.0,13571733.0,12.0,3.0,18.0,1647.0,1665.0,15.0,1402.0,1418.0,1.0,1345.0,1346.0,0.000249781,0.16810291,0.168227801,543,964,1288809,nova,fa2f139e22ed1317f4afe01faaf7ee3943444715,1,1,incorrect allocation,"Bug #1288809 in OpenStack Compute (nova): ""pci passthrough: incorrect device allocation ""","The PCI filter for scheduling runs on the basis of pci stats pools available in a compute node. If the PCI requests match one or more pools, the number of devices will be subtracted from the total number of devices available in those pools.
On the compute node, PCI device allocation is performed based on the list of free devices that are available on the node. The PCI requests are used to match against the device itself, rather than the pci stats pools.
The unsymmetrical handling of scheduling versus allocating could cause incorrect pci stats, and thus incorrect scheduling, and nova instances failed to boot.","makes sure correct PCI device allocation

With this patch, on the compute node, a stats pool will be associated with a
list of devices that belongs to the pool. This makes sure that PCI devices are
allocated out of the same stats pools that are used by the nova scheduler to
satisfy the PCI requests. And therefore, stats pools on the compute nodes will
be kept in sync with their counterparts in the nova scheduler.

Change-Id: I2d97c6354215e2ac5ff659e3203c33771abe1c09
Closes-bug: #1288809"
1830,fa72e49845179e29cf918e0742ef735ae868bb4c,1392449567,,1.0,17,1,2,2,1,0.503258335,True,15.0,5676290.0,161.0,54.0,False,90.0,330354.0,292.0,8.0,362.0,4651.0,4654.0,362.0,3739.0,3742.0,344.0,3519.0,3521.0,0.046855901,0.478066006,0.478337634,416,831,1280532,nova,fa72e49845179e29cf918e0742ef735ae868bb4c,1,1,,"Bug #1280532 in OpenStack Compute (nova): ""Detach volume fails with ""Unexpected KeyError"" in EC2 interface.""","Detach volume fails with ""Unexpected KeyError"" in EC2 interface when I detach a ""attaching"" status volume.
The volume with ""attaching"" status don't contain a property""instance_uuid"", a KeyError will be raised at the following function.
    def _get_instance_from_volume(self, context, volume):
        if volume['instance_uuid']:
            ......
Attaching volume dict:
{
 'status': u'attaching',
 'volume_type_id': u'None',
 'display_name': None,
 'attach_time': '',
 'availability_zone': u'nova',
 'created_at': u'2014-02-13T16: 50: 53.620080',
 'attach_status': 'detached',
 'display_description': None,
 'volume_metadata': {
 },
 'snapshot_id': None,
 'mountpoint': '',
 'id': u'99d118ee-3666-4983-8825-f8c096bccbd1',
 'size': 1
}","Fix bug detach volume fails with ""KeyError"" in EC2

Modified volume['instance_uuid'] to volume.get('instance_uuid'),
to avoid ""KeyError"", when detach a volume with ""attaching"" status.

Change-Id: Ic1aacab212c2a2026d18ef57ad9d77fdc6d86b47
Closes-Bug: #1280532"
1831,fa73a5d037471dbec11893ad83bd93276968b9af,1394061189,,1.0,1,1,1,1,1,0.0,True,2.0,1121873.0,27.0,7.0,False,22.0,746570.0,37.0,6.0,698.0,3879.0,4456.0,598.0,3099.0,3578.0,206.0,3651.0,3743.0,0.027526596,0.485638298,0.49787234,537,958,1288463,nova,fa73a5d037471dbec11893ad83bd93276968b9af,1,1,Security bug,"Bug #1288463 in OpenStack Compute (nova): ""neutron_metadata_proxy_shared_secret should not be written to log file""",neutron_metadata_proxy_shared_secret should not be written to log file,"neutron_metadata_proxy_shared_secret should not be written to log file

This patch sets secret=True for the following config option
neutron_metadata_proxy_shared_secret

Change-Id: I381ea9e89f76beac11b03def6e3ccfee2a55e48f
Closes-bug: #1288463"
1832,fb12fa03ad21c4c8924175ae77bc6bd30139f089,1391094614,0.0,1.0,224,17,12,2,1,0.706124165,True,25.0,1820186.0,196.0,65.0,False,69.0,43294.16667,216.0,7.0,1694.0,983.0,2343.0,1444.0,969.0,2080.0,819.0,959.0,1447.0,0.113652114,0.133056133,0.200693001,368,780,1276539,nova,fb12fa03ad21c4c8924175ae77bc6bd30139f089,0,0,Raise more specific exceptions,"Bug #1276539 in OpenStack Compute (nova): ""VMware: exception handling throws error - AttributeError: 'NoneType' object has no attribute 'getChildren'""","[req-83b880af-7ed2-417f-8d91-ae9b7d624be1 FixedIPsTestJson-1652350262 FixedIPsTestJson-1562534047] In vmwareapi: _call_method (session=52e6b7a6-4745-7303-17f1-52ed4da676a8)
Traceback (most recent call last):
  File ""/opt/stack/nova/nova/virt/vmwareapi/driver.py"", line 817, in _call_method
    return temp_module(*args, **kwargs)
  File ""/opt/stack/nova/nova/virt/vmwareapi/vim.py"", line 197, in vim_request_handler
    for child in detail.getChildren():
AttributeError: 'NoneType' object has no attribute 'getChildren'","VMware: raise more specific exceptions

In certain cases the exception handling for backend errors
would be handled too broadly. This patch elevates the backend exception
to the application so that it can treat specific errors.

In the application we can now handle the following specific exceptions:
 - FileAlreadyExists - for example when moving a directory to a directory
                       that already exists
 - InvalidProperty - for example when using neutron and opaque networks
                     are not supported
 - AlreadyExists - for example a port group already exists
 - NotAuthenticated - for example the operation is denied as because a
                      session is not established

Related-Bug: #1266579
Related-Bug: #1266580
Closes-bug: #1276539

Co-authored-by: Matthew Booth <mbooth@redhat.com>
Change-Id: I0873f570d97345208c8d9b5bcbb5a980a7a58f9d"
1833,fb25917fe17fc80c1ae704759c8f6487ac2e9a22,1393960019,1.0,1.0,64,6,2,2,1,0.9905577,True,5.0,537122.0,33.0,9.0,False,18.0,345880.0,75.0,3.0,15.0,762.0,766.0,15.0,723.0,727.0,15.0,733.0,737.0,0.010688043,0.490313961,0.492985972,495,914,1285906,cinder,fb25917fe17fc80c1ae704759c8f6487ac2e9a22,1,1,,"Bug #1285906 in Cinder: ""3PAR: Extend a volume based on a snapshot fails""","1. Create a volume
2. Create a snapshot of the volume
3. Create a volume based on the snapshot
4. Extend the volume.
5. Horizon shows the volume with status = Error Extending
2014-02-27 15:27:55.028 ERROR cinder.volume.manager [req-bb0716d0-5d3c-430a-83fc-df0e2d85444c dd968af1de0d457b8d43217a821edf1a 63be499ec75e48d597b83679112d32bd] volume 7f440946-8b67-4b0a-85de-9b94add2d258: Error trying to extend volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager Traceback (most recent call last):
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/manager.py"", line 1114, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     volume_id)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     retval = f(*args, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_iscsi.py"", line 437, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     try:
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 251, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     self.client.growVolume(volume_name, growth_size)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/openstack/common/excutils.py"", line 68, in __exit__
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     six.reraise(self.type_, self.value, self.tb)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""/opt/stack/cinder/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 248, in extend_volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     LOG.debug(""Extending Volume %s from %s to %s, by %s GB."" %
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/client.py"", line 452, in growVolume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     response, body = self.http.put('/volumes/%s' % name, body=info)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 306, in put
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     return self._cs_request(url, 'PUT', **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 239, in _cs_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self._do_reauth(url, method, ex, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 216, in _do_reauth
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self._time_request(self.api_url + url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 205, in _time_request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     resp, body = self.request(url, method, **kwargs)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager   File ""build/bdist.linux-x86_64/egg/hp3parclient/http.py"", line 199, in request
2014-02-27 15:27:55.028 TRACE cinder.volume.manager     raise exceptions.from_response(resp, body)
2014-02-27 15:27:55.028 TRACE cinder.volume.manager HTTPForbidden: Forbidden (HTTP 403) 150 - invalid operation:  Cannot grow this type of volume
2014-02-27 15:27:55.028 TRACE cinder.volume.manager","3PAR: Support extend volume based on snapshot

If extend volume fails with an HTTP Forbidden error 150,
convert the volume to a base volume and retry extending.

Change-Id: Id407058e954b2630f4a7f31c6149361301b502f2
Closes-Bug: #1285906"
1834,fb9886b903434321e62373cb4c11ba014921e4df,1394486969,1.0,1.0,195,103,3,3,1,0.715925756,True,13.0,891621.0,237.0,15.0,False,10.0,5197544.0,13.0,2.0,295.0,615.0,866.0,273.0,483.0,714.0,228.0,278.0,466.0,0.24335813,0.296493092,0.496280553,555,976,1289130,neutron,fb9886b903434321e62373cb4c11ba014921e4df,1,1,Fix bad error msg in some cases,"Bug #1289130 in neutron: ""ERROR log ""No DHCP agents are associated with network"" is not an error in most cases""","dhcp-agent notifier outputs ERROR log ""No DHCP agents are associated with network""
but it is not an error and valid cases in most cases. It is so annoying for debugging and monitoring.
No dhcp-agent association for a network was logged as ERROR level. However it is completely incorrect for network_create_end, and usually wrong for subnet_create_end (because a subnet is created just after a network is created in most cases). We should not log it for these cases.
For other cases, a dhcp-agent is usually associated with a network, and no dhcp-agent assocation might be a symptom of some error. On the other hand there are valid cases where no dhcp-agent is associated (e.g., delete dhcp-agent association intentionally, network/subnet_update_end before a port is created). Considering these and the fact that error will be logged in
dhcp-agent scheduler, it would be better to be logged as INFO level now.
2014-03-07 02:29:08.533 5415 ERROR neutron.api.rpc.agentnotifiers.dhcp_rpc_agent_api [req-0babab8c-d638-4f1e-aef3-d58ad6af8b86 None] No DHCP agents are associated with network '6c3c60c7-2fce-4513-971c-b49b98d153c8'. Unable to send notification for 'network_create_end' with payload: {'network': {'status': 'ACTIVE', 'subnets': [], 'name': u'private', 'provider:physical_network': None, 'admin_state_up': True, 'tenant_id': u'33f39be0287d4316a29a4247dcd6db5c', 'provider:network_type': u'local', 'shared': False, 'id': '6c3c60c7-2fce-4513-971c-b49b98d153c8', 'provider:segmentation_id': None}}
http://logs.openstack.org/35/78835/1/check/check-tempest-dsvm-neutron/cd96f2f/logs/screen-q-svc.txt.gz?level=ERROR","Fix misleading error message about failed dhcp notifications

This is achieved by adjusting the log traces and ensuring
that the right log errors are emitted based on the status
of the network.

Also, the patch drastically simplifies the structure of the
notification method and unit tests to increase coverage.

Closes-bug: #1289130

Change-Id: I7cc78bba81c516380fc93a68aa7b295312a88e29"
1835,fb9886b903434321e62373cb4c11ba014921e4df,1394486969,1.0,1.0,195,103,3,3,1,0.715925756,True,13.0,891621.0,237.0,15.0,False,10.0,5197544.0,13.0,2.0,295.0,615.0,866.0,273.0,483.0,714.0,228.0,278.0,466.0,0.24335813,0.296493092,0.496280553,1846,1327000,1327000,Neutron,fb9886b903434321e62373cb4c11ba014921e4df,1,1, ,"Bug #1327000 in neutron: ""Should not schedule dhcp-agent when dhcp port creation""","Intended operation:
---
1. create network and subnet
  neutron net-create net
  neutron subnet-create --name sub net 20.0.0.0/24
2. create port for dhcp server. It is intended to assign specific ip address.
 neutron port-create --name dhcp --device-id reserved_dhcp_port --fixed-ip ip_address=20.0.0.10,subnet_id=sub net -- --device_owner network:dhcp
3. then schedule dhcp-agent manually
 neutron dhcp-agent-network-add 275f7a3f-0251-485c-aea1-9913e173dd1e net
---
But now force scheduling occurs at port creation (2.). So it is not available to schedule manually (3.).","Fix misleading error message about failed dhcp notifications

This is achieved by adjusting the log traces and ensuring
that the right log errors are emitted based on the status
of the network.

Also, the patch drastically simplifies the structure of the
notification method and unit tests to increase coverage.

Closes-bug: #1289130

Change-Id: I7cc78bba81c516380fc93a68aa7b295312a88e29"
1836,fb9b60fb5d3084d4cab29936fd1f534e5599f231,1398148466,,1.0,0,8,1,1,1,0.0,True,1.0,94402.0,6.0,2.0,False,25.0,2995235.0,63.0,2.0,28.0,2661.0,2681.0,21.0,2330.0,2343.0,22.0,1738.0,1752.0,0.002933299,0.221782936,0.223568422,126,525,1257168,nova,fb9b60fb5d3084d4cab29936fd1f534e5599f231,1,1,Race condition,"Bug #1257168 in OpenStack Compute (nova): ""Cells: Error unlocking instances""","Getting the following error when trying to unlock an instance
2013-12-03 15:14:31.198 31688 DEBUG nova.compute.api [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] [instance: fb468a1d-2e64-4560-850d-31a5fd698305] Unlocking unlock /usr/lib/python2.7/dist-packages/nova/compute/api.py:2612
2013-12-03 15:14:31.200 31688 ERROR nova.cells.messaging [req-266c1f02-b77c-440a-8983-948f6ab2357c 671dcaba8087487c8a28afe42b6672fa e4eee8dbc16a49dcbc76edac96674e96] Error processing message locally: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging Traceback (most recent call last):
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 205, in _process_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging resp_value = self.msg_runner._process_message_locally(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 1476, in _process_message_locally
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message, **message.method_kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 708, in run_compute_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return self._run_api_method(message, method_info, fn)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/cells/messaging.py"", line 702, in _run_api_method
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return fn(message.ctxt, *args, **method_info['method_kwargs'])
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 198, in wrapped
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging return func(self, context, target, *args, **kwargs)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/compute/api.py"", line 2615, in unlock
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging instance.save()
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/nova/openstack/common/db/sqlalchemy/models.py"", line 52, in save
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging session.add(self)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1399, in add
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_state(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1411, in _save_or_update_state
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._save_or_update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1667, in _save_or_update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._update_impl(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1661, in _update_impl
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging self._attach(state)
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging File ""/usr/lib/python2.7/dist-packages/sqlalchemy/orm/session.py"", line 1749, in _attach
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging state.session_id, self.hash_key))
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging InvalidRequestError: Object '<Instance at 0x5948a90>' is already attached to session '1247' (this is '1248')
2013-12-03 15:14:31.200 31688 TRACE nova.cells.messaging","Remove cell api overrides for lock and unlock

This isn't actually needed as these methods handle objects and is
triggered when the instance object is saved.
It actually caused a race condition with unlock as it triggers 2
instance updates from a compute cell.

Change-Id: I42eea6a2984f70527d4fc37f48ad973a2248721c
Closes-Bug: #1257168"
1837,fbc02fd569983122a9f5284a71f3c3aaaa03d260,1379108900,1.0,,33,13,2,2,1,0.79359123,True,4.0,405809.0,24.0,11.0,False,10.0,183512.0,11.0,4.0,695.0,481.0,961.0,663.0,463.0,915.0,247.0,163.0,297.0,0.74251497,0.491017964,0.892215569,1842,1315430,1315430,Neutron,fbc02fd569983122a9f5284a71f3c3aaaa03d260,1,1, ,"Bug #1315430 in neutron: ""dhcp_lease_duration of 'infinite' is not configurable""","In the dhcp-agent, it is not possible to set a dhcp_lease_duration of 'infinite' since the variable is set as an IntOpt in neutron/common/config.py.  There are instances, however, where an infinite lease duration may be desirable (particularly in a tripleo installation where the seed may go away but the undercloud and overcloud are expected to persist).  Additionally, dnsmasq itself supports the 'infinite' value for the lease-time in the dhcp-range configuration.","Dynamically adjust max number of leases

This change dynamically adjusts the maximum number of leases based on
the size of the subnets associated with a network.  The upper bound is
limited by a configurable option to keep the max reasonable and prevent
denial of service.

Closes bug: 1225200

Change-Id: I75c3907bcf45cd991eadf5dd8c8ad7f1eaab3c85"
1838,fbc6b991a79a147bf1411b643f5c304062e5956a,1392973045,,1.0,131,8,4,4,1,0.754782362,True,30.0,7378695.0,478.0,66.0,False,9.0,2036652.0,10.0,4.0,2.0,164.0,166.0,2.0,163.0,165.0,1.0,155.0,156.0,0.002358491,0.183962264,0.185141509,1605,1230323,1230323,neutron,fbc6b991a79a147bf1411b643f5c304062e5956a,1,1, ,"Bug #1230323 in neutron: ""Race condition with multiple neutron-servers can allow a router to be scheduled twice""","In an environment with multiple neutron-servers, I have observed that a router can get scheduled to an l3-agent more than once.  A ""neutron l3-agent-list-hosting-router <router id>""  will show the router scheduled twice to the same l3-agent or perhaps to two different agents.  This can be reproduced using devstack.  A second neutron-server on another host has to be configured.  Executing a script against each of the neutron-servers which adds (neutron l3-agent-router-add)  and removes (neutron l3-agent-router-remove) a router from an l3 agent is the quickest way to reproduce the race condition.  There is no locking or other coordination across multiple neutron-server processes to prevent this.","Race condition of L3-agent to add/remove routers

This race condition happens when repeatedly calling
l3-agent-router-add and l3-agent-router-remove
by different neutron-servers at the same time.

The primary key constraint is added for the pair of
(router_id and l3_agent_id).

During migration, verification is done if the current
records violate the PK constraint defined in this bug
fix, and sanitize the data before schema modification.

Due to different dialects of database engines, different
sql statements are executed correspondingly to do
the verification.

Change-Id: Ia541e023b757b2e77c4eec9bb1670632c7a271fa
Closes-Bug: #1230323"
1839,fbd04f71ed28efc85e51465527e91c21bd0d7328,1413370771,,1.0,1,1,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1406,1876,1381464,cinder,fbd04f71ed28efc85e51465527e91c21bd0d7328,1,1,,"Bug #1381464 in Cinder: ""Incorrectly formatted debug message in Prophetstor driver""","correct the message string: %(volumeid) -->%(volumeid)s in cinder/volume/drivers/prophetstor/dpl_fc.py
msg = _('Volume %(volumeid) failed to send assign command, '
                    'ret: %(status)s output: %(output)s') % \
                {'volumeid': volumeid, 'status': ret, 'output': output}","Correct the message string

Change %(volumeid) to %(volumeid)s in message string.

Change-Id: Ia780a8687a784a6d8d8f0c5e6fe9b1a884416608
Closes-Bug: #1381464"
1840,fbe3c734a8675ed36d7e021584612d1413444667,1381324478,,1.0,45,3,2,2,1,0.776555785,True,2.0,5010634.0,30.0,15.0,False,12.0,5172.0,18.0,6.0,51.0,3511.0,3529.0,50.0,3127.0,3145.0,49.0,2559.0,2576.0,0.007993605,0.409272582,0.411990408,1460,1208707,1208707,nova,fbe3c734a8675ed36d7e021584612d1413444667,1,0,“Previously baremetal volume driver used DB API to get fixed ips and DBNotAllowed was raised.”,"Bug #1208707 in OpenStack Compute (nova): ""Cannot attach a volume to a bare-metal instance""","virt/baremetal/volume_driver.py calls db api directly then fails.
2013-08-06 15:59:47.239 ERROR nova.compute [req-117555a1-bb81-43ec-9eee-ce976729093b demo demo] No db access allowed in nova-compute:   File ""/usr/lib/python2.7/dist-packages/eventlet/greenpool.py"", line 80, in _spawn_n_impl
    func(*args, **kwargs)
  File ""/opt/stack/nova/nova/openstack/common/rpc/amqp.py"", line 421, in _process_data
    **args)
  File ""/opt/stack/nova/nova/openstack/common/rpc/dispatcher.py"", line 172, in dispatch
    result = getattr(proxyobj, method)(ctxt, **kwargs)
  File ""/opt/stack/nova/nova/exception.py"", line 77, in wrapped
    return f(self, context, *args, **kw)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 216, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 245, in decorated_function
    return function(self, context, *args, **kwargs)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3424, in attach_volume
    mountpoint, instance)
  File ""/opt/stack/nova/nova/compute/manager.py"", line 3456, in _attach_volume
    mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/driver.py"", line 357, in attach_volume
    instance, mountpoint)
  File ""/opt/stack/nova/nova/virt/baremetal/volume_driver.py"", line 225, in attach_volume
    fixed_ips = nova_db_api.fixed_ip_get_by_instance(ctx, instance['uuid'])
  File ""/opt/stack/nova/nova/db/api.py"", line 496, in fixed_ip_get_by_instance
    return IMPL.fixed_ip_get_by_instance(context, instance_uuid)
  File ""/opt/stack/nova/nova/cmd/compute.py"", line 47, in __call__
    stacktrace = """".join(traceback.format_stack())","baremetal: Make volume driver use a correct source device

attach_volume() have to take the source device path from returning of
LibVirtVolumeDriver.connect_volume() rather than connection_info.

It also adds a test for attach_volume().

Change-Id: I3bee80560342104e2df614066301171624497a04
Closes-Bug: #1208707"
1841,fbf0e621399027d5004bbc8c888d67a3c2851686,1389937243,2.0,1.0,63,47,21,14,1,0.934516696,True,8.0,3871963.0,150.0,38.0,False,61.0,199541.1905,151.0,2.0,59.0,483.0,513.0,59.0,416.0,446.0,52.0,204.0,231.0,0.074438202,0.287921348,0.325842697,309,718,1270863,neutron,fbf0e621399027d5004bbc8c888d67a3c2851686,0,0,Use of constants instead of variables,"Bug #1270863 in neutron: ""db_base_plugin_v2 AUTO_DELETE_PORT_OWNERS doesn't use constant""",db_base_plugin_v2 AUTO_DELETE_PORT_OWNERS should use the constant defined for network:dhcp.,"Replaces network:* strings by constants

This patch replaces all occurences of the strings
prefixed by network:* by their constant equivalent.

Closes-bug: #1270863
Change-Id: I149cc0ab7bde08ea83057e6c0697f668edbe29db"
1842,fbf0e621399027d5004bbc8c888d67a3c2851686,1389937243,2.0,1.0,63,47,21,14,1,0.934516696,True,8.0,3871963.0,150.0,38.0,False,61.0,199541.1905,151.0,2.0,59.0,483.0,513.0,59.0,416.0,446.0,52.0,204.0,231.0,0.074438202,0.287921348,0.325842697,1854,1339401,1339401,Neutron,fbf0e621399027d5004bbc8c888d67a3c2851686,1,1,logic is now obsolete and can be removed’,"Bug #1339401 in neutron: ""NSX: don't check router interface on network delete""","since commit b50e66f the router interfaces will not be automatically delete anymore when a network is deleted.
Instead a 409 response code will be returned.
The NSX plugin still has logic to ensure correct backend state in case router interfaces are present on network_delete. This logic is useless and can be removed.
Also it performs some unnecessary queries, so it affects also plugin performance.","Replaces network:* strings by constants

This patch replaces all occurences of the strings
prefixed by network:* by their constant equivalent.

Closes-bug: #1270863
Change-Id: I149cc0ab7bde08ea83057e6c0697f668edbe29db"
1843,fbf50ad8cba857b62ecb24fccf778175db5b5534,1353571077,1.0,,104,54,1,1,1,0.0,True,7.0,-18332723.0,31.0,7.0,False,12.0,884.0,29.0,2.0,19.0,383.0,384.0,19.0,379.0,380.0,19.0,357.0,358.0,0.041928721,0.750524109,0.752620545,1807,1255142,1255142,Neutron,fbf50ad8cba857b62ecb24fccf778175db5b5534,1,1, ,"Bug #1255142 in neutron: ""unable to get router's external IP when non admin (blocker for VPNaaS)""","In order to set up VPNaaS, a user needs to know his router's external IP (to configure it as endpoint).
PROBLEM : When a user is not admin, the external IP of a router is not visible:
source openrc demo demo
neutron router-list
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| id                                   | name    | external_gateway_info                                                       |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
| 2bd1f015-6c98-4861-a078-5a69256ca7b0 | router1 | {""network_id"": ""8ae6890d-5bb5-4f07-9059-77499628048c"", ""enable_snat"": true} |
+--------------------------------------+---------+-----------------------------------------------------------------------------+
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                         |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e |      | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""} |
+--------------------------------------+------+-------------------+-----------------------------------------------------------------------------------+
It's visible only as admin:
source openrc admin demo
neutron router-port-list 2bd1f015-6c98-4861-a078-5a69256ca7b0
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                             |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| 8ae7206d-19af-4a2a-a15b-0f8cdb98861e |      | fa:16:3e:0a:ee:14 | {""subnet_id"": ""c69b14f9-c2e4-4877-8516-57ff2bdeaa9e"", ""ip_address"": ""172.17.0.1""}     |
| fd56a686-480d-4ede-b021-010253c3de42 |      | fa:16:3e:a5:d2:92 | {""subnet_id"": ""29f5737c-417f-4aa9-a95e-2bef3a04729e"", ""ip_address"": ""192.168.57.226""} |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
Since users need to know the external IP of their router in order to set up VPNaaS this is quite blocking because it requires users to be admin in order to use this feature. It's not an issue for a private cloud, but a big issue for public clouds.","Add router testcases that missing in L3NatDBTestCase

Fixes bug 1083504
Add router testcases in L3NatDBTestCase
Correct router testcases name. Use the same naming rule as test_db_plugin

Change-Id: Icfbef5ec7861aee07df3d451c09f06265fbafe21"
1844,fc01ddaaa5f1170b746f47837104352bfe36eaa5,1404252491,,1.0,17,1,2,2,1,0.764204507,False,,,,,True,,,,,,,,,,,,,,,,,1032,1476,1336566,neutron,fc01ddaaa5f1170b746f47837104352bfe36eaa5,1,1,,"Bug #1336566 in neutron: ""unboundlocalerror during sync_routers_task""","I observed this stacktrace:
http://paste.openstack.org/show/85277/
It looks like there may be circumstances where routers do not get initialized. It's better to be more defensive in this regard.","Fix UnboundLocalError raised during L3 router sync task

This can be fixed in a number of ways: a) consolidating the
two except clauses into one; b) adding a 'return' after the
last except clause c) by calling the cleanup method only on
success; d) initializing 'routers' before usage.

Approach c) has the benefit of stating the developer's intent
more explicitly and minimize chances of regression.

Closes-bug: #1336566

Change-Id: Ib91ac5bb07869cbec6825b60d7c4c5b23c4c4d3a"
1845,fc69f038bb3dca554475bbdd6844996d2d07a23e,1328493006,0.0,,288,302,28,8,1,0.811026813,True,6.0,858942.0,16.0,5.0,False,60.0,220277.5714,253.0,2.0,14.0,425.0,433.0,13.0,425.0,432.0,10.0,323.0,327.0,0.014686248,0.432576769,0.437917223,1861,1348624,1348624,Nova,fc69f038bb3dca554475bbdd6844996d2d07a23e,1,0,"“Unfortunately the Xen hypervisor uses a architecture name of 'x86_32' for i686 platforms which means it won't match the standard OS ‘uname'""","Bug #1348624 in OpenStack Compute (nova): ""XenAPI driver uses a bogus architecture type for i686 platforms""",The XenAPI driver simply parses the Xen hypervisor capabilities to report the architecture type in the supported instances list. Unfortunately the Xen hypervisor uses a architecture name of 'x86_32' for i686 platforms which means it won't match the standard OS 'uname' reported architecture used by other drivers.,"Backslash continuations (nova.tests)

Fixes bug #925285

Backslash continuations removal for package nova.tests

Change-Id: I089dfb9a06a807e58ebb21329800a4eff40ed2bb"
1846,fc786dd469a15bda1b9d3c7bacf9f1771b9b9956,1368280098,2.0,,56,12,2,2,1,0.984343203,True,5.0,458639.0,32.0,10.0,False,30.0,1382.0,47.0,6.0,1765.0,2816.0,3867.0,1465.0,2496.0,3320.0,1634.0,2083.0,3102.0,0.363010657,0.462699822,0.688943162,1543,1223975,1223975,Nova,fc786dd469a15bda1b9d3c7bacf9f1771b9b9956,1,1,The BFC ESTA MAL,"Bug #1223975 in OpenStack Compute (nova): ""multipath tool sends IOs periodically after volume detached""","When detaching a volume, Nova doesn't disconnect the iSCSI portal and just return for there are other volumes attaching to the host. But the mutipath mapping device descriptor is there and multipath tool sends IOs periodically to storage array. This leads to some cinder storage drivers, such as huawei, can not unmap that volume for it detect periodic IOs.
So we need to remove the multipath device descriptor in this case.","Detach volume fails when using multipath iscsi

Enabled multipath for libvirt iscsi volume driver. Attach volume worked,
however, detach volume failed. It failed because it can't find multipath
device ""/dev/mapper/xxxxxxxx"" from connection_info and block device
mapping table.

The fix is to retrieve multipath device info dynamically at detach time.

Fixes: bug #1178893

Change-Id: I53a3888daef93b57bf7f57438f74cee05e794b52"
1847,fc7cffedbe60ae9da7963373e9072c55700fce5f,1398380770,,1.0,59,21,6,4,1,0.833162235,True,8.0,1979875.0,98.0,11.0,False,47.0,11727.0,98.0,2.0,126.0,872.0,937.0,126.0,717.0,782.0,119.0,519.0,577.0,0.101608806,0.440304826,0.489415749,834,1263,1312467,neutron,fc7cffedbe60ae9da7963373e9072c55700fce5f,1,1,,"Bug #1312467 in neutron: ""On external networks with multiple subnets, routers need onlink routes for all subnets""","This subject came up on IRC here [1]. It relates to the blueprint about pluggable external network connections and so I jumped in.
There are two reasons that using multiple external networks to allow multiple floating ip subnets [2] is not optimal.
- Extra L2 infrastructure needed.
- A neutron router cannot have a gateway connection to more than one external network. So, floating IPs wouldn't be able to float as freely as we'd like them to.
I cracked open devstack and started playing with it. I tried this first just to add a second subnet full of floating IPs.
neutron subnet-create ext-net 10.224.24.0/24 --disable-dhcp
In devstack, I needed to add a ""gateway router"". I did this by adding an IP to the br-ex interface. In a real cloud, we'd need to configure the upstream router as a gateway on the second subnet.
sudo ip addr add 10.224.24.1/24 dev br-ex
At this point, I was able to get a router to host floating IPs on both subnets! Pretty cool! I was very surprised it worked so easily.
There is one bug which this bug report addresses! Traffic between floating IPs on the second subnet went up to the router and then back down. The upstream router sent ICMP redirect packets periodically back to the Neutron router sourcing the traffic. These did the router no good because what it really needed to know was that the IP was on link but the upstream router couldn't tell it that.  Some upstream routers may not be configured to send redirects or route back through the port of origin.
The answer to this is to add an on-link route for each subnet on the external network to each router's gateway interface. This will require an L3 agent change but should not be very difficult.
[1] http://eavesdrop.openstack.org/irclogs/%23openstack-neutron/%23openstack-neutron.2014-04-08.log starting at 2014-04-08T23:23:51 (near the bottom)
[2] http://docs.openstack.org/admin-guide-cloud/content/adv_cfg_l3_agent_multi_extnet.html","Set onlink routes for all subnets on an external network

The addition of the on-link routes gives us some freedom to allocate a
router's IP address from any one of multiple subnets on one external
network.  Different routers can get their IPs from different subnets and
they still have direct on-link connectivity to each other.  For example,
one router with its primary IP from 10.0.0.0/24 and another from
192.168.0.0/24 can communicate directly.  It is important that each
router has on-link routes to *all* of the subnets.

Any router can host floating ips from any of the subnets regardless of
which subnet the primary IP address comes from.

This is an alternative to the ""Multiple floating IP pools"" section in
the administration guide.  It is a simpler alternative that avoids
having to create multiple external networks.  It is also more flexible
because routers will no longer be restricted to getting floating IPs
from the pool to which they happen to be connected.

DocImpact
Document the procedure for adding subnets to the external network.
Potentially remove the existing procedure for ""Multiple floating IP
pools"" from the docs.

Change-Id: I2c283f5be0cbb6b5d350cafc1b636c300b796a7b
Closes-Bug: #1312467"
1848,fc82c6dbbd0fa1cdc130cefea534967e273d5570,1341376928,0.0,,221,114,16,12,1,0.744481645,True,5.0,1509699.0,25.0,7.0,False,87.0,112778.0625,380.0,2.0,100.0,882.0,923.0,100.0,882.0,923.0,99.0,845.0,885.0,0.055617353,0.470522803,0.492769744,1870,1365606,1365606,Nova,fc82c6dbbd0fa1cdc130cefea534967e273d5570,1,1,The BFC ESTA MAL,"Bug #1365606 in OpenStack Compute (nova): ""Network deallocation can fail if a network has been deleted""","The call to get_instance_nw_info fails with an error if a network is deleted during the deallocation. Networks are not supposed to be able to be deleted if they have fixed ips in use, but there is a race where a network can be deleted while an allocation is still in process. This is make many times worse by the fact that get_instance_nw_info makes 3*networks + 1 (db + rpc) calls. This should be converted to a) get everything in a single db request, b) handle networks not existing gracefully.
The traceback from get_nw_info failing can look like:
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     address, instance=instance)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     instance_uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     None, None)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 602, in get_instance_nw_info
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     rxtx_factor, host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 625, in build_network_info_model
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     instance_host)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 701, in _get_subnets_from_network
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     network['project_id'], network['uuid'], vif.uuid)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/nova_ipam_lib.py"", line 46, in get_subnets_by_net_id
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     n = network_obj.Network.get_by_uuid(context.elevated(), net_id)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     args, kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-09-03 23:45:03.060 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFoundForUUID_Remote: Network could not be found for uuid 3af0a6e6-59f8-4d7e-90ec-b5b866f578f8
or:
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher Traceback (most recent call last):
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 133, in _dispatch_and_reply
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     incoming.message))
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 176, in _dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return self._do_dispatch(endpoint, method, ctxt, args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/dispatcher.py"", line 122, in _do_dispatch
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     result = getattr(endpoint, method)(ctxt, **new_args)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 242, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     address, instance=instance)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 931, in deallocate_fixed_ip
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     instance_uuid)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 395, in _do_trigger_security_group_members_refresh_for_instance
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     None, None)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/server.py"", line 139, in inner
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return func(*args, **kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 598, in get_instance_nw_info
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     network = self._get_network_by_id(context, vif.network_id)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/network/manager.py"", line 1445, in _get_network_by_id
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     project_only='allow_none')
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/objects/base.py"", line 110, in wrapper
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     args, kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/nova/conductor/rpcapi.py"", line 425, in object_class_action
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     objver=objver, args=args, kwargs=kwargs)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/rpc/client.py"", line 150, in call
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     wait_for_reply=True, timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/transport.py"", line 90, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     timeout=timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 412, in send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     return self._send(target, ctxt, message, wait_for_reply, timeout)
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher   File ""/usr/lib/python2.7/dist-packages/oslo/messaging/_drivers/amqpdriver.py"", line 405, in _send
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher     raise result
2014-09-03 23:57:39.153 10725 TRACE oslo.messaging.rpc.dispatcher NetworkNotFound_Remote: Network 23 could not be found.
depending on where in the call stack the race occurs.","Convert fixed_ips to using instance_uuid.

This should be the second last blueprint finish-uuid-conversion change.

Change-Id: Idd47c5ed3c30af24d60eb23b8e3881d61b4c7976"
1849,fcad26e394cd6021aab2c94f1179533cc7866f8c,1392394650,,1.0,11,19,2,2,1,0.948078244,True,4.0,621859.0,122.0,14.0,False,17.0,328462.0,32.0,2.0,602.0,1414.0,1673.0,473.0,1236.0,1410.0,251.0,616.0,677.0,0.312655087,0.765508685,0.841191067,407,821,1280035,neutron,fcad26e394cd6021aab2c94f1179533cc7866f8c,0,0,tests,"Bug #1280035 in neutron: ""Intermittent unit tests failures in NSX plugin""","Observed here: http://logs.openstack.org/92/71692/12/check/gate-neutron-python26/496b1b1/console.html
This behaviour might have been introduced by the patch for introducing a mapping between neutron and nsx uuids for networks.","Use different context for each API request in unit tests

test_router_add_interface_subnet_with_port_from_other_tenant in
neutron.tests.unit.test_l3_plugin.L3NatTestCaseBase was mocking
neutron.context.Context thus performing multiple API requests
with the same context instance. As a context instance also has
a DB session attribute, this might cause unexpected side effects,
especially for plugins which process request asynchronously.

The plugin neutron.plugins.nicira.NeutronServicePlugin was being
affected.

This patch ensures each request has a different context object
without changing the unit test semantics.

It also refactors slightly test_edge_router.py in the nicira
unit test package to avoid executing twice the same unit tests.

Change-Id: I4895faa00ebd915eb9e259930db2d004a9e78a86
Closes-Bug: #1280035"
1850,fcc17a6b09cbcfaa8aa9db2ee5d187de7f7649aa,1398393700,1.0,1.0,61,3,2,2,1,0.585873293,True,8.0,2369376.0,51.0,13.0,True,,,,,,,,,,,,,,,,,823,1251,1310991,cinder,fcc17a6b09cbcfaa8aa9db2ee5d187de7f7649aa,1,1,,"Bug #1310991 in Cinder: ""available snapshot exists which doesn't have snapshot metadata""","Create snapshot from bootable volume.
Let's say cinder process down suddenly, when snapshot status is changed available.
Restarting cinder process, there is available snapshot that doesn't have snapshot metadata.
When the snapshot doesn't have the snapshot metadata, the status is available is not good.(data and status are mismatched)","Ensure metadata is saved before updating volume status

This patch ensures the volume metadata is saved to
the snapshot metadata before updating the status
of the snapshot to be available.

Change-Id: Iada010520dc3c086fcd410a6c7fd152cf4a31561
Closes-Bug: #1310991"
1851,fcceb93bc842ee2ba61dd21205d09c66d1f08634,1378359943,1.0,1.0,3,3,3,3,1,1.0,True,2.0,67265.0,11.0,3.0,False,127.0,486300.0,469.0,2.0,37.0,1177.0,1213.0,37.0,1007.0,1043.0,3.0,1066.0,1068.0,0.000673741,0.179720398,0.180057268,1514,1221026,1221026,nova,fcceb93bc842ee2ba61dd21205d09c66d1f08634,1,1,“fix conversion type missing”,"Bug #1221026 in OpenStack Compute (nova): ""conversion type missing in log message""","target codes:
def _log_progress_if_required(left, last_log_time, virtual_size):
    if timeutils.is_older_than(last_log_time, PROGRESS_INTERVAL_SECONDS):
        last_log_time = timeutils.utcnow()
        complete_pct = float(virtual_size - left) / virtual_size * 100
        LOG.debug(_(""Sparse copy in progress, ""
                    ""%(complete_pct).2f%% complete. ""
                    ""%(left) bytes left to copy""),                       <====== here miss a conversion type like 's'
            {""complete_pct"": complete_pct, ""left"": left})
    return last_log_time","fix conversion type missing

Conversion type is missing in some places which would cause some
unexcepted error. By using 'grep -rn ""%(\w\+)[^\%a-z0-9\.\-]""',
we could find all cases of '%(variable_a)' and fix them.

Change-Id: Id4b376a92ad23db2cb5380be85d4fe937f1a5cb7
Closes-Bug: #1221026"
1852,fd37ce7d943ab1c2dbc1cf3b6f0187c227f658ad,1413456192,,1.0,1,4,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1401,1871,1381094,neutron,fd37ce7d943ab1c2dbc1cf3b6f0187c227f658ad,1,1,,"Bug #1381094 in neutron: ""NSX plugin request retry failure with ""[Errno 104] Connection reset by peer""""","Http connections in connection pool to NSX controller can be reset due to reasons such as when LB is in the middle, and connection idle timeout.
In such case, while recreating the connection, NSX plugin would retry the request with ""next"" connection, which could also be reset already because of idle timeout. This would leads to continuous retry failure and the request fails finally, with a misleading error log: ""Request timeout ..."".
Error log example:
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,163 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,164 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,164 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,165 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,165 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,166 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,166 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,166 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Acquired connection https://os-nvp.vip.ppp01.corp.com:443. 14 connection(s) available.
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.request] [13168] Issuing - request GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,167 43793136    DEBUG [neutron.plugins.nicira.api_client.request] Setting X-Nvp-Wait-For-Config-Generation request header: '377462645'
2014-10-12 11:07:18,167 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Exception issuing request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,167 43793136  WARNING [neutron.plugins.nicira.api_client.request] [13168] Failed request 'GET https://os-nvp.vip.ppp01.corp.com:443//ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus': '[Errno 104] Connection reset by peer' (0.00 seconds)
2014-10-12 11:07:18,168 43793136  WARNING [neutron.plugins.nicira.api_client.client] [13168] Connection returned in bad state, reconnecting to https://os-nvp.vip.ppp01.corp.com:443
2014-10-12 11:07:18,168 43793136    DEBUG [neutron.plugins.nicira.api_client.client] [13168] Released connection https://os-nvp.vip.ppp01.corp.com:443. 15 connection(s) available.
2014-10-12 11:07:18,168 43793136     INFO [neutron.plugins.nicira.api_client.request_eventlet] [13168] Error while handling request: [Errno 104] Connection reset by peer
2014-10-12 11:07:18,168 133917104    ERROR [NVPApiHelper] Request timed out: GET to /ws.v1/lswitch/7d13735a-ca65-43b3-a3fb-ef8b1ca0f552?relations=LogicalSwitchStatus
2014-10-12 11:07:18,169 133917104    ERROR [NeutronPlugin] An exception occured while selecting logical switch for the port
...
Suggestion to the fix issue by using the newly created connection when retry instead of using the ""next"" connection.","nsx plugin: keep old priority when reconnecting bad connection

Change-Id: Id05012ec04d23a5eec8441fc85f87611e08271fd
Closes-Bug: #1381094"
1853,fd772afb697b70bf9da009976681a2a67d6fa5f1,1399040465,,1.0,2,1,1,1,1,0.0,True,2.0,449368.0,35.0,4.0,False,3.0,1212633.0,5.0,4.0,16.0,321.0,326.0,16.0,317.0,322.0,16.0,309.0,314.0,0.014166667,0.258333333,0.2625,1028,1472,1336198,neutron,fd772afb697b70bf9da009976681a2a67d6fa5f1,0,0,Bug in test,"Bug #1336198 in neutron: ""Metering test plugin includes initialization code as well as tests""",Currently the metering test case includes setUp code as well as the tests themselves. Splitting up the test case class into two allows other test cases to inherit the metering plugin class without inherting (And implicitly re-running) the tests themselves.,"Split up metering test case into plugin + test case

Currently the metering test case includes setUp code as well
as the tests themselves. Splitting up the test case class
into two allows other test cases to inherit the metering
plugin class without inherting (And implicitly re-running)
the tests themselves.

Closes-Bug: #1336198
Change-Id: I4427fcd17cc3d28189f9f74df487084cf35c1634"
1854,fd901be0c6718c26c078bb5ae85ba804b153833c,1398413020,,1.0,25,1,4,3,1,0.79517403,True,9.0,10856163.0,163.0,56.0,False,158.0,2390290.5,718.0,2.0,241.0,2522.0,2654.0,241.0,2057.0,2189.0,223.0,2468.0,2583.0,0.02845167,0.313603455,0.328210339,296,704,1269684,nova,fd901be0c6718c26c078bb5ae85ba804b153833c,1,1,missing information in the payload.. bug i think in this case,"Bug #1269684 in OpenStack Compute (nova): ""payload is empty when remove metadata with event updatemetadata.end ""","liugya@liugya-ubuntu:~/src/nova-ce$ nova  aggregate-set-metadata 1 a=a1
Aggregate 1 has been successfully updated.
+----+------+-------------------+-------+----------+
| Id | Name | Availability Zone | Hosts | Metadata |
+----+------+-------------------+-------+----------+
| 1  | agg1 | None              |       | 'a=a1'   |
+----+------+-------------------+-------+----------+
liugya@liugya-ubuntu:~/src/nova-ce$ nova  aggregate-set-metadata 1 a
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(100)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(101)update_metadata()
-> ""updatemetadata.start"",
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(102)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {u'a': None}, 'aggregate_id': 1}
(Pdb) n
> /opt/stack/nova/nova/objects/aggregate.py(118)update_metadata()
-> payload['meta_data'] = to_add
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(119)update_metadata()
-> compute_utils.notify_about_aggregate_update(context,
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(120)update_metadata()
-> ""updatemetadata.end"",
(Pdb)
> /opt/stack/nova/nova/objects/aggregate.py(121)update_metadata()
-> payload)
(Pdb) payload
{'meta_data': {}, 'aggregate_id': 1} <<< meta_data is empty, this caused 3rd party do not know which meta_data was now removed after get notification of updatemetadata.end","Payload meta_data is empty when remove metadata

When update metadata for a host aggregation, all of the values will
be in the variable of ""updates"", if remove metadata, the format would
be {""foo"": None, ""bar"": None}; If adding metadata, the format would
be {""foo"": ""foo1"", ""bar"": ""bar1""}.

The current logic of update_metadata() is as following:
1) Get all metadata from variable ""updates"".
2) Traverse updates to see which values are new added and which needs
to be removed. For the new added values, put the metadata to
variable ""to_add""; for the values which need to be removed, just
remove them from metadata.
3) Set the values of variable ""to_add"" as payload metadata.

The above logic will make the values which was removed from metadata
cannot be in payload, this caused the third party receiver cannot know
which metadata was removed.

The fix was always using the values from variable ""updates"" as the
metadata for payload as it include values removed and new added.

Change-Id: I7b78795e325f52cfb9c24eabd9ef42990666c02d
Closes-Bug: #1269684"
1855,fdbaba877f98bf20c1ff71c9bd0d04956120f845,1391409842,0.0,1.0,112,188,5,3,1,0.987697205,True,14.0,2044231.0,198.0,21.0,False,19.0,551740.4,27.0,3.0,30.0,1042.0,1062.0,20.0,919.0,931.0,9.0,563.0,565.0,0.013315579,0.750998668,0.753661784,356,767,1275654,neutron,fdbaba877f98bf20c1ff71c9bd0d04956120f845,0,0,tests,"Bug #1275654 in neutron: ""neutron.unit.services code duplication""","code duplication between neutron.unit.services.loadbalancer.test_loadbalancer_plugin
and neutron.unit.services.vpn.test_vpnaas_extension","tests/service: consolidate setUp/tearDown logic

Consolidate the duplicate code for extension api tests.
This will be also used by servicevm extensions later.

At the same time, a bug in test_extension_firewall.py is also fixed.
double call of super().setUp().

Closes-bug: #1275654
Change-Id: I72f185ef6e5af55729ced51fc0b72fae799d46ac"
1856,fdbaba877f98bf20c1ff71c9bd0d04956120f845,1391409842,0.0,1.0,112,188,5,3,1,0.987697205,True,14.0,2044231.0,198.0,21.0,False,19.0,551740.4,27.0,3.0,30.0,1042.0,1062.0,20.0,919.0,931.0,9.0,563.0,565.0,0.013315579,0.750998668,0.753661784,1832,1291130,1291130,Neutron,fdbaba877f98bf20c1ff71c9bd0d04956120f845,0,0,Bug in tests,"Bug #1291130 in neutron: ""Calling stopall not necessary in individual unit tests""","Once https://bugs.launchpad.net/neutron/+bug/1290550 is closed, calling stopall on mock.patch from individual unit tests is no longer required. Calls to stopall in existing code should be removed to prevent duplication.","tests/service: consolidate setUp/tearDown logic

Consolidate the duplicate code for extension api tests.
This will be also used by servicevm extensions later.

At the same time, a bug in test_extension_firewall.py is also fixed.
double call of super().setUp().

Closes-bug: #1275654
Change-Id: I72f185ef6e5af55729ced51fc0b72fae799d46ac"
1857,fdc775d6d52150c8314ec43ef5ab14a5b751e2c7,1385744387,,1.0,267,2,1,1,1,0.0,True,3.0,981400.0,30.0,17.0,False,12.0,783224.0,18.0,8.0,0.0,539.0,539.0,0.0,460.0,460.0,0.0,342.0,342.0,0.001760563,0.603873239,0.603873239,1411,948179,948179,Swift,fdc775d6d52150c8314ec43ef5ab14a5b751e2c7,0,0,"""db_replicator.py needs more/better tests""","Bug #948179 in OpenStack Object Storage (swift): ""db_replicator.py needs more/better tests""","the db_replicator.py is probably one of the more complex pieces of swift code, and one of the least tested.  We need more tests, and better tests for this functionality.","Increases the UT coverage of db_replicator.py

Adds 20 unit tests to increase the coverage of db_replicator.py
from 71% to 90%

Change-Id: Ia63cb8f2049fb3182bbf7af695087bfe15cede54
Closes-Bug: #948179"
1858,fdce41ac4f8de00769fbe6ba32265e35266df621,1375349535,2.0,,145,7,6,2,1,0.782773566,True,29.0,5263320.0,104.0,41.0,False,33.0,3099673.167,69.0,4.0,12.0,2002.0,2011.0,11.0,1541.0,1549.0,12.0,1845.0,1854.0,0.002378339,0.337724113,0.339370655,1839,1314008,1314008,Nova,fdce41ac4f8de00769fbe6ba32265e35266df621,1,1,"The BFC ESTA MAL “In _report_final_resource_view() method, Nova will never record
""free pci"" in log message because 'pci_devices' is not in dict ‘resources'.""","Bug #1314008 in OpenStack Compute (nova): """"free pci"" in log message  never record""","In _report_final_resource_view() method
 if 'pci_devices' in resources:
            LOG.audit(_(""Free PCI devices: %s"") % resources['pci_devices'])
but  in update_available_resource() method
if self.pci_tracker:
            self.pci_tracker.clean_usage(instances, migrations, orphans)
            resources['pci_stats'] = jsonutils.dumps(self.pci_tracker.stats)
        else:
            resources['pci_stats'] = jsonutils.dumps([])
resources has key ""pci_stats"" not  ""pci_devices""
https://review.openstack.org/#/c/90671/","Add PCI device tracker to compute resource tracker

PCI device tracker manages all the PCI devices life cycle.

The resource tracker on a compute node uses the PCI device tracker's
interface to query for resources, claim PCI devices, and alloc and
free PCI devices for instances.

bp:pci-passthrough-base

Change-Id: I766d7f92e26fb75c1d6095dc0237c44f938d4170
Signed-off-by: Yongli He <yongli.he@intel.com>
Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>"
1859,fdce41ac4f8de00769fbe6ba32265e35266df621,1375349535,2.0,,145,7,6,2,1,0.782773566,True,29.0,5263320.0,104.0,41.0,False,33.0,3099673.167,69.0,4.0,12.0,2002.0,2011.0,11.0,1541.0,1549.0,12.0,1845.0,1854.0,0.002378339,0.337724113,0.339370655,1843,1316916,1316916,Nova,fdce41ac4f8de00769fbe6ba32265e35266df621,1,1,"The BFC ESTA MAL. “
we need to avoid this kind of audit logs and give more accurate info”","Bug #1316916 in OpenStack Compute (nova): ""resource tracker report negative value for memory and vcpu will be confusing""","2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
ram (MB): -1559
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
disk (GB): 29
2014-05-05 10:51:33.732 4992 AUDIT nova.compute.resource_tracker [-] Free
VCPUS: -3
was showed in my compute node logs which make me confusing
we need to avoid this kind of audit logs and give more accurate info
discussions here:
http://lists.openstack.org/pipermail/openstack-dev/2014-May/034312.html","Add PCI device tracker to compute resource tracker

PCI device tracker manages all the PCI devices life cycle.

The resource tracker on a compute node uses the PCI device tracker's
interface to query for resources, claim PCI devices, and alloc and
free PCI devices for instances.

bp:pci-passthrough-base

Change-Id: I766d7f92e26fb75c1d6095dc0237c44f938d4170
Signed-off-by: Yongli He <yongli.he@intel.com>
Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>"
1860,fdee7801248b5605112e5e2ce0f06f8a5e9ecc6b,1405642969,2.0,,124,54,9,8,1,0.743532129,False,,,,,True,,,,,,,,,,,,,,,,,1865,1352857,1352857,Neutron,fdee7801248b5605112e5e2ce0f06f8a5e9ecc6b,1,1, ,"Bug #1352857 in neutron: ""few VMs fail to get ip address in devstack""","I am running Juno int code on multi node devstack environment. I am trying to boot 10 VMs in each different network. If so, few VMs, will fail to boot. If i delete & recreate  a VM in the same network, it will fail to get the ip address.
Here are the steps followed to hit this issue
1. Create a network
2. Create a subnet
3. Create a distributed router. Add the subnet to this DVR
4. Boot a VM.
5. Follow the steps 1-4 for 9 more VMs.
6. In this case, few VMs will not get the ip address.
7. I see that dhcp request is not reaching the NN. Please note that few more VMs hosted on the same CN are getting the ip address.
On both Openstack Controller & Compute node logs does not show any Errors.","Encapsulate some port properties in the PortContext

Bindings to host or status may need further encapsulation
to avoid exposing mechanism drivers to underlying DB model
details. This was particularly true in the case of the
l2pop mech driver.

As a result, some docstrings were reworded, and the newly
introduced properties used directly in the mech drivers.

Partially-implements: blueprint neutron-ovs-dvr
Supports blueprint: ml2-hierarchical-port-binding

Change-Id: If2a373ef04d19b164585fb4bde4fe6e0cfaf43be"
1861,fe02cc830f9c9e1dac234164bc1f0caa0e2072d7,1397834086,,1.0,11,7,1,1,1,0.0,True,4.0,634410.0,63.0,17.0,False,6.0,7043798.0,7.0,4.0,24.0,2862.0,2872.0,24.0,2417.0,2427.0,24.0,2780.0,2790.0,0.00319081,0.354945756,0.35622208,799,1227,1308418,nova,fe02cc830f9c9e1dac234164bc1f0caa0e2072d7,1,1,,"Bug #1308418 in OpenStack Compute (nova): ""nova-spiceproxy ignores spicehtml5proxy_host setting = ""::""""","Hello Stackers!
I'm trying SPICE Consoles and it is working smoothly! But, I detected a problem, it is ignoring the entry ""spicehtml5proxy_host"" in nova.conf for Dual-Stacked environment (IPv4 / IPv6).
So, it isn't listening on ""::"".
The following setup doesn't work....
---
[spice]
enabled = True
spicehtml5proxy_host = ::
html5proxy_base_url = http://controller.yourdomain.com:6082/spice_auto.html
keymap = en-us
---
Unless I patch the following file:
/usr/lib/python2.7/dist-packages/nova/cmd/spicehtml5proxy.py into this:
---
opts = [
    cfg.StrOpt('spicehtml5proxy_host',
               default='::',
               help='Host on which to listen for incoming requests'),
---
As you guys can see, I replaced the default ""0.0.0.0"" to ""::"" and now SPICE Proxy listens on both IPv4 and IPv6! But, this is a hack...
I don't know why it is ignoring the ""spicehtml5proxy_host = ::"" entry at nova.conf.
BTW, the ""novncproxy_host = ::"" works as expected for NoVNC but, I'm disabling VNC from my cloud in favor of SPICE.
Cheers!
Thiago","Spice proxy config setting to be read from the spice group in nova.conf

All spice configuration should be read from the spice group
in the nova config. As it's currently being done in the nova.spice
module.
Renaming spicehtml5proxy_{host, port} parameters to
html5proxy_{host, port}

DocImpact
Change-Id: I36c014c8b8a0796cda494f9450a1fc30104504a1
Closes-Bug: #1308418"
1862,fe172d61145ce2cca388f880806b7cf3a6cae6c5,1403244610,,1.0,35,5,4,4,1,0.741693613,False,,,,,True,,,,,,,,,,,,,,,,,1849,1332103,1332103,Glance,fe172d61145ce2cca388f880806b7cf3a6cae6c5,1,1,Im not sure if this bug was caused because of something changed in V2,"Bug #1332103 in Glance: ""Glance v2 api can not set or delete a property of a image when the value of the property is an empty string""","Glance v2 api can not set or delete a property of a image when the value of the property is an empty string.
__delitem__ and __setitem__ methods of ExtraPropertiesProxy class in glance/api/property_protections.py may have some problems. When the value of a property is an empty string, the methods will raise KeyError.","Fixes the failure of updating or deleting image empty property

When property protection feature enabled, end user couldn't uses
Glance v2 API to update or delete a property of an image successfully
which value is empty, client will receive a http 500 error and a
relevant error log could be found in glance-api service log.

Closes-bug: #1332103

Change-Id: I1f9f181cea714e6ba26388d125bb7023e7a14305
Signed-off-by: Zhi Yan Liu <zhiyanl@cn.ibm.com>"
1863,fe18aa12d48377e9dae7990396fdaf59c960f687,1373840626,1.0,,1,1,1,1,1,0.0,True,1.0,20276.0,8.0,4.0,False,2.0,154.0,2.0,3.0,367.0,579.0,813.0,335.0,540.0,752.0,20.0,30.0,38.0,0.291666667,0.430555556,0.541666667,1722,1245335,1245335,Neutron,fe18aa12d48377e9dae7990396fdaf59c960f687,0,0,Test files,"Bug #1245335 in neutron: ""Mix use of test_config['plugin_name_v2'] and plugin argument in DbPluginV2TestCase""","In NeutronDbPluginV2TestCase and its subclasses (most DB-based tests), there are two ways to specify a core plugin and an extension manager in the unit tests: test_config and ""plugin"" arguments of the constructor. Both are used and it sometimes makes it a bit difficult to debug.
It is better to unify the way to pass ""core plugin"" and ""extension manager"" into one.
I think it is better to remove test_config['plugin_name_v2'] and use ""plugin"" argument.","Ensure PortSecurityDBTestCase loads correct plugin

Bug 1201569

If a plugin is passed to the setUp method, that plugin
should be loaded instead od DB_PLUGIN_KLASS

Change-Id: I81df9f62e7399f5a76a07e504679ee8311e5855e"
1864,fe2ca9a75878a445a54ecfe4a97c79b696abf503,1394150231,1.0,1.0,247,41,2,2,1,0.857148437,True,10.0,6368502.0,169.0,16.0,False,27.0,243136.0,46.0,2.0,92.0,139.0,212.0,92.0,100.0,173.0,86.0,53.0,120.0,0.094874591,0.058887677,0.131952017,552,973,1289066,neutron,fe2ca9a75878a445a54ecfe4a97c79b696abf503,1,1,,"Bug #1289066 in neutron: ""L3 Agent cannot process RPC messages until _sync_routers_task is finished""","When L3 agent starts or restarts, it almost immediately goes in to a _sync_routers_task run.  This task is synchronized with _rpc_loop so that only one can happen at a time.
The problem with this is that -- at least at scale -- the _sync_routers_task can take a VERY LONG time to run.  I've observed it take 1-2 hours!  This is WAY too long to wait before I can do something with my router like add a floating ip.
The thing is, _sync_routers_task is important to do periodically but it is mostly just checking that things are still in the right state.  It should never take precedence over responding to RPC messages.  The RPC messages represent work that the system has just been asked to perform.  It is silly to make it wait a long time for a maintenance task to complete.","L3 agent prefers RPC messages over full sync

When the L3 agent starts up and runs the sync task it doesn't process
any incoming RPC events until the sync task is complete.

This change combines the work from _rpc_loop and _sync_routers_task in
to a single loop called _process_routers_loop.  This loop spawns
threads that pull from a priority queue.  The queue ensures that RPC
messages are handled before _process_routers_loop.  The latter is
generally maintenance tasks triggered by the agent rather than user
triggered tasks.

Synchronization between RPC and sync routers loops is no longer
necessary since they both feed in to a single queue.  There were
places where it was necessary to reorder some things to allow for the
lack of synchronization.  For example, it is necessary to list
namespaces before fetching the full list of routers to ensure that it
doesn't delete a new namespace that gets created after listing
namespaces.  The lack of the need for synchronization between loops is
probably the main strength of this patch.

With multiple worker threads, need to handle the case where an RPC
message came in while a thread was working on a router.  Another
thread should not handle the same router that is already in progress.
Adds a mechanism to signal to the working thread that an update came
in for the router it is working on.  The original thread will repeat
processing the router when it is finished to get the update.
Multiple rapid updates to the same router will be consolidated.
Essentially, there is still synchronization of work for a given router
but not between routers.  Much better than before.

blueprint l3-agent-responsiveness
Closes-Bug: #1289066
Change-Id: I39afe86c66f864d71adf865d7bd1c9db35511505"
1865,fe2ef9f0dd1f33b61fe11eebf2ae3d577a73ef53,1394147295,1.0,1.0,3,1,2,1,1,0.811278124,True,2.0,338190.0,31.0,7.0,False,8.0,92644.0,8.0,4.0,1.0,1238.0,1238.0,1.0,1032.0,1032.0,1.0,661.0,661.0,0.002183406,0.722707424,0.722707424,551,972,1289039,neutron,fe2ef9f0dd1f33b61fe11eebf2ae3d577a73ef53,1,0,Bug due to evolution,"Bug #1289039 in neutron: ""Cisco Neutron plugin fails in DB migration""","For Cisco Neutron plugin:
Alembic migration from revision f44ab9871cd6 to 2eeaf963a447 fails because the floatingips table doesn't exist.
The fix is to add the plugin to the DB migration path for L3.","Cisco Neutron plugin fails DB migration

Alembic migration from revision f44ab9871cd6 to 2eeaf963a447 fails
because the floatingips table doesn't exist.

The fix is to add the plugin to the DB migration path for l3 and ext_gw_mode.

Change-Id: Ic7a9c1d624d2838e9313fa95ed6ddbfea9b6e1c9
Closes-Bug: #1289039"
1866,fe3fae10698ed465f45aec4a4022bc784307b4c3,1384260880,,1.0,3,3,1,1,1,0.0,True,4.0,1828526.0,30.0,16.0,False,3.0,8204769.0,3.0,8.0,27.0,676.0,695.0,27.0,617.0,636.0,9.0,307.0,308.0,0.018761726,0.577861163,0.579737336,1750,1250454,1250454,neutron,fe3fae10698ed465f45aec4a4022bc784307b4c3,1,0,“mismatch between 2a6d0b51f4bb_cisco_plugin_cleanup and the initial definition.”,"Bug #1250454 in neutron: ""create_table in downgrade does not match table's content before migration""","create_table in downgrade in migration 2a6d0b51f4bb_cisco_plugin_cleanup does not match table's content created by migration folsom_initial, because of this there is a problem with downgrade.
File ""/opt/stack/neutron/neutron/db/migration/alembic_migrations/versions/2a6d0b51f4bb_cisco_plugin_cleanup.py"", line 87, in downgrade
    sa.PrimaryKeyConstraint(u'id')
  File ""<string>"", line 7, in create_table
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 631, in create_table
    self._table(name, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/alembic/operations.py"", line 117, in _table
    t = sa_schema.Table(name, m, *columns, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 318, in __new__
    table._init(name, metadata, *args, **kw)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 385, in _init
    self._init_items(*args)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 64, in _init_items
    item._set_parent_with_dispatch(self)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/events.py"", line 234, in _set_parent_with_dispatch
    self._set_parent(parent)
  File ""/opt/stack/neutron/.tox/py27/local/lib/python2.7/site-packages/sqlalchemy/schema.py"", line 2131, in _set_parent
    ""named '%s' is present."" % (table.description, col))
ArgumentError: Can't create ForeignKeyConstraint on table 'portprofile_bindings': no column named 'ports' is present.","Fix downgrade in migration

Fixes a downgrade bug caused by a portprofile_bindings table
definition mismatch between 2a6d0b51f4bb_cisco_plugin_cleanup
and the initial definition.

Change-Id: I051e5b41a4285b6cf24270ea6229b1bbca42106d
Closes-Bug: #1250454"
1867,fe513ec3707afd529270f0c4eef7c468ab2c6b91,1380135313,,1.0,16,5,7,7,1,0.877106612,True,4.0,1209632.0,24.0,10.0,False,205.0,157121.4286,1051.0,4.0,70.0,2331.0,2374.0,66.0,1909.0,1949.0,69.0,2024.0,2066.0,0.011430438,0.330666231,0.337524494,1603,1230282,1230282,nova,fe513ec3707afd529270f0c4eef7c468ab2c6b91,1,1, ,"Bug #1230282 in OpenStack Compute (nova): ""Incorrect exception raised during evacuate instance""","An incorrect exception, which looks to contradict the corresponding log message logged
at this scenario seems to be raised in the evacuate instance API.
Evacuate instance needs to check that the compute service is actually down to perform evacuation of an instance,
but the exception raised when this check fails is incorrect:
https://github.com/openstack/nova/blob/master/nova/compute/api.py#L2907
    if self.servicegroup_api.service_is_up(service):
       ....
logs the message ""Instance compute service state on <compute_host> expected to be down, but it was up.""
if the check passes, but raises the exception exception.ComputeServiceUnavailable(Instance compute service state on <compute_host> expected to be down, but it was up.)
The exception raised and the message logged are contradictory.","Fix incorrect exception raised during evacuate

This change creates a new exception 'ComputeServiceInUse'
to better represent the error when trying to evacuate
an instance with a running compute service. Also, it adds
a missing test for evacuate API v3.

Closes-Bug: #1230282

Change-Id: I1c4edb01e84cd2a89879bca9c99f5e38f47e16fb"
1868,fefad06eabc7fb23ccb3ca1a6093d3460065adc7,1375997083,,1.0,0,8,2,2,1,1.0,True,1.0,8745.0,5.0,2.0,False,6.0,6693.0,6.0,2.0,101.0,696.0,794.0,101.0,661.0,759.0,1.0,133.0,133.0,0.012048193,0.807228916,0.807228916,1463,1210276,1210276,neutron,fefad06eabc7fb23ccb3ca1a6093d3460065adc7,0,0, “The neutron.common.exceptions.AlreadyAttached exception is not used.” Refactorings,"Bug #1210276 in neutron: ""exceptions.AlreadyAttached is not used""","The neutron.common.exceptions.AlreadyAttached exception is not used.  It looks like it's replaced by this exception in the nicira plugin:
https://github.com/openstack/neutron/blob/master/neutron/plugins/nicira/common/exceptions.py#L51
Either common.exceptions.AlreadyAttached should be removed or NvpPortAlreadyAttached should extend the common AlreadyAttached exception class.","Remove unused *AlreadyAttached exceptions

This patch simply removes two exceptions which are not being used.

Closes-Bug: #1210276

Change-Id: I5a3412160f49f6e37f3d9182ce4d4a59b1d7bc09"
1869,fefea36baff5f65c56984ae27074e4ad95a3b511,1357857766,,,446,56,7,1,1,0.775033141,True,6.0,1143489.0,33.0,13.0,False,29.0,3147.142857,45.0,2.0,1.0,910.0,910.0,1.0,903.0,903.0,1.0,833.0,833.0,0.000571755,0.238421955,0.238421955,1707,1241350,1241350,Nova,fefea36baff5f65c56984ae27074e4ad95a3b511,1,1,The BFC ESTA MAL,"Bug #1241350 in OpenStack Compute (nova): ""VMware: Detaching a volume from an instance also deletes the volume's backing vmdk (ESXDriver only)""","I found that when I run:
% nova volume-detach my_instance c54ad11f-4e51-41a0-97db-7e551776db59
where the volume with given id is currently attached to my running instance named my_instance, the operation completes successfully. Nevertheless a subsequent attach of the same volume again will fail. So:
% nova volume-attach my_instance c54ad11f-4e51-41a0-97db-7e551776db59 /dev/sdb
fails with the error that the volume's vmdk file is not found.
Cause:
During volume detach a delete_virtual_disk_spec is used to remove the device from the running instance. This spec also ""destroy""s the underlying vmdk file. The offending line is : https://github.com/openstack/nova/blob/master/nova/virt/vmwareapi/vm_util.py#L471
Possible fix:
The fileOperation field of the device config during this reconfigure operation should be left unset. We should continue setting device_config.operation field to ""remove"". This will remove the device from the VM without deleting the underlying vmdk backing.","VMware Compute Driver Volume Management

blueprint vmware-compute-driver

Attach and Detach iSCSI volume
Get volume connector

Change-Id: I25e0c79ffb0b762726fb931233f7beeb53092a34"
1870,fefea36baff5f65c56984ae27074e4ad95a3b511,1357857766,,,446,56,7,1,1,0.775033141,True,6.0,1143489.0,33.0,13.0,False,29.0,3147.142857,45.0,2.0,1.0,910.0,910.0,1.0,903.0,903.0,1.0,833.0,833.0,0.000571755,0.238421955,0.238421955,1812,1259981,1259981,Nova,fefea36baff5f65c56984ae27074e4ad95a3b511,0,0,It is a bug or a feature3,"Bug #1259981 in OpenStack Compute (nova): ""VMware: factor out the management of unit numbers""","Virtual devices need a unit number when they are attached to a controller. We cannot have two devices on the same controller with the same unit number.
Currently, the selection of unit numbers is spread all over the driver code, leaking to high-level functions like spawn() and rescue(). We need to factor this out into helper functions which take care of choosing a proper unit number and creating additional controllers if needed.
High-level functions need to communicate only the intent like 'attach CDROM' or 'attach disk' and shouldn't bother with details like unit numbers.","VMware Compute Driver Volume Management

blueprint vmware-compute-driver

Attach and Detach iSCSI volume
Get volume connector

Change-Id: I25e0c79ffb0b762726fb931233f7beeb53092a34"
1871,ff007a57efa82bc73cbe4a1e60b1cde42c7efa82,1401951808,,1.0,0,45,2,2,1,0.99964375,False,,,,,True,,,,,,,,,,,,,,,,,949,1385,1326666,neutron,ff007a57efa82bc73cbe4a1e60b1cde42c7efa82,0,0,“dead code in ovs/ofagent agents”,"Bug #1326666 in neutron: ""dead code in ovs/ofagent agents""","""class Port"" seems like a leftover from days when agents had direct database accesses.","ovs, ofagent: Remove dead code

Remove ""class Port"", which seems like a leftover from
the old days when agents had direct database accesses.

Change-Id: Ibc0bf5ab48f08ad620f87eb4952c7ff0f4701fda
Closes-Bug: #1326666"
1872,ff1ce62a0b9f504fd1dd46fb569ab85f5b56e4ff,1405029304,,1.0,23,6,2,2,1,0.978449329,False,,,,,True,,,,,,,,,,,,,,,,,1055,1501,1340431,neutron,ff1ce62a0b9f504fd1dd46fb569ab85f5b56e4ff,1,1,,"Bug #1340431 in neutron: ""NSX: network gateway connection doesn't validate vlan id""","when the transport type for a network gateway connection is vlan, the neutron code does not validate that the segmentation id is between 0 and 4095.
The requests is then sent to NSX where it fails. However a 500 error is returned to the neutron API user because of the backend failure.
The operation should return a 400 and possibly not go at all to the backend.","NSX: fix validation logic on network gateway connect

This patch adds validation for the segmentation ID when the
network type for the gateway connection is vlan.
This will avoid requests with invalid vlan IDs are sent to
the backend resulting in 500 error responses being
returned to API users.

To this aim this patch slightly alters the current validation
logic due to the fact that some checks are unnecessary since
the same routine sets default values which avoid the
conditions being checked.

Change-Id: If0e71f6fdf27a49f0eda727e21405cffbc260a7a
Closes-Bug: #1340431"
1873,ff1f6607e71ee261a9499b4a11de34e3648c570b,1389239396,0.0,1.0,5,6,1,1,1,0.0,True,4.0,356013.0,16.0,9.0,False,27.0,245407.0,64.0,1.0,1.0,608.0,609.0,1.0,445.0,446.0,1.0,476.0,477.0,0.001805054,0.430505415,0.431407942,269,673,1267074,glance,ff1f6607e71ee261a9499b4a11de34e3648c570b,0,0,It’s better to use…,"Bug #1267074 in Glance: ""type() method should be replaced with isinstance() in stone/__init__.py""","In store/__init__.py, there is two places use the ""type"" method to determine the type. It's bertter to use the ""isinstance"" method instead.
The code is:
def check_location_metadata(val, key=''):
    t = type(val)
    if t == dict:
        for key in val:
            check_location_metadata(val[key], key=key)
    elif t == list:
        ndx = 0
        for v in val:
            check_location_metadata(v, key='%s[%d]' % (key, ndx))
            ndx = ndx + 1
    elif t != unicode:
        raise BackendException(_(""The image metadata key %s has an invalid ""
                                 ""type of %s.  Only dict, list, and unicode ""
                                 ""are supported."") % (key, str(t)))
def store_add_to_backend(image_id, data, size, store):
    (location, size, checksum, metadata) = store.add(image_id, data, size)
    if metadata is not None:
        if type(metadata) != dict:
            msg = (_(""The storage driver %s returned invalid metadata %s""
                     ""This must be a dictionary type"") %
                   (str(store), str(metadata)))
            LOG.error(msg)
            raise BackendException(msg)","replace type calls with isinstance

In store/__init__.py, there are two uses of the ""type"" method to
check type compatibility. It is better to use the ""isinstance"" method
since it will work for subtypes as well.

Change-Id: I20958aa449822cac2cb03e12557fded3f45a9e05
Closes-Bug: #1267074"
1874,ff221c6cc61f7b98b62462ab4f0e0e1cf691a9a8,1385419706,,1.0,76,1,15,2,1,0.961459742,True,10.0,1581097.0,51.0,15.0,False,21.0,777840.4667,133.0,1.0,947.0,1031.0,1721.0,891.0,757.0,1398.0,928.0,1005.0,1676.0,0.138843222,0.150351218,0.250635182,77,474,1254902,nova,ff221c6cc61f7b98b62462ab4f0e0e1cf691a9a8,0,0,I think is evolution of the code. No bug,"Bug #1254902 in OpenStack Compute (nova): ""Conductor returns new instance objects to older clients in list operations""","Conductor will return new Instance objects to older clients when they do actions on the InstanceList object. This is because we only consider the version of their InstanceList object, and happily fill it with objects newer than they can handle.","Require List objects to be able to backlevel their contents

Right now, a client declares its supported version of a given object
automatically in the remoted calls it makes to conductor. However,
in the case of things like InstanceList.get_by_foo(), they are
reporting the version of their InstanceList object, not their
Instance object. Conductor fills a version-matching InstanceList
object with brand new Instance objects, which the client, of course,
barfs on.

There may be a better way to handle this going forward, but for now,
stop the bleeding by requiring a version bump to the corresponding
List object whenever the object type it contains takes a version
bump. This adds a test to validate that all the objects registered
have a suitable mapping for the current version in the tree.

Since this actually caused a breakage in the Instance object
recently, this also bumps the InstanceList version so that
conductors running this commit (or later) will properly send
version 1.9 Instance objects to Havana clients and version 1.10+
to newer ones.

Change-Id: I2668dead4784fbd0411d1b6372a9a8006eeb2e84
Related-Bug: #1258256
Closes-Bug: #1254902"
1875,ff56055a081a622e90b393b7f40dbe6b482abf65,1385784915,,1.0,8,4,3,3,1,0.980834038,True,6.0,696463.0,42.0,14.0,False,42.0,4048212.0,79.0,6.0,4.0,2042.0,2045.0,4.0,1737.0,1740.0,0.0,1932.0,1932.0,0.000148302,0.286667655,0.286667655,1634,1235088,1235088,nova,ff56055a081a622e90b393b7f40dbe6b482abf65,1,0,"Changed requirements ""some logs have to be changed in order to support translation”","Bug #1235088 in OpenStack Compute (nova): ""LOG.warn() should support translation""","$ grep 'LOG.warn([^_]' * -r
$ grep 'LOG.warn([^_]' * -r
nova/virt/xenapi/vm_utils.py:        LOG.warn(msg % e)
nova/virt/libvirt/driver.py:                LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/virt/libvirt/volume.py:            LOG.warn(msg)
nova/tests/scheduler/test_host_manager.py:        host_manager.LOG.warn(""No service for compute ID 5"")
nova/tests/image/fake.py:        LOG.warn('Unable to find image id %s.  Have images: %s',
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:            LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:        LOG.warn(msg)
nova/db/sqlalchemy/api.py:                    LOG.warn(msg)
nova/db/sqlalchemy/migrate_repo/versions/186_new_bdm_format.py:            LOG.warn(""Got an unexpected block device %s""
nova/openstack/common/db/sqlalchemy/session.py:            LOG.warn(msg % remaining)
nova/openstack/common/rpc/impl_zmq.py:                        LOG.warn(emsg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/api.py:                LOG.warn(msg)
nova/compute/manager.py:                LOG.warn(err_str % exc, instance=instance)
nova/compute/manager.py:                LOG.warn(e, instance=instance)
nova/network/linux_net.py:        LOG.warn(msg % {'num': num_rules, 'float': floating_ip})
nova/network/manager.py:                    LOG.warn(oversize_msg)
nova/api/openstack/compute/plugins/v3/quota_sets.py:                    LOG.warn(msg)
nova/api/openstack/compute/contrib/quotas.py:                    LOG.warn(msg)
nova/api/metadata/vendordata_json.py:                    LOG.warn(logprefix + _(""file does not exist""))
nova/api/metadata/vendordata_json.py:                    LOG.warn(logprefix + _(""Unexpected IOError when reading""))
nova/api/metadata/vendordata_json.py:                LOG.warn(logprefix + _(""failed to load json""))
They shoud make use of ""_"" defined in nova/openstack/common/gettextutils.py, like this:
  LOG.warn(_(""parent device '%s' not found""), dev)","LOG.warn() and LOG.error() should support translation

Based on a search done in nova code,
some logs have to be changed in order to support translation

Change-Id: I371b714d004bcd0091ae569d4b82ae8d19fba708
Closes-Bug: #1235088"
1876,ff693722639387a7a3ea62e291a8306569392ab9,1375349145,2.0,,113,6,6,3,1,0.908565625,True,31.0,5224700.0,128.0,40.0,False,61.0,1297470.5,175.0,3.0,0.0,2376.0,2376.0,0.0,1849.0,1849.0,0.0,2185.0,2185.0,0.000182983,0.4,0.4,1564,1226348,1226348,Nova,ff693722639387a7a3ea62e291a8306569392ab9,1,1, ,"Bug #1226348 in OpenStack Compute (nova): ""pci passthrough scheduler hasattr dones not work for dict""","2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     legacy_bdm_in_spec)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 87, in schedule_run_instance
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     filter_properties, instance_uuids)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filter_scheduler.py"", line 336, in _schedule
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     filter_properties, index=num)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/host_manager.py"", line 397, in get_filtered_hosts
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     hosts, filter_properties, index)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/filters.py"", line 82, in get_filtered_objects
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     list_objs = list(objs)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/filters.py"", line 43, in filter_all
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     if self._filter_one(obj, filter_properties):
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filters/__init__.py"", line 27, in _filter_one
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     return self.host_passes(obj, filter_properties)
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp   File ""/opt/stack/nova/nova/scheduler/filters/pci_passthrough_filter.py"", line 41, in host_passes
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp     return host_state.pci_stats.support_requests(
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp AttributeError: 'NoneType' object has no attribute 'support_requests'
2013-09-17 08:51:06.675 TRACE nova.openstack.common.rpc.amqp","Add scheduler support for PCI passthrough

PCI tracker updates the PCI stats information which provide an abstract
resource view to the scheduler

Openstack clients request PCI devices using PCI aliases, which are
converted to PCI requests. The PCI passthrough filter checks the PCI stats
for nodes that can satisfy the request of the instance, and gives the
schedule result.

bp:pci-passthrough-base

Change-Id: I0ede4caefab8b22126d0c8b9293f023aa4780e36
Signed-off-by: Yongli He <yongli.he@intel.com>
Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>"
1877,ff693722639387a7a3ea62e291a8306569392ab9,1375349145,2.0,,113,6,6,3,1,0.908565625,True,31.0,5224700.0,128.0,40.0,False,61.0,1297470.5,175.0,3.0,0.0,2376.0,2376.0,0.0,1849.0,1849.0,0.0,2185.0,2185.0,0.000182983,0.4,0.4,1631,1234902,1234902,Nova,ff693722639387a7a3ea62e291a8306569392ab9,0,0,"Fixes several misc typos in scheduler code
","Bug #1234902 in OpenStack Compute (nova): ""Misc typos in scheduler code""",Misc typos in scheduler code (comments),"Add scheduler support for PCI passthrough

PCI tracker updates the PCI stats information which provide an abstract
resource view to the scheduler

Openstack clients request PCI devices using PCI aliases, which are
converted to PCI requests. The PCI passthrough filter checks the PCI stats
for nodes that can satisfy the request of the instance, and gives the
schedule result.

bp:pci-passthrough-base

Change-Id: I0ede4caefab8b22126d0c8b9293f023aa4780e36
Signed-off-by: Yongli He <yongli.he@intel.com>
Signed-off-by: Yunhong Jiang <yunhong.jiang@intel.com>"
1878,ffa12c98971e35056a7867cd61ba5a39630cc0a7,1401227374,,1.0,67,17,3,2,1,0.899799661,False,,,,,True,,,,,,,,,,,,,,,,,870,1304,1317134,cinder,ffa12c98971e35056a7867cd61ba5a39630cc0a7,1,1,,"Bug #1317134 in Cinder: ""Volume detach hangs. HP 3par client fails to delete host when node host in 3par host sets""","When an HP 3par host in host set, the cinder volume detach will hang. Looking at the log files, the exception shows the following exception after deleting all the vluns, the host entry is a pre-existing one since the host storage is using the same 3par system:
cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
in master/cinder/volume/drivers/san/hp/hp_3par_common.py
def delete_vlun(self, volume, hostname):
        volume_name = self._get_3par_vol_name(volume['id'])
        vlun = self._get_vlun(volume_name, hostname)
        if vlun is not None:
            # VLUN Type of MATCHED_SET 4 requires the port to be provided
            if self.VLUN_TYPE_MATCHED_SET == vlun['type']:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname,
                                       vlun['portPos'])
            else:
                self.client.deleteVLUN(volume_name, vlun['lun'], hostname)
        try:
            self._delete_3par_host(hostname)
            self._remove_hosts_naming_dict_host(hostname)
 >>        except hpexceptions.HTTPConflict as ex:
 >>            # host will only be removed after all vluns
 >>           # have been removed
 >>           if 'has exported VLUN' in ex.get_description():
 >>              pass
 >>           else:
 >>               raise
Yes, the problem occurred in Havana. I believe the problem is in Icehouse as well, but I have not tested it yet.
Log info:
ccontrol': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.698 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.699 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/hosts/kvm1-4 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.866 22459 DEBUG hp3parclient.http [-] RESP:{'connection': 'close',
 'content-type': 'application/json',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'server': 'hp3par-wsapi',
 'status': '409'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.867 22459 DEBUG hp3parclient.http [-] RESP BODY:{""code"":77,""desc"":""host is a member of a set""}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.868 22459 DEBUG hp3parclient.http [-]
REQ: curl -i https://192.168.3.20:8080/api/v1/credentials/272d1-c810c89243580a0047fec3b7c713b650-a1d66353 DELETE -H ""X-Hp3Par-Wsapi-Sessionkey: 272d1-c810c89243580a0047fec3b7c713b650-a1d66353"" -H ""Accept: application/json"" -H ""User-Agent: python-3parclient""
 _http_log_req /usr/lib/python2.6/site-packages/hp3parclient/http.py:159
2014-05-02 17:31:23.887 22459 DEBUG hp3parclient.http [-] RESP:{'cache-control': 'no-cache',
 'connection': 'close',
 'date': 'Fri, 02 May 2014 17:32:17 GMT',
 'pragma': 'no-cache',
 'server': 'hp3par-wsapi',
 'status': '200'}
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:166
2014-05-02 17:31:23.888 22459 DEBUG hp3parclient.http [-] RESP BODY:
 _http_log_resp /usr/lib/python2.6/site-packages/hp3parclient/http.py:167
2014-05-02 17:31:23.889 22459 DEBUG cinder.volume.drivers.san.hp.hp_3par_common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Disconnect from 3PAR client_logout /usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py:201
2014-05-02 17:31:23.890 22459 DEBUG cinder.openstack.common.lockutils [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Released file lock ""3par"" at /var/lib/cinder/tmp/cinder-3par for method ""terminate_connection""... inner /usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py:239
2014-05-02 17:31:23.891 22459 ERROR cinder.openstack.common.rpc.amqp [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Exception during message handling
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp Traceback (most recent call last):
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/amqp.py"", line 441, in _process_data
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     **args)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/rpc/dispatcher.py"", line 148, in dispatch
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return getattr(proxyobj, method)(ctxt, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/utils.py"", line 819, in wrapper
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return func(self, *args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/manager.py"", line 658, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.driver.terminate_connection(volume_ref, connector, force=force)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/openstack/common/lockutils.py"", line 233, in inner
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     retval = f(*args, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_fc.py"", line 233, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     wwn=connector['wwpns'])
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 1099, in terminate_connection
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.delete_vlun(volume, hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 546, in delete_vlun
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self._delete_3par_host(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/cinder/volume/drivers/san/hp/hp_3par_common.py"", line 408, in _delete_3par_host
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     self.client.deleteHost(hostname)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/client.py"", line 388, in deleteHost
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     reponse, body = self.http.delete('/hosts/%s' % name)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 327, in delete
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     return self._cs_request(url, 'DELETE', **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 231, in _cs_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 205, in _time_request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     resp, body = self.request(url, method, **kwargs)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp   File ""/usr/lib/python2.6/site-packages/hp3parclient/http.py"", line 199, in request
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp     raise exceptions.from_response(resp, body)
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp HTTPConflict: Conflict (HTTP 409) 77 - host is a member of a set
2014-05-02 17:31:23.891 22459 TRACE cinder.openstack.common.rpc.amqp
2014-05-02 17:31:23.897 22459 ERROR cinder.openstack.common.rpc.common [req-2118f4aa-4468-41cd-bfba-2c055d2d3275 e08faaac660b4cefa8371d37e2717113 66c49057ea4d478e975c576a926a1415] Returning exception Conflict (HTTP 409) 77 - host is a member of a set to caller","3PAR volume detach with host in a host set

When deleting the last VLUN for a host, try to
delete the host.  If the host is in a host set
this will fail.  When that happens, log a nice
message including the reason, but don't fail
or hang the delete VLUN.  Leave hosts in host
sets.

Closes-Bug: #1317134

Change-Id: Ie3f8f94c05a6a318de866598f9e9c4c5d84926f4"
1879,ffcb30c4fbee334f9903d2719418bc6aa9d7721c,1413441063,,1.0,2,2,1,1,1,0.0,False,,,,,True,,,,,,,,,,,,,,,,,1407,1877,1381900,neutron,ffcb30c4fbee334f9903d2719418bc6aa9d7721c,1,1,,"Bug #1381900 in neutron: ""l3 agent context name conflict""",context module imported in l3_agent conflicts with argument name.,"l3_agent: avoid name conflict with context

module name, context, conflicts with argument name in many place in
l3_agent. In order to avoid such conflict, import context as n_context
following Neutron practice.

Change-Id: Ic3754818f84064d2c8da04914826fc912437b2f0
Closes-Bug: #1381900"
